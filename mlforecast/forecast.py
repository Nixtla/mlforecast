# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/forecast.ipynb.

# %% auto 0
__all__ = ['Forecast']

# %% ../nbs/forecast.ipynb 3
from typing import Callable, Dict, List, Optional, Tuple, Union

import numpy as np
import pandas as pd
from sklearn.base import RegressorMixin, clone

from .core import TimeSeries
from .compat import dd_Frame
from .utils import backtest_splits

# %% ../nbs/forecast.ipynb 6
class Forecast:
    """Full pipeline encapsulation."""

    def __init__(
        self,
        models: Union[
            RegressorMixin, List[RegressorMixin]
        ],  # model or list of models that follow the scikit-learn API
        freq: Optional[str] = None,  # pandas offset alias, e.g. D, W, M
        lags: List[int] = [],  # list of lags to use as features
        lag_transforms: Dict[
            int, List[Tuple]
        ] = {},  # list of transformations to apply to each lag
        date_features: List[
            str
        ] = [],  # list of names of pandas date attributes to use as features, e.g. dayofweek
        num_threads: int = 1,  # number of threads to use when computing lag features
    ):
        if not isinstance(models, list):
            models = [clone(models)]
        self.models = [clone(m) for m in models]
        self.ts = TimeSeries(freq, lags, lag_transforms, date_features, num_threads)

    def __repr__(self):
        return (
            f'Forecast(models=[{", ".join(m.__class__.__name__ for m in self.models)}], '
            f"freq={self.freq}, "
            f"lag_features={list(self.ts.transforms.keys())}, "
            f"date_features={self.ts.date_features}, "
            f"num_threads={self.ts.num_threads})"
        )

    @property
    def freq(self):
        return self.ts.freq

    def preprocess(
        self,
        data: pd.DataFrame,  # dataframe with the series' data
        id_col: str = "index",  # column that identifies each serie, can also be the index.
        time_col: str = "ds",  # column with the timestamps
        target_col: str = "y",  # column with the series values
        static_features: Optional[
            List[str]
        ] = None,  # column names of the features that don't change in time
        dropna: bool = True,  # drop rows with missing values created by lags
        keep_last_n: Optional[
            int
        ] = None,  # keep only this many observations of each serie for computing the updates
    ) -> pd.DataFrame:
        return self.ts.fit_transform(
            data, id_col, time_col, target_col, static_features, dropna, keep_last_n
        )

    def fit(
        self,
        data: pd.DataFrame,  # dataframe with the series' data
        id_col: str = "index",  # column that identifies each serie. If 'index', the index is taken as the identifier of each serie
        time_col: str = "ds",  # column with the timestamps
        target_col: str = "y",  # column with the series values
        static_features: Optional[
            List[str]
        ] = None,  # column names of the features that don't change in time
        dropna: bool = True,  # drop rows with missing values created by lags
        keep_last_n: Optional[
            int
        ] = None,  # keep only this many observations of each serie for computing the updates
    ) -> "Forecast":
        """Preprocesses `data` and fits `models` using it."""
        series_df = self.preprocess(
            data, id_col, time_col, target_col, static_features, dropna, keep_last_n
        )
        X, y = (
            series_df.drop(columns=[time_col, target_col]),
            series_df[target_col].values,
        )
        del series_df
        self.models_ = []
        for i, model in enumerate(self.models):
            self.models_.append(clone(model).fit(X, y))
        return self

    def predict(
        self,
        horizon: int,  # number of periods to predict in the future
        dynamic_dfs: Optional[
            List[pd.DataFrame]
        ] = None,  # future values for dynamic features
        predict_fn: Optional[Callable] = None,  # custom function to compute predictions
        **predict_fn_kwargs,  # additional arguments passed to predict_fn
    ) -> pd.DataFrame:
        """Compute the predictions for the next `horizon` steps.

        `predict_fn(model, new_x, dynamic_dfs, features_order, **predict_fn_kwargs)` is called in every timestep, where:
        `model` is the trained model.
        `new_x` is a dataframe with the same format as the input plus the computed features.
        `dynamic_dfs` is a list containing the dynamic dataframes.
        `features_order` is the list of column names that were used in the training step.
        """
        return self.ts.predict(
            self.models_, horizon, dynamic_dfs, predict_fn, **predict_fn_kwargs
        )

    def cross_validation(
        self,
        data: pd.DataFrame,  # time series
        n_windows: int,  # number of windows to evaluate
        window_size: int,  # test size in each window
        id_col: str = "index",  # column that identifies each serie, can also be the index.
        time_col: str = "ds",  # column with the timestamps
        target_col: str = "y",  # column with the series values
        static_features: Optional[
            List[str]
        ] = None,  # column names of the features that don't change in time
        dropna: bool = True,  # drop rows with missing values created by lags
        keep_last_n: Optional[
            int
        ] = None,  # keep only this many observations of each serie for computing the updates
        dynamic_dfs: Optional[
            List[pd.DataFrame]
        ] = None,  # future values for dynamic features
        predict_fn: Optional[Callable] = None,  # custom function to compute predictions
        **predict_fn_kwargs,  # additional arguments passed to predict_fn
    ):
        """Creates `n_windows` splits of `window_size` from `data`, trains the model
        on the training set, predicts the window and merges the actuals and the predictions
        in a dataframe.

        Returns a dataframe containing the datestamps, actual values, train ends and predictions."""
        results = []
        self.cv_models_ = []
        if id_col != "index":
            data = data.set_index(id_col)

        def renames(df):
            mapper = {time_col: "ds", target_col: "y"}
            df = df.rename(columns=mapper, copy=False)
            df.index.name = "unique_id"
            return df

        if isinstance(data, dd_Frame):
            data = data.map_partitions(renames)
        else:
            data = renames(data)

        if np.issubdtype(data["ds"].dtype.type, np.integer):
            freq = 1
        else:
            freq = self.freq
        for train_end, train, valid in backtest_splits(
            data, n_windows, window_size, freq
        ):
            self.fit(train, "index", "ds", "y", static_features, dropna, keep_last_n)
            self.cv_models_.append(self.models_)
            y_pred = self.predict(
                window_size, dynamic_dfs, predict_fn, **predict_fn_kwargs
            )
            result = valid[["ds", "y"]].copy()
            result["cutoff"] = train_end

            def merge_fn(res, pred):
                return res.merge(pred, on=["unique_id", "ds"], how="left")

            if isinstance(result, dd_Frame):
                meta = {**result.dtypes.to_dict(), **y_pred.dtypes.to_dict()}
                result = result.map_partitions(
                    merge_fn, y_pred, align_dataframes=False, meta=meta
                )
            else:
                result = merge_fn(result, y_pred)

            if id_col != "index":
                result = result.reset_index()
            result = result.rename(
                columns={"ds": time_col, "y": target_col, "unique_id": id_col}
            )
            results.append(result)

        if isinstance(data, dd_Frame):
            import dask.dataframe as dd

            return dd.concat(results)
        return pd.concat(results)
