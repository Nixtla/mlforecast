# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/distributed.forecast.ipynb.

# %% auto 0
__all__ = ['DistributedForecast']

# %% ../../nbs/distributed.forecast.ipynb 5
import warnings
from typing import Callable, Dict, List, Optional, Tuple, Union

import dask.dataframe as dd
import numpy as np
import pandas as pd
from dask.distributed import Client, default_client
from sklearn.base import clone

from .. import Forecast, TimeSeries
from .core import DistributedTimeSeries
from ..utils import backtest_splits

# %% ../../nbs/distributed.forecast.ipynb 7
class DistributedForecast:
    """Distributed pipeline encapsulation."""

    def __init__(
        self,
        models,  # model or list of mlforecast.distributed.models
        freq: Optional[
            str
        ] = None,  # pandas offset alias, e.g. D, W, M. Don't set if you're using integer times.
        lags: List[int] = [],  # list of lags to use as features
        lag_transforms: Dict[
            int, List[Tuple]
        ] = {},  # list of transformations to apply to each lag
        date_features: List[
            Union[str, Callable]
        ] = [],  # list of names of pandas date attributes or functions to use as features, e.g. dayofweek
        differences: Optional[
            List[int]
        ] = None,  # differences to apply to the series before fitting
        num_threads: int = 1,  # number of threads to use when computing lag features
        client: Optional[Client] = None,  # dask client to use for computations
    ):
        if not isinstance(models, list):
            models = [clone(models)]
        self.models = [clone(m) for m in models]
        self.client = client or default_client()
        self.dts = DistributedTimeSeries(
            TimeSeries(
                freq, lags, lag_transforms, date_features, differences, num_threads
            ),
            self.client,
        )

    def __repr__(self) -> str:
        return (
            f'DistributedForecast(models=[{", ".join(m.__class__.__name__ for m in self.models)}], '
            f"freq={self.freq}, "
            f"lag_features={list(self.dts._base_ts.transforms.keys())}, "
            f"date_features={self.dts._base_ts.date_features}, "
            f"num_threads={self.dts._base_ts.num_threads}, "
            f"client={self.client})"
        )

    @property
    def freq(self):
        return self.dts._base_ts.freq

    def preprocess(
        self,
        data: dd.DataFrame,
        id_col: str = "index",  # column that identifies each serie, it's recommended to have this as the index.
        time_col: str = "ds",  # column with the timestamps
        target_col: str = "y",  # column with the series values
        static_features: Optional[
            List[str]
        ] = None,  # column names of the features that don't change in time
        dropna: bool = True,  # drop rows with missing values created by lags
        keep_last_n: Optional[
            int
        ] = None,  # keep only this many observations of each serie for computing the updates
    ) -> dd.DataFrame:
        """Computes the transformations on each partition of `data` and
        saves the required information for the forecasting step.
        Returns a dask dataframe with the computed features."""
        if id_col in data:
            warnings.warn(
                "It is recommended to have id_col as the index, since setting the index is a slow operation."
            )
            data = data.set_index(id_col)
            id_col = "index"
        return self.dts.fit_transform(
            data, id_col, time_col, target_col, static_features, dropna, keep_last_n
        )

    def fit(
        self,
        data: dd.DataFrame,
        id_col: str = "index",  # column that identifies each serie, it's recommended to have this as the index.
        time_col: str = "ds",  # column with the timestamps
        target_col: str = "y",  # column with the series values
        static_features: Optional[
            List[str]
        ] = None,  # column names of the features that don't change in time
        dropna: bool = True,  # drop rows with missing values created by lags
        keep_last_n: Optional[
            int
        ] = None,  # keep only this many observations of each serie for computing the updates
    ) -> "DistributedForecast":
        """Perform the preprocessing and fit the model."""
        train_ddf = self.preprocess(
            data, id_col, time_col, target_col, static_features, dropna, keep_last_n
        )
        X, y = train_ddf.drop(columns=[time_col, target_col]), train_ddf[target_col]
        self.models_ = []
        for i, model in enumerate(self.models):
            model = clone(model)
            model.client = self.client
            self.models_.append(model.fit(X, y))
        return self

    def predict(
        self,
        horizon: int,
        dynamic_dfs: Optional[List[pd.DataFrame]] = None,
        predict_fn: Optional[Callable] = None,
        **predict_fn_kwargs,
    ) -> dd.DataFrame:
        return self.dts.predict(
            [m.model_ for m in self.models_],
            horizon,
            dynamic_dfs,
            predict_fn,
            **predict_fn_kwargs,
        )

    predict.__doc__ = Forecast.predict.__doc__

    def cross_validation(
        self,
        data: dd.DataFrame,  # time series
        n_windows: int,  # number of windows to evaluate
        window_size: int,  # test size in each window
        id_col: str = "index",  # column that identifies each serie, can also be the index.
        time_col: str = "ds",  # column with the timestamps
        target_col: str = "y",  # column with the series values
        static_features: Optional[
            List[str]
        ] = None,  # column names of the features that don't change in time
        dropna: bool = True,  # drop rows with missing values created by lags
        keep_last_n: Optional[
            int
        ] = None,  # keep only this many observations of each serie for computing the updates
        dynamic_dfs: Optional[
            List[pd.DataFrame]
        ] = None,  # future values for dynamic features
        predict_fn: Optional[Callable] = None,  # custom function to compute predictions
        **predict_fn_kwargs,  # additional arguments passed to predict_fn
    ):
        """Creates `n_windows` splits of `window_size` from `data`, trains the model
        on the training set, predicts the window and merges the actuals and the predictions
        in a dataframe.

        Returns a dataframe containing the datestamps, actual values, train ends and predictions."""
        results = []
        self.cv_models_ = []
        if id_col != "index":
            data = data.set_index(id_col)

        def renames(df):
            mapper = {time_col: "ds", target_col: "y"}
            df = df.rename(columns=mapper, copy=False)
            df.index.name = "unique_id"
            return df

        data = data.map_partitions(renames)

        if np.issubdtype(data["ds"].dtype.type, np.integer):
            freq = 1
        else:
            freq = self.freq
        for train_end, train, valid in backtest_splits(
            data, n_windows, window_size, freq
        ):
            self.fit(train, "index", "ds", "y", static_features, dropna, keep_last_n)
            self.cv_models_.append(self.models_)
            y_pred = self.predict(
                window_size, dynamic_dfs, predict_fn, **predict_fn_kwargs
            )
            result = valid[["ds", "y"]].copy()
            result["cutoff"] = train_end

            def merge_fn(res, pred):
                return res.merge(pred, on=["unique_id", "ds"], how="left")

            meta = {**result.dtypes.to_dict(), **y_pred.dtypes.to_dict()}
            result = result.map_partitions(
                merge_fn, y_pred, align_dataframes=False, meta=meta
            )
            if id_col != "index":
                result = result.reset_index()
            result = result.rename(
                columns={"ds": time_col, "y": target_col, "unique_id": id_col}
            )
            results.append(result)

        return dd.concat(results)
