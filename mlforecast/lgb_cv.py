# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/lgb_cv.ipynb.

# %% auto 0
__all__ = ['LightGBMCV']

# %% ../nbs/lgb_cv.ipynb 3
import copy
import os
from concurrent.futures import ThreadPoolExecutor
from functools import partial
from typing import Any, Callable, Dict, List, Optional, Sequence, Tuple

import lightgbm as lgb
import numpy as np
import pandas as pd
from sklearn.base import clone

from . import Forecast, TimeSeries
from .utils import backtest_splits

# %% ../nbs/lgb_cv.ipynb 4
class EarlyStopException(BaseException):
    ...


def _update(bst, n):
    for _ in range(n):
        bst.update()


def _predict(ts, bst, valid, h):
    preds = ts.predict(bst, h)
    preds = preds.merge(valid, on=["unique_id", "ds"])
    preds["sq_err"] = (preds["Booster"] - preds["y"]) ** 2
    rmse = preds.groupby("unique_id")["sq_err"].mean().pow(0.5).mean()
    return rmse


def _update_and_predict(ts, bst, valid, n, h):
    _update(bst, n)
    return _predict(ts, bst, valid, h)

# %% ../nbs/lgb_cv.ipynb 5
class LightGBMCV:
    def __init__(
        self,
        freq: Optional[str] = None,  # pandas offset alias, e.g. D, W, M
        lags: List[int] = [],  # list of lags to use as features
        lag_transforms: Dict[
            int, List[Tuple]
        ] = {},  # list of transformations to apply to each lag
        date_features: List[
            str
        ] = [],  # list of names of pandas date attributes to use as features, e.g. dayofweek
        num_threads: int = 1,  # number of threads to use when computing the predictions of each window.
    ):
        self.num_threads = num_threads
        self.ts = TimeSeries(freq, lags, lag_transforms, date_features, 1)

    def _should_stop(self, hist, early_stopping_evals, early_stopping_pct):
        if len(hist) < early_stopping_evals + 1:
            return False
        improvement_pct = 1 - hist[-1][1] / hist[-(early_stopping_evals + 1)][1]
        return improvement_pct < early_stopping_pct

    def fit(
        self,
        data: pd.DataFrame,  # time series
        n_windows: int,  # number of windows to evaluate
        window_size: int,  # test size in each window
        params: Dict[str, Any] = {},  # lightgbm parameters
        id_col: str = "index",  # column that identifies each serie, can also be the index.
        time_col: str = "ds",  # column with the timestamps
        target_col: str = "y",  # column with the series values
        static_features: Optional[
            List[str]
        ] = None,  # column names of the features that don't change in time
        dropna: bool = True,  # drop rows with missing values created by lags
        keep_last_n: Optional[
            int
        ] = None,  # keep only this many observations of each serie for computing the updates
        dynamic_dfs: Optional[
            List[pd.DataFrame]
        ] = None,  # future values for dynamic features
        weights: Sequence[float] = None,  # weight for each window
        eval_every: int = 10,  # number of iterations to train before evaluating the full window
        fit_on_all: bool = True,  # return model fitted on all data
        verbose_eval: bool = True,  # print evaluation metrics
        early_stopping_evals: int = 2,  # stop if the score doesn't improve in these many evaluations
        early_stopping_pct: float = 0.01,  # score must improve at least in this percentage to keep training
        predict_fn: Optional[Callable] = None,  # custom function to compute predictions
        **predict_fn_kwargs,  # additional arguments passed to predict_fn
    ):
        if eval_every <= 0:
            raise ValueError(
                "eval_every should be > 0. If you don't want to evaluate the complete horizon use "
                "Forecast.cross_validation instead."
            )
        if weights is None:
            weights = np.full(n_windows, 1 / n_windows)
        elif len(weights) != n_windows:
            raise ValueError("Must specify as many weights as the number of windows")
        else:
            weights = np.asarray(weights)

        orig = data.copy(deep=False)
        if id_col != "index":
            data = data.set_index(id_col)
        data = data.rename(columns={time_col: "ds", target_col: "y"}, copy=False)
        data.index.name = "unique_id"

        if np.issubdtype(data["ds"].dtype.type, np.integer):
            freq = 1
        else:
            freq = self.ts.freq
        items = []
        bst_threads = os.cpu_count() // self.num_threads
        for _, train, valid in backtest_splits(data, n_windows, window_size, freq):
            ts = copy.deepcopy(self.ts)
            prep = ts.fit_transform(train, static_features=static_features)
            ds = lgb.Dataset(prep.drop(columns=["ds", "y"]), prep["y"]).construct()
            bst = lgb.Booster({**params, "num_threads": bst_threads}, ds)
            bst.predict = partial(bst.predict, num_threads=bst_threads)
            items.append((ts, bst, valid))

        hist = []
        n_iter = lgb.basic._choose_param_value("num_iterations", params, 100)[
            "num_iterations"
        ]
        rmses = np.empty(n_windows)

        if self.num_threads == 1:
            try:
                for i in range(0, n_iter, eval_every):
                    for j, (ts, bst, valid) in enumerate(items):
                        rmses[j] = _update_and_predict(
                            ts, bst, valid, eval_every, window_size
                        )
                    rmse = rmses @ weights
                    rounds = eval_every + i
                    hist.append((rounds, rmse))
                    if verbose_eval:
                        print(f"[{rounds:,d}] RMSE: {rmse:,f}")
                    if self._should_stop(
                        hist, early_stopping_evals, early_stopping_pct
                    ):
                        raise EarlyStopException
            except EarlyStopException:
                print(f"Early stopping at round {rounds:,}")
        else:
            try:
                with ThreadPoolExecutor(self.num_threads) as executor:
                    for i in range(0, n_iter, eval_every):
                        futures = []
                        for ts, bst, valid in items:
                            _update(bst, eval_every)
                            future = executor.submit(
                                _predict, ts, bst, valid, window_size
                            )
                            futures.append(future)
                        rmses[:] = [f.result() for f in futures]
                        rmse = rmses @ weights
                        rounds = eval_every + i
                        hist.append((rounds, rmse))
                        if verbose_eval:
                            print(f"[{rounds:,d}] RMSE: {rmse:,f}")
                        if self._should_stop(
                            hist, early_stopping_evals, early_stopping_pct
                        ):
                            raise EarlyStopException
            except EarlyStopException:
                print(f"Early stopping at round {rounds:,}.")

        self.cv_models_ = [item[1] for item in items]

        if fit_on_all:
            self.fcst = Forecast([])
            self.fcst.ts = self.ts
            self.fcst.models = [lgb.LGBMRegressor(**params)]
            self.fcst.fit(
                orig,
                id_col,
                time_col,
                target_col,
                static_features,
                dropna,
                keep_last_n,
            )
        return hist

    def predict(
        self,
        horizon: int,  # number of periods to predict in the future
        dynamic_dfs: Optional[
            List[pd.DataFrame]
        ] = None,  # future values for dynamic features
        predict_fn: Optional[Callable] = None,  # custom function to compute predictions
        **predict_fn_kwargs,  # additional arguments passed to predict_fn
    ):
        return self.fcst.predict(horizon, dynamic_dfs, predict_fn, **predict_fn_kwargs)

    def cv_predict(self, horizon):
        return self.ts.predict(self.cv_models_, horizon)
