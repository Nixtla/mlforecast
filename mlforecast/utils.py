# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/utils.ipynb.

# %% auto 0
__all__ = ['generate_daily_series', 'generate_prices_for_series', 'backtest_splits']

# %% ../nbs/utils.ipynb 3
import random
import reprlib
from dataclasses import dataclass
from itertools import chain
from math import ceil, log10
from typing import Any, Optional, Union

import numpy as np
import pandas as pd
from fugue.collections.yielded import Yielded
from fugue.constants import FUGUE_CONF_WORKFLOW_EXCEPTION_INJECT
from fugue.dataframe import DataFrame
from fugue.workflow import FugueWorkflow

# %% ../nbs/utils.ipynb 5
def generate_daily_series(
    n_series: int,
    min_length: int = 50,
    max_length: int = 500,
    n_static_features: int = 0,
    equal_ends: bool = False,
    static_as_categorical: bool = True,
    with_trend: bool = False,
    seed: int = 0,
) -> pd.DataFrame:
    """Generates `n_series` of different lengths in the interval [`min_length`, `max_length`].

    If `n_static_features > 0`, then each serie gets static features with random values.
    If `equal_ends == True` then all series end at the same date."""
    rng = np.random.RandomState(seed)
    random.seed(seed)
    series_lengths = rng.randint(min_length, max_length + 1, n_series)
    total_length = series_lengths.sum()
    n_digits = ceil(log10(n_series))

    dates = pd.date_range("2000-01-01", periods=max_length, freq="D").values
    uids = [
        [f"id_{i:0{n_digits}}"] * serie_length
        for i, serie_length in enumerate(series_lengths)
    ]
    if equal_ends:
        ds = [dates[-serie_length:] for serie_length in series_lengths]
    else:
        ds = [dates[:serie_length] for serie_length in series_lengths]
    y = np.arange(total_length) % 7 + rng.rand(total_length) * 0.5
    series = pd.DataFrame(
        {
            "unique_id": list(chain.from_iterable(uids)),
            "ds": list(chain.from_iterable(ds)),
            "y": y,
        }
    )
    for i in range(n_static_features):
        static_values = np.repeat(rng.randint(0, 100, n_series), series_lengths)
        series[f"static_{i}"] = static_values
        if static_as_categorical:
            series[f"static_{i}"] = series[f"static_{i}"].astype("category")
        if i == 0:
            series["y"] = series["y"] * 0.1 * (1 + static_values)
    series["unique_id"] = series["unique_id"].astype("category")
    series["unique_id"] = series["unique_id"].cat.as_ordered()
    series = series.set_index("unique_id")
    if with_trend:
        coefs = pd.Series(
            rng.rand(n_series), index=[f"id_{i:0{n_digits}}" for i in range(n_series)]
        )
        trends = series.groupby("unique_id").cumcount()
        trends.index = series.index
        series["y"] += coefs * trends
    return series

# %% ../nbs/utils.ipynb 16
def generate_prices_for_series(
    series: pd.DataFrame, horizon: int = 7, seed: int = 0
) -> pd.DataFrame:
    rng = np.random.RandomState(seed)
    unique_last_dates = series.groupby("unique_id")["ds"].max().nunique()
    if unique_last_dates > 1:
        raise ValueError("series must have equal ends.")
    if "product_id" not in series:
        raise ValueError("series must have a product_id column.")
    day_offset = pd.tseries.frequencies.Day()
    starts_ends = series.groupby("product_id")["ds"].agg([min, max])
    dfs = []
    for idx, (start, end) in starts_ends.iterrows():
        product_df = pd.DataFrame(
            {
                "product_id": idx,
                "price": rng.rand((end - start).days + 1 + horizon),
            },
            index=pd.date_range(start, end + horizon * day_offset, name="ds"),
        )
        dfs.append(product_df)
    prices_catalog = pd.concat(dfs).reset_index()
    return prices_catalog

# %% ../nbs/utils.ipynb 19
def _split_info(
    data: pd.DataFrame,
    offset: int,
    window_size: int,
    freq: Union[pd.offsets.BaseOffset, int],
    time_col: str,
):
    # TODO: try computing this once and passing it to this fn
    last_dates = data.groupby(level=0, observed=True)[time_col].transform("max")
    train_ends = last_dates - offset * freq
    valid_ends = train_ends + window_size * freq
    train_mask = data[time_col].le(train_ends)
    train_sizes = train_mask.groupby(data.index, observed=True).sum()
    if train_sizes.eq(0).any():
        ids = reprlib.repr(train_sizes[train_sizes.eq(0)].index.tolist())
        raise ValueError(f"The following series are too short for the window: {ids}")
    valid_mask = data[time_col].gt(train_ends) & data[time_col].le(valid_ends)
    return pd.DataFrame({"train_end": train_ends, "is_valid": valid_mask})

# %% ../nbs/utils.ipynb 20
def backtest_splits(
    data,
    n_windows: int,
    window_size: int,
    freq: Union[pd.offsets.BaseOffset, int],
    step_size: Optional[int] = None,
    time_col: str = "ds",
):
    if step_size is None:
        step_size = window_size
    test_size = window_size + step_size * (n_windows - 1)
    for i in range(n_windows):
        offset = test_size - i * step_size
        if isinstance(data, pd.DataFrame):
            splits = _split_info(data, offset, window_size, freq, time_col)
        else:
            end_dtype = int if isinstance(freq, int) else "datetime64[ns]"
            splits = data.map_partitions(
                _split_info,
                offset=offset,
                window_size=window_size,
                freq=freq,
                time_col=time_col,
                meta={"train_end": end_dtype, "is_valid": bool},
            )
        train_mask = data[time_col].le(splits["train_end"])
        train, valid = data[train_mask], data[splits["is_valid"]]
        yield splits.loc[splits["is_valid"], "train_end"], train, valid

# %% ../nbs/utils.ipynb 24
@dataclass
class PredictionIntervals:
    """Class for storing prediction intervals metadata information."""

    n_windows: int = 2
    window_size: int = 1

    def __post_init__(self):
        if self.n_windows < 2:
            raise ValueError(
                "You need at least two windows to compute conformal intervals"
            )

# %% ../nbs/utils.ipynb 25
def _cotransform(
    df1: Any,
    df2: Any,
    using: Any,
    schema: Any = None,
    params: Any = None,
    partition: Any = None,
    engine: Any = None,
    engine_conf: Any = None,
    force_output_fugue_dataframe: bool = False,
    as_local: bool = False,
) -> Any:
    dag = FugueWorkflow(compile_conf={FUGUE_CONF_WORKFLOW_EXCEPTION_INJECT: 0})

    src = dag.create_data(df1).zip(dag.create_data(df2), partition=partition)
    tdf = src.transform(
        using=using,
        schema=schema,
        params=params,
        pre_partition=partition,
    )
    tdf.yield_dataframe_as("result", as_local=as_local)
    dag.run(engine, conf=engine_conf)
    result = dag.yields["result"].result  # type:ignore
    if force_output_fugue_dataframe or isinstance(df1, (DataFrame, Yielded)):
        return result
    return result.as_pandas() if result.is_local else result.native
