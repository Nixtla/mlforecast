{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf5b1d7-bc13-4ed3-af30-f15cacc861f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp distributed.forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503920c4-0e1f-4cef-a7ed-dea1005027ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2921fbf1-b7be-4f3d-b8e6-ce58b49fbd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "import warnings\n",
    "\n",
    "from fastcore.test import test_warns, test_eq, test_ne\n",
    "from nbdev import show_doc\n",
    "from sklearn import set_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bf1328-ca14-4169-8608-31dbb62107d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "warnings.simplefilter('ignore', FutureWarning)\n",
    "set_config(display='text')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc9a984-a442-41d2-8132-471d73e64d96",
   "metadata": {},
   "source": [
    "# DistributedMLForecast\n",
    "\n",
    "> Distributed pipeline encapsulation\n",
    "\n",
    "**This interface is only tested on Linux**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8108c14-6d27-4dfd-b3e8-a2b315eb5f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import copy\n",
    "from collections import namedtuple\n",
    "from typing import Any, Callable, Iterable, List, Optional\n",
    "\n",
    "import cloudpickle\n",
    "try:\n",
    "    import dask.dataframe as dd\n",
    "    DASK_INSTALLED = True\n",
    "except ModuleNotFoundError:\n",
    "    DASK_INSTALLED = False\n",
    "import fugue\n",
    "import fugue.api as fa\n",
    "import pandas as pd\n",
    "try:\n",
    "    from pyspark.ml.feature import VectorAssembler\n",
    "    from pyspark.sql import DataFrame as SparkDataFrame\n",
    "    SPARK_INSTALLED = True\n",
    "except ModuleNotFoundError:\n",
    "    SPARK_INSTALLED = False\n",
    "try:\n",
    "    from ray.air.checkpoint import Checkpoint as RayCheckpoint\n",
    "    from ray.data import Dataset as RayDataset\n",
    "    from ray.train.sklearn import SklearnTrainer, SklearnPredictor\n",
    "    RAY_INSTALLED = True\n",
    "except ModuleNotFoundError:\n",
    "    RAY_INSTALLED = False\n",
    "from sklearn.base import clone\n",
    "\n",
    "from mlforecast.core import (\n",
    "    DateFeature,\n",
    "    Differences,\n",
    "    Freq,\n",
    "    LagTransforms,\n",
    "    Lags,\n",
    "    TimeSeries,\n",
    "    _name_models,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c505b5b8-2c00-456b-8d61-2301516bc347",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "WindowInfo = namedtuple('WindowInfo', ['n_windows', 'window_size', 'step_size', 'i_window'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227f19b4-4582-4c51-94cd-c7c220d35dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class DistributedMLForecast:\n",
    "    \"\"\"Multi backend distributed pipeline\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        models,\n",
    "        freq: Optional[Freq] = None,\n",
    "        lags: Optional[Lags] = None,\n",
    "        lag_transforms: Optional[LagTransforms] = None,\n",
    "        date_features: Optional[Iterable[DateFeature]] = None,\n",
    "        differences: Optional[Differences] = None,\n",
    "        num_threads: int = 1,\n",
    "        engine = None,\n",
    "    ):\n",
    "        \"\"\"Create distributed forecast object\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        models : regressor or list of regressors\n",
    "            Models that will be trained and used to compute the forecasts.\n",
    "        freq : str or int, optional (default=None)\n",
    "            Pandas offset alias, e.g. 'D', 'W-THU' or integer denoting the frequency of the series.\n",
    "        lags : list of int, optional (default=None)\n",
    "            Lags of the target to use as features.\n",
    "        lag_transforms : dict of int to list of functions, optional (default=None)\n",
    "            Mapping of target lags to their transformations.\n",
    "        date_features : list of str or callable, optional (default=None)\n",
    "            Features computed from the dates. Can be pandas date attributes or functions that will take the dates as input.\n",
    "        differences : list of int, optional (default=None)\n",
    "            Differences to take of the target before computing the features. These are restored at the forecasting step.\n",
    "        num_threads : int (default=1)\n",
    "            Number of threads to use when computing the features.\n",
    "        engine : fugue execution engine, optional (default=None)\n",
    "            Dask Client, Spark Session, etc to use for the distributed computation.\n",
    "            If None will infer depending on the input type.\n",
    "        \"\"\"        \n",
    "        if not isinstance(models, dict) and not isinstance(models, list):\n",
    "            models = [models]\n",
    "        if isinstance(models, list):\n",
    "            model_names = _name_models([m.__class__.__name__ for m in models])\n",
    "            models_with_names = dict(zip(model_names, models))\n",
    "        else:\n",
    "            models_with_names = models\n",
    "        self.models = models_with_names\n",
    "        self._base_ts = TimeSeries(\n",
    "            freq, lags, lag_transforms, date_features, differences, num_threads\n",
    "        )\n",
    "        self.engine = engine\n",
    "        \n",
    "    def __repr__(self) -> str:\n",
    "        return (\n",
    "            f'{self.__class__.__name__}(models=[{\", \".join(self.models.keys())}], '\n",
    "            f\"freq={self._base_ts.freq}, \"\n",
    "            f\"lag_features={list(self._base_ts.transforms.keys())}, \"\n",
    "            f\"date_features={self._base_ts.date_features}, \"\n",
    "            f\"num_threads={self._base_ts.num_threads}, \"\n",
    "            f\"engine={self.engine})\"\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _preprocess_partition(\n",
    "        part: pd.DataFrame,\n",
    "        base_ts: TimeSeries,        \n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "        window_info: Optional[WindowInfo] = None,\n",
    "        fit_ts_only: bool = False,\n",
    "    ) -> List[List[Any]]:\n",
    "        ts = copy.deepcopy(base_ts)\n",
    "        if fit_ts_only:\n",
    "            ts._fit(\n",
    "                part,\n",
    "                id_col=id_col,\n",
    "                time_col=time_col,\n",
    "                target_col=target_col,\n",
    "                static_features=static_features,\n",
    "                keep_last_n=keep_last_n,                \n",
    "            )\n",
    "            return [[cloudpickle.dumps(ts), cloudpickle.dumps(None), cloudpickle.dumps(None)]]        \n",
    "        if window_info is None:\n",
    "            train = part\n",
    "            valid = None\n",
    "        else:\n",
    "            n_windows, window_size, step_size, i_window = window_info\n",
    "            if step_size is None:\n",
    "                step_size = window_size\n",
    "            test_size = window_size + step_size * (n_windows - 1)\n",
    "            offset = test_size - i_window * step_size\n",
    "            max_dates = part.groupby(id_col)[time_col].transform('max')\n",
    "            train_ends = max_dates - offset * base_ts.freq\n",
    "            valid_ends = train_ends + window_size * base_ts.freq\n",
    "            train_mask = part[time_col].le(train_ends)\n",
    "            valid_mask = part[time_col].gt(train_ends) & part[time_col].le(valid_ends)\n",
    "            train = part[train_mask]\n",
    "            valid_keep_cols = part.columns\n",
    "            if static_features is not None:\n",
    "                valid_keep_cols.drop(static_features)\n",
    "            cutoffs = part.groupby(id_col)[time_col].max().rename('cutoff') - offset * base_ts.freq\n",
    "            valid = part.loc[valid_mask, valid_keep_cols].merge(cutoffs, on=id_col)\n",
    "        transformed = ts.fit_transform(\n",
    "            train,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            static_features=static_features,\n",
    "            dropna=dropna,\n",
    "            keep_last_n=keep_last_n,\n",
    "        )\n",
    "        return [[cloudpickle.dumps(ts), cloudpickle.dumps(transformed), cloudpickle.dumps(valid)]]\n",
    "\n",
    "    @staticmethod\n",
    "    def _retrieve_df(items: List[List[Any]]) -> Iterable[pd.DataFrame]:\n",
    "        for _, serialized_train, _ in items:\n",
    "            yield cloudpickle.loads(serialized_train)\n",
    "            \n",
    "    def _preprocess_partitions(\n",
    "        self,\n",
    "        data: fugue.AnyDataFrame,\n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "        window_info: Optional[WindowInfo] = None,\n",
    "        fit_ts_only: bool = False,\n",
    "    ) -> List[Any]:\n",
    "        return fa.transform(\n",
    "            data,\n",
    "            DistributedMLForecast._preprocess_partition,\n",
    "            params={\n",
    "                'base_ts': self._base_ts,\n",
    "                'id_col': id_col,\n",
    "                'time_col': time_col,\n",
    "                'target_col': target_col,\n",
    "                'static_features': static_features,\n",
    "                'dropna': dropna,\n",
    "                'keep_last_n': keep_last_n,\n",
    "                'window_info': window_info,\n",
    "                'fit_ts_only': fit_ts_only,\n",
    "            },\n",
    "            schema='ts:binary,train:binary,valid:binary',\n",
    "            engine=self.engine,\n",
    "            as_fugue=True,\n",
    "            partition=id_col,\n",
    "        )        \n",
    "\n",
    "    def _preprocess(\n",
    "        self,\n",
    "        data: fugue.AnyDataFrame,\n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "        window_info: Optional[WindowInfo] = None,\n",
    "    ) -> fugue.AnyDataFrame:\n",
    "        self.id_col = id_col\n",
    "        self.time_col = time_col\n",
    "        self.target_col = target_col\n",
    "        self.static_features = static_features\n",
    "        self.dropna = dropna\n",
    "        self.keep_last_n = keep_last_n\n",
    "        self.partition_results = self._preprocess_partitions(\n",
    "            data=data,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            static_features=static_features,\n",
    "            dropna=dropna,\n",
    "            keep_last_n=keep_last_n,\n",
    "            window_info=window_info,\n",
    "        )\n",
    "        base_schema = str(fa.get_schema(data))\n",
    "        features_schema = ','.join(f'{feat}:double' for feat in self._base_ts.features)\n",
    "        res = fa.transform(\n",
    "            self.partition_results,\n",
    "            DistributedMLForecast._retrieve_df,\n",
    "            schema=f'{base_schema},{features_schema}',\n",
    "            engine=self.engine,\n",
    "        )\n",
    "        return fa.get_native_as_df(res)\n",
    "    \n",
    "    def preprocess(\n",
    "        self,\n",
    "        data: fugue.AnyDataFrame,\n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "    ) -> fugue.AnyDataFrame:\n",
    "        \"\"\"Add the features to `data`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : dask or spark DataFrame.\n",
    "            Series data in long format.\n",
    "        id_col : str\n",
    "            Column that identifies each serie. If 'index' then the index is used.\n",
    "        time_col : str\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str\n",
    "            Column that contains the target.\n",
    "        static_features : list of str, optional (default=None)\n",
    "            Names of the features that are static and will be repeated when forecasting.\n",
    "        dropna : bool (default=True)\n",
    "            Drop rows with missing values produced by the transformations.\n",
    "        keep_last_n : int, optional (default=None)\n",
    "            Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        result : same type as input\n",
    "            data with added features.\n",
    "        \"\"\"        \n",
    "        return self._preprocess(\n",
    "            data,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            static_features=static_features,\n",
    "            dropna=dropna,\n",
    "            keep_last_n=keep_last_n,\n",
    "        )\n",
    "    \n",
    "    def _fit(\n",
    "        self,\n",
    "        data: fugue.AnyDataFrame,\n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "        window_info: Optional[WindowInfo] = None,\n",
    "    ) -> 'DistributedMLForecast':\n",
    "        prep = self._preprocess(\n",
    "            data,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            static_features=static_features,\n",
    "            dropna=dropna,\n",
    "            keep_last_n=keep_last_n,\n",
    "            window_info=window_info,\n",
    "        )\n",
    "        features = [x for x in fa.get_column_names(prep) if x not in {id_col, time_col, target_col}]\n",
    "        self.models_ = {}\n",
    "        if SPARK_INSTALLED and isinstance(data, SparkDataFrame):\n",
    "            featurizer = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "            train_data = featurizer.transform(prep)[target_col, \"features\"]\n",
    "            for name, model in self.models.items():\n",
    "                trained_model = model._pre_fit(target_col).fit(train_data)\n",
    "                self.models_[name] = model.extract_local_model(trained_model)\n",
    "        elif DASK_INSTALLED and isinstance(data, dd.DataFrame):\n",
    "            X, y = prep[features], prep[target_col]\n",
    "            for name, model in self.models.items():\n",
    "                trained_model = clone(model).fit(X, y)\n",
    "                self.models_[name] = trained_model.model_\n",
    "        elif RAY_INSTALLED and isinstance(data, RayDataset):\n",
    "            for name, model in self.models.items():\n",
    "                trainer = SklearnTrainer(\n",
    "                    estimator=clone(model),\n",
    "                    label_column=target_col,\n",
    "                    datasets={\"train\": prep.select_columns(cols=features + [target_col])}\n",
    "                )\n",
    "                trained_model = trainer.fit()\n",
    "                self.models_[name] = trained_model.checkpoint\n",
    "        else:\n",
    "            raise NotImplementedError('Only spark, dask, and ray engines are supported.')\n",
    "        return self\n",
    "    \n",
    "    def fit(\n",
    "        self,\n",
    "        data: fugue.AnyDataFrame,\n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,        \n",
    "    ) -> 'DistributedMLForecast':\n",
    "        \"\"\"Apply the feature engineering and train the models.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : dask or spark DataFrame\n",
    "            Series data in long format.\n",
    "        id_col : str\n",
    "            Column that identifies each serie. If 'index' then the index is used.\n",
    "        time_col : str\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str\n",
    "            Column that contains the target.\n",
    "        static_features : list of str, optional (default=None)\n",
    "            Names of the features that are static and will be repeated when forecasting.\n",
    "        dropna : bool (default=True)\n",
    "            Drop rows with missing values produced by the transformations.\n",
    "        keep_last_n : int, optional (default=None)\n",
    "            Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : DistributedMLForecast\n",
    "            Forecast object with series values and trained models.\n",
    "        \"\"\"        \n",
    "        return self._fit(\n",
    "            data,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            static_features=static_features,\n",
    "            dropna=dropna,\n",
    "            keep_last_n=keep_last_n,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _predict(\n",
    "        items: List[List[Any]],\n",
    "        models,\n",
    "        horizon,\n",
    "        dynamic_dfs=None,\n",
    "        before_predict_callback=None,\n",
    "        after_predict_callback=None,\n",
    "    ) -> Iterable[pd.DataFrame]:\n",
    "        for serialized_ts, _, serialized_valid in items:\n",
    "            valid = cloudpickle.loads(serialized_valid)\n",
    "            ts = cloudpickle.loads(serialized_ts)\n",
    "            if valid is not None:\n",
    "                dynamic_features = valid.columns.drop(\n",
    "                    [ts.id_col, ts.time_col, ts.target_col]\n",
    "                )\n",
    "                if not dynamic_features.empty:\n",
    "                    dynamic_dfs = [valid.drop(columns=ts.target_col)]\n",
    "            res = ts.predict(\n",
    "                models={\n",
    "                    name: SklearnPredictor.from_checkpoint(model) if isinstance(model, RayCheckpoint) \\\n",
    "                    else model for name, model in models.items()\n",
    "                },\n",
    "                horizon=horizon,\n",
    "                dynamic_dfs=dynamic_dfs,\n",
    "                before_predict_callback=before_predict_callback,\n",
    "                after_predict_callback=after_predict_callback,\n",
    "            ).reset_index()\n",
    "            if valid is not None:\n",
    "                res = res.merge(valid, how='left')\n",
    "            yield res\n",
    "            \n",
    "    def _get_predict_schema(self) -> str:\n",
    "        model_names = self.models.keys()\n",
    "        models_schema = ','.join(f'{model_name}:double' for model_name in model_names)\n",
    "        schema = f'{self.id_col}:string,{self.time_col}:datetime,' + models_schema\n",
    "        return schema\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        horizon: int,\n",
    "        dynamic_dfs: Optional[List[pd.DataFrame]] = None,\n",
    "        before_predict_callback: Optional[Callable] = None,\n",
    "        after_predict_callback: Optional[Callable] = None,\n",
    "        new_data: Optional[fugue.AnyDataFrame] = None,\n",
    "    ) -> fugue.AnyDataFrame:\n",
    "        \"\"\"Compute the predictions for the next `horizon` steps.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        horizon : int\n",
    "            Number of periods to predict.\n",
    "        dynamic_dfs : list of pandas DataFrame, optional (default=None)\n",
    "            Future values of the dynamic features, e.g. prices.\n",
    "        before_predict_callback : callable, optional (default=None)\n",
    "            Function to call on the features before computing the predictions.\n",
    "                This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure.\n",
    "                The series identifier is on the index.\n",
    "        after_predict_callback : callable, optional (default=None)\n",
    "            Function to call on the predictions before updating the targets.\n",
    "                This function will take a pandas Series with the predictions and should return another one with the same structure.\n",
    "                The series identifier is on the index.\n",
    "        new_data : dask or spark DataFrame, optional (default=None)\n",
    "            Series data of new observations for which forecasts are to be generated.\n",
    "                This dataframe should have the same structure as the one used to fit the model, including any features and time series data.\n",
    "                If `new_data` is not None, the method will generate forecasts for the new observations.                \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        result : dask, spark or ray DataFrame\n",
    "            Predictions for each serie and timestep, with one column per model.\n",
    "        \"\"\"        \n",
    "        if new_data is not None:\n",
    "            partition_results = self._preprocess_partitions(\n",
    "                data=new_data,\n",
    "                id_col=self.id_col,\n",
    "                time_col=self.time_col,\n",
    "                target_col=self.target_col,\n",
    "                static_features=self.static_features,\n",
    "                dropna=self.dropna,\n",
    "                keep_last_n=self.keep_last_n,\n",
    "                fit_ts_only=True,\n",
    "            )\n",
    "        else:\n",
    "            partition_results = self.partition_results\n",
    "        schema = self._get_predict_schema()\n",
    "        res = fa.transform(\n",
    "            partition_results,\n",
    "            DistributedMLForecast._predict,\n",
    "            params={\n",
    "                'models': self.models_,\n",
    "                'horizon': horizon,\n",
    "                'dynamic_dfs': dynamic_dfs,\n",
    "                'before_predict_callback': before_predict_callback,\n",
    "                'after_predict_callback': after_predict_callback,\n",
    "            },\n",
    "            schema=schema,\n",
    "            engine=self.engine,\n",
    "        )\n",
    "        return fa.get_native_as_df(res)\n",
    "\n",
    "    def cross_validation(\n",
    "        self,\n",
    "        data: fugue.AnyDataFrame,\n",
    "        n_windows: int,\n",
    "        window_size: int,\n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        step_size: Optional[int] = None, \n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "        refit: bool = True,\n",
    "        before_predict_callback: Optional[Callable] = None,\n",
    "        after_predict_callback: Optional[Callable] = None,\n",
    "    ) -> fugue.AnyDataFrame:\n",
    "        \"\"\"Perform time series cross validation.\n",
    "        Creates `n_windows` splits where each window has `window_size` test periods,\n",
    "        trains the models, computes the predictions and merges the actuals.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : dask, spark or ray DataFrame\n",
    "            Series data in long format.\n",
    "        n_windows : int\n",
    "            Number of windows to evaluate.\n",
    "        window_size : int\n",
    "            Number of test periods in each window.\n",
    "        id_col : str\n",
    "            Column that identifies each serie. If 'index' then the index is used.\n",
    "        time_col : str\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str\n",
    "            Column that contains the target.\n",
    "        step_size : int, optional (default=None)\n",
    "            Step size between each cross validation window. If None it will be equal to `window_size`.\n",
    "        static_features : list of str, optional (default=None)\n",
    "            Names of the features that are static and will be repeated when forecasting.\n",
    "        dropna : bool (default=True)\n",
    "            Drop rows with missing values produced by the transformations.\n",
    "        keep_last_n : int, optional (default=None)\n",
    "            Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it.\n",
    "        refit : bool (default=True)\n",
    "            Retrain model for each cross validation window.\n",
    "            If False, the models are trained at the beginning and then used to predict each window.            \n",
    "        before_predict_callback : callable, optional (default=None)\n",
    "            Function to call on the features before computing the predictions.\n",
    "                This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure.\n",
    "                The series identifier is on the index.\n",
    "        after_predict_callback : callable, optional (default=None)\n",
    "            Function to call on the predictions before updating the targets.\n",
    "                This function will take a pandas Series with the predictions and should return another one with the same structure.\n",
    "                The series identifier is on the index.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        result : dask, spark or ray DataFrame\n",
    "            Predictions for each window with the series id, timestamp, target value and predictions from each model.\n",
    "        \"\"\"            \n",
    "        self.cv_models_ = []\n",
    "        results = []\n",
    "        for i in range(n_windows):\n",
    "            window_info = WindowInfo(n_windows, window_size, step_size, i)            \n",
    "            if refit or i == 0:\n",
    "                self._fit(\n",
    "                    data,\n",
    "                    id_col=id_col,\n",
    "                    time_col=time_col,\n",
    "                    target_col=target_col,\n",
    "                    static_features=static_features,\n",
    "                    dropna=dropna,\n",
    "                    keep_last_n=keep_last_n,\n",
    "                    window_info=window_info,\n",
    "                )\n",
    "                self.cv_models_.append(self.models_)\n",
    "                partition_results = self.partition_results\n",
    "            elif not refit:\n",
    "                partition_results = self._preprocess_partitions(\n",
    "                    data=data,\n",
    "                    id_col=id_col,\n",
    "                    time_col=time_col,\n",
    "                    target_col=target_col,\n",
    "                    static_features=static_features,\n",
    "                    dropna=dropna,\n",
    "                    keep_last_n=keep_last_n,\n",
    "                    window_info=window_info,\n",
    "                )\n",
    "            schema = self._get_predict_schema() + f',cutoff:datetime,{self.target_col}:double'\n",
    "            preds = fa.transform(\n",
    "                partition_results,\n",
    "                DistributedMLForecast._predict,\n",
    "                params={\n",
    "                    'models': self.models_,\n",
    "                    'horizon': window_size,\n",
    "                    'before_predict_callback': before_predict_callback,\n",
    "                    'after_predict_callback': after_predict_callback,\n",
    "                },\n",
    "                schema=schema,\n",
    "                engine=self.engine,\n",
    "            )\n",
    "            results.append(fa.get_native_as_df(preds))\n",
    "        if len(results) == 1:\n",
    "            return results[0]\n",
    "        if len(results) == 2:\n",
    "            return fa.union(results[0], results[1])\n",
    "        return fa.union(results[0], results[1], results[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18b1388-3c79-4d1c-98cf-d748d64119e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda/envs/mlforecast/lib/python3.10/site-packages/statsforecast/core.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### DistributedMLForecast\n",
       "\n",
       ">      DistributedMLForecast (models,\n",
       ">                             freq:Union[int,str,pandas._libs.tslibs.offsets.Bas\n",
       ">                             eOffset,NoneType]=None,\n",
       ">                             lags:Optional[Iterable[int]]=None, lag_transforms:\n",
       ">                             Optional[Dict[int,List[Union[Callable,Tuple[Callab\n",
       ">                             le,Any]]]]]=None, date_features:Optional[Iterable[\n",
       ">                             Union[str,Callable]]]=None,\n",
       ">                             differences:Optional[Iterable[int]]=None,\n",
       ">                             num_threads:int=1, engine=None)\n",
       "\n",
       "Multi backend distributed pipeline"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### DistributedMLForecast\n",
       "\n",
       ">      DistributedMLForecast (models,\n",
       ">                             freq:Union[int,str,pandas._libs.tslibs.offsets.Bas\n",
       ">                             eOffset,NoneType]=None,\n",
       ">                             lags:Optional[Iterable[int]]=None, lag_transforms:\n",
       ">                             Optional[Dict[int,List[Union[Callable,Tuple[Callab\n",
       ">                             le,Any]]]]]=None, date_features:Optional[Iterable[\n",
       ">                             Union[str,Callable]]]=None,\n",
       ">                             differences:Optional[Iterable[int]]=None,\n",
       ">                             num_threads:int=1, engine=None)\n",
       "\n",
       "Multi backend distributed pipeline"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DistributedMLForecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ec88ed-1306-4a4b-bca4-0a254ada08a5",
   "metadata": {},
   "source": [
    "The `DistributedMLForecast` class is a high level abstraction that encapsulates all the steps in the pipeline (preprocessing, fitting the model and computing predictions) and applies them in a distributed way.\n",
    "\n",
    "The different things that you need to use `DistributedMLForecast` (as opposed to `MLForecast`) are:\n",
    "\n",
    "1. You need to set up a cluster. We currently support dask and spark (ray is on the roadmap).\n",
    "2. Your data needs to be a distributed collection. We currently support dask and spark dataframes.\n",
    "3. You need to use a model that implements distributed training in your framework of choice, e.g. SynapseML for LightGBM in spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbda5c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dask.distributed import Client\n",
    "from window_ops.expanding import expanding_mean\n",
    "from window_ops.rolling import rolling_mean\n",
    "\n",
    "from mlforecast.utils import backtest_splits, generate_daily_series, generate_prices_for_series\n",
    "from mlforecast.distributed.models.dask.lgb import DaskLGBMForecast\n",
    "from mlforecast.distributed.models.dask.xgb import DaskXGBForecast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73eb8f10-1b95-453a-8f1c-91f9c1813af1",
   "metadata": {},
   "source": [
    "## Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92b13a2-f6bd-4d76-840d-cadb1d672147",
   "metadata": {},
   "source": [
    "### Client setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9fe50e-1b4a-4f58-8bbf-d1c087fb7d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(n_workers=2, threads_per_worker=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e838b0d-a0f3-4672-a794-75301a4aced3",
   "metadata": {},
   "source": [
    "Here we define a client that connects to a `dask.distributed.LocalCluster`, however it could be any other kind of cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f15a4a-79d2-4b72-8b9b-290edefe340f",
   "metadata": {},
   "source": [
    "### Data setup\n",
    "\n",
    "For dask, the data must be a `dask.dataframe.DataFrame`. You need to make sure that each time serie is only in one partition and it is recommended that you have as many partitions as you have workers. If you have more partitions than workers make sure to set `num_threads=1` to avoid having nested parallelism.\n",
    "\n",
    "The required input format is the same as for `MLForecast`, except that it's a `dask.dataframe.DataFrame` instead of a `pandas.Dataframe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552cea5f-b826-4c9b-ae9f-2532f92f31bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>static_0</th>\n",
       "      <th>static_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=10</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>object</td>\n",
       "      <td>datetime64[ns]</td>\n",
       "      <td>float64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_10</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_89</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_99</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: assign, 5 graph layers</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "               unique_id              ds        y static_0 static_1\n",
       "npartitions=10                                                     \n",
       "id_00             object  datetime64[ns]  float64    int64    int64\n",
       "id_10                ...             ...      ...      ...      ...\n",
       "...                  ...             ...      ...      ...      ...\n",
       "id_89                ...             ...      ...      ...      ...\n",
       "id_99                ...             ...      ...      ...      ...\n",
       "Dask Name: assign, 5 graph layers"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series = generate_daily_series(100, n_static_features=2, equal_ends=True, static_as_categorical=False)\n",
    "partitioned_series = dd.from_pandas(series, npartitions=10).map_partitions(lambda df: df.reset_index())\n",
    "partitioned_series['unique_id'] = partitioned_series['unique_id'].astype(str)\n",
    "partitioned_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf40d477-eddd-4c4d-b6b1-57fd4bd7c58f",
   "metadata": {},
   "source": [
    "### Models\n",
    "In order to perform distributed forecasting, we need to use a model that is able to train in a distributed way using `dask`. The current implementations are in `DaskLGBMForecast` and `DaskXGBForecast` which are just wrappers around the native implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e3eabd-f9d3-4d7e-b1e8-edccf8a27f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [DaskXGBForecast(random_state=0), DaskLGBMForecast(random_state=0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfb3968-0ee4-4141-ad7a-ee202dd8c044",
   "metadata": {},
   "source": [
    "### Training\n",
    "Once we have our models we instantiate a `DistributedMLForecast` object defining our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f56da13-0e3e-43d8-897d-af44fb89b037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistributedMLForecast(models=[DaskXGBForecast, DaskLGBMForecast], freq=<Day>, lag_features=['lag7', 'expanding_mean_lag1', 'rolling_mean_lag7_window_size14'], date_features=['dayofweek', 'month'], num_threads=1, engine=<Client: 'tcp://127.0.0.1:43409' processes=2 threads=2, memory=184.69 GiB>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcst = DistributedMLForecast(\n",
    "    models=models,\n",
    "    freq='D',\n",
    "    lags=[7],\n",
    "    lag_transforms={\n",
    "        1: [expanding_mean],\n",
    "        7: [(rolling_mean, 14)]\n",
    "    },\n",
    "    date_features=['dayofweek', 'month'],\n",
    "    num_threads=1,\n",
    "    engine=client,\n",
    ")\n",
    "fcst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67af6771-3484-4b54-82b1-ef6f284bb289",
   "metadata": {},
   "source": [
    "Here where we say that:\n",
    "\n",
    "* Our series have daily frequency.\n",
    "* We want to use lag 7 as a feature\n",
    "* We want the lag transformations to be:\n",
    "   * expanding mean of the lag 1\n",
    "   * rolling mean of the lag 7 over a window of size 14\n",
    "* We want to use dayofweek and month as date features.\n",
    "* We want to perform the preprocessing and the forecasting steps using 1 thread, because we have 10 partitions and 2 workers.\n",
    "\n",
    "From this point we have two options:\n",
    "\n",
    "1. Compute the features and fit our models.\n",
    "2. Compute the features and get them back as a dataframe to do some custom splitting or adding additional features, then training the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ea6591-d9bd-4273-8920-c40f678e2f2c",
   "metadata": {},
   "source": [
    "### 1. Using all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3201af6c-4915-4447-8bca-01fe9a1f7cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "## DistributedMLForecast.fit\n",
       "\n",
       ">      DistributedMLForecast.fit (data:~AnyDataFrame, id_col:str, time_col:str,\n",
       ">                                 target_col:str,\n",
       ">                                 static_features:Optional[List[str]]=None,\n",
       ">                                 dropna:bool=True,\n",
       ">                                 keep_last_n:Optional[int]=None)\n",
       "\n",
       "Apply the feature engineering and train the models.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| data | AnyDataFrame |  | Series data in long format. |\n",
       "| id_col | str |  | Column that identifies each serie. If 'index' then the index is used. |\n",
       "| time_col | str |  | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str |  | Column that contains the target. |\n",
       "| static_features | Optional | None | Names of the features that are static and will be repeated when forecasting. |\n",
       "| dropna | bool | True | Drop rows with missing values produced by the transformations. |\n",
       "| keep_last_n | Optional | None | Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it. |\n",
       "| **Returns** | **DistributedMLForecast** |  | **Forecast object with series values and trained models.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "## DistributedMLForecast.fit\n",
       "\n",
       ">      DistributedMLForecast.fit (data:~AnyDataFrame, id_col:str, time_col:str,\n",
       ">                                 target_col:str,\n",
       ">                                 static_features:Optional[List[str]]=None,\n",
       ">                                 dropna:bool=True,\n",
       ">                                 keep_last_n:Optional[int]=None)\n",
       "\n",
       "Apply the feature engineering and train the models.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| data | AnyDataFrame |  | Series data in long format. |\n",
       "| id_col | str |  | Column that identifies each serie. If 'index' then the index is used. |\n",
       "| time_col | str |  | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str |  | Column that contains the target. |\n",
       "| static_features | Optional | None | Names of the features that are static and will be repeated when forecasting. |\n",
       "| dropna | bool | True | Drop rows with missing values produced by the transformations. |\n",
       "| keep_last_n | Optional | None | Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it. |\n",
       "| **Returns** | **DistributedMLForecast** |  | **Forecast object with series values and trained models.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DistributedMLForecast.fit, title_level=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa359fc-e73b-4178-9421-848742b027ae",
   "metadata": {},
   "source": [
    "Calling `fit` on our data computes the features independently for each partition and performs distributed training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bf69ca-1723-43ef-a527-239aed06bd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst.fit(partitioned_series, id_col='unique_id', time_col='ds', target_col='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06df485-8496-45da-87e0-e45cf9b1f9ea",
   "metadata": {},
   "source": [
    "### Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc8f139-ae37-4927-8449-ed949dc254db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "## DistributedMLForecast.predict\n",
       "\n",
       ">      DistributedMLForecast.predict (horizon:int,\n",
       ">                                     dynamic_dfs:Optional[List[pandas.core.fram\n",
       ">                                     e.DataFrame]]=None, before_predict_callbac\n",
       ">                                     k:Optional[Callable]=None, after_predict_c\n",
       ">                                     allback:Optional[Callable]=None,\n",
       ">                                     new_data:Optional[~AnyDataFrame]=None)\n",
       "\n",
       "Compute the predictions for the next `horizon` steps.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| horizon | int |  | Number of periods to predict. |\n",
       "| dynamic_dfs | Optional | None | Future values of the dynamic features, e.g. prices. |\n",
       "| before_predict_callback | Optional | None | Function to call on the features before computing the predictions.<br>    This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure.<br>    The series identifier is on the index. |\n",
       "| after_predict_callback | Optional | None | Function to call on the predictions before updating the targets.<br>    This function will take a pandas Series with the predictions and should return another one with the same structure.<br>    The series identifier is on the index. |\n",
       "| new_data | Optional | None | Series data of new observations for which forecasts are to be generated.<br>    This dataframe should have the same structure as the one used to fit the model, including any features and time series data.<br>    If `new_data` is not None, the method will generate forecasts for the new observations.                 |\n",
       "| **Returns** | **AnyDataFrame** |  | **Predictions for each serie and timestep, with one column per model.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "## DistributedMLForecast.predict\n",
       "\n",
       ">      DistributedMLForecast.predict (horizon:int,\n",
       ">                                     dynamic_dfs:Optional[List[pandas.core.fram\n",
       ">                                     e.DataFrame]]=None, before_predict_callbac\n",
       ">                                     k:Optional[Callable]=None, after_predict_c\n",
       ">                                     allback:Optional[Callable]=None,\n",
       ">                                     new_data:Optional[~AnyDataFrame]=None)\n",
       "\n",
       "Compute the predictions for the next `horizon` steps.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| horizon | int |  | Number of periods to predict. |\n",
       "| dynamic_dfs | Optional | None | Future values of the dynamic features, e.g. prices. |\n",
       "| before_predict_callback | Optional | None | Function to call on the features before computing the predictions.<br>    This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure.<br>    The series identifier is on the index. |\n",
       "| after_predict_callback | Optional | None | Function to call on the predictions before updating the targets.<br>    This function will take a pandas Series with the predictions and should return another one with the same structure.<br>    The series identifier is on the index. |\n",
       "| new_data | Optional | None | Series data of new observations for which forecasts are to be generated.<br>    This dataframe should have the same structure as the one used to fit the model, including any features and time series data.<br>    If `new_data` is not None, the method will generate forecasts for the new observations.                 |\n",
       "| **Returns** | **AnyDataFrame** |  | **Predictions for each serie and timestep, with one column per model.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DistributedMLForecast.predict, title_level=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8162b4-4dc7-48cd-8a0e-f59f36825f1e",
   "metadata": {},
   "source": [
    "Once we have our fitted models we can compute the predictions for the next 7 timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694a1c30-40d0-4552-ab79-73eaf6179b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>DaskXGBForecast</th>\n",
       "      <th>DaskLGBMForecast</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=10</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>object</td>\n",
       "      <td>datetime64[ns]</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: map, 23 graph layers</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "               unique_id              ds DaskXGBForecast DaskLGBMForecast\n",
       "npartitions=10                                                           \n",
       "                  object  datetime64[ns]         float64          float64\n",
       "                     ...             ...             ...              ...\n",
       "...                  ...             ...             ...              ...\n",
       "                     ...             ...             ...              ...\n",
       "                     ...             ...             ...              ...\n",
       "Dask Name: map, 23 graph layers"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = fcst.predict(7)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba35d44-0916-4fb0-9a2d-ee98efbbe931",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "preds = preds.compute()\n",
    "preds2 = fcst.predict(7).compute()\n",
    "preds3 = fcst.predict(7, new_data=partitioned_series).compute()\n",
    "pd.testing.assert_frame_equal(preds, preds2)\n",
    "pd.testing.assert_frame_equal(preds, preds3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d85e0d-b167-43a2-a480-708e20aba44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "non_std_series = partitioned_series.copy()\n",
    "non_std_series['ds'] = non_std_series.map_partitions(lambda part: part.groupby('unique_id').cumcount())\n",
    "non_std_series = non_std_series.rename(columns={'ds': 'time', 'y': 'value', 'unique_id': 'some_id'})\n",
    "flow_params = dict(\n",
    "    models=[DaskXGBForecast(random_state=0)],\n",
    "    lags=[7],\n",
    "    lag_transforms={\n",
    "        1: [expanding_mean],\n",
    "        7: [(rolling_mean, 14)]\n",
    "    },\n",
    "    num_threads=1,\n",
    ")\n",
    "fcst = DistributedMLForecast(freq='D', **flow_params)\n",
    "fcst.fit(partitioned_series, id_col='unique_id', time_col='ds', target_col='y')\n",
    "preds = fcst.predict(7).compute()\n",
    "fcst2 = DistributedMLForecast(**flow_params)\n",
    "fcst2.preprocess(non_std_series, id_col='some_id', time_col='time', target_col='value')\n",
    "fcst2.models_ = fcst.models_  # distributed training can end up with different fits\n",
    "non_std_preds = fcst2.predict(7).compute()\n",
    "pd.testing.assert_frame_equal(\n",
    "    preds.drop(columns='ds'),\n",
    "    non_std_preds.drop(columns='time').rename(columns={'some_id': 'unique_id'})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6438efd-365c-444f-a447-03f44f1cc61a",
   "metadata": {},
   "source": [
    "### 2. Preprocess and train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b989f8-945e-4864-854d-92e846fe240d",
   "metadata": {},
   "source": [
    "If we only want to perform the preprocessing step we call `preprocess` with our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d825fee7-6b8f-4613-93b0-e5769a7abe39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "## DistributedMLForecast.preprocess\n",
       "\n",
       ">      DistributedMLForecast.preprocess (data:~AnyDataFrame, id_col:str,\n",
       ">                                        time_col:str, target_col:str, static_fe\n",
       ">                                        atures:Optional[List[str]]=None,\n",
       ">                                        dropna:bool=True,\n",
       ">                                        keep_last_n:Optional[int]=None)\n",
       "\n",
       "Add the features to `data`.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| data | AnyDataFrame |  | Series data in long format. |\n",
       "| id_col | str |  | Column that identifies each serie. If 'index' then the index is used. |\n",
       "| time_col | str |  | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str |  | Column that contains the target. |\n",
       "| static_features | Optional | None | Names of the features that are static and will be repeated when forecasting. |\n",
       "| dropna | bool | True | Drop rows with missing values produced by the transformations. |\n",
       "| keep_last_n | Optional | None | Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it. |\n",
       "| **Returns** | **AnyDataFrame** |  | **data with added features.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "## DistributedMLForecast.preprocess\n",
       "\n",
       ">      DistributedMLForecast.preprocess (data:~AnyDataFrame, id_col:str,\n",
       ">                                        time_col:str, target_col:str, static_fe\n",
       ">                                        atures:Optional[List[str]]=None,\n",
       ">                                        dropna:bool=True,\n",
       ">                                        keep_last_n:Optional[int]=None)\n",
       "\n",
       "Add the features to `data`.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| data | AnyDataFrame |  | Series data in long format. |\n",
       "| id_col | str |  | Column that identifies each serie. If 'index' then the index is used. |\n",
       "| time_col | str |  | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str |  | Column that contains the target. |\n",
       "| static_features | Optional | None | Names of the features that are static and will be repeated when forecasting. |\n",
       "| dropna | bool | True | Drop rows with missing values produced by the transformations. |\n",
       "| keep_last_n | Optional | None | Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it. |\n",
       "| **Returns** | **AnyDataFrame** |  | **data with added features.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DistributedMLForecast.preprocess, title_level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbd1a34-f3ad-4aca-8d94-62a2bc13299e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>static_0</th>\n",
       "      <th>static_1</th>\n",
       "      <th>lag7</th>\n",
       "      <th>expanding_mean_lag1</th>\n",
       "      <th>rolling_mean_lag7_window_size14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>id_17</td>\n",
       "      <td>2000-06-05</td>\n",
       "      <td>31.928572</td>\n",
       "      <td>59</td>\n",
       "      <td>13</td>\n",
       "      <td>30.434638</td>\n",
       "      <td>18.874181</td>\n",
       "      <td>19.224775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>id_17</td>\n",
       "      <td>2000-06-06</td>\n",
       "      <td>36.128788</td>\n",
       "      <td>59</td>\n",
       "      <td>13</td>\n",
       "      <td>38.283496</td>\n",
       "      <td>19.495819</td>\n",
       "      <td>19.336628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>id_17</td>\n",
       "      <td>2000-06-07</td>\n",
       "      <td>0.473843</td>\n",
       "      <td>59</td>\n",
       "      <td>13</td>\n",
       "      <td>2.816317</td>\n",
       "      <td>20.251863</td>\n",
       "      <td>19.330477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>id_17</td>\n",
       "      <td>2000-06-08</td>\n",
       "      <td>8.521403</td>\n",
       "      <td>59</td>\n",
       "      <td>13</td>\n",
       "      <td>6.770670</td>\n",
       "      <td>19.391949</td>\n",
       "      <td>19.176714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>id_17</td>\n",
       "      <td>2000-06-09</td>\n",
       "      <td>14.435941</td>\n",
       "      <td>59</td>\n",
       "      <td>13</td>\n",
       "      <td>13.585938</td>\n",
       "      <td>18.939009</td>\n",
       "      <td>19.185958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   unique_id         ds          y  static_0  static_1       lag7  \\\n",
       "20     id_17 2000-06-05  31.928572        59        13  30.434638   \n",
       "21     id_17 2000-06-06  36.128788        59        13  38.283496   \n",
       "22     id_17 2000-06-07   0.473843        59        13   2.816317   \n",
       "23     id_17 2000-06-08   8.521403        59        13   6.770670   \n",
       "24     id_17 2000-06-09  14.435941        59        13  13.585938   \n",
       "\n",
       "    expanding_mean_lag1  rolling_mean_lag7_window_size14  \n",
       "20            18.874181                        19.224775  \n",
       "21            19.495819                        19.336628  \n",
       "22            20.251863                        19.330477  \n",
       "23            19.391949                        19.176714  \n",
       "24            18.939009                        19.185958  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_ddf = fcst.preprocess(partitioned_series, id_col='unique_id', time_col='ds', target_col='y')\n",
    "features_ddf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15a97f1-7285-48fc-a016-6b8076c154ec",
   "metadata": {},
   "source": [
    "This is useful if we want to inspect the data the model will be trained. If we do this we must manually train our models and add a local version of them to the `models_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece0fb49-c02c-4336-af7c-e007bd19aaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = features_ddf.drop(columns=['unique_id', 'ds', 'y']), features_ddf['y']\n",
    "model = DaskXGBForecast(random_state=0).fit(X, y)\n",
    "fcst.models_ = {'DaskXGBForecast': model.model_}\n",
    "fcst.predict(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0380885-e914-4caf-ad85-7d9d1e6c0b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "fcst.models_ = fcst2.models_\n",
    "preds2 = fcst.predict(7).compute()\n",
    "pd.testing.assert_frame_equal(preds, preds2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e6f46c-8222-4cef-8955-a2b9a80a4735",
   "metadata": {},
   "source": [
    "### Dynamic features\n",
    "By default the predict method repeats the static features and updates the transformations and the date features. If you have dynamic features like prices or a calendar with holidays you can pass them as a list to the `dynamic_dfs` argument of `DistributedMLForecast.predict`, which will call `pd.DataFrame.merge` on each of them in order.\n",
    "\n",
    "Here's an example:\n",
    "\n",
    "Suppose that we have a `product_id` column and we have a catalog for prices based on that `product_id` and the date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a3b1df-530e-4673-b500-e26ffd378914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>product_id</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-06-09</td>\n",
       "      <td>1</td>\n",
       "      <td>0.548814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-06-10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.715189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-06-11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.602763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-06-12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.544883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-06-13</td>\n",
       "      <td>1</td>\n",
       "      <td>0.423655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20180</th>\n",
       "      <td>2001-05-17</td>\n",
       "      <td>99</td>\n",
       "      <td>0.223520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20181</th>\n",
       "      <td>2001-05-18</td>\n",
       "      <td>99</td>\n",
       "      <td>0.446104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20182</th>\n",
       "      <td>2001-05-19</td>\n",
       "      <td>99</td>\n",
       "      <td>0.044783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20183</th>\n",
       "      <td>2001-05-20</td>\n",
       "      <td>99</td>\n",
       "      <td>0.483216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20184</th>\n",
       "      <td>2001-05-21</td>\n",
       "      <td>99</td>\n",
       "      <td>0.799660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20185 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ds  product_id     price\n",
       "0     2000-06-09           1  0.548814\n",
       "1     2000-06-10           1  0.715189\n",
       "2     2000-06-11           1  0.602763\n",
       "3     2000-06-12           1  0.544883\n",
       "4     2000-06-13           1  0.423655\n",
       "...          ...         ...       ...\n",
       "20180 2001-05-17          99  0.223520\n",
       "20181 2001-05-18          99  0.446104\n",
       "20182 2001-05-19          99  0.044783\n",
       "20183 2001-05-20          99  0.483216\n",
       "20184 2001-05-21          99  0.799660\n",
       "\n",
       "[20185 rows x 3 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynamic_series = series.rename(columns={'static_1': 'product_id'})\n",
    "prices_catalog = generate_prices_for_series(dynamic_series)\n",
    "prices_catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5aec529-856e-4028-b8aa-4347eefd422f",
   "metadata": {},
   "source": [
    "And you have already merged these prices into your series dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0557a5a8-bfad-4c6e-a0a5-a3a2181d6c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>static_0</th>\n",
       "      <th>product_id</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-10-05</td>\n",
       "      <td>3.981198</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>0.570826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-10-06</td>\n",
       "      <td>10.327401</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>0.260562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-10-07</td>\n",
       "      <td>17.657474</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>0.274048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-10-08</td>\n",
       "      <td>25.898790</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>0.433878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-10-09</td>\n",
       "      <td>34.494040</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>0.653738</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  unique_id         ds          y  static_0  product_id     price\n",
       "0     id_00 2000-10-05   3.981198        79          45  0.570826\n",
       "1     id_00 2000-10-06  10.327401        79          45  0.260562\n",
       "2     id_00 2000-10-07  17.657474        79          45  0.274048\n",
       "3     id_00 2000-10-08  25.898790        79          45  0.433878\n",
       "4     id_00 2000-10-09  34.494040        79          45  0.653738"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynamic_series = partitioned_series.rename(columns={'static_1': 'product_id'})\n",
    "dynamic_series = dynamic_series\n",
    "series_with_prices = dynamic_series.merge(prices_catalog, how='left')\n",
    "series_with_prices.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951f5066-3685-43d0-91a6-09867790ab35",
   "metadata": {},
   "source": [
    "This dataframe will be passed to `DistributedMLForecast.fit` (or `DistributedMLForecast.preprocess`), however since the price is dynamic we have to tell that method that only `static_0` and `product_id` are static and we'll have to update `price` in every timestep, which basically involves merging the updated features with the prices catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ca112d-cbdb-41dc-88b4-fad613cb8b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst = DistributedMLForecast(\n",
    "    models,\n",
    "    freq='D',\n",
    "    lags=[7],\n",
    "    lag_transforms={\n",
    "        1: [expanding_mean],\n",
    "        7: [(rolling_mean, 14)]\n",
    "    },\n",
    "    date_features=['dayofweek', 'month'],\n",
    "    num_threads=1,\n",
    ")\n",
    "series_with_prices = series_with_prices\n",
    "fcst.fit(\n",
    "    series_with_prices,\n",
    "    id_col='unique_id',\n",
    "    time_col='ds',\n",
    "    target_col='y',\n",
    "    static_features=['static_0', 'product_id'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4191f9c1-d2cd-4896-9ff2-18c078cfc503",
   "metadata": {},
   "source": [
    "So in order to update the price in each timestep we just call `DistributedForecast.predict` with our forecast horizon and pass the prices catalog as a dynamic dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c4e580-73af-4b0f-943a-32bc3838cbb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>DaskXGBForecast</th>\n",
       "      <th>DaskLGBMForecast</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_17</td>\n",
       "      <td>2001-05-15</td>\n",
       "      <td>37.718147</td>\n",
       "      <td>37.627785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_17</td>\n",
       "      <td>2001-05-16</td>\n",
       "      <td>1.317483</td>\n",
       "      <td>1.414693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_17</td>\n",
       "      <td>2001-05-17</td>\n",
       "      <td>7.393958</td>\n",
       "      <td>7.572545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_17</td>\n",
       "      <td>2001-05-18</td>\n",
       "      <td>13.076118</td>\n",
       "      <td>12.791319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_17</td>\n",
       "      <td>2001-05-19</td>\n",
       "      <td>18.812704</td>\n",
       "      <td>18.705727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-17</td>\n",
       "      <td>43.616680</td>\n",
       "      <td>44.301239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-18</td>\n",
       "      <td>1.754835</td>\n",
       "      <td>2.004719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-19</td>\n",
       "      <td>9.136863</td>\n",
       "      <td>9.261787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-20</td>\n",
       "      <td>15.539311</td>\n",
       "      <td>15.362283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-21</td>\n",
       "      <td>22.682920</td>\n",
       "      <td>22.878186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>700 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   unique_id         ds  DaskXGBForecast  DaskLGBMForecast\n",
       "0      id_17 2001-05-15        37.718147         37.627785\n",
       "1      id_17 2001-05-16         1.317483          1.414693\n",
       "2      id_17 2001-05-17         7.393958          7.572545\n",
       "3      id_17 2001-05-18        13.076118         12.791319\n",
       "4      id_17 2001-05-19        18.812704         18.705727\n",
       "..       ...        ...              ...               ...\n",
       "2      id_99 2001-05-17        43.616680         44.301239\n",
       "3      id_99 2001-05-18         1.754835          2.004719\n",
       "4      id_99 2001-05-19         9.136863          9.261787\n",
       "5      id_99 2001-05-20        15.539311         15.362283\n",
       "6      id_99 2001-05-21        22.682920         22.878186\n",
       "\n",
       "[700 rows x 4 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = fcst.predict(7, dynamic_dfs=[prices_catalog])\n",
    "preds.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06edbe8-27e1-4662-bf5a-60a67c7f6713",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test we can compute cross validation with\n",
    "# exougenous variables without adding extra information\n",
    "# later a more robust test is performed\n",
    "cv_with_ex = fcst.cross_validation(\n",
    "    series_with_prices,\n",
    "    window_size=7,\n",
    "    n_windows=2,\n",
    "    id_col='unique_id',\n",
    "    time_col='ds',\n",
    "    target_col='y',\n",
    "    static_features=['static_0', 'product_id'],\n",
    ").compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c37303b-d2a9-4041-8cbe-f7921e5a7316",
   "metadata": {},
   "source": [
    "### Custom predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3063a465-11bb-4e85-be4e-a3f4978e6fc7",
   "metadata": {},
   "source": [
    "If you want to do something like scaling the predictions you can define a function and pass it to `DistributedMLForecast.predict` as described in <a href=\"/forecast.html#custom-predictions\">Custom predictions</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9461e8-2891-4c52-ab1c-0b1417c9ce60",
   "metadata": {},
   "source": [
    "#### Cross validation\n",
    "Refer to `MLForecast.cross_validation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3faef3-89d4-45dd-bcda-78aba4414a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "## DistributedMLForecast.cross_validation\n",
       "\n",
       ">      DistributedMLForecast.cross_validation (data:~AnyDataFrame,\n",
       ">                                              n_windows:int, window_size:int,\n",
       ">                                              id_col:str, time_col:str,\n",
       ">                                              target_col:str,\n",
       ">                                              step_size:Optional[int]=None, sta\n",
       ">                                              tic_features:Optional[List[str]]=\n",
       ">                                              None, dropna:bool=True,\n",
       ">                                              keep_last_n:Optional[int]=None,\n",
       ">                                              refit:bool=True, before_predict_c\n",
       ">                                              allback:Optional[Callable]=None, \n",
       ">                                              after_predict_callback:Optional[C\n",
       ">                                              allable]=None)\n",
       "\n",
       "Perform time series cross validation.\n",
       "Creates `n_windows` splits where each window has `window_size` test periods,\n",
       "trains the models, computes the predictions and merges the actuals.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| data | AnyDataFrame |  | Series data in long format. |\n",
       "| n_windows | int |  | Number of windows to evaluate. |\n",
       "| window_size | int |  | Number of test periods in each window. |\n",
       "| id_col | str |  | Column that identifies each serie. If 'index' then the index is used. |\n",
       "| time_col | str |  | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str |  | Column that contains the target. |\n",
       "| step_size | Optional | None | Step size between each cross validation window. If None it will be equal to `window_size`. |\n",
       "| static_features | Optional | None | Names of the features that are static and will be repeated when forecasting. |\n",
       "| dropna | bool | True | Drop rows with missing values produced by the transformations. |\n",
       "| keep_last_n | Optional | None | Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it. |\n",
       "| refit | bool | True | Retrain model for each cross validation window.<br>If False, the models are trained at the beginning and then used to predict each window.             |\n",
       "| before_predict_callback | Optional | None | Function to call on the features before computing the predictions.<br>    This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure.<br>    The series identifier is on the index. |\n",
       "| after_predict_callback | Optional | None | Function to call on the predictions before updating the targets.<br>    This function will take a pandas Series with the predictions and should return another one with the same structure.<br>    The series identifier is on the index. |\n",
       "| **Returns** | **AnyDataFrame** |  | **Predictions for each window with the series id, timestamp, target value and predictions from each model.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "## DistributedMLForecast.cross_validation\n",
       "\n",
       ">      DistributedMLForecast.cross_validation (data:~AnyDataFrame,\n",
       ">                                              n_windows:int, window_size:int,\n",
       ">                                              id_col:str, time_col:str,\n",
       ">                                              target_col:str,\n",
       ">                                              step_size:Optional[int]=None, sta\n",
       ">                                              tic_features:Optional[List[str]]=\n",
       ">                                              None, dropna:bool=True,\n",
       ">                                              keep_last_n:Optional[int]=None,\n",
       ">                                              refit:bool=True, before_predict_c\n",
       ">                                              allback:Optional[Callable]=None, \n",
       ">                                              after_predict_callback:Optional[C\n",
       ">                                              allable]=None)\n",
       "\n",
       "Perform time series cross validation.\n",
       "Creates `n_windows` splits where each window has `window_size` test periods,\n",
       "trains the models, computes the predictions and merges the actuals.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| data | AnyDataFrame |  | Series data in long format. |\n",
       "| n_windows | int |  | Number of windows to evaluate. |\n",
       "| window_size | int |  | Number of test periods in each window. |\n",
       "| id_col | str |  | Column that identifies each serie. If 'index' then the index is used. |\n",
       "| time_col | str |  | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str |  | Column that contains the target. |\n",
       "| step_size | Optional | None | Step size between each cross validation window. If None it will be equal to `window_size`. |\n",
       "| static_features | Optional | None | Names of the features that are static and will be repeated when forecasting. |\n",
       "| dropna | bool | True | Drop rows with missing values produced by the transformations. |\n",
       "| keep_last_n | Optional | None | Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it. |\n",
       "| refit | bool | True | Retrain model for each cross validation window.<br>If False, the models are trained at the beginning and then used to predict each window.             |\n",
       "| before_predict_callback | Optional | None | Function to call on the features before computing the predictions.<br>    This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure.<br>    The series identifier is on the index. |\n",
       "| after_predict_callback | Optional | None | Function to call on the predictions before updating the targets.<br>    This function will take a pandas Series with the predictions and should return another one with the same structure.<br>    The series identifier is on the index. |\n",
       "| **Returns** | **AnyDataFrame** |  | **Predictions for each window with the series id, timestamp, target value and predictions from each model.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DistributedMLForecast.cross_validation, title_level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e006da05-b056-4859-8005-9ece21dd3c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst = DistributedMLForecast(\n",
    "    models=[DaskLGBMForecast(), DaskXGBForecast()],\n",
    "    freq='D',\n",
    "    lags=[7],\n",
    "    lag_transforms={\n",
    "        1: [expanding_mean],\n",
    "        7: [(rolling_mean, 14)]\n",
    "    },\n",
    "    date_features=['dayofweek', 'month'],\n",
    "    num_threads=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8eb4ac-1622-4eed-8892-e1272e131925",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_windows = 2\n",
    "window_size = 14\n",
    "\n",
    "cv_results = fcst.cross_validation(\n",
    "    partitioned_series,\n",
    "    n_windows,\n",
    "    window_size,\n",
    "    id_col='unique_id',\n",
    "    time_col='ds',\n",
    "    target_col='y',\n",
    ")\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a173f86-eeca-4e66-9df3-9282bf4afcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "cv_results_no_refit = fcst.cross_validation(\n",
    "    partitioned_series,\n",
    "    n_windows,\n",
    "    window_size,\n",
    "    id_col='unique_id',\n",
    "    time_col='ds',\n",
    "    target_col='y',\n",
    "    refit=False\n",
    ")\n",
    "cv_results_df = cv_results.compute()\n",
    "cv_results_no_refit_df = cv_results_no_refit.compute()\n",
    "# test we recover the same \"metadata\"\n",
    "models = ['DaskXGBForecast', 'DaskLGBMForecast']\n",
    "test_eq(\n",
    "    cv_results_no_refit_df.drop(columns=models),\n",
    "    cv_results_df.drop(columns=models)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df86990-35ff-4b44-b96a-cabd22b03adb",
   "metadata": {},
   "source": [
    "We can aggregate these by date to get a rough estimate of how our model is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1776f7a5-996d-4598-90d6-c5217b6da9e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DaskLGBMForecast</th>\n",
       "      <th>DaskXGBForecast</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ds</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2001-04-17</th>\n",
       "      <td>16.177839</td>\n",
       "      <td>16.207031</td>\n",
       "      <td>16.123231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-04-18</th>\n",
       "      <td>15.117420</td>\n",
       "      <td>15.148521</td>\n",
       "      <td>15.213920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-04-19</th>\n",
       "      <td>17.120856</td>\n",
       "      <td>17.104460</td>\n",
       "      <td>16.985699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-04-20</th>\n",
       "      <td>18.009654</td>\n",
       "      <td>18.052552</td>\n",
       "      <td>18.068340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-04-21</th>\n",
       "      <td>18.119445</td>\n",
       "      <td>18.133831</td>\n",
       "      <td>18.200609</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            DaskLGBMForecast  DaskXGBForecast          y\n",
       "ds                                                      \n",
       "2001-04-17         16.177839        16.207031  16.123231\n",
       "2001-04-18         15.117420        15.148521  15.213920\n",
       "2001-04-19         17.120856        17.104460  16.985699\n",
       "2001-04-20         18.009654        18.052552  18.068340\n",
       "2001-04-21         18.119445        18.133831  18.200609"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_results = cv_results_df.drop(columns='cutoff').groupby('ds').mean()\n",
    "agg_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccb8da2-6009-4143-8c3b-3425921f52de",
   "metadata": {},
   "source": [
    "We can also compute the error for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c4a38f-70a7-4606-a902-bd3944228cc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>DaskLGBMForecast</th>\n",
       "      <th>DaskXGBForecast</th>\n",
       "      <th>cutoff</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>object</td>\n",
       "      <td>datetime64[ns]</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>datetime64[ns]</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: drop-duplicates-agg, 40 graph layers</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "              unique_id              ds DaskLGBMForecast DaskXGBForecast          cutoff        y\n",
       "npartitions=1                                                                                    \n",
       "                 object  datetime64[ns]          float64         float64  datetime64[ns]  float64\n",
       "                    ...             ...              ...             ...             ...      ...\n",
       "Dask Name: drop-duplicates-agg, 40 graph layers"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a449b9e-7b68-443a-b3c2-4b0977d32143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DaskLGBMForecast': 0.93, 'DaskXGBForecast': 0.88}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mse_from_dask_dataframe(ddf):\n",
    "    mses = {}\n",
    "    for model_name in ddf.columns.drop(['unique_id', 'ds', 'y', 'cutoff']):\n",
    "        mses[model_name] = (ddf['y'] - ddf[model_name]).pow(2).mean()\n",
    "    return client.gather(client.compute(mses))\n",
    "\n",
    "{k: round(v, 2) for k, v in mse_from_dask_dataframe(cv_results).items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa70236c-dfd0-4cc7-9a0a-46a9769567ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bd1c31-9270-4176-b9b1-eaaaeae3cf28",
   "metadata": {},
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e7f9e1-4bac-4998-b0b7-a9897e5a3ef3",
   "metadata": {},
   "source": [
    "### Session setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81348566-8200-419f-9921-c6b5b655652d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9713172-2263-48a5-99fd-f66b13117cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"MyApp\")\n",
    "    .config(\"spark.jars.packages\", \"com.microsoft.azure:synapseml_2.12:0.10.2\")\n",
    "    .config(\"spark.jars.repositories\", \"https://mmlspark.azureedge.net/maven\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadaef9e-29a6-4423-8f32-573ea7b4edc7",
   "metadata": {},
   "source": [
    "### Data setup\n",
    "For spark, the data must be a `pyspark DataFrame`. You need to make sure that each time serie is only in one partition (which you can do using `repartitionByRange`, for example) and it is recommended that you have as many partitions as you have workers. If you have more partitions than workers make sure to set `num_threads=1` to avoid having nested parallelism.\n",
    "\n",
    "The required input format is the same as for `MLForecast`, i.e. it should have at least an id column, a time column and a target column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9efe100-a3b8-4b23-93c5-8235cbe4e6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "series = generate_daily_series(100, n_static_features=2, equal_ends=True, static_as_categorical=False)\n",
    "spark_series = spark.createDataFrame(series.reset_index()).repartitionByRange(4, 'unique_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609ef79b-259d-4939-bb9f-b9bec54e326e",
   "metadata": {},
   "source": [
    "### Models\n",
    "In order to perform distributed forecasting, we need to use a model that is able to train in a distributed way using `spark`. The current implementations are in `SparkLGBMForecast` and `SparkXGBForecast` which are just wrappers around the native implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071e6bb6-7cfe-4208-bae9-8e8b4cc0cfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlforecast.distributed.models.spark.lgb import SparkLGBMForecast\n",
    "\n",
    "models = [SparkLGBMForecast()]\n",
    "try:\n",
    "    from xgboost.spark import SparkXGBRegressor\n",
    "    from mlforecast.distributed.models.spark.xgb import SparkXGBForecast\n",
    "    models.append(SparkXGBForecast())\n",
    "except ModuleNotFoundError:  # py < 38\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff83d8e-a96c-4bba-9305-9c8a89153ac4",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91acad4-55fd-43c6-838d-3102816664fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst = DistributedMLForecast(\n",
    "    models,\n",
    "    freq='D',\n",
    "    lags=[1],\n",
    "    lag_transforms={\n",
    "        1: [expanding_mean]\n",
    "    },\n",
    "    date_features=['dayofweek'],\n",
    ")\n",
    "fcst.fit(\n",
    "    spark_series,\n",
    "    id_col='unique_id',\n",
    "    time_col='ds',\n",
    "    target_col='y',\n",
    "    static_features=['static_0', 'static_1'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473f10d8-6e41-4096-a239-cd8bc8febc4e",
   "metadata": {},
   "source": [
    "### Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d003e12c-ffa4-42b7-8387-5fe78baa1b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = fcst.predict(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f2fd37-39d1-4025-82b6-7cc9b76a1e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>SparkLGBMForecast</th>\n",
       "      <th>SparkXGBForecast</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-15</td>\n",
       "      <td>42.213984</td>\n",
       "      <td>42.305004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-16</td>\n",
       "      <td>49.718021</td>\n",
       "      <td>50.262386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-17</td>\n",
       "      <td>1.306248</td>\n",
       "      <td>1.912686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-18</td>\n",
       "      <td>10.060104</td>\n",
       "      <td>10.240939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-19</td>\n",
       "      <td>18.070785</td>\n",
       "      <td>18.265749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-24</td>\n",
       "      <td>43.426901</td>\n",
       "      <td>43.780163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-25</td>\n",
       "      <td>1.361680</td>\n",
       "      <td>2.097803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-26</td>\n",
       "      <td>8.787283</td>\n",
       "      <td>8.593580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-27</td>\n",
       "      <td>15.551965</td>\n",
       "      <td>15.622238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-28</td>\n",
       "      <td>22.518518</td>\n",
       "      <td>22.943216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1400 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     unique_id         ds  SparkLGBMForecast  SparkXGBForecast\n",
       "0        id_00 2001-05-15          42.213984         42.305004\n",
       "1        id_00 2001-05-16          49.718021         50.262386\n",
       "2        id_00 2001-05-17           1.306248          1.912686\n",
       "3        id_00 2001-05-18          10.060104         10.240939\n",
       "4        id_00 2001-05-19          18.070785         18.265749\n",
       "...        ...        ...                ...               ...\n",
       "1395     id_99 2001-05-24          43.426901         43.780163\n",
       "1396     id_99 2001-05-25           1.361680          2.097803\n",
       "1397     id_99 2001-05-26           8.787283          8.593580\n",
       "1398     id_99 2001-05-27          15.551965         15.622238\n",
       "1399     id_99 2001-05-28          22.518518         22.943216\n",
       "\n",
       "[1400 rows x 4 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7006bea-6774-49dd-8a34-b3f07320aee6",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df462c44-f309-45bc-97c5-39b946030d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_res = fcst.cross_validation(\n",
    "    spark_series,\n",
    "    n_windows=2,\n",
    "    window_size=14,\n",
    "    id_col='unique_id',\n",
    "    time_col='ds',\n",
    "    target_col='y',\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d9d284-4be4-45e1-9ba1-d285257d6607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>SparkLGBMForecast</th>\n",
       "      <th>SparkXGBForecast</th>\n",
       "      <th>cutoff</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_26</td>\n",
       "      <td>2001-04-17</td>\n",
       "      <td>0.770200</td>\n",
       "      <td>1.325263</td>\n",
       "      <td>2001-04-16</td>\n",
       "      <td>0.661216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_27</td>\n",
       "      <td>2001-04-18</td>\n",
       "      <td>24.101212</td>\n",
       "      <td>23.596254</td>\n",
       "      <td>2001-04-16</td>\n",
       "      <td>23.218062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_02</td>\n",
       "      <td>2001-04-28</td>\n",
       "      <td>0.837139</td>\n",
       "      <td>1.110391</td>\n",
       "      <td>2001-04-16</td>\n",
       "      <td>1.035494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_12</td>\n",
       "      <td>2001-04-20</td>\n",
       "      <td>7.965585</td>\n",
       "      <td>7.741258</td>\n",
       "      <td>2001-04-16</td>\n",
       "      <td>9.436190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_03</td>\n",
       "      <td>2001-04-20</td>\n",
       "      <td>16.628429</td>\n",
       "      <td>16.529549</td>\n",
       "      <td>2001-04-16</td>\n",
       "      <td>17.446369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2795</th>\n",
       "      <td>id_80</td>\n",
       "      <td>2001-05-06</td>\n",
       "      <td>29.935527</td>\n",
       "      <td>30.079792</td>\n",
       "      <td>2001-04-30</td>\n",
       "      <td>30.289962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2796</th>\n",
       "      <td>id_88</td>\n",
       "      <td>2001-05-05</td>\n",
       "      <td>3.749097</td>\n",
       "      <td>0.514714</td>\n",
       "      <td>2001-04-30</td>\n",
       "      <td>0.953977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2797</th>\n",
       "      <td>id_89</td>\n",
       "      <td>2001-05-03</td>\n",
       "      <td>16.633254</td>\n",
       "      <td>17.505013</td>\n",
       "      <td>2001-04-30</td>\n",
       "      <td>16.616387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2798</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-07</td>\n",
       "      <td>23.385428</td>\n",
       "      <td>23.057673</td>\n",
       "      <td>2001-04-30</td>\n",
       "      <td>21.497185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2799</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-11</td>\n",
       "      <td>1.971762</td>\n",
       "      <td>1.749306</td>\n",
       "      <td>2001-04-30</td>\n",
       "      <td>3.022948</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2800 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     unique_id         ds  SparkLGBMForecast  SparkXGBForecast     cutoff  \\\n",
       "0        id_26 2001-04-17           0.770200          1.325263 2001-04-16   \n",
       "1        id_27 2001-04-18          24.101212         23.596254 2001-04-16   \n",
       "2        id_02 2001-04-28           0.837139          1.110391 2001-04-16   \n",
       "3        id_12 2001-04-20           7.965585          7.741258 2001-04-16   \n",
       "4        id_03 2001-04-20          16.628429         16.529549 2001-04-16   \n",
       "...        ...        ...                ...               ...        ...   \n",
       "2795     id_80 2001-05-06          29.935527         30.079792 2001-04-30   \n",
       "2796     id_88 2001-05-05           3.749097          0.514714 2001-04-30   \n",
       "2797     id_89 2001-05-03          16.633254         17.505013 2001-04-30   \n",
       "2798     id_99 2001-05-07          23.385428         23.057673 2001-04-30   \n",
       "2799     id_99 2001-05-11           1.971762          1.749306 2001-04-30   \n",
       "\n",
       "              y  \n",
       "0      0.661216  \n",
       "1     23.218062  \n",
       "2      1.035494  \n",
       "3      9.436190  \n",
       "4     17.446369  \n",
       "...         ...  \n",
       "2795  30.289962  \n",
       "2796   0.953977  \n",
       "2797  16.616387  \n",
       "2798  21.497185  \n",
       "2799   3.022948  \n",
       "\n",
       "[2800 rows x 6 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d426eef-f002-4456-805e-4f61667abcac",
   "metadata": {},
   "source": [
    "## Ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ed8f06-cdc6-4507-94ad-39bed3554552",
   "metadata": {},
   "source": [
    "### Session setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87a927e-ceda-4c09-a1f3-ce542d00ecc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aadd62-cc24-4c29-857a-aa0c28fa2159",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf4f53e-18be-4d15-b93e-536efe0d3f17",
   "metadata": {},
   "source": [
    "### Data setup\n",
    "For ray, the data must be a `ray DataFrame`. It is recommended that you have as many partitions as you have workers. If you have more partitions than workers make sure to set `num_threads=1` to avoid having nested parallelism.\n",
    "\n",
    "The required input format is the same as for `MLForecast`, i.e. it should have at least an id column, a time column and a target column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c910c858-f038-4d49-b844-1e9a2932f16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "series = generate_daily_series(100, n_static_features=2, equal_ends=True, static_as_categorical=False)\n",
    "series = series.reset_index()\n",
    "# we need noncategory unique_id\n",
    "series['unique_id'] = series['unique_id'].astype(str)\n",
    "ray_series = ray.data.from_pandas(series).repartition(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6487397d-09dc-49d4-a909-cf37759b29f3",
   "metadata": {},
   "source": [
    "### Models\n",
    "The ray integration allows to include `sklearn` models and also `lightgbm` (`LGBMRegressor`), and `xgboost` (`XGBRegressor`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6db2d8-34d4-483e-9d4e-75993c0c8683",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "models = [LinearRegression(), LGBMRegressor(), XGBRegressor()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c25924-dae1-4d25-95e2-261814f4a1a0",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d84521-e796-4a8c-9b39-cd1b9310fdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "fcst = DistributedMLForecast(\n",
    "    models,\n",
    "    freq='D',\n",
    "    lags=[1],\n",
    "    lag_transforms={\n",
    "        1: [expanding_mean]\n",
    "    },\n",
    "    date_features=['dayofweek'],\n",
    ")\n",
    "fcst.fit(\n",
    "    ray_series,\n",
    "    id_col='unique_id',\n",
    "    time_col='ds',\n",
    "    target_col='y',\n",
    "    static_features=['static_0', 'static_1'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa79da49-3c1a-41c6-8597-410d382d443d",
   "metadata": {},
   "source": [
    "### Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af59dab-5458-49e2-b794-6b80529210b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = fcst.predict(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657802d4-cb7c-450c-9360-6420522a47db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>LinearRegression</th>\n",
       "      <th>LGBMRegressor</th>\n",
       "      <th>XGBRegressor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-15</td>\n",
       "      <td>27.550165</td>\n",
       "      <td>42.213984</td>\n",
       "      <td>42.196953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-16</td>\n",
       "      <td>26.032221</td>\n",
       "      <td>49.718021</td>\n",
       "      <td>50.788631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-17</td>\n",
       "      <td>26.045137</td>\n",
       "      <td>1.306248</td>\n",
       "      <td>2.322584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-18</td>\n",
       "      <td>26.460494</td>\n",
       "      <td>10.060104</td>\n",
       "      <td>10.294286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-19</td>\n",
       "      <td>26.979665</td>\n",
       "      <td>18.070785</td>\n",
       "      <td>18.293076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-24</td>\n",
       "      <td>22.605174</td>\n",
       "      <td>43.426901</td>\n",
       "      <td>44.132332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-25</td>\n",
       "      <td>23.152805</td>\n",
       "      <td>1.361680</td>\n",
       "      <td>1.684340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-26</td>\n",
       "      <td>23.706656</td>\n",
       "      <td>8.787283</td>\n",
       "      <td>8.621625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-27</td>\n",
       "      <td>24.259044</td>\n",
       "      <td>15.551965</td>\n",
       "      <td>15.587831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-28</td>\n",
       "      <td>21.919096</td>\n",
       "      <td>22.518518</td>\n",
       "      <td>22.936302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1400 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     unique_id         ds  LinearRegression  LGBMRegressor  XGBRegressor\n",
       "0        id_00 2001-05-15         27.550165      42.213984     42.196953\n",
       "1        id_00 2001-05-16         26.032221      49.718021     50.788631\n",
       "2        id_00 2001-05-17         26.045137       1.306248      2.322584\n",
       "3        id_00 2001-05-18         26.460494      10.060104     10.294286\n",
       "4        id_00 2001-05-19         26.979665      18.070785     18.293076\n",
       "...        ...        ...               ...            ...           ...\n",
       "1395     id_99 2001-05-24         22.605174      43.426901     44.132332\n",
       "1396     id_99 2001-05-25         23.152805       1.361680      1.684340\n",
       "1397     id_99 2001-05-26         23.706656       8.787283      8.621625\n",
       "1398     id_99 2001-05-27         24.259044      15.551965     15.587831\n",
       "1399     id_99 2001-05-28         21.919096      22.518518     22.936302\n",
       "\n",
       "[1400 rows x 5 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8349b33f-a892-4b7d-9379-c1edc4c87d78",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f9b4b1-f923-49eb-9c78-8388f4d2b4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "cv_res = fcst.cross_validation(\n",
    "    ray_series,\n",
    "    n_windows=2,\n",
    "    window_size=14,\n",
    "    id_col='unique_id',\n",
    "    time_col='ds',\n",
    "    target_col='y',\n",
    ").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfca612-25bb-4f7f-8f28-55fe7151d6fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>LinearRegression</th>\n",
       "      <th>LGBMRegressor</th>\n",
       "      <th>XGBRegressor</th>\n",
       "      <th>cutoff</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_03</td>\n",
       "      <td>2001-04-17</td>\n",
       "      <td>19.354797</td>\n",
       "      <td>0.733232</td>\n",
       "      <td>1.582030</td>\n",
       "      <td>2001-04-16</td>\n",
       "      <td>0.082253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_03</td>\n",
       "      <td>2001-04-18</td>\n",
       "      <td>16.478312</td>\n",
       "      <td>6.277368</td>\n",
       "      <td>6.020199</td>\n",
       "      <td>2001-04-16</td>\n",
       "      <td>6.064865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_03</td>\n",
       "      <td>2001-04-19</td>\n",
       "      <td>16.146571</td>\n",
       "      <td>11.605073</td>\n",
       "      <td>10.429543</td>\n",
       "      <td>2001-04-16</td>\n",
       "      <td>10.234736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_03</td>\n",
       "      <td>2001-04-20</td>\n",
       "      <td>16.489043</td>\n",
       "      <td>16.628429</td>\n",
       "      <td>15.988829</td>\n",
       "      <td>2001-04-16</td>\n",
       "      <td>17.446369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_03</td>\n",
       "      <td>2001-04-21</td>\n",
       "      <td>17.008040</td>\n",
       "      <td>21.325320</td>\n",
       "      <td>21.530821</td>\n",
       "      <td>2001-04-16</td>\n",
       "      <td>21.438031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2795</th>\n",
       "      <td>id_94</td>\n",
       "      <td>2001-05-11</td>\n",
       "      <td>9.215405</td>\n",
       "      <td>14.789303</td>\n",
       "      <td>15.383530</td>\n",
       "      <td>2001-04-30</td>\n",
       "      <td>1.106124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2796</th>\n",
       "      <td>id_94</td>\n",
       "      <td>2001-05-12</td>\n",
       "      <td>9.780934</td>\n",
       "      <td>13.922064</td>\n",
       "      <td>7.933564</td>\n",
       "      <td>2001-04-30</td>\n",
       "      <td>3.317492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2797</th>\n",
       "      <td>id_94</td>\n",
       "      <td>2001-05-13</td>\n",
       "      <td>10.346017</td>\n",
       "      <td>14.789303</td>\n",
       "      <td>11.291733</td>\n",
       "      <td>2001-04-30</td>\n",
       "      <td>6.409359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2798</th>\n",
       "      <td>id_94</td>\n",
       "      <td>2001-05-14</td>\n",
       "      <td>7.967080</td>\n",
       "      <td>9.974652</td>\n",
       "      <td>13.469623</td>\n",
       "      <td>2001-04-30</td>\n",
       "      <td>8.486872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2799</th>\n",
       "      <td>id_94</td>\n",
       "      <td>2001-05-05</td>\n",
       "      <td>9.785551</td>\n",
       "      <td>11.749486</td>\n",
       "      <td>14.556999</td>\n",
       "      <td>2001-04-30</td>\n",
       "      <td>3.664288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2800 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     unique_id         ds  LinearRegression  LGBMRegressor  XGBRegressor  \\\n",
       "0        id_03 2001-04-17         19.354797       0.733232      1.582030   \n",
       "1        id_03 2001-04-18         16.478312       6.277368      6.020199   \n",
       "2        id_03 2001-04-19         16.146571      11.605073     10.429543   \n",
       "3        id_03 2001-04-20         16.489043      16.628429     15.988829   \n",
       "4        id_03 2001-04-21         17.008040      21.325320     21.530821   \n",
       "...        ...        ...               ...            ...           ...   \n",
       "2795     id_94 2001-05-11          9.215405      14.789303     15.383530   \n",
       "2796     id_94 2001-05-12          9.780934      13.922064      7.933564   \n",
       "2797     id_94 2001-05-13         10.346017      14.789303     11.291733   \n",
       "2798     id_94 2001-05-14          7.967080       9.974652     13.469623   \n",
       "2799     id_94 2001-05-05          9.785551      11.749486     14.556999   \n",
       "\n",
       "         cutoff          y  \n",
       "0    2001-04-16   0.082253  \n",
       "1    2001-04-16   6.064865  \n",
       "2    2001-04-16  10.234736  \n",
       "3    2001-04-16  17.446369  \n",
       "4    2001-04-16  21.438031  \n",
       "...         ...        ...  \n",
       "2795 2001-04-30   1.106124  \n",
       "2796 2001-04-30   3.317492  \n",
       "2797 2001-04-30   6.409359  \n",
       "2798 2001-04-30   8.486872  \n",
       "2799 2001-04-30   3.664288  \n",
       "\n",
       "[2800 rows x 7 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f38f46e-f36a-4e84-ab67-77bee7f7f9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
