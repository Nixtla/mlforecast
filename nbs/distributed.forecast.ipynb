{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71e077d-b6cc-4913-b206-77b652053251",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_distributed\n",
    "#default_exp distributed.forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a7d947-ce42-40ec-8b54-623ec2189dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c82054e-db3d-43ca-a4fe-c397e351ad1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import warnings\n",
    "\n",
    "from nbdev import show_doc\n",
    "\n",
    "warnings.simplefilter('ignore', UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933ec908-b746-4b80-8da3-fcb0039145d4",
   "metadata": {},
   "source": [
    "# Distributed forecast\n",
    "\n",
    "> Distributed pipeline encapsulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6d15cd-5734-4585-aaca-de860e07f9ab",
   "metadata": {},
   "source": [
    "**This interface is only available on Linux**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3e3690-1951-487a-8c66-ba1b2cc01756",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from typing import Callable, Generator, List, Optional\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "from dask.distributed import Client, default_client\n",
    "\n",
    "from mlforecast.core import TimeSeries, simple_predict\n",
    "from mlforecast.utils import backtest_splits\n",
    "from mlforecast.distributed.core import DistributedTimeSeries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434b7a1a-716f-433c-ab86-8ec33f0f185e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DistributedForecast:\n",
    "    \"\"\"Distributed pipeline encapsulation.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, ts: TimeSeries, client: Optional[Client] = None):\n",
    "        self.model = model\n",
    "        self.client = client or default_client()\n",
    "        self.dts = DistributedTimeSeries(ts, self.client)\n",
    "        self.model.client = self.client\n",
    "        \n",
    "    def __repr__(self) -> str:\n",
    "        return f'DistributedForecast(model={self.model}, dts={self.dts})'\n",
    "\n",
    "    def preprocess(\n",
    "        self,\n",
    "        data: dd.DataFrame,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,        \n",
    "    ) -> dd.DataFrame:\n",
    "        \"\"\"Computes the transformations on each partition of `data`.\n",
    "\n",
    "        Saves the resulting `TimeSeries` objects as well as the divisions in `data` for the forecasting step.\n",
    "        Returns a dask dataframe with the computed features.\"\"\"\n",
    "        return self.dts.fit_transform(data, static_features, dropna, keep_last_n)\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        data: dd.DataFrame,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,            \n",
    "        **fit_kwargs,\n",
    "    ) -> 'DistributedForecast':\n",
    "        \"\"\"Perform the preprocessing and fit the model.\"\"\"\n",
    "        train_ddf = self.preprocess(data, static_features, dropna, keep_last_n)\n",
    "        X, y = train_ddf.drop(columns=['ds', 'y']), train_ddf.y\n",
    "        self.model.fit(X, y, **fit_kwargs)\n",
    "        return self\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        horizon: int,\n",
    "        dynamic_dfs: Optional[List[pd.DataFrame]] = None,\n",
    "        predict_fn: Optional[Callable] = None,\n",
    "        **predict_fn_kwargs,\n",
    "    ) -> dd.DataFrame:\n",
    "        \"\"\"Compute the predictions for the next `horizon` steps.\n",
    "        \n",
    "        `predict_fn(model, new_x, features_order, **predict_fn_kwargs)` is called in every timestep, where:\n",
    "        `model` is the trained model.\n",
    "        `new_x` is a dataframe with the same format as the input plus the computed features.\n",
    "        `features_order` is the list of column names that were used in the training step.\n",
    "        \"\"\"\n",
    "        return self.dts.predict(\n",
    "            self.model.model_, horizon, dynamic_dfs, predict_fn, **predict_fn_kwargs\n",
    "        )\n",
    "\n",
    "    def backtest(\n",
    "        self,\n",
    "        data: dd.DataFrame,\n",
    "        n_windows: int,\n",
    "        window_size: int,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "        dynamic_dfs: Optional[List[pd.DataFrame]] = None,\n",
    "        predict_fn: Callable = simple_predict,\n",
    "        **predict_fn_kwargs,\n",
    "    ) -> Generator[dd.DataFrame, None, None]:\n",
    "        \"\"\"Creates `n_windows` splits of `window_size` from `data`, trains the model\n",
    "        on the training set, predicts the window and merges the actuals and the predictions\n",
    "        in a dataframe.\n",
    "\n",
    "        Returns a generator to the dataframes containing the datestamps, actual values \n",
    "        and predictions.\"\"\"\n",
    "        for train, valid in backtest_splits(data, n_windows, window_size):\n",
    "            self.fit(train, static_features, dropna, keep_last_n)\n",
    "            y_pred = self.predict(\n",
    "                window_size, dynamic_dfs, predict_fn, **predict_fn_kwargs\n",
    "            )\n",
    "            y_valid = valid[['ds', 'y']]\n",
    "            result = y_valid.merge(y_pred, on=['unique_id', 'ds'], how='left')\n",
    "            yield result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2b21c7-23fa-4628-be0d-4da2448fa382",
   "metadata": {},
   "source": [
    "The `DistributedForecast` class is a high level abstraction that encapsulates all the steps in the pipeline (preprocessing, fitting the model and computing predictions) and applies them in a distributed way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3009c1fe-caba-4412-be41-69588a512bbf",
   "metadata": {},
   "source": [
    "## Example\n",
    "This shows an example with simulated data, for a real world example in a remote cluster you can check the [M5 distributed example](https://www.kaggle.com/lemuz90/m5-mlforecast-distributed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bc621d-43b8-4160-a694-08d3259e202d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from window_ops.expanding import expanding_mean\n",
    "from window_ops.rolling import rolling_mean\n",
    "\n",
    "from mlforecast.utils import generate_daily_series, generate_prices_for_series\n",
    "from mlforecast.distributed.models.lgb import LGBMForecast\n",
    "from mlforecast.distributed.models.xgb import XGBForecast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a9b623-af02-4acb-9d09-cda1debead4e",
   "metadata": {},
   "source": [
    "The different things that you need to use `DistributedForecast` (as opposed to `Forecast`) are:\n",
    "1. You need to set up a `dask.distributed.Client`. If this client is connected to a remote cluster then the process will run there.\n",
    "2. Your data needs to be a `dask.dataframe.DataFrame`.\n",
    "3. You need to use a model that implements distributed training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6479a531-60db-4c1e-ac28-189b8628d3ef",
   "metadata": {},
   "source": [
    "### Client setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb8d3ef-d1f3-48da-b2dc-0579ea058626",
   "metadata": {},
   "source": [
    "Here we define a client that connects to a `dask.distributed.LocalCluster`, however it could be any other kind of cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fea1b0f-f8d3-4be5-89fe-3dcbe00dff77",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(n_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62cfccf-5898-4046-a6c8-091fec26f0e4",
   "metadata": {},
   "source": [
    "### Data setup\n",
    "\n",
    "The data is given as a `dask.dataframe.DataFrame`, you need to make sure that each time serie is only in one partition and it is recommended that you have as many partitions as you have workers. If you have more partitions than workers make sure to set `num_threads=1` in `TimeSeries` to avoid having nested parallelism.\n",
    "\n",
    "The required input format is the same as for `Forecast`, except that it's a `dask.dataframe.DataFrame` instead of a `pandas.Dataframe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1388b1cf-78cb-4cf9-b146-6d2043d4d3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "series = generate_daily_series(100, n_static_features=2, equal_ends=True)\n",
    "for col in series.select_dtypes(include='category'):\n",
    "    series[col] = series[col].cat.codes  # encode categories for xgboost\n",
    "partitioned_series = dd.from_pandas(series, npartitions=10)\n",
    "partitioned_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b92d81-3a70-424d-81dc-8a995af1fb9c",
   "metadata": {},
   "source": [
    "### Model\n",
    "In order to perform distributed forecasting, we need to use a model that is able to train in a distributed way using `dask`. The current implementations are in `LGBMForecast` and `XGBForecast` which are just wrappers around `lightgbm.dask.DaskLGBMRegressor` and `xgboost.dask.DaskXGBRegressor` that add a `model_` property to get the trained model from them and send it to every worker to perform the predictions step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8012f8-3325-41a2-8fb0-7eaa07a24b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBForecast()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b6281f-1e65-433b-aad2-2d9895a64bcb",
   "metadata": {},
   "source": [
    "### TimeSeries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce63471-cd49-451f-9ce9-f7a484c61549",
   "metadata": {},
   "source": [
    "As in the local version (`Forecast`) a `TimeSeries` object is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc418ad-6b0a-4c33-b813-cc28f62fdec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = TimeSeries(\n",
    "    freq='D',\n",
    "    lags=[7],\n",
    "    lag_transforms={\n",
    "        1: [expanding_mean],\n",
    "        7: [(rolling_mean, 14)]\n",
    "    },\n",
    "    date_features=['dayofweek', 'month'],\n",
    "    num_threads=1,    \n",
    ")\n",
    "ts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc7459d-9b74-4a04-9389-93770c10ef62",
   "metadata": {},
   "source": [
    "Here where we say that:\n",
    "* Our series have daily frequency.\n",
    "* We want to use lag 7 as a feature\n",
    "* We want the lag transformations to be:\n",
    "   * expanding mean of the lag 1\n",
    "   * rolling mean of the lag 7 over a window of size 14\n",
    "* We want to use dayofweek and month as date features.\n",
    "* We want to perform the preprocessing and the forecasting steps using 1 thread, because we have 10 partitions and 2 workers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078e1983-7235-4930-9b44-36c76e35be69",
   "metadata": {},
   "source": [
    "### Training\n",
    "Once we have our model and time series we instantiate a `DistributedForecast` with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334d20c9-5ad7-42e8-a6b2-40d8005467bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst = DistributedForecast(model, ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5713b6a-1436-4554-878f-cc47bf87b2d0",
   "metadata": {},
   "source": [
    "From this point we have two options:\n",
    "\n",
    "1. Preprocess the data and fit our model using all of it.\n",
    "2. Preprocess the data and get it back as a dataframe to do some custom splitting or adding additional features. And then training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bff858-7e8a-49dd-963d-3acd6214e7a8",
   "metadata": {},
   "source": [
    "#### 1. Using all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2882214c-877f-4f5c-af51-3b509fafc3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(DistributedForecast.fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8545b806-d828-42dc-be00-d71dab8499b6",
   "metadata": {},
   "source": [
    "Calling `.fit` on our data computes the features independently for each partition and performs distributed training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8472ae1e-4b26-4414-8c58-9cc9a9e4d65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst.fit(partitioned_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f27b9a-ebc3-4673-8459-ad5843a43dac",
   "metadata": {},
   "source": [
    "#### 2. Preprocess and train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0b1833-a63a-45da-b3dc-8e1e5bfeb589",
   "metadata": {},
   "source": [
    "If we only want to perform the preprocessing step we call `.preprocess` on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5c34f3-faa9-44a8-b070-d28225d73cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(DistributedForecast.preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f5dae0-3313-403b-8eff-475f7736355e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_ddf = fcst.preprocess(partitioned_series)\n",
    "features_ddf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113b647b-7d05-42d7-b626-35c35fcf517b",
   "metadata": {},
   "source": [
    "This is useful if we want to inspect the data the model will be trained, adding additional features or performing some custom train-valid split. Here we perform a 80-20 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace9f0b3-63d6-457f-be72-39c4adc97309",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(0)\n",
    "\n",
    "def mask_as_series(df):\n",
    "    return pd.Series(rng.rand(df.shape[0]) < 0.8, index=df.index)\n",
    "\n",
    "train_mask = features_ddf.map_partitions(mask_as_series)\n",
    "train, valid = features_ddf[train_mask], features_ddf[~train_mask]\n",
    "X_train, y_train = train.drop(columns=['ds', 'y']), train.y\n",
    "X_valid, y_valid = valid.drop(columns=['ds', 'y']), valid.y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9205a1c-1ea5-4dfd-ba69-9756fb59b1ca",
   "metadata": {},
   "source": [
    "If we do this we must \"manually\" train our model calling `DistributedForecast.model.fit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4055884-0525-4c5f-9898-1afe85e07057",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst.model.fit(X_train, y_train,\n",
    "               eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "               verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79760ff9-aa75-454f-b5d9-39f495f692d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lab in fcst.model.evals_result_.keys():\n",
    "    plt.plot(fcst.model.evals_result_[lab]['rmse'], label=lab)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b961fb48-54de-48f1-a868-824bed51f596",
   "metadata": {},
   "source": [
    "### Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e95a58-0fee-41ee-900b-d2b61825042f",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(DistributedForecast.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed8e91f-122a-47b1-a1dd-ad811532b113",
   "metadata": {},
   "source": [
    "Once we have our fitted model we can compute the predictions for the next 7 timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8a439d-e0d0-4f2f-925c-fccd92361c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = fcst.predict(7)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7a8518-0e2a-4699-8da0-e63f29a2f7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "preds = preds.compute()\n",
    "preds2 = fcst.predict(7).compute()\n",
    "np.testing.assert_equal(preds['y_pred'].values, preds2['y_pred'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7d1a41-a56c-4113-a7d9-a79f643021ab",
   "metadata": {},
   "source": [
    "#### Dynamic features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e66766-a8e9-4af0-96e3-9f5b847c4f0b",
   "metadata": {},
   "source": [
    "By default the predict method repeats the static features and updates the transformations and the date features. If you have dynamic features like prices or a calendar with holidays you can pass them as a list to the `dynamic_dfs` argument of `Forecast.predict`, which will call `pd.DataFrame.merge` on each of them in order.\n",
    "\n",
    "Here's an example:\n",
    "\n",
    "Suppose that we have a `product_id` column and we have a catalog for prices based on that `product_id` and the date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee52992-0b18-43b0-833d-62f7246951c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_series = series.rename(columns={'static_1': 'product_id'})\n",
    "prices_catalog = generate_prices_for_series(dynamic_series)\n",
    "prices_catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7730055-6bab-4b52-bc83-e8456358309e",
   "metadata": {},
   "source": [
    "And you have already merged these prices into your series dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2e34f7-3ee0-48a1-93d1-4cc77f1afdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_series = partitioned_series.rename(columns={'static_1': 'product_id'})\n",
    "dynamic_series = dynamic_series.reset_index()\n",
    "series_with_prices = dynamic_series.merge(prices_catalog, how='left')\n",
    "series_with_prices = series_with_prices.set_index('unique_id', sorted=True)\n",
    "series_with_prices.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e348d3-a449-4823-8d5c-1c8b13f9b133",
   "metadata": {},
   "source": [
    "This dataframe will be passed to `DistributedForecast.fit` (or `DistributedForecast.preprocess`), however since the price is dynamic we have to tell that method that only `static_0` and `product_id` are static and we'll have to update `price` in every timestep, which basically involves merging the updated features with the prices catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33ea277-66d3-4bf8-aeaa-cc117cfa8e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst = DistributedForecast(XGBForecast(), ts)\n",
    "fcst.fit(series_with_prices, static_features=['static_0', 'product_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9bba71-8bb1-491b-bb0c-7ef8c6004f7b",
   "metadata": {},
   "source": [
    "So in order to update the price in each timestep we just call `DistributedForecast.predict` with our forecast horizon and pass the prices catalog as a dynamic dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3bc484-5e79-4139-acd5-c9ee3cf7270a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = fcst.predict(7, dynamic_dfs=[prices_catalog])\n",
    "preds.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b03634-d3e0-46e5-b05b-df7291fc8fdc",
   "metadata": {},
   "source": [
    "#### Custom predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa29c8f-65a6-4048-bf6a-0aa9874e2a2c",
   "metadata": {},
   "source": [
    "If you want to do something like scaling the predictions you can define a function and pass it to `DistributedForecast.predict` as described in <a href=\"/mlforecast/forecast.html#Custom-predictions\">Custom predictions</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec33846e-6d3c-4199-afe8-c5e064a53aac",
   "metadata": {},
   "source": [
    "### Backtesting\n",
    "\n",
    "If we would like to know how good our forecast will be for a specific model and set of features then we can perform backtesting. What backtesting does is take our data and split it in two parts, where the first part is used for training and the second one for validation. Since the data is time dependant we usually take the last *x* observations from our data as the validation set.\n",
    "\n",
    "This process is implemented in `DistributedForecast.backtest`, which takes our data and performs the process described above for `n_windows` times where each window is of size `window_size`. For example, if we have 100 samples and we want to perform 2 backtests each of size 14, the splits will be as follows:\n",
    "\n",
    "1. Train: 1 to 72. Validation: 73 to 86.\n",
    "2. Train: 1 to 86. Validation: 87 to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c97388-73f5-460d-9299-ab5cc6f31720",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(DistributedForecast.backtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fab05e3-3d72-4afa-a74e-6961a3d9186a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_windows = 2\n",
    "window_size = 14\n",
    "\n",
    "fcst = DistributedForecast(model, ts)\n",
    "backtest_results = fcst.backtest(partitioned_series, n_windows, window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff391af-3c38-45b1-8e16-34da01acac77",
   "metadata": {},
   "source": [
    "This returns a generator that yields the results of each window one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fded328-f113-48dc-80e6-913677eb692c",
   "metadata": {},
   "outputs": [],
   "source": [
    "window1_result = next(backtest_results)\n",
    "window1_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe1698f-6f21-4389-b00c-83713ce36935",
   "metadata": {},
   "outputs": [],
   "source": [
    "window2_result = next(backtest_results)\n",
    "results = pd.concat([window1_result.compute(), window2_result.compute()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fffbd79-f544-4cfc-8b62-ba009165c071",
   "metadata": {},
   "source": [
    "We can aggregate these by date to get a rough estimate of how our model is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a4a701-1779-4007-b9be-dbdee3454bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_results = results.groupby('ds').sum()\n",
    "agg_results.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc8651d-bf63-4a42-bd19-95e1271fed95",
   "metadata": {},
   "source": [
    "We can include some more context by using the values in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd06319-4031-422e-bd1e-93c56ca2bf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = series[series.ds < agg_results.index.min()]\n",
    "agg_history = history.groupby('ds')[['y']].sum().tail(50)\n",
    "agg_history.append(agg_results).plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da220459-5595-4bd4-901f-4c4f5ee8090c",
   "metadata": {},
   "source": [
    "Note that since the backtest results are returned as a generator we can also compute a single statistic on them and not keep the whole results in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54c787d-e68b-4a46-bf1b-8c865dc5cb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_from_dask_dataframe(ddf):\n",
    "    ddf['sq_err'] = (ddf['y'] - ddf['y_pred'])**2\n",
    "    mse = ddf['sq_err'].mean()\n",
    "    return mse.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d5d0e9-531f-45ad-aadb-250db1accf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst = DistributedForecast(XGBForecast(), ts)\n",
    "backtest_results = fcst.backtest(partitioned_series, n_windows, window_size)\n",
    "\n",
    "losses = [mse_from_dask_dataframe(res) for res in backtest_results]\n",
    "np.round(losses, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577baf77-56cc-4224-a1cc-722332aa7dd2",
   "metadata": {},
   "source": [
    "We can try `LGBMForecast` as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d5628d-0272-4aa0-b294-3b7514a09cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst = DistributedForecast(LGBMForecast(), ts)\n",
    "backtest_results = fcst.backtest(partitioned_series, n_windows, window_size)\n",
    "\n",
    "losses = [mse_from_dask_dataframe(res) for res in backtest_results]\n",
    "np.round(losses, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd8d63c-7ba8-48b1-ac2d-b4bdd51d61b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "model = XGBForecast()\n",
    "ts = TimeSeries(lags=[7, 14])\n",
    "fcst = DistributedForecast(model, ts)\n",
    "backtest_results = [res.compute() for res in fcst.backtest(partitioned_series, n_windows, window_size)]\n",
    "manual_results = []\n",
    "for train, valid in backtest_splits(partitioned_series, n_windows, window_size):\n",
    "    model = XGBForecast()\n",
    "    ts = TimeSeries(lags=[7, 14], num_threads=1)\n",
    "    fcst = DistributedForecast(model, ts).fit(train)\n",
    "    pred = fcst.predict(window_size).compute()\n",
    "    res = valid[['ds', 'y']].compute()\n",
    "    manual_results.append(res.merge(pred, on=['unique_id', 'ds'], how='left'))\n",
    "backtest_results = pd.concat(backtest_results)\n",
    "manual_results = pd.concat(manual_results)\n",
    "pd.testing.assert_frame_equal(backtest_results, manual_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0086c2a-c5b3-4a40-9901-b05139c3d6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
