{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf5b1d7-bc13-4ed3-af30-f15cacc861f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp distributed.forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503920c4-0e1f-4cef-a7ed-dea1005027ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2921fbf1-b7be-4f3d-b8e6-ce58b49fbd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "import warnings\n",
    "\n",
    "from fastcore.test import test_warns, test_eq, test_ne\n",
    "from nbdev import show_doc\n",
    "from sklearn import set_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bf1328-ca14-4169-8608-31dbb62107d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "warnings.simplefilter('ignore', FutureWarning)\n",
    "set_config(display='text')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc9a984-a442-41d2-8132-471d73e64d96",
   "metadata": {},
   "source": [
    "# Distributed Forecast\n",
    "\n",
    "> Distributed pipeline encapsulation\n",
    "\n",
    "**This interface is only tested on Linux**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8108c14-6d27-4dfd-b3e8-a2b315eb5f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import copy\n",
    "from collections import namedtuple\n",
    "from typing import Any, Callable, Iterable, List, Optional\n",
    "\n",
    "import cloudpickle\n",
    "import fsspec\n",
    "try:\n",
    "    import dask.dataframe as dd\n",
    "    DASK_INSTALLED = True\n",
    "except ModuleNotFoundError:\n",
    "    DASK_INSTALLED = False\n",
    "import fugue\n",
    "import fugue.api as fa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utilsforecast.processing as ufp\n",
    "try:\n",
    "    from pyspark.ml.feature import VectorAssembler\n",
    "    from pyspark.sql import DataFrame as SparkDataFrame\n",
    "    SPARK_INSTALLED = True\n",
    "except ModuleNotFoundError:\n",
    "    SPARK_INSTALLED = False\n",
    "try:\n",
    "    from lightgbm_ray import RayDMatrix\n",
    "    from ray.data import Dataset as RayDataset\n",
    "    RAY_INSTALLED = True\n",
    "except ModuleNotFoundError:\n",
    "    RAY_INSTALLED = False\n",
    "from sklearn.base import clone\n",
    "\n",
    "from mlforecast.core import (\n",
    "    DateFeature,\n",
    "    Freq,\n",
    "    LagTransforms,\n",
    "    Lags,\n",
    "    TargetTransform,\n",
    "    TimeSeries,\n",
    "    _build_transform_name,\n",
    "    _name_models,\n",
    ")\n",
    "from mlforecast.forecast import MLForecast\n",
    "from mlforecast.grouped_array import GroupedArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c505b5b8-2c00-456b-8d61-2301516bc347",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "WindowInfo = namedtuple('WindowInfo', ['n_windows', 'window_size', 'step_size', 'i_window', 'input_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227f19b4-4582-4c51-94cd-c7c220d35dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class DistributedMLForecast:\n",
    "    \"\"\"Multi backend distributed pipeline\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        models,\n",
    "        freq: Freq,\n",
    "        lags: Optional[Lags] = None,\n",
    "        lag_transforms: Optional[LagTransforms] = None,\n",
    "        date_features: Optional[Iterable[DateFeature]] = None,\n",
    "        num_threads: int = 1,\n",
    "        target_transforms: Optional[List[TargetTransform]] = None,        \n",
    "        engine = None,\n",
    "        num_partitions: Optional[int] = None,\n",
    "        lag_transforms_namer: Optional[Callable] = None,\n",
    "    ):\n",
    "        \"\"\"Create distributed forecast object\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        models : regressor or list of regressors\n",
    "            Models that will be trained and used to compute the forecasts.\n",
    "        freq : str or int, optional (default=None)\n",
    "            Pandas offset alias, e.g. 'D', 'W-THU' or integer denoting the frequency of the series.\n",
    "        lags : list of int, optional (default=None)\n",
    "            Lags of the target to use as features.\n",
    "        lag_transforms : dict of int to list of functions, optional (default=None)\n",
    "            Mapping of target lags to their transformations.\n",
    "        date_features : list of str or callable, optional (default=None)\n",
    "            Features computed from the dates. Can be pandas date attributes or functions that will take the dates as input.\n",
    "        num_threads : int (default=1)\n",
    "            Number of threads to use when computing the features.\n",
    "        target_transforms : list of transformers, optional(default=None)\n",
    "            Transformations that will be applied to the target before computing the features and restored after the forecasting step.            \n",
    "        engine : fugue execution engine, optional (default=None)\n",
    "            Dask Client, Spark Session, etc to use for the distributed computation.\n",
    "            If None will infer depending on the input type.\n",
    "        num_partitions: number of data partitions to use, optional (default=None)\n",
    "            If None, the default partitions provided by the AnyDataFrame used\n",
    "            by the `fit` and `cross_validation` methods will be used. If a Ray\n",
    "            Dataset is provided and `num_partitions` is None, the partitioning\n",
    "            will be done by the `id_col`.\n",
    "        lag_transforms_namer : callable, optional(default=None)\n",
    "            Function that takes a transformation (either function or class), a lag and extra arguments and produces a name\n",
    "        \"\"\"        \n",
    "        if not isinstance(models, dict) and not isinstance(models, list):\n",
    "            models = [models]\n",
    "        if isinstance(models, list):\n",
    "            model_names = _name_models([m.__class__.__name__ for m in models])\n",
    "            models_with_names = dict(zip(model_names, models))\n",
    "        else:\n",
    "            models_with_names = models\n",
    "        self.models = models_with_names\n",
    "        if lag_transforms_namer is None:\n",
    "            def name_without_dots(tfm, lag, *args):\n",
    "                name = _build_transform_name(tfm, lag, args)\n",
    "                return name.replace('.', '_')\n",
    "            lag_transforms_namer = name_without_dots\n",
    "        self._base_ts = TimeSeries(\n",
    "            freq=freq,\n",
    "            lags=lags,\n",
    "            lag_transforms=lag_transforms,\n",
    "            date_features=date_features,\n",
    "            num_threads=num_threads,\n",
    "            target_transforms=target_transforms,\n",
    "            lag_transforms_namer=lag_transforms_namer,\n",
    "        )\n",
    "        self.engine = engine\n",
    "        self.num_partitions = num_partitions\n",
    "        \n",
    "    def __repr__(self) -> str:\n",
    "        return (\n",
    "            f'{self.__class__.__name__}(models=[{\", \".join(self.models.keys())}], '\n",
    "            f\"freq={self._base_ts.freq}, \"\n",
    "            f\"lag_features={list(self._base_ts.transforms.keys())}, \"\n",
    "            f\"date_features={self._base_ts.date_features}, \"\n",
    "            f\"num_threads={self._base_ts.num_threads}, \"\n",
    "            f\"engine={self.engine})\"\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _preprocess_partition(\n",
    "        part: pd.DataFrame,\n",
    "        base_ts: TimeSeries,        \n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "        window_info: Optional[WindowInfo] = None,\n",
    "        fit_ts_only: bool = False,\n",
    "    ) -> List[List[Any]]:\n",
    "        ts = copy.deepcopy(base_ts)\n",
    "        if fit_ts_only:\n",
    "            ts._fit(\n",
    "                part,\n",
    "                id_col=id_col,\n",
    "                time_col=time_col,\n",
    "                target_col=target_col,\n",
    "                static_features=static_features,\n",
    "                keep_last_n=keep_last_n,                \n",
    "            )\n",
    "            core_tfms = ts._get_core_lag_tfms()\n",
    "            if core_tfms:\n",
    "                # populate the stats needed for the updates\n",
    "                ts._compute_transforms(core_tfms, updates_only=False)\n",
    "            ts.as_numpy = False\n",
    "            return [[cloudpickle.dumps(ts), cloudpickle.dumps(None), cloudpickle.dumps(None)]]        \n",
    "        if window_info is None:\n",
    "            train = part\n",
    "            valid = None\n",
    "        else:\n",
    "            max_dates = part.groupby(id_col, observed=True)[time_col].transform('max')\n",
    "            cutoffs, train_mask, valid_mask = ufp._single_split(\n",
    "                part,\n",
    "                i_window=window_info.i_window,\n",
    "                n_windows=window_info.n_windows,\n",
    "                h=window_info.window_size,\n",
    "                id_col=id_col,\n",
    "                time_col=time_col,\n",
    "                freq=ts.freq,\n",
    "                max_dates=max_dates,\n",
    "                step_size=window_info.step_size,\n",
    "                input_size=window_info.input_size,\n",
    "            )\n",
    "            train = part[train_mask]\n",
    "            valid_keep_cols = part.columns\n",
    "            if static_features is not None:\n",
    "                valid_keep_cols.drop(static_features)\n",
    "            valid = part.loc[valid_mask, valid_keep_cols].merge(cutoffs, on=id_col)\n",
    "        transformed = ts.fit_transform(\n",
    "            train,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            static_features=static_features,\n",
    "            dropna=dropna,\n",
    "            keep_last_n=keep_last_n,\n",
    "        )\n",
    "        return [[cloudpickle.dumps(ts), cloudpickle.dumps(transformed), cloudpickle.dumps(valid)]]\n",
    "\n",
    "    @staticmethod\n",
    "    def _retrieve_df(items: List[List[Any]]) -> Iterable[pd.DataFrame]:\n",
    "        for _, serialized_train, _ in items:\n",
    "            yield cloudpickle.loads(serialized_train)\n",
    "\n",
    "    def _preprocess_partitions(\n",
    "        self,\n",
    "        data: fugue.AnyDataFrame,\n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "        window_info: Optional[WindowInfo] = None,\n",
    "        fit_ts_only: bool = False,\n",
    "    ) -> List[Any]:\n",
    "        if self.num_partitions:\n",
    "            partition = dict(by=id_col, num=self.num_partitions, algo='coarse')\n",
    "        elif RAY_INSTALLED and isinstance(data, RayDataset): # num partitions is None but data is a RayDataset\n",
    "            # We need to add this because \n",
    "            # currently ray doesnt support partitioning a Dataset\n",
    "            # based on a column.\n",
    "            # If a Dataset is partitioned using `.repartition(num_partitions)`\n",
    "            # we will have akward results.\n",
    "            partition = dict(by=id_col)\n",
    "        else:\n",
    "            partition = None\n",
    "        res = fa.transform(\n",
    "            data,\n",
    "            DistributedMLForecast._preprocess_partition,\n",
    "            params={\n",
    "                'base_ts': self._base_ts,\n",
    "                'id_col': id_col,\n",
    "                'time_col': time_col,\n",
    "                'target_col': target_col,\n",
    "                'static_features': static_features,\n",
    "                'dropna': dropna,\n",
    "                'keep_last_n': keep_last_n,\n",
    "                'window_info': window_info,\n",
    "                'fit_ts_only': fit_ts_only,\n",
    "            },\n",
    "            schema='ts:binary,train:binary,valid:binary',\n",
    "            engine=self.engine,\n",
    "            as_fugue=True,\n",
    "            partition=partition,\n",
    "        )\n",
    "        # so that we don't need to recompute this on predict\n",
    "        return fa.persist(res, lazy=False, engine=self.engine, as_fugue=True)\n",
    "\n",
    "    def _preprocess(\n",
    "        self,\n",
    "        data: fugue.AnyDataFrame,\n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "        window_info: Optional[WindowInfo] = None,\n",
    "    ) -> fugue.AnyDataFrame:\n",
    "        self._base_ts.id_col = id_col\n",
    "        self._base_ts.time_col = time_col\n",
    "        self._base_ts.target_col = target_col\n",
    "        self._base_ts.static_features = static_features\n",
    "        self._base_ts.dropna = dropna\n",
    "        self._base_ts.keep_last_n = keep_last_n\n",
    "        self._partition_results = self._preprocess_partitions(\n",
    "            data=data,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            static_features=static_features,\n",
    "            dropna=dropna,\n",
    "            keep_last_n=keep_last_n,\n",
    "            window_info=window_info,\n",
    "        )\n",
    "        base_schema = fa.get_schema(data)\n",
    "        features_schema = {\n",
    "            f: 'double' for f in self._base_ts.features if f not in base_schema\n",
    "        }\n",
    "        res = fa.transform(\n",
    "            self._partition_results,\n",
    "            DistributedMLForecast._retrieve_df,\n",
    "            schema=base_schema + features_schema,\n",
    "            engine=self.engine,\n",
    "        )\n",
    "        return fa.get_native_as_df(res)\n",
    "\n",
    "    def preprocess(\n",
    "        self,\n",
    "        df: fugue.AnyDataFrame,\n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        target_col: str = 'y',\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "    ) -> fugue.AnyDataFrame:\n",
    "        \"\"\"Add the features to `data`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : dask, spark or ray DataFrame.\n",
    "            Series data in long format.\n",
    "        id_col : str (default='unique_id')\n",
    "            Column that identifies each serie.\n",
    "        time_col : str (default='ds')\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str (default='y')\n",
    "            Column that contains the target.\n",
    "        static_features : list of str, optional (default=None)\n",
    "            Names of the features that are static and will be repeated when forecasting.\n",
    "        dropna : bool (default=True)\n",
    "            Drop rows with missing values produced by the transformations.\n",
    "        keep_last_n : int, optional (default=None)\n",
    "            Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        result : same type as df\n",
    "            `df` with added features.\n",
    "        \"\"\"        \n",
    "        return self._preprocess(\n",
    "            df,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            static_features=static_features,\n",
    "            dropna=dropna,\n",
    "            keep_last_n=keep_last_n,\n",
    "        )\n",
    "    \n",
    "    def _fit(\n",
    "        self,\n",
    "        data: fugue.AnyDataFrame,\n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "        window_info: Optional[WindowInfo] = None,\n",
    "    ) -> 'DistributedMLForecast':\n",
    "        prep = self._preprocess(\n",
    "            data,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            static_features=static_features,\n",
    "            dropna=dropna,\n",
    "            keep_last_n=keep_last_n,\n",
    "            window_info=window_info,\n",
    "        )\n",
    "        features = [x for x in fa.get_column_names(prep) if x not in {id_col, time_col, target_col}]\n",
    "        self.models_ = {}\n",
    "        if SPARK_INSTALLED and isinstance(data, SparkDataFrame):\n",
    "            featurizer = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "            train_data = featurizer.transform(prep)[target_col, \"features\"]\n",
    "            for name, model in self.models.items():\n",
    "                trained_model = model._pre_fit(target_col).fit(train_data)\n",
    "                self.models_[name] = model.extract_local_model(trained_model)\n",
    "        elif DASK_INSTALLED and isinstance(data, dd.DataFrame):\n",
    "            X, y = prep[features], prep[target_col]\n",
    "            for name, model in self.models.items():\n",
    "                trained_model = clone(model).fit(X, y)\n",
    "                self.models_[name] = trained_model.model_\n",
    "        elif RAY_INSTALLED and isinstance(data, RayDataset):\n",
    "            X = RayDMatrix(\n",
    "                prep.select_columns(cols=features + [target_col]),\n",
    "                label=target_col,\n",
    "            )\n",
    "            for name, model in self.models.items():\n",
    "                trained_model = clone(model).fit(X, y=None)\n",
    "                self.models_[name] = trained_model.model_\n",
    "        else:\n",
    "            raise NotImplementedError('Only spark, dask, and ray engines are supported.')\n",
    "        return self\n",
    "    \n",
    "    def fit(\n",
    "        self,\n",
    "        df: fugue.AnyDataFrame,\n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        target_col: str = 'y',\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "    ) -> 'DistributedMLForecast':\n",
    "        \"\"\"Apply the feature engineering and train the models.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : dask, spark or ray DataFrame\n",
    "            Series data in long format.\n",
    "        id_col : str (default='unique_id')\n",
    "            Column that identifies each serie.\n",
    "        time_col : str (default='ds')\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str (default='y')\n",
    "            Column that contains the target.\n",
    "        static_features : list of str, optional (default=None)\n",
    "            Names of the features that are static and will be repeated when forecasting.\n",
    "        dropna : bool (default=True)\n",
    "            Drop rows with missing values produced by the transformations.\n",
    "        keep_last_n : int, optional (default=None)\n",
    "            Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : DistributedMLForecast\n",
    "            Forecast object with series values and trained models.\n",
    "        \"\"\"        \n",
    "        return self._fit(\n",
    "            df,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            static_features=static_features,\n",
    "            dropna=dropna,\n",
    "            keep_last_n=keep_last_n,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _predict(\n",
    "        items: List[List[Any]],\n",
    "        models,\n",
    "        horizon,\n",
    "        before_predict_callback=None,\n",
    "        after_predict_callback=None,\n",
    "        X_df=None,        \n",
    "    ) -> Iterable[pd.DataFrame]:\n",
    "        for serialized_ts, _, serialized_valid in items:\n",
    "            valid = cloudpickle.loads(serialized_valid)\n",
    "            ts = cloudpickle.loads(serialized_ts)\n",
    "            res = ts.predict(\n",
    "                models=models,\n",
    "                horizon=horizon,\n",
    "                before_predict_callback=before_predict_callback,\n",
    "                after_predict_callback=after_predict_callback,\n",
    "                X_df=X_df,\n",
    "            )\n",
    "            if valid is not None:\n",
    "                res = res.merge(valid, how='left')\n",
    "            yield res\n",
    "            \n",
    "    def _get_predict_schema(self) -> str:\n",
    "        model_names = self.models.keys()\n",
    "        models_schema = ','.join(f'{model_name}:double' for model_name in model_names)\n",
    "        schema = f'{self._base_ts.id_col}:string,{self._base_ts.time_col}:datetime,' + models_schema\n",
    "        return schema\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        h: int,\n",
    "        before_predict_callback: Optional[Callable] = None,\n",
    "        after_predict_callback: Optional[Callable] = None,\n",
    "        X_df: Optional[pd.DataFrame] = None,\n",
    "        new_df: Optional[fugue.AnyDataFrame] = None,\n",
    "    ) -> fugue.AnyDataFrame:\n",
    "        \"\"\"Compute the predictions for the next `horizon` steps.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        h : int\n",
    "            Forecast horizon.\n",
    "        before_predict_callback : callable, optional (default=None)\n",
    "            Function to call on the features before computing the predictions.\n",
    "                This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure.\n",
    "                The series identifier is on the index.\n",
    "        after_predict_callback : callable, optional (default=None)\n",
    "            Function to call on the predictions before updating the targets.\n",
    "                This function will take a pandas Series with the predictions and should return another one with the same structure.\n",
    "                The series identifier is on the index.\n",
    "        X_df : pandas DataFrame, optional (default=None)\n",
    "            Dataframe with the future exogenous features. Should have the id column and the time column.                \n",
    "        new_df : dask or spark DataFrame, optional (default=None)\n",
    "            Series data of new observations for which forecasts are to be generated.\n",
    "                This dataframe should have the same structure as the one used to fit the model, including any features and time series data.\n",
    "                If `new_df` is not None, the method will generate forecasts for the new observations.                \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        result : dask, spark or ray DataFrame\n",
    "            Predictions for each serie and timestep, with one column per model.\n",
    "        \"\"\"       \n",
    "        if new_df is not None:\n",
    "            partition_results = self._preprocess_partitions(\n",
    "                new_df,\n",
    "                id_col=self._base_ts.id_col,\n",
    "                time_col=self._base_ts.time_col,\n",
    "                target_col=self._base_ts.target_col,\n",
    "                static_features=self._base_ts.static_features,\n",
    "                dropna=self._base_ts.dropna,\n",
    "                keep_last_n=self._base_ts.keep_last_n,\n",
    "                fit_ts_only=True,\n",
    "            )\n",
    "        else:\n",
    "            partition_results = self._partition_results\n",
    "        schema = self._get_predict_schema()\n",
    "        if X_df is not None and not isinstance(X_df, pd.DataFrame):\n",
    "            raise ValueError('`X_df` should be a pandas DataFrame')\n",
    "        res = fa.transform(\n",
    "            partition_results,\n",
    "            DistributedMLForecast._predict,\n",
    "            params={\n",
    "                'models': self.models_,\n",
    "                'horizon': h,\n",
    "                'before_predict_callback': before_predict_callback,\n",
    "                'after_predict_callback': after_predict_callback,\n",
    "                'X_df': X_df,\n",
    "            },\n",
    "            schema=schema,\n",
    "            engine=self.engine,\n",
    "        )\n",
    "        return fa.get_native_as_df(res)\n",
    "\n",
    "    def cross_validation(\n",
    "        self,\n",
    "        df: fugue.AnyDataFrame,\n",
    "        n_windows: int,\n",
    "        h: int,\n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        target_col: str = 'y',\n",
    "        step_size: Optional[int] = None,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "        refit: bool = True,\n",
    "        before_predict_callback: Optional[Callable] = None,\n",
    "        after_predict_callback: Optional[Callable] = None,\n",
    "        input_size: Optional[int] = None,\n",
    "    ) -> fugue.AnyDataFrame:\n",
    "        \"\"\"Perform time series cross validation.\n",
    "        Creates `n_windows` splits where each window has `h` test periods,\n",
    "        trains the models, computes the predictions and merges the actuals.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : dask, spark or ray DataFrame\n",
    "            Series data in long format.\n",
    "        n_windows : int\n",
    "            Number of windows to evaluate.\n",
    "        h : int\n",
    "            Number of test periods in each window.\n",
    "        id_col : str (default='unique_id')\n",
    "            Column that identifies each serie.\n",
    "        time_col : str (default='ds')\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str (default='y')\n",
    "            Column that contains the target.        \n",
    "        step_size : int, optional (default=None)\n",
    "            Step size between each cross validation window. If None it will be equal to `h`.\n",
    "        static_features : list of str, optional (default=None)\n",
    "            Names of the features that are static and will be repeated when forecasting.\n",
    "        dropna : bool (default=True)\n",
    "            Drop rows with missing values produced by the transformations.\n",
    "        keep_last_n : int, optional (default=None)\n",
    "            Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it.\n",
    "        refit : bool (default=True)\n",
    "            Retrain model for each cross validation window.\n",
    "            If False, the models are trained at the beginning and then used to predict each window.            \n",
    "        before_predict_callback : callable, optional (default=None)\n",
    "            Function to call on the features before computing the predictions.\n",
    "                This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure.\n",
    "                The series identifier is on the index.\n",
    "        after_predict_callback : callable, optional (default=None)\n",
    "            Function to call on the predictions before updating the targets.\n",
    "                This function will take a pandas Series with the predictions and should return another one with the same structure.\n",
    "                The series identifier is on the index.\n",
    "        input_size : int, optional (default=None)\n",
    "            Maximum training samples per serie in each window. If None, will use an expanding window.                \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        result : dask, spark or ray DataFrame\n",
    "            Predictions for each window with the series id, timestamp, target value and predictions from each model.\n",
    "        \"\"\"            \n",
    "        self.cv_models_ = []\n",
    "        results = []\n",
    "        for i in range(n_windows):\n",
    "            window_info = WindowInfo(n_windows, h, step_size, i, input_size)            \n",
    "            if refit or i == 0:\n",
    "                self._fit(\n",
    "                    df,\n",
    "                    id_col=id_col,\n",
    "                    time_col=time_col,\n",
    "                    target_col=target_col,\n",
    "                    static_features=static_features,\n",
    "                    dropna=dropna,\n",
    "                    keep_last_n=keep_last_n,\n",
    "                    window_info=window_info,\n",
    "                )\n",
    "                self.cv_models_.append(self.models_)\n",
    "                partition_results = self._partition_results\n",
    "            elif not refit:\n",
    "                partition_results = self._preprocess_partitions(\n",
    "                    df,\n",
    "                    id_col=id_col,\n",
    "                    time_col=time_col,\n",
    "                    target_col=target_col,\n",
    "                    static_features=static_features,\n",
    "                    dropna=dropna,\n",
    "                    keep_last_n=keep_last_n,\n",
    "                    window_info=window_info,\n",
    "                )\n",
    "            schema = self._get_predict_schema() + f',cutoff:datetime,{self._base_ts.target_col}:double'\n",
    "            preds = fa.transform(\n",
    "                partition_results,\n",
    "                DistributedMLForecast._predict,\n",
    "                params={\n",
    "                    'models': self.models_,\n",
    "                    'horizon': h,\n",
    "                    'before_predict_callback': before_predict_callback,\n",
    "                    'after_predict_callback': after_predict_callback,\n",
    "                },\n",
    "                schema=schema,\n",
    "                engine=self.engine,\n",
    "            )\n",
    "            results.append(fa.get_native_as_df(preds))\n",
    "        return fa.union(*results)\n",
    "\n",
    "    @staticmethod\n",
    "    def _save_ts(items: List[List[Any]], path: str) -> Iterable[pd.DataFrame]:\n",
    "        for serialized_ts, _, _ in items:\n",
    "            ts = cloudpickle.loads(serialized_ts)\n",
    "            first_uid = ts.uids[0]\n",
    "            last_uid = ts.uids[-1]\n",
    "            ts.save(f'{path}/ts_{first_uid}-{last_uid}.pkl')\n",
    "            yield pd.DataFrame({'x': [True]})    \n",
    "\n",
    "    def save(self, path: str) -> None:\n",
    "        \"\"\"Save forecast object\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        path : str\n",
    "            Directory where artifacts will be stored.\"\"\"\n",
    "        dummy_df = fa.transform(\n",
    "            self._partition_results,\n",
    "            DistributedMLForecast._save_ts,\n",
    "            schema='x:bool',\n",
    "            params={'path': path},\n",
    "            engine=self.engine,\n",
    "        )\n",
    "        # trigger computation\n",
    "        dummy_df.as_pandas()\n",
    "        with fsspec.open(f'{path}/models.pkl', 'wb') as f:\n",
    "            cloudpickle.dump(self.models_, f)\n",
    "        self._base_ts.save(f'{path}/_base_ts.pkl')\n",
    "\n",
    "    @staticmethod\n",
    "    def _load_ts(paths: List[List[Any]], protocol: str) -> Iterable[pd.DataFrame]:\n",
    "        for [path] in paths:\n",
    "            ts = TimeSeries.load(path, protocol=protocol)\n",
    "            yield pd.DataFrame(\n",
    "                {\n",
    "                    'ts': [cloudpickle.dumps(ts)],\n",
    "                    'train': [cloudpickle.dumps(None)],\n",
    "                    'valid': [cloudpickle.dumps(None)],\n",
    "                }\n",
    "            )\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path: str, engine) -> 'DistributedMLForecast':\n",
    "        \"\"\"Load forecast object\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        path : str\n",
    "            Directory with saved artifacts.\n",
    "        engine : fugue execution engine\n",
    "            Dask Client, Spark Session, etc to use for the distributed computation.\n",
    "        \"\"\"\n",
    "        fs, _, paths = fsspec.get_fs_token_paths(f'{path}/ts*')\n",
    "        protocol = fs.protocol\n",
    "        if isinstance(protocol, tuple):\n",
    "            protocol = protocol[0]\n",
    "        names_df = pd.DataFrame({'path': paths})\n",
    "        partition_results = fa.transform(\n",
    "            names_df,\n",
    "            DistributedMLForecast._load_ts,\n",
    "            schema='ts:binary,train:binary,valid:binary',\n",
    "            partition='per_row',\n",
    "            params={'protocol': protocol},\n",
    "            engine=engine,\n",
    "            as_fugue=True,\n",
    "        )\n",
    "        with fsspec.open(f'{path}/models.pkl', 'rb') as f:\n",
    "            models = cloudpickle.load(f)\n",
    "        base_ts = TimeSeries.load(f'{path}/_base_ts.pkl')\n",
    "        fcst = DistributedMLForecast(models=models, freq=base_ts.freq)\n",
    "        fcst._base_ts = base_ts\n",
    "        fcst._partition_results = fa.persist(\n",
    "            partition_results, lazy=False, engine=engine, as_fugue=True\n",
    "        )\n",
    "        fcst.models_ = models        \n",
    "        fcst.engine = engine\n",
    "        fcst.num_partitions = len(paths)\n",
    "        return fcst\n",
    "\n",
    "    @staticmethod\n",
    "    def _update(items: List[List[Any]], new_df) -> Iterable[List[Any]]:\n",
    "        for serialized_ts, serialized_transformed, serialized_valid in items:\n",
    "            ts = cloudpickle.loads(serialized_ts)\n",
    "            partition_mask = ufp.is_in(new_df[ts.id_col], ts.uids)\n",
    "            partition_df = ufp.filter_with_mask(new_df, partition_mask)\n",
    "            ts.update(partition_df)\n",
    "            yield [cloudpickle.dumps(ts), serialized_transformed, serialized_valid]\n",
    "\n",
    "    def update(self, df: pd.DataFrame) -> None:\n",
    "        \"\"\"Update the values of the stored series.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas DataFrame\n",
    "            Dataframe with new observations.\"\"\"\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            raise ValueError(\"`df` must be a pandas DataFrame.\")\n",
    "        res = fa.transform(\n",
    "            self._partition_results,\n",
    "            DistributedMLForecast._update,\n",
    "            params={\"new_df\": df},\n",
    "            schema=\"ts:binary,train:binary,valid:binary\",\n",
    "            engine=self.engine,\n",
    "            as_fugue=True,\n",
    "        )\n",
    "        self._partition_results = fa.persist(res)\n",
    "\n",
    "    def to_local(self) -> MLForecast:\n",
    "        \"\"\"Convert this distributed forecast object into a local one\n",
    "        \n",
    "        This pulls all the data from the remote machines, so you have to be sure that \n",
    "        it fits in the scheduler/driver. If you're not sure use the save method instead.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        MLForecast\n",
    "            Local forecast object.\"\"\"\n",
    "        serialized_ts = fa.select_columns(\n",
    "            self._partition_results,\n",
    "            columns=['ts'],\n",
    "            as_fugue=True,\n",
    "        ).as_pandas()['ts'].tolist()\n",
    "        all_ts = [cloudpickle.loads(ts) for ts in serialized_ts]\n",
    "        # sort by ids (these should already be sorted within each partition)\n",
    "        all_ts = sorted(all_ts, key=lambda ts: ts.uids[0])\n",
    "        \n",
    "        # combine attributes. since fugue works on pandas these are all pandas.\n",
    "        # we're using utilsforecast here in case we add support for polars\n",
    "        def possibly_concat_indices(collection):\n",
    "            items_are_indices = isinstance(collection[0], pd.Index)\n",
    "            if items_are_indices:\n",
    "                collection = [pd.Series(item) for item in collection]\n",
    "            combined = ufp.vertical_concat(collection)\n",
    "            if items_are_indices:\n",
    "                combined = pd.Index(combined)\n",
    "            return combined\n",
    "\n",
    "        def combine_target_tfms(by_partition):\n",
    "            if by_partition[0] is None:\n",
    "                return None\n",
    "            by_transform = [\n",
    "                [part[i] for part in by_partition]\n",
    "                for i in range(len(by_partition[0]))\n",
    "            ]\n",
    "            out = []\n",
    "            for tfms in by_transform:\n",
    "                out.append(tfms[0].stack(tfms))\n",
    "            return out\n",
    "\n",
    "        def combine_core_lag_tfms(by_partition):\n",
    "            by_transform = [\n",
    "                (name, [part[name] for part in by_partition])\n",
    "                for name in by_partition[0].keys()\n",
    "            ]\n",
    "            out = {}            \n",
    "            for name, partition_tfms in by_transform:\n",
    "                out[name] = partition_tfms[0].stack(partition_tfms)\n",
    "            return out\n",
    "\n",
    "        uids = possibly_concat_indices([ts.uids for ts in all_ts])\n",
    "        last_dates = possibly_concat_indices([ts.last_dates for ts in all_ts])\n",
    "        statics = ufp.vertical_concat([ts.static_features_ for ts in all_ts])\n",
    "        combined_target_tfms = combine_target_tfms(\n",
    "            [ts.target_transforms for ts in all_ts]\n",
    "        )\n",
    "        combined_core_lag_tfms = combine_core_lag_tfms(\n",
    "            [ts._get_core_lag_tfms() for ts in all_ts]\n",
    "        )\n",
    "        sizes = np.hstack([np.diff(ts.ga.indptr) for ts in all_ts])        \n",
    "        data = np.hstack([ts.ga.data for ts in all_ts])\n",
    "        indptr = np.append(0, sizes).cumsum()\n",
    "        if isinstance(uids, pd.Index):\n",
    "            uids_idx = uids\n",
    "        else:\n",
    "            # uids is polars series\n",
    "            uids_idx = pd.Index(uids)\n",
    "        if not uids_idx.is_monotonic_increasing:\n",
    "            # this seems to happen only with ray\n",
    "            # we have to sort all data related to the series\n",
    "            sort_idxs = uids_idx.argsort()\n",
    "            uids = uids[sort_idxs]\n",
    "            last_dates = last_dates[sort_idxs]\n",
    "            statics = ufp.take_rows(statics, sort_idxs)\n",
    "            statics = ufp.drop_index_if_pandas(statics)\n",
    "            for tfm in combined_core_lag_tfms.values():\n",
    "                tfm._core_tfm = tfm._core_tfm.take(sort_idxs)\n",
    "            if combined_target_tfms is not None:\n",
    "                combined_target_tfms = [\n",
    "                    tfm.take(sort_idxs) for tfm in combined_target_tfms\n",
    "                ]\n",
    "            old_data = data.copy()\n",
    "            old_indptr = indptr.copy()\n",
    "            indptr = np.append(0, sizes[sort_idxs]).cumsum()\n",
    "            # this loop takes 500ms for 100,000 series of sizes between 500 and 2,000\n",
    "            # so it may not be that much of a bottleneck, but try to implement in core\n",
    "            for i, sort_idx in enumerate(sort_idxs):\n",
    "                old_slice = slice(old_indptr[sort_idx], old_indptr[sort_idx + 1])\n",
    "                new_slice = slice(indptr[i], indptr[i + 1])\n",
    "                data[new_slice] = old_data[old_slice]\n",
    "        ga = GroupedArray(data, indptr)\n",
    "\n",
    "        # all other attributes should be the same, so we just override the first serie\n",
    "        ts = all_ts[0]\n",
    "        ts.uids = uids\n",
    "        ts.last_dates = last_dates\n",
    "        ts.ga = ga\n",
    "        ts.static_features_ = statics\n",
    "        ts.transforms.update(combined_core_lag_tfms)\n",
    "        ts.target_transforms = combined_target_tfms\n",
    "        fcst = MLForecast(models=self.models_, freq=ts.freq)\n",
    "        fcst.ts = ts\n",
    "        fcst.models_ = self.models_\n",
    "        return fcst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18b1388-3c79-4d1c-98cf-d748d64119e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/mlforecast/blob/main/mlforecast/distributed/forecast.py#L61){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DistributedMLForecast\n",
       "\n",
       ">      DistributedMLForecast (models, freq:Union[int,str],\n",
       ">                             lags:Optional[Iterable[int]]=None, lag_transforms:\n",
       ">                             Optional[Dict[int,List[Union[Callable,Tuple[Callab\n",
       ">                             le,Any]]]]]=None, date_features:Optional[Iterable[\n",
       ">                             Union[str,Callable]]]=None, num_threads:int=1, tar\n",
       ">                             get_transforms:Optional[List[Union[mlforecast.targ\n",
       ">                             et_transforms.BaseTargetTransform,mlforecast.targe\n",
       ">                             t_transforms._BaseGroupedArrayTargetTransform]]]=N\n",
       ">                             one, engine=None,\n",
       ">                             num_partitions:Optional[int]=None,\n",
       ">                             lag_transforms_namer:Optional[Callable]=None)\n",
       "\n",
       "*Multi backend distributed pipeline*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/mlforecast/blob/main/mlforecast/distributed/forecast.py#L61){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DistributedMLForecast\n",
       "\n",
       ">      DistributedMLForecast (models, freq:Union[int,str],\n",
       ">                             lags:Optional[Iterable[int]]=None, lag_transforms:\n",
       ">                             Optional[Dict[int,List[Union[Callable,Tuple[Callab\n",
       ">                             le,Any]]]]]=None, date_features:Optional[Iterable[\n",
       ">                             Union[str,Callable]]]=None, num_threads:int=1, tar\n",
       ">                             get_transforms:Optional[List[Union[mlforecast.targ\n",
       ">                             et_transforms.BaseTargetTransform,mlforecast.targe\n",
       ">                             t_transforms._BaseGroupedArrayTargetTransform]]]=N\n",
       ">                             one, engine=None,\n",
       ">                             num_partitions:Optional[int]=None,\n",
       ">                             lag_transforms_namer:Optional[Callable]=None)\n",
       "\n",
       "*Multi backend distributed pipeline*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DistributedMLForecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3201af6c-4915-4447-8bca-01fe9a1f7cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/mlforecast/blob/main/mlforecast/distributed/forecast.py#L402){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DistributedMLForecast.fit\n",
       "\n",
       ">      DistributedMLForecast.fit (df:~AnyDataFrame, id_col:str='unique_id',\n",
       ">                                 time_col:str='ds', target_col:str='y',\n",
       ">                                 static_features:Optional[List[str]]=None,\n",
       ">                                 dropna:bool=True,\n",
       ">                                 keep_last_n:Optional[int]=None)\n",
       "\n",
       "*Apply the feature engineering and train the models.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | AnyDataFrame |  | Series data in long format. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| time_col | str | ds | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str | y | Column that contains the target. |\n",
       "| static_features | Optional | None | Names of the features that are static and will be repeated when forecasting. |\n",
       "| dropna | bool | True | Drop rows with missing values produced by the transformations. |\n",
       "| keep_last_n | Optional | None | Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it. |\n",
       "| **Returns** | **DistributedMLForecast** |  | **Forecast object with series values and trained models.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/mlforecast/blob/main/mlforecast/distributed/forecast.py#L402){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DistributedMLForecast.fit\n",
       "\n",
       ">      DistributedMLForecast.fit (df:~AnyDataFrame, id_col:str='unique_id',\n",
       ">                                 time_col:str='ds', target_col:str='y',\n",
       ">                                 static_features:Optional[List[str]]=None,\n",
       ">                                 dropna:bool=True,\n",
       ">                                 keep_last_n:Optional[int]=None)\n",
       "\n",
       "*Apply the feature engineering and train the models.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | AnyDataFrame |  | Series data in long format. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| time_col | str | ds | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str | y | Column that contains the target. |\n",
       "| static_features | Optional | None | Names of the features that are static and will be repeated when forecasting. |\n",
       "| dropna | bool | True | Drop rows with missing values produced by the transformations. |\n",
       "| keep_last_n | Optional | None | Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it. |\n",
       "| **Returns** | **DistributedMLForecast** |  | **Forecast object with series values and trained models.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DistributedMLForecast.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc8f139-ae37-4927-8449-ed949dc254db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/mlforecast/blob/main/mlforecast/distributed/forecast.py#L478){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DistributedMLForecast.predict\n",
       "\n",
       ">      DistributedMLForecast.predict (h:int,\n",
       ">                                     before_predict_callback:Optional[Callable]\n",
       ">                                     =None, after_predict_callback:Optional[Cal\n",
       ">                                     lable]=None, X_df:Optional[pandas.core.fra\n",
       ">                                     me.DataFrame]=None,\n",
       ">                                     new_df:Optional[~AnyDataFrame]=None)\n",
       "\n",
       "*Compute the predictions for the next `horizon` steps.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| h | int |  | Forecast horizon. |\n",
       "| before_predict_callback | Optional | None | Function to call on the features before computing the predictions.<br>    This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure.<br>    The series identifier is on the index. |\n",
       "| after_predict_callback | Optional | None | Function to call on the predictions before updating the targets.<br>    This function will take a pandas Series with the predictions and should return another one with the same structure.<br>    The series identifier is on the index. |\n",
       "| X_df | Optional | None | Dataframe with the future exogenous features. Should have the id column and the time column.                 |\n",
       "| new_df | Optional | None | Series data of new observations for which forecasts are to be generated.<br>    This dataframe should have the same structure as the one used to fit the model, including any features and time series data.<br>    If `new_df` is not None, the method will generate forecasts for the new observations.                 |\n",
       "| **Returns** | **AnyDataFrame** |  | **Predictions for each serie and timestep, with one column per model.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/mlforecast/blob/main/mlforecast/distributed/forecast.py#L478){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DistributedMLForecast.predict\n",
       "\n",
       ">      DistributedMLForecast.predict (h:int,\n",
       ">                                     before_predict_callback:Optional[Callable]\n",
       ">                                     =None, after_predict_callback:Optional[Cal\n",
       ">                                     lable]=None, X_df:Optional[pandas.core.fra\n",
       ">                                     me.DataFrame]=None,\n",
       ">                                     new_df:Optional[~AnyDataFrame]=None)\n",
       "\n",
       "*Compute the predictions for the next `horizon` steps.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| h | int |  | Forecast horizon. |\n",
       "| before_predict_callback | Optional | None | Function to call on the features before computing the predictions.<br>    This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure.<br>    The series identifier is on the index. |\n",
       "| after_predict_callback | Optional | None | Function to call on the predictions before updating the targets.<br>    This function will take a pandas Series with the predictions and should return another one with the same structure.<br>    The series identifier is on the index. |\n",
       "| X_df | Optional | None | Dataframe with the future exogenous features. Should have the id column and the time column.                 |\n",
       "| new_df | Optional | None | Series data of new observations for which forecasts are to be generated.<br>    This dataframe should have the same structure as the one used to fit the model, including any features and time series data.<br>    If `new_df` is not None, the method will generate forecasts for the new observations.                 |\n",
       "| **Returns** | **AnyDataFrame** |  | **Predictions for each serie and timestep, with one column per model.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DistributedMLForecast.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3326fd-638d-43ba-93d7-3b3d289c68ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/mlforecast/blob/main/mlforecast/distributed/forecast.py#L661){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DistributedMLForecast.save\n",
       "\n",
       ">      DistributedMLForecast.save (path:str)\n",
       "\n",
       "*Save forecast object*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| path | str | Directory where artifacts will be stored. |\n",
       "| **Returns** | **None** |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/mlforecast/blob/main/mlforecast/distributed/forecast.py#L661){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DistributedMLForecast.save\n",
       "\n",
       ">      DistributedMLForecast.save (path:str)\n",
       "\n",
       "*Save forecast object*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| path | str | Directory where artifacts will be stored. |\n",
       "| **Returns** | **None** |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DistributedMLForecast.save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309d0b0f-f2ce-4afa-b192-f7bcd6f5044f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/mlforecast/blob/main/mlforecast/distributed/forecast.py#L694){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DistributedMLForecast.load\n",
       "\n",
       ">      DistributedMLForecast.load (path:str, engine)\n",
       "\n",
       "*Load forecast object*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| path | str | Directory with saved artifacts. |\n",
       "| engine | fugue execution engine | Dask Client, Spark Session, etc to use for the distributed computation. |\n",
       "| **Returns** | **DistributedMLForecast** |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/mlforecast/blob/main/mlforecast/distributed/forecast.py#L694){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DistributedMLForecast.load\n",
       "\n",
       ">      DistributedMLForecast.load (path:str, engine)\n",
       "\n",
       "*Load forecast object*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| path | str | Directory with saved artifacts. |\n",
       "| engine | fugue execution engine | Dask Client, Spark Session, etc to use for the distributed computation. |\n",
       "| **Returns** | **DistributedMLForecast** |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DistributedMLForecast.load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee581ec-35a7-4443-bb08-1edf15f6c783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/mlforecast/blob/main/mlforecast/distributed/forecast.py#L740){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DistributedMLForecast.update\n",
       "\n",
       ">      DistributedMLForecast.update (df:pandas.core.frame.DataFrame)\n",
       "\n",
       "*Update the values of the stored series.*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| df | DataFrame | Dataframe with new observations. |\n",
       "| **Returns** | **None** |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/mlforecast/blob/main/mlforecast/distributed/forecast.py#L740){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DistributedMLForecast.update\n",
       "\n",
       ">      DistributedMLForecast.update (df:pandas.core.frame.DataFrame)\n",
       "\n",
       "*Update the values of the stored series.*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| df | DataFrame | Dataframe with new observations. |\n",
       "| **Returns** | **None** |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DistributedMLForecast.update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4916ff-ad5c-4fb5-a8c4-e7befb52fa67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/mlforecast/blob/main/mlforecast/distributed/forecast.py#L759){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DistributedMLForecast.to_local\n",
       "\n",
       ">      DistributedMLForecast.to_local ()\n",
       "\n",
       "*Convert this distributed forecast object into a local one\n",
       "\n",
       "This pulls all the data from the remote machines, so you have to be sure that \n",
       "it fits in the scheduler/driver. If you're not sure use the save method instead.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/mlforecast/blob/main/mlforecast/distributed/forecast.py#L759){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DistributedMLForecast.to_local\n",
       "\n",
       ">      DistributedMLForecast.to_local ()\n",
       "\n",
       "*Convert this distributed forecast object into a local one\n",
       "\n",
       "This pulls all the data from the remote machines, so you have to be sure that \n",
       "it fits in the scheduler/driver. If you're not sure use the save method instead.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DistributedMLForecast.to_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d825fee7-6b8f-4613-93b0-e5769a7abe39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/mlforecast/blob/main/mlforecast/distributed/forecast.py#L306){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DistributedMLForecast.preprocess\n",
       "\n",
       ">      DistributedMLForecast.preprocess (df:~AnyDataFrame,\n",
       ">                                        id_col:str='unique_id',\n",
       ">                                        time_col:str='ds', target_col:str='y', \n",
       ">                                        static_features:Optional[List[str]]=Non\n",
       ">                                        e, dropna:bool=True,\n",
       ">                                        keep_last_n:Optional[int]=None)\n",
       "\n",
       "*Add the features to `data`.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | AnyDataFrame |  | Series data in long format. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| time_col | str | ds | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str | y | Column that contains the target. |\n",
       "| static_features | Optional | None | Names of the features that are static and will be repeated when forecasting. |\n",
       "| dropna | bool | True | Drop rows with missing values produced by the transformations. |\n",
       "| keep_last_n | Optional | None | Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it. |\n",
       "| **Returns** | **AnyDataFrame** |  | **`df` with added features.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/mlforecast/blob/main/mlforecast/distributed/forecast.py#L306){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DistributedMLForecast.preprocess\n",
       "\n",
       ">      DistributedMLForecast.preprocess (df:~AnyDataFrame,\n",
       ">                                        id_col:str='unique_id',\n",
       ">                                        time_col:str='ds', target_col:str='y', \n",
       ">                                        static_features:Optional[List[str]]=Non\n",
       ">                                        e, dropna:bool=True,\n",
       ">                                        keep_last_n:Optional[int]=None)\n",
       "\n",
       "*Add the features to `data`.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | AnyDataFrame |  | Series data in long format. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| time_col | str | ds | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str | y | Column that contains the target. |\n",
       "| static_features | Optional | None | Names of the features that are static and will be repeated when forecasting. |\n",
       "| dropna | bool | True | Drop rows with missing values produced by the transformations. |\n",
       "| keep_last_n | Optional | None | Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it. |\n",
       "| **Returns** | **AnyDataFrame** |  | **`df` with added features.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DistributedMLForecast.preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3faef3-89d4-45dd-bcda-78aba4414a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/mlforecast/blob/main/mlforecast/distributed/forecast.py#L543){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DistributedMLForecast.cross_validation\n",
       "\n",
       ">      DistributedMLForecast.cross_validation (df:~AnyDataFrame, n_windows:int,\n",
       ">                                              h:int, id_col:str='unique_id',\n",
       ">                                              time_col:str='ds',\n",
       ">                                              target_col:str='y',\n",
       ">                                              step_size:Optional[int]=None, sta\n",
       ">                                              tic_features:Optional[List[str]]=\n",
       ">                                              None, dropna:bool=True,\n",
       ">                                              keep_last_n:Optional[int]=None,\n",
       ">                                              refit:bool=True, before_predict_c\n",
       ">                                              allback:Optional[Callable]=None, \n",
       ">                                              after_predict_callback:Optional[C\n",
       ">                                              allable]=None,\n",
       ">                                              input_size:Optional[int]=None)\n",
       "\n",
       "*Perform time series cross validation.\n",
       "Creates `n_windows` splits where each window has `h` test periods,\n",
       "trains the models, computes the predictions and merges the actuals.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | AnyDataFrame |  | Series data in long format. |\n",
       "| n_windows | int |  | Number of windows to evaluate. |\n",
       "| h | int |  | Number of test periods in each window. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| time_col | str | ds | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str | y | Column that contains the target.         |\n",
       "| step_size | Optional | None | Step size between each cross validation window. If None it will be equal to `h`. |\n",
       "| static_features | Optional | None | Names of the features that are static and will be repeated when forecasting. |\n",
       "| dropna | bool | True | Drop rows with missing values produced by the transformations. |\n",
       "| keep_last_n | Optional | None | Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it. |\n",
       "| refit | bool | True | Retrain model for each cross validation window.<br>If False, the models are trained at the beginning and then used to predict each window.             |\n",
       "| before_predict_callback | Optional | None | Function to call on the features before computing the predictions.<br>    This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure.<br>    The series identifier is on the index. |\n",
       "| after_predict_callback | Optional | None | Function to call on the predictions before updating the targets.<br>    This function will take a pandas Series with the predictions and should return another one with the same structure.<br>    The series identifier is on the index. |\n",
       "| input_size | Optional | None | Maximum training samples per serie in each window. If None, will use an expanding window.                 |\n",
       "| **Returns** | **AnyDataFrame** |  | **Predictions for each window with the series id, timestamp, target value and predictions from each model.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/mlforecast/blob/main/mlforecast/distributed/forecast.py#L543){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DistributedMLForecast.cross_validation\n",
       "\n",
       ">      DistributedMLForecast.cross_validation (df:~AnyDataFrame, n_windows:int,\n",
       ">                                              h:int, id_col:str='unique_id',\n",
       ">                                              time_col:str='ds',\n",
       ">                                              target_col:str='y',\n",
       ">                                              step_size:Optional[int]=None, sta\n",
       ">                                              tic_features:Optional[List[str]]=\n",
       ">                                              None, dropna:bool=True,\n",
       ">                                              keep_last_n:Optional[int]=None,\n",
       ">                                              refit:bool=True, before_predict_c\n",
       ">                                              allback:Optional[Callable]=None, \n",
       ">                                              after_predict_callback:Optional[C\n",
       ">                                              allable]=None,\n",
       ">                                              input_size:Optional[int]=None)\n",
       "\n",
       "*Perform time series cross validation.\n",
       "Creates `n_windows` splits where each window has `h` test periods,\n",
       "trains the models, computes the predictions and merges the actuals.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | AnyDataFrame |  | Series data in long format. |\n",
       "| n_windows | int |  | Number of windows to evaluate. |\n",
       "| h | int |  | Number of test periods in each window. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| time_col | str | ds | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str | y | Column that contains the target.         |\n",
       "| step_size | Optional | None | Step size between each cross validation window. If None it will be equal to `h`. |\n",
       "| static_features | Optional | None | Names of the features that are static and will be repeated when forecasting. |\n",
       "| dropna | bool | True | Drop rows with missing values produced by the transformations. |\n",
       "| keep_last_n | Optional | None | Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it. |\n",
       "| refit | bool | True | Retrain model for each cross validation window.<br>If False, the models are trained at the beginning and then used to predict each window.             |\n",
       "| before_predict_callback | Optional | None | Function to call on the features before computing the predictions.<br>    This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure.<br>    The series identifier is on the index. |\n",
       "| after_predict_callback | Optional | None | Function to call on the predictions before updating the targets.<br>    This function will take a pandas Series with the predictions and should return another one with the same structure.<br>    The series identifier is on the index. |\n",
       "| input_size | Optional | None | Maximum training samples per serie in each window. If None, will use an expanding window.                 |\n",
       "| **Returns** | **AnyDataFrame** |  | **Predictions for each window with the series id, timestamp, target value and predictions from each model.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DistributedMLForecast.cross_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4cba4c-44e3-43c0-ab8a-c472fd697f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "\n",
    "from mlforecast.distributed.models.dask.lgb import DaskLGBMForecast\n",
    "from mlforecast.lag_transforms import ExpandingMean, RollingMean\n",
    "from mlforecast.utils import generate_daily_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de387b23-f87b-40cd-b37c-9964026bd8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "series = generate_daily_series(100, equal_ends=True, min_length=500, max_length=1_000)\n",
    "npartitions = 4\n",
    "partitioned_series = dd.from_pandas(series.set_index('unique_id'), npartitions=npartitions)  # make sure we split by the id_col\n",
    "partitioned_series = partitioned_series.map_partitions(lambda df: df.reset_index())\n",
    "partitioned_series['unique_id'] = partitioned_series['unique_id'].astype(str)  # can't handle categoricals atm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0245aedc-e5fb-4d11-86f6-fef7c2437d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test existing features provide the same result\n",
    "fcst = DistributedMLForecast(\n",
    "    models=[DaskLGBMForecast(verbosity=-1, random_state=0)],\n",
    "    freq='D',\n",
    "    lags=[1,2,3,4,5,6,7],    \n",
    "    lag_transforms={\n",
    "        1: [RollingMean(7),RollingMean(30), ExpandingMean()],\n",
    "        335: [RollingMean(30)],\n",
    "    },\n",
    "    date_features=['dayofweek']    \n",
    ")\n",
    "# df with features\n",
    "training_df_featured = fcst.preprocess(partitioned_series, static_features=[], dropna=False)\n",
    "fcst.fit(training_df_featured, static_features=[], dropna=False)\n",
    "preds1 = fcst.predict(10).compute()\n",
    "\n",
    "# df without features\n",
    "fcst.preprocess(partitioned_series, static_features=[], dropna=False)\n",
    "preds2 = fcst.predict(10).compute()\n",
    "pd.testing.assert_frame_equal(preds1, preds2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
