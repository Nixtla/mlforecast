{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71e077d-b6cc-4913-b206-77b652053251",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp distributed.forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a7d947-ce42-40ec-8b54-623ec2189dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c82054e-db3d-43ca-a4fe-c397e351ad1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from nbdev import show_doc\n",
    "from sklearn import set_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933ec908-b746-4b80-8da3-fcb0039145d4",
   "metadata": {},
   "source": [
    "# Distributed forecast\n",
    "\n",
    "> Distributed pipeline encapsulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6d15cd-5734-4585-aaca-de860e07f9ab",
   "metadata": {},
   "source": [
    "**This interface is only tested on Linux**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3e3690-1951-487a-8c66-ba1b2cc01756",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import warnings\n",
    "from typing import Callable, Dict, List, Optional, Tuple\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "from dask.distributed import Client, default_client\n",
    "from sklearn.base import clone\n",
    "\n",
    "from mlforecast import Forecast, TimeSeries\n",
    "from mlforecast.distributed.core import DistributedTimeSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55dee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "set_config(display='text')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434b7a1a-716f-433c-ab86-8ec33f0f185e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class DistributedForecast:\n",
    "    \"\"\"Distributed pipeline encapsulation.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        models,  # model or list of mlforecast.distributed.models\n",
    "        freq: Optional[str] = None,  # pandas offset alias, e.g. D, W, M. Don't set if you're using integer times.\n",
    "        lags: List[int] = [],  # list of lags to use as features\n",
    "        lag_transforms: Dict[int, List[Tuple]] = {},  # list of transformations to apply to each lag\n",
    "        date_features: List[str] = [],  # list of names of pandas date attributes to use as features, e.g. dayofweek\n",
    "        num_threads: int = 1,  # number of threads to use when computing lag features\n",
    "        client: Optional[Client] = None  # dask client to use for computations\n",
    "    ):\n",
    "        if not isinstance(models, list):\n",
    "            models = [clone(models)]\n",
    "        self.models = [clone(m) for m in models]\n",
    "        self.client = client or default_client()\n",
    "        self.dts = DistributedTimeSeries(\n",
    "            TimeSeries(freq, lags, lag_transforms, date_features, num_threads),\n",
    "            self.client,\n",
    "        )\n",
    "        \n",
    "    def __repr__(self) -> str:\n",
    "        return (\n",
    "            f'DistributedForecast(models=[{\", \".join(m.__class__.__name__ for m in self.models)}], '\n",
    "            f'freq={self.freq}, '\n",
    "            f'lag_features={list(self.dts._base_ts.transforms.keys())}, '\n",
    "            f'date_features={self.dts._base_ts.date_features}, '\n",
    "            f'num_threads={self.dts._base_ts.num_threads}, '\n",
    "            f'client={self.client})'\n",
    "        )\n",
    "    \n",
    "    @property\n",
    "    def freq(self):\n",
    "        return self.dts._base_ts.freq\n",
    "\n",
    "    def preprocess(\n",
    "        self,\n",
    "        data: dd.DataFrame,\n",
    "        id_col: str = 'index',  # column that identifies each serie, it's recommended to have this as the index.\n",
    "        time_col: str = 'ds',  # column with the timestamps\n",
    "        target_col: str = 'y',  # column with the series values        \n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,        \n",
    "    ) -> dd.DataFrame:\n",
    "        \"\"\"Computes the transformations on each partition of `data` and\n",
    "        saves the required information for the forecasting step.\n",
    "        Returns a dask dataframe with the computed features.\"\"\"\n",
    "        if id_col in data:\n",
    "            warnings.warn('It is recommended to have id_col as the index, since setting the index is a slow operation.')\n",
    "            data = data.set_index(id_col)\n",
    "            id_col = 'index'\n",
    "        return self.dts.fit_transform(data, id_col, time_col, target_col, static_features, dropna, keep_last_n)\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        data: dd.DataFrame,\n",
    "        id_col: str = 'index',  # column that identifies each serie, it's recommended to have this as the index.\n",
    "        time_col: str = 'ds',  # column with the timestamps\n",
    "        target_col: str = 'y',  # column with the series values\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None) -> 'DistributedForecast':\n",
    "        \"\"\"Perform the preprocessing and fit the model.\"\"\"\n",
    "        train_ddf = self.preprocess(data, id_col, time_col, target_col, static_features, dropna, keep_last_n)\n",
    "        X, y = train_ddf.drop(columns=[time_col, target_col]), train_ddf[target_col]\n",
    "        self.models_ = []\n",
    "        for i, model in enumerate(self.models):\n",
    "            model = clone(model)\n",
    "            model.client = self.client\n",
    "            self.models_.append(model.fit(X, y))\n",
    "        return self\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        horizon: int,\n",
    "        dynamic_dfs: Optional[List[pd.DataFrame]] = None,\n",
    "        predict_fn: Optional[Callable] = None,\n",
    "        **predict_fn_kwargs,\n",
    "    ) -> dd.DataFrame:\n",
    "        return self.dts.predict(\n",
    "            [m.model_ for m in self.models_], horizon, dynamic_dfs, predict_fn, **predict_fn_kwargs\n",
    "        )\n",
    "    \n",
    "    predict.__doc__ = Forecast.predict.__doc__\n",
    "    \n",
    "    def cross_validation(\n",
    "        self,\n",
    "        data: dd.DataFrame,  # time series\n",
    "        n_windows: int,  # number of windows to evaluate\n",
    "        window_size: int,  # test size in each window\n",
    "        id_col: str = 'index',  # column that identifies each serie, can also be the index.\n",
    "        time_col: str = 'ds',  # column with the timestamps\n",
    "        target_col: str = 'y',  # column with the series values\n",
    "        static_features: Optional[List[str]] = None,  # column names of the features that don't change in time\n",
    "        dropna: bool = True,  # drop rows with missing values created by lags\n",
    "        keep_last_n: Optional[int] = None,  # keep only this many observations of each serie for computing the updates\n",
    "        dynamic_dfs: Optional[List[pd.DataFrame]] = None,  # future values for dynamic features\n",
    "        predict_fn: Optional[Callable] = None,  # custom function to compute predictions\n",
    "        **predict_fn_kwargs,  # additional arguments passed to predict_fn\n",
    "    ):\n",
    "        \"\"\"Creates `n_windows` splits of `window_size` from `data`, trains the model\n",
    "        on the training set, predicts the window and merges the actuals and the predictions\n",
    "        in a dataframe.\n",
    "\n",
    "        Returns a dataframe containing the datestamps, actual values, train ends and predictions.\"\"\"\n",
    "        results = []\n",
    "        self.cv_models_ = []\n",
    "        if id_col != 'index':\n",
    "            data = data.set_index(id_col)\n",
    "\n",
    "        def renames(df):\n",
    "            mapper = {time_col: 'ds', target_col: 'y'}\n",
    "            df = df.rename(columns=mapper, copy=False)\n",
    "            df.index.name = 'unique_id'\n",
    "            return df\n",
    "        data = data.map_partitions(renames)\n",
    "\n",
    "        if np.issubdtype(data['ds'].dtype.type, np.integer):\n",
    "            freq = 1\n",
    "        else:\n",
    "            freq = self.freq\n",
    "        for train_end, train, valid in backtest_splits(data, n_windows, window_size, freq):\n",
    "            self.fit(train, 'index', 'ds', 'y', static_features, dropna, keep_last_n)\n",
    "            self.cv_models_.append(self.models_)\n",
    "            y_pred = self.predict(\n",
    "                window_size, dynamic_dfs, predict_fn, **predict_fn_kwargs\n",
    "            )\n",
    "            result = valid[['ds', 'y']].copy()\n",
    "            result['cutoff'] = train_end\n",
    "            \n",
    "            def merge_fn(res, pred):\n",
    "                return res.merge(pred, on=['unique_id', 'ds'], how='left')\n",
    "            meta = {**result.dtypes.to_dict(), **y_pred.dtypes.to_dict()}\n",
    "            result = result.map_partitions(merge_fn, y_pred, align_dataframes=False, meta=meta)\n",
    "            if id_col != 'index':\n",
    "                result = result.reset_index()\n",
    "            result = result.rename(columns={'ds': time_col, 'y': target_col, 'unique_id': id_col})\n",
    "            results.append(result)\n",
    "\n",
    "        return dd.concat(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2b21c7-23fa-4628-be0d-4da2448fa382",
   "metadata": {},
   "source": [
    "The `DistributedForecast` class is a high level abstraction that encapsulates all the steps in the pipeline (preprocessing, fitting the model and computing predictions) and applies them in a distributed way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3009c1fe-caba-4412-be41-69588a512bbf",
   "metadata": {},
   "source": [
    "## Example\n",
    "This shows an example with simulated data, for a real world example in a remote cluster you can check the [M5 distributed example](https://www.kaggle.com/lemuz90/m5-mlforecast-distributed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bc621d-43b8-4160-a694-08d3259e202d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from window_ops.expanding import expanding_mean\n",
    "from window_ops.rolling import rolling_mean\n",
    "\n",
    "from mlforecast.utils import backtest_splits, generate_daily_series, generate_prices_for_series\n",
    "from mlforecast.distributed.models.lgb import LGBMForecast\n",
    "from mlforecast.distributed.models.xgb import XGBForecast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a9b623-af02-4acb-9d09-cda1debead4e",
   "metadata": {},
   "source": [
    "The different things that you need to use `DistributedForecast` (as opposed to `Forecast`) are:\n",
    "\n",
    "1. You need to set up a `dask.distributed.Client`. If this client is connected to a remote cluster then the process will run there.\n",
    "2. Your data needs to be a `dask.dataframe.DataFrame`.\n",
    "3. You need to use a model that implements distributed training (either XGBForecast or LGBMForecast)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6479a531-60db-4c1e-ac28-189b8628d3ef",
   "metadata": {},
   "source": [
    "### Client setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb8d3ef-d1f3-48da-b2dc-0579ea058626",
   "metadata": {},
   "source": [
    "Here we define a client that connects to a `dask.distributed.LocalCluster`, however it could be any other kind of cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fea1b0f-f8d3-4be5-89fe-3dcbe00dff77",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(n_workers=2, threads_per_worker=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62cfccf-5898-4046-a6c8-091fec26f0e4",
   "metadata": {},
   "source": [
    "### Data setup\n",
    "\n",
    "The data is given as a `dask.dataframe.DataFrame`, you need to make sure that each time serie is only in one partition and it is recommended that you have as many partitions as you have workers. If you have more partitions than workers make sure to set `num_threads=1` to avoid having nested parallelism.\n",
    "\n",
    "The required input format is the same as for `Forecast`, except that it's a `dask.dataframe.DataFrame` instead of a `pandas.Dataframe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1388b1cf-78cb-4cf9-b146-6d2043d4d3ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>static_0</th>\n",
       "      <th>static_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=10</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>datetime64[ns]</td>\n",
       "      <td>float64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_11</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_90</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_99</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: from_pandas, 10 tasks</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                            ds        y static_0 static_1\n",
       "npartitions=10                                           \n",
       "id_00           datetime64[ns]  float64    int64    int64\n",
       "id_11                      ...      ...      ...      ...\n",
       "...                        ...      ...      ...      ...\n",
       "id_90                      ...      ...      ...      ...\n",
       "id_99                      ...      ...      ...      ...\n",
       "Dask Name: from_pandas, 10 tasks"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series = generate_daily_series(100, n_static_features=2, equal_ends=True, static_as_categorical=False)\n",
    "partitioned_series = dd.from_pandas(series, npartitions=10)\n",
    "partitioned_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b92d81-3a70-424d-81dc-8a995af1fb9c",
   "metadata": {},
   "source": [
    "### Models\n",
    "In order to perform distributed forecasting, we need to use a model that is able to train in a distributed way using `dask`. The current implementations are in `LGBMForecast` and `XGBForecast` which are just wrappers around `lightgbm.dask.DaskLGBMRegressor` and `xgboost.dask.DaskXGBRegressor` that add a `model_` property to get the trained model from them and send it to every worker to perform the predictions step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8012f8-3325-41a2-8fb0-7eaa07a24b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [XGBForecast(random_state=0), LGBMForecast(random_state=0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078e1983-7235-4930-9b44-36c76e35be69",
   "metadata": {},
   "source": [
    "### Training\n",
    "Once we have our models we instantiate a `DistributedForecast` object defining our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334d20c9-5ad7-42e8-a6b2-40d8005467bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistributedForecast(models=[XGBForecast, LGBMForecast], freq=<Day>, lag_features=['lag-7', 'expanding_mean_lag-1', 'rolling_mean_lag-7_window_size-14'], date_features=['dayofweek', 'month'], num_threads=1, client=<Client: 'tcp://127.0.0.1:39927' processes=2 threads=2, memory=15.50 GiB>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcst = DistributedForecast(\n",
    "    models=models,\n",
    "    freq='D',\n",
    "    lags=[7],\n",
    "    lag_transforms={\n",
    "        1: [expanding_mean],\n",
    "        7: [(rolling_mean, 14)]\n",
    "    },\n",
    "    date_features=['dayofweek', 'month'],\n",
    "    num_threads=1,\n",
    ")\n",
    "fcst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234a2f97-ea31-4359-b57b-bb5db5cdcfc2",
   "metadata": {},
   "source": [
    "Here where we say that:\n",
    "\n",
    "* Our series have daily frequency.\n",
    "* We want to use lag 7 as a feature\n",
    "* We want the lag transformations to be:\n",
    "   * expanding mean of the lag 1\n",
    "   * rolling mean of the lag 7 over a window of size 14\n",
    "* We want to use dayofweek and month as date features.\n",
    "* We want to perform the preprocessing and the forecasting steps using 1 thread, because we have 10 partitions and 2 workers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5713b6a-1436-4554-878f-cc47bf87b2d0",
   "metadata": {},
   "source": [
    "From this point we have two options:\n",
    "\n",
    "1. Compute the features and fit our models.\n",
    "2. Compute the features and get them back as a dataframe to do some custom splitting or adding additional features, then training the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bff858-7e8a-49dd-963d-3acd6214e7a8",
   "metadata": {},
   "source": [
    "#### 1. Using all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2882214c-877f-4f5c-af51-3b509fafc3fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### DistributedForecast.fit\n",
       "\n",
       ">      DistributedForecast.fit (data:dask.dataframe.core.DataFrame,\n",
       ">                               id_col:str='index', time_col:str='ds',\n",
       ">                               target_col:str='y',\n",
       ">                               static_features:Optional[List[str]]=None,\n",
       ">                               dropna:bool=True,\n",
       ">                               keep_last_n:Optional[int]=None)\n",
       "\n",
       "Perform the preprocessing and fit the model.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| data | DataFrame |  |  |\n",
       "| id_col | str | index | column that identifies each serie, it's recommended to have this as the index. |\n",
       "| time_col | str | ds | column with the timestamps |\n",
       "| target_col | str | y | column with the series values |\n",
       "| static_features | typing.Optional[typing.List[str]] | None |  |\n",
       "| dropna | bool | True |  |\n",
       "| keep_last_n | typing.Optional[int] | None |  |\n",
       "| **Returns** | **DistributedForecast** |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### DistributedForecast.fit\n",
       "\n",
       ">      DistributedForecast.fit (data:dask.dataframe.core.DataFrame,\n",
       ">                               id_col:str='index', time_col:str='ds',\n",
       ">                               target_col:str='y',\n",
       ">                               static_features:Optional[List[str]]=None,\n",
       ">                               dropna:bool=True,\n",
       ">                               keep_last_n:Optional[int]=None)\n",
       "\n",
       "Perform the preprocessing and fit the model.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| data | DataFrame |  |  |\n",
       "| id_col | str | index | column that identifies each serie, it's recommended to have this as the index. |\n",
       "| time_col | str | ds | column with the timestamps |\n",
       "| target_col | str | y | column with the series values |\n",
       "| static_features | typing.Optional[typing.List[str]] | None |  |\n",
       "| dropna | bool | True |  |\n",
       "| keep_last_n | typing.Optional[int] | None |  |\n",
       "| **Returns** | **DistributedForecast** |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DistributedForecast.fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8545b806-d828-42dc-be00-d71dab8499b6",
   "metadata": {},
   "source": [
    "Calling `fit` on our data computes the features independently for each partition and performs distributed training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8472ae1e-4b26-4414-8c58-9cc9a9e4d65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20:14:15] task [xgboost.dask]:tcp://127.0.0.1:41527 got new rank 0\n",
      "[20:14:15] task [xgboost.dask]:tcp://127.0.0.1:39163 got new rank 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding random open ports for workers\n",
      "[LightGBM] [Info] Trying to bind port 37517...\n",
      "[LightGBM] [Info] Binding port 37517 succeeded\n",
      "[LightGBM] [Info] Listening...\n",
      "[LightGBM] [Warning] Connecting to rank 1 failed, waiting for 200 milliseconds\n",
      "[LightGBM] [Info] Trying to bind port 35081...\n",
      "[LightGBM] [Info] Binding port 35081 succeeded\n",
      "[LightGBM] [Info] Listening...\n",
      "[LightGBM] [Info] Connected to rank 1\n",
      "[LightGBM] [Info] Connected to rank 0\n",
      "[LightGBM] [Info] Local rank: 0, total number of machines: 2\n",
      "[LightGBM] [Info] Local rank: 1, total number of machines: 2\n",
      "[LightGBM] [Warning] num_threads is set=1, n_jobs=-1 will be ignored. Current value: num_threads=1\n",
      "[LightGBM] [Warning] num_threads is set=1, n_jobs=-1 will be ignored. Current value: num_threads=1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistributedForecast(models=[XGBForecast, LGBMForecast], freq=<Day>, lag_features=['lag-7', 'expanding_mean_lag-1', 'rolling_mean_lag-7_window_size-14'], date_features=['dayofweek', 'month'], num_threads=1, client=<Client: 'tcp://127.0.0.1:39927' processes=2 threads=2, memory=15.50 GiB>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcst.fit(partitioned_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0b5ec6-bee0-4f68-8c91-b99c54576774",
   "metadata": {},
   "source": [
    "### Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e95a58-0fee-41ee-900b-d2b61825042f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### DistributedForecast.predict\n",
       "\n",
       ">      DistributedForecast.predict (horizon:int,\n",
       ">                                   dynamic_dfs:Optional[List[pandas.core.frame.\n",
       ">                                   DataFrame]]=None,\n",
       ">                                   predict_fn:Optional[Callable]=None,\n",
       ">                                   **predict_fn_kwargs)\n",
       "\n",
       "Compute the predictions for the next `horizon` steps.\n",
       "\n",
       "`predict_fn(model, new_x, dynamic_dfs, features_order, **predict_fn_kwargs)` is called in every timestep, where:\n",
       "`model` is the trained model.\n",
       "`new_x` is a dataframe with the same format as the input plus the computed features.\n",
       "`dynamic_dfs` is a list containing the dynamic dataframes.\n",
       "`features_order` is the list of column names that were used in the training step."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### DistributedForecast.predict\n",
       "\n",
       ">      DistributedForecast.predict (horizon:int,\n",
       ">                                   dynamic_dfs:Optional[List[pandas.core.frame.\n",
       ">                                   DataFrame]]=None,\n",
       ">                                   predict_fn:Optional[Callable]=None,\n",
       ">                                   **predict_fn_kwargs)\n",
       "\n",
       "Compute the predictions for the next `horizon` steps.\n",
       "\n",
       "`predict_fn(model, new_x, dynamic_dfs, features_order, **predict_fn_kwargs)` is called in every timestep, where:\n",
       "`model` is the trained model.\n",
       "`new_x` is a dataframe with the same format as the input plus the computed features.\n",
       "`dynamic_dfs` is a list containing the dynamic dataframes.\n",
       "`features_order` is the list of column names that were used in the training step."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DistributedForecast.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7417aa8d-e724-4ffa-a902-2ab822574030",
   "metadata": {},
   "source": [
    "Once we have our fitted models we can compute the predictions for the next 7 timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8a439d-e0d0-4f2f-925c-fccd92361c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>XGBRegressor</th>\n",
       "      <th>LGBMRegressor</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=10</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>datetime64[ns]</td>\n",
       "      <td>float32</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_11</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_90</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_99</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: from-delayed, 20 tasks</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                            ds XGBRegressor LGBMRegressor\n",
       "npartitions=10                                           \n",
       "id_00           datetime64[ns]      float32       float64\n",
       "id_11                      ...          ...           ...\n",
       "...                        ...          ...           ...\n",
       "id_90                      ...          ...           ...\n",
       "id_99                      ...          ...           ...\n",
       "Dask Name: from-delayed, 20 tasks"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = fcst.predict(7)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7a8518-0e2a-4699-8da0-e63f29a2f7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "preds = preds.compute()\n",
    "preds2 = fcst.predict(7).compute()\n",
    "pd.testing.assert_frame_equal(preds, preds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e8d462-8318-4d10-98e5-13739dcd4023",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20:14:19] task [xgboost.dask]:tcp://127.0.0.1:41527 got new rank 0\n",
      "[20:14:19] task [xgboost.dask]:tcp://127.0.0.1:39163 got new rank 1\n"
     ]
    }
   ],
   "source": [
    "##|hide\n",
    "non_std_series = partitioned_series.copy()\n",
    "non_std_series['ds'] = non_std_series.map_partitions(lambda part: part.groupby('unique_id').cumcount())\n",
    "non_std_series = non_std_series.reset_index().rename(columns={'ds': 'time', 'y': 'value', 'unique_id': 'some_id'})\n",
    "flow_params = dict(\n",
    "    models=[XGBForecast(random_state=0)],\n",
    "    lags=[7],\n",
    "    lag_transforms={\n",
    "        1: [expanding_mean],\n",
    "        7: [(rolling_mean, 14)]\n",
    "    },\n",
    "    num_threads=1,\n",
    ")\n",
    "fcst = DistributedForecast(freq='D', **flow_params)\n",
    "preds = fcst.fit(partitioned_series).predict(7).compute()\n",
    "fcst2 = DistributedForecast(**flow_params)\n",
    "fcst2.preprocess(non_std_series, id_col='some_id', time_col='time', target_col='value')\n",
    "fcst2.models_ = fcst.models_  # distributed training can end up with different fits\n",
    "non_std_preds = fcst2.predict(7).compute()\n",
    "non_std_preds.index.name = 'unique_id'\n",
    "# non_std_preds.index.name = 'unique_id'\n",
    "pd.testing.assert_frame_equal(preds.drop(columns='ds'), non_std_preds.drop(columns='time'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f27b9a-ebc3-4673-8459-ad5843a43dac",
   "metadata": {},
   "source": [
    "#### 2. Preprocess and train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0b1833-a63a-45da-b3dc-8e1e5bfeb589",
   "metadata": {},
   "source": [
    "If we only want to perform the preprocessing step we call `preprocess` with our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5c34f3-faa9-44a8-b070-d28225d73cae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### DistributedForecast.preprocess\n",
       "\n",
       ">      DistributedForecast.preprocess (data:dask.dataframe.core.DataFrame,\n",
       ">                                      id_col:str='index', time_col:str='ds',\n",
       ">                                      target_col:str='y',\n",
       ">                                      static_features:Optional[List[str]]=None,\n",
       ">                                      dropna:bool=True,\n",
       ">                                      keep_last_n:Optional[int]=None)\n",
       "\n",
       "Computes the transformations on each partition of `data` and\n",
       "saves the required information for the forecasting step.\n",
       "Returns a dask dataframe with the computed features.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| data | DataFrame |  |  |\n",
       "| id_col | str | index | column that identifies each serie, it's recommended to have this as the index. |\n",
       "| time_col | str | ds | column with the timestamps |\n",
       "| target_col | str | y | column with the series values |\n",
       "| static_features | typing.Optional[typing.List[str]] | None |  |\n",
       "| dropna | bool | True |  |\n",
       "| keep_last_n | typing.Optional[int] | None |  |\n",
       "| **Returns** | **DataFrame** |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### DistributedForecast.preprocess\n",
       "\n",
       ">      DistributedForecast.preprocess (data:dask.dataframe.core.DataFrame,\n",
       ">                                      id_col:str='index', time_col:str='ds',\n",
       ">                                      target_col:str='y',\n",
       ">                                      static_features:Optional[List[str]]=None,\n",
       ">                                      dropna:bool=True,\n",
       ">                                      keep_last_n:Optional[int]=None)\n",
       "\n",
       "Computes the transformations on each partition of `data` and\n",
       "saves the required information for the forecasting step.\n",
       "Returns a dask dataframe with the computed features.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| data | DataFrame |  |  |\n",
       "| id_col | str | index | column that identifies each serie, it's recommended to have this as the index. |\n",
       "| time_col | str | ds | column with the timestamps |\n",
       "| target_col | str | y | column with the series values |\n",
       "| static_features | typing.Optional[typing.List[str]] | None |  |\n",
       "| dropna | bool | True |  |\n",
       "| keep_last_n | typing.Optional[int] | None |  |\n",
       "| **Returns** | **DataFrame** |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DistributedForecast.preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a9c699-8768-4485-bc22-4da64bea46d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>static_0</th>\n",
       "      <th>static_1</th>\n",
       "      <th>lag-7</th>\n",
       "      <th>expanding_mean_lag-1</th>\n",
       "      <th>rolling_mean_lag-7_window_size-14</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2000-10-25</td>\n",
       "      <td>497.668437</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>506.946385</td>\n",
       "      <td>250.013666</td>\n",
       "      <td>263.200596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2000-10-26</td>\n",
       "      <td>39.183469</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>38.877800</td>\n",
       "      <td>261.806750</td>\n",
       "      <td>263.133868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2000-10-27</td>\n",
       "      <td>94.377779</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>115.127739</td>\n",
       "      <td>251.687510</td>\n",
       "      <td>263.980563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2000-10-28</td>\n",
       "      <td>179.235741</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>180.384975</td>\n",
       "      <td>244.847957</td>\n",
       "      <td>264.252723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2000-10-29</td>\n",
       "      <td>267.546447</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>242.228588</td>\n",
       "      <td>242.114114</td>\n",
       "      <td>263.055629</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ds           y  static_0  static_1       lag-7  \\\n",
       "unique_id                                                          \n",
       "id_00     2000-10-25  497.668437        79        45  506.946385   \n",
       "id_00     2000-10-26   39.183469        79        45   38.877800   \n",
       "id_00     2000-10-27   94.377779        79        45  115.127739   \n",
       "id_00     2000-10-28  179.235741        79        45  180.384975   \n",
       "id_00     2000-10-29  267.546447        79        45  242.228588   \n",
       "\n",
       "           expanding_mean_lag-1  rolling_mean_lag-7_window_size-14  \n",
       "unique_id                                                           \n",
       "id_00                250.013666                         263.200596  \n",
       "id_00                261.806750                         263.133868  \n",
       "id_00                251.687510                         263.980563  \n",
       "id_00                244.847957                         264.252723  \n",
       "id_00                242.114114                         263.055629  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_ddf = fcst.preprocess(partitioned_series)\n",
    "features_ddf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113b647b-7d05-42d7-b626-35c35fcf517b",
   "metadata": {},
   "source": [
    "This is useful if we want to inspect the data the model will be trained, adding additional features or performing some custom train-valid split. Here we perform a 80-20 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace9f0b3-63d6-457f-be72-39c4adc97309",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(0)\n",
    "\n",
    "def mask_as_series(df):\n",
    "    return pd.Series(rng.rand(df.shape[0]) < 0.8, index=df.index)\n",
    "\n",
    "train_mask = features_ddf.map_partitions(mask_as_series)\n",
    "train, valid = features_ddf[train_mask], features_ddf[~train_mask]\n",
    "X_train, y_train = train.drop(columns=['ds', 'y']), train.y\n",
    "X_valid, y_valid = valid.drop(columns=['ds', 'y']), valid.y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9205a1c-1ea5-4dfd-ba69-9756fb59b1ca",
   "metadata": {},
   "source": [
    "If we do this we must \"manually\" train our models and assing them to the `models_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4055884-0525-4c5f-9898-1afe85e07057",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20:14:21] task [xgboost.dask]:tcp://127.0.0.1:39163 got new rank 0\n",
      "[20:14:21] task [xgboost.dask]:tcp://127.0.0.1:41527 got new rank 1\n"
     ]
    }
   ],
   "source": [
    "fitted = models[0].fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "    eval_metric='rmse',\n",
    "    verbose=False,\n",
    ")\n",
    "fcst.models_ = [fitted]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1424717d-153a-4e5e-94b1-11d4991656de",
   "metadata": {},
   "source": [
    "We can see the RMSE by iteration for each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c864900-88fa-4d2a-86d3-5a9638808a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>validation_0</th>\n",
       "      <th>validation_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>159.12</td>\n",
       "      <td>158.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>111.81</td>\n",
       "      <td>111.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>78.76</td>\n",
       "      <td>78.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55.74</td>\n",
       "      <td>55.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39.80</td>\n",
       "      <td>39.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>7.76</td>\n",
       "      <td>9.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>7.74</td>\n",
       "      <td>9.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>7.73</td>\n",
       "      <td>9.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>7.70</td>\n",
       "      <td>9.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>7.69</td>\n",
       "      <td>9.63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    validation_0  validation_1\n",
       "0         159.12        158.98\n",
       "1         111.81        111.59\n",
       "2          78.76         78.50\n",
       "3          55.74         55.44\n",
       "4          39.80         39.52\n",
       "..           ...           ...\n",
       "95          7.76          9.64\n",
       "96          7.74          9.64\n",
       "97          7.73          9.64\n",
       "98          7.70          9.63\n",
       "99          7.69          9.63\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\n",
    "    k: np.round(fcst.models_[0].evals_result_[k]['rmse'], 2)\n",
    "    for k in ('validation_0', 'validation_1')\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7d1a41-a56c-4113-a7d9-a79f643021ab",
   "metadata": {},
   "source": [
    "#### Dynamic features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e66766-a8e9-4af0-96e3-9f5b847c4f0b",
   "metadata": {},
   "source": [
    "By default the predict method repeats the static features and updates the transformations and the date features. If you have dynamic features like prices or a calendar with holidays you can pass them as a list to the `dynamic_dfs` argument of `Forecast.predict`, which will call `pd.DataFrame.merge` on each of them in order.\n",
    "\n",
    "Here's an example:\n",
    "\n",
    "Suppose that we have a `product_id` column and we have a catalog for prices based on that `product_id` and the date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee52992-0b18-43b0-833d-62f7246951c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>product_id</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-06-09</td>\n",
       "      <td>1</td>\n",
       "      <td>0.548814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-06-10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.715189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-06-11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.602763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-06-12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.544883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-06-13</td>\n",
       "      <td>1</td>\n",
       "      <td>0.423655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20180</th>\n",
       "      <td>2001-05-17</td>\n",
       "      <td>99</td>\n",
       "      <td>0.223520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20181</th>\n",
       "      <td>2001-05-18</td>\n",
       "      <td>99</td>\n",
       "      <td>0.446104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20182</th>\n",
       "      <td>2001-05-19</td>\n",
       "      <td>99</td>\n",
       "      <td>0.044783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20183</th>\n",
       "      <td>2001-05-20</td>\n",
       "      <td>99</td>\n",
       "      <td>0.483216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20184</th>\n",
       "      <td>2001-05-21</td>\n",
       "      <td>99</td>\n",
       "      <td>0.799660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20185 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ds  product_id     price\n",
       "0     2000-06-09           1  0.548814\n",
       "1     2000-06-10           1  0.715189\n",
       "2     2000-06-11           1  0.602763\n",
       "3     2000-06-12           1  0.544883\n",
       "4     2000-06-13           1  0.423655\n",
       "...          ...         ...       ...\n",
       "20180 2001-05-17          99  0.223520\n",
       "20181 2001-05-18          99  0.446104\n",
       "20182 2001-05-19          99  0.044783\n",
       "20183 2001-05-20          99  0.483216\n",
       "20184 2001-05-21          99  0.799660\n",
       "\n",
       "[20185 rows x 3 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynamic_series = series.rename(columns={'static_1': 'product_id'})\n",
    "prices_catalog = generate_prices_for_series(dynamic_series)\n",
    "prices_catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7730055-6bab-4b52-bc83-e8456358309e",
   "metadata": {},
   "source": [
    "And you have already merged these prices into your series dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2e34f7-3ee0-48a1-93d1-4cc77f1afdfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>static_0</th>\n",
       "      <th>product_id</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2000-10-05</td>\n",
       "      <td>39.811983</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>0.570826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2000-10-06</td>\n",
       "      <td>103.274013</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>0.260562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2000-10-07</td>\n",
       "      <td>176.574744</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>0.274048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2000-10-08</td>\n",
       "      <td>258.987900</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>0.433878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2000-10-09</td>\n",
       "      <td>344.940404</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>0.653738</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ds           y  static_0  product_id     price\n",
       "unique_id                                                       \n",
       "id_00     2000-10-05   39.811983        79          45  0.570826\n",
       "id_00     2000-10-06  103.274013        79          45  0.260562\n",
       "id_00     2000-10-07  176.574744        79          45  0.274048\n",
       "id_00     2000-10-08  258.987900        79          45  0.433878\n",
       "id_00     2000-10-09  344.940404        79          45  0.653738"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynamic_series = partitioned_series.rename(columns={'static_1': 'product_id'})\n",
    "dynamic_series = dynamic_series.reset_index()\n",
    "series_with_prices = dynamic_series.merge(prices_catalog, how='left')\n",
    "series_with_prices = series_with_prices.set_index('unique_id', sorted=True)\n",
    "series_with_prices.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e348d3-a449-4823-8d5c-1c8b13f9b133",
   "metadata": {},
   "source": [
    "This dataframe will be passed to `DistributedForecast.fit` (or `DistributedForecast.preprocess`), however since the price is dynamic we have to tell that method that only `static_0` and `product_id` are static and we'll have to update `price` in every timestep, which basically involves merging the updated features with the prices catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33ea277-66d3-4bf8-aeaa-cc117cfa8e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20:14:22] task [xgboost.dask]:tcp://127.0.0.1:41527 got new rank 0\n",
      "[20:14:22] task [xgboost.dask]:tcp://127.0.0.1:39163 got new rank 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding random open ports for workers\n",
      "[LightGBM] [Info] Trying to bind port 59265...\n",
      "[LightGBM] [Info] Binding port 59265 succeeded\n",
      "[LightGBM] [Info] Listening...\n",
      "[LightGBM] [Info] Trying to bind port 48485...\n",
      "[LightGBM] [Info] Binding port 48485 succeeded\n",
      "[LightGBM] [Info] Listening...\n",
      "[LightGBM] [Info] Connected to rank 1\n",
      "[LightGBM] [Info] Local rank: 0, total number of machines: 2\n",
      "[LightGBM] [Info] Connected to rank 0\n",
      "[LightGBM] [Info] Local rank: 1, total number of machines: 2\n",
      "[LightGBM] [Warning] num_threads is set=1, n_jobs=-1 will be ignored. Current value: num_threads=1\n",
      "[LightGBM] [Warning] num_threads is set=1, n_jobs=-1 will be ignored. Current value: num_threads=1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistributedForecast(models=[XGBForecast, LGBMForecast], freq=<Day>, lag_features=['lag-7', 'expanding_mean_lag-1', 'rolling_mean_lag-7_window_size-14'], date_features=['dayofweek', 'month'], num_threads=1, client=<Client: 'tcp://127.0.0.1:39927' processes=2 threads=2, memory=15.50 GiB>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcst = DistributedForecast(\n",
    "    models,\n",
    "    freq='D',\n",
    "    lags=[7],\n",
    "    lag_transforms={\n",
    "        1: [expanding_mean],\n",
    "        7: [(rolling_mean, 14)]\n",
    "    },\n",
    "    date_features=['dayofweek', 'month'],\n",
    "    num_threads=1,\n",
    ")\n",
    "series_with_prices = series_with_prices\n",
    "fcst.fit(series_with_prices, static_features=['static_0', 'product_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9bba71-8bb1-491b-bb0c-7ef8c6004f7b",
   "metadata": {},
   "source": [
    "So in order to update the price in each timestep we just call `DistributedForecast.predict` with our forecast horizon and pass the prices catalog as a dynamic dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3bc484-5e79-4139-acd5-c9ee3cf7270a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>XGBRegressor</th>\n",
       "      <th>LGBMRegressor</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2001-05-15</td>\n",
       "      <td>417.990967</td>\n",
       "      <td>431.913959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2001-05-16</td>\n",
       "      <td>501.354095</td>\n",
       "      <td>502.759544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2001-05-17</td>\n",
       "      <td>23.075724</td>\n",
       "      <td>19.760072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2001-05-18</td>\n",
       "      <td>102.658348</td>\n",
       "      <td>101.563043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2001-05-19</td>\n",
       "      <td>178.322525</td>\n",
       "      <td>187.509875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_99</th>\n",
       "      <td>2001-05-17</td>\n",
       "      <td>442.907318</td>\n",
       "      <td>442.109678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_99</th>\n",
       "      <td>2001-05-18</td>\n",
       "      <td>18.299212</td>\n",
       "      <td>20.339088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_99</th>\n",
       "      <td>2001-05-19</td>\n",
       "      <td>90.773727</td>\n",
       "      <td>91.373232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_99</th>\n",
       "      <td>2001-05-20</td>\n",
       "      <td>154.936508</td>\n",
       "      <td>152.878285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_99</th>\n",
       "      <td>2001-05-21</td>\n",
       "      <td>227.398514</td>\n",
       "      <td>228.649412</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>700 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ds  XGBRegressor  LGBMRegressor\n",
       "unique_id                                        \n",
       "id_00     2001-05-15    417.990967     431.913959\n",
       "id_00     2001-05-16    501.354095     502.759544\n",
       "id_00     2001-05-17     23.075724      19.760072\n",
       "id_00     2001-05-18    102.658348     101.563043\n",
       "id_00     2001-05-19    178.322525     187.509875\n",
       "...              ...           ...            ...\n",
       "id_99     2001-05-17    442.907318     442.109678\n",
       "id_99     2001-05-18     18.299212      20.339088\n",
       "id_99     2001-05-19     90.773727      91.373232\n",
       "id_99     2001-05-20    154.936508     152.878285\n",
       "id_99     2001-05-21    227.398514     228.649412\n",
       "\n",
       "[700 rows x 3 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = fcst.predict(7, dynamic_dfs=[prices_catalog])\n",
    "preds.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b03634-d3e0-46e5-b05b-df7291fc8fdc",
   "metadata": {},
   "source": [
    "#### Custom predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa29c8f-65a6-4048-bf6a-0aa9874e2a2c",
   "metadata": {},
   "source": [
    "If you want to do something like scaling the predictions you can define a function and pass it to `DistributedForecast.predict` as described in <a href=\"/mlforecast/forecast.html#Custom-predictions\">Custom predictions</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec33846e-6d3c-4199-afe8-c5e064a53aac",
   "metadata": {},
   "source": [
    "### Cross validation\n",
    "Refer to `Forecast.cross_validation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c97388-73f5-460d-9299-ab5cc6f31720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### DistributedForecast.cross_validation\n",
       "\n",
       ">      DistributedForecast.cross_validation (data:dask.dataframe.core.DataFrame,\n",
       ">                                            n_windows:int, window_size:int,\n",
       ">                                            id_col:str='index',\n",
       ">                                            time_col:str='ds',\n",
       ">                                            target_col:str='y', static_features\n",
       ">                                            :Optional[List[str]]=None,\n",
       ">                                            dropna:bool=True,\n",
       ">                                            keep_last_n:Optional[int]=None, dyn\n",
       ">                                            amic_dfs:Optional[List[pandas.core.\n",
       ">                                            frame.DataFrame]]=None,\n",
       ">                                            predict_fn:Optional[Callable]=None,\n",
       ">                                            **predict_fn_kwargs)\n",
       "\n",
       "Creates `n_windows` splits of `window_size` from `data`, trains the model\n",
       "on the training set, predicts the window and merges the actuals and the predictions\n",
       "in a dataframe.\n",
       "\n",
       "Returns a dataframe containing the datestamps, actual values, train ends and predictions.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| data | DataFrame |  | time series |\n",
       "| n_windows | int |  | number of windows to evaluate |\n",
       "| window_size | int |  | test size in each window |\n",
       "| id_col | str | index | column that identifies each serie, can also be the index. |\n",
       "| time_col | str | ds | column with the timestamps |\n",
       "| target_col | str | y | column with the series values |\n",
       "| static_features | typing.Optional[typing.List[str]] | None | column names of the features that don't change in time |\n",
       "| dropna | bool | True | drop rows with missing values created by lags |\n",
       "| keep_last_n | typing.Optional[int] | None | keep only this many observations of each serie for computing the updates |\n",
       "| dynamic_dfs | typing.Optional[typing.List[pandas.core.frame.DataFrame]] | None | future values for dynamic features |\n",
       "| predict_fn | typing.Optional[typing.Callable] | None | custom function to compute predictions |\n",
       "| predict_fn_kwargs |  |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### DistributedForecast.cross_validation\n",
       "\n",
       ">      DistributedForecast.cross_validation (data:dask.dataframe.core.DataFrame,\n",
       ">                                            n_windows:int, window_size:int,\n",
       ">                                            id_col:str='index',\n",
       ">                                            time_col:str='ds',\n",
       ">                                            target_col:str='y', static_features\n",
       ">                                            :Optional[List[str]]=None,\n",
       ">                                            dropna:bool=True,\n",
       ">                                            keep_last_n:Optional[int]=None, dyn\n",
       ">                                            amic_dfs:Optional[List[pandas.core.\n",
       ">                                            frame.DataFrame]]=None,\n",
       ">                                            predict_fn:Optional[Callable]=None,\n",
       ">                                            **predict_fn_kwargs)\n",
       "\n",
       "Creates `n_windows` splits of `window_size` from `data`, trains the model\n",
       "on the training set, predicts the window and merges the actuals and the predictions\n",
       "in a dataframe.\n",
       "\n",
       "Returns a dataframe containing the datestamps, actual values, train ends and predictions.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| data | DataFrame |  | time series |\n",
       "| n_windows | int |  | number of windows to evaluate |\n",
       "| window_size | int |  | test size in each window |\n",
       "| id_col | str | index | column that identifies each serie, can also be the index. |\n",
       "| time_col | str | ds | column with the timestamps |\n",
       "| target_col | str | y | column with the series values |\n",
       "| static_features | typing.Optional[typing.List[str]] | None | column names of the features that don't change in time |\n",
       "| dropna | bool | True | drop rows with missing values created by lags |\n",
       "| keep_last_n | typing.Optional[int] | None | keep only this many observations of each serie for computing the updates |\n",
       "| dynamic_dfs | typing.Optional[typing.List[pandas.core.frame.DataFrame]] | None | future values for dynamic features |\n",
       "| predict_fn | typing.Optional[typing.Callable] | None | custom function to compute predictions |\n",
       "| predict_fn_kwargs |  |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DistributedForecast.cross_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fab05e3-3d72-4afa-a74e-6961a3d9186a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20:14:24] task [xgboost.dask]:tcp://127.0.0.1:39163 got new rank 0\n",
      "[20:14:24] task [xgboost.dask]:tcp://127.0.0.1:41527 got new rank 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding random open ports for workers\n",
      "[LightGBM] [Info] Trying to bind port 57797...\n",
      "[LightGBM] [Info] Binding port 57797 succeeded\n",
      "[LightGBM] [Info] Listening...\n",
      "[LightGBM] [Warning] Connecting to rank 1 failed, waiting for 200 milliseconds\n",
      "[LightGBM] [Info] Trying to bind port 48891...\n",
      "[LightGBM] [Info] Binding port 48891 succeeded\n",
      "[LightGBM] [Info] Listening...\n",
      "[LightGBM] [Info] Connected to rank 1\n",
      "[LightGBM] [Info] Local rank: 0, total number of machines: 2\n",
      "[LightGBM] [Info] Connected to rank 0\n",
      "[LightGBM] [Info] Local rank: 1, total number of machines: 2\n",
      "[LightGBM] [Warning] num_threads is set=1, n_jobs=-1 will be ignored. Current value: num_threads=1\n",
      "[LightGBM] [Warning] num_threads is set=1, n_jobs=-1 will be ignored. Current value: num_threads=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20:14:26] task [xgboost.dask]:tcp://127.0.0.1:39163 got new rank 0\n",
      "[20:14:26] task [xgboost.dask]:tcp://127.0.0.1:41527 got new rank 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding random open ports for workers\n",
      "[LightGBM] [Info] Trying to bind port 53565...\n",
      "[LightGBM] [Info] Binding port 53565 succeeded\n",
      "[LightGBM] [Info] Listening...\n",
      "[LightGBM] [Warning] Connecting to rank 1 failed, waiting for 200 milliseconds\n",
      "[LightGBM] [Info] Trying to bind port 36255...\n",
      "[LightGBM] [Info] Binding port 36255 succeeded\n",
      "[LightGBM] [Info] Listening...\n",
      "[LightGBM] [Info] Connected to rank 1\n",
      "[LightGBM] [Info] Local rank: 0, total number of machines: 2\n",
      "[LightGBM] [Info] Connected to rank 0\n",
      "[LightGBM] [Info] Local rank: 1, total number of machines: 2\n",
      "[LightGBM] [Warning] num_threads is set=1, n_jobs=-1 will be ignored. Current value: num_threads=1\n",
      "[LightGBM] [Warning] num_threads is set=1, n_jobs=-1 will be ignored. Current value: num_threads=1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>cutoff</th>\n",
       "      <th>XGBRegressor</th>\n",
       "      <th>LGBMRegressor</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=20</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>datetime64[ns]</td>\n",
       "      <td>float64</td>\n",
       "      <td>datetime64[ns]</td>\n",
       "      <td>float32</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: concat, 240 tasks</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                            ds        y          cutoff XGBRegressor LGBMRegressor\n",
       "npartitions=20                                                                    \n",
       "                datetime64[ns]  float64  datetime64[ns]      float32       float64\n",
       "                           ...      ...             ...          ...           ...\n",
       "...                        ...      ...             ...          ...           ...\n",
       "                           ...      ...             ...          ...           ...\n",
       "                           ...      ...             ...          ...           ...\n",
       "Dask Name: concat, 240 tasks"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_windows = 2\n",
    "window_size = 14\n",
    "\n",
    "backtest_results = fcst.cross_validation(partitioned_series, n_windows, window_size)\n",
    "backtest_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fffbd79-f544-4cfc-8b62-ba009165c071",
   "metadata": {},
   "source": [
    "We can aggregate these by date to get a rough estimate of how our model is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a4a701-1779-4007-b9be-dbdee3454bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>XGBRegressor</th>\n",
       "      <th>LGBMRegressor</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ds</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2001-04-17</th>\n",
       "      <td>161.232312</td>\n",
       "      <td>161.527481</td>\n",
       "      <td>162.094722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-04-18</th>\n",
       "      <td>152.139197</td>\n",
       "      <td>151.559235</td>\n",
       "      <td>151.311011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-04-19</th>\n",
       "      <td>169.856989</td>\n",
       "      <td>171.319366</td>\n",
       "      <td>171.463442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-04-20</th>\n",
       "      <td>180.683402</td>\n",
       "      <td>180.790359</td>\n",
       "      <td>180.209600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-04-21</th>\n",
       "      <td>182.006090</td>\n",
       "      <td>181.458237</td>\n",
       "      <td>181.380985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     y  XGBRegressor  LGBMRegressor\n",
       "ds                                                 \n",
       "2001-04-17  161.232312    161.527481     162.094722\n",
       "2001-04-18  152.139197    151.559235     151.311011\n",
       "2001-04-19  169.856989    171.319366     171.463442\n",
       "2001-04-20  180.683402    180.790359     180.209600\n",
       "2001-04-21  182.006090    181.458237     181.380985"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_results = backtest_results.compute().groupby('ds').mean()\n",
    "agg_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da220459-5595-4bd4-901f-4c4f5ee8090c",
   "metadata": {},
   "source": [
    "We can also compute the error for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71efc3c1-f6ba-490f-8d19-9e4587c8e1c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'XGBRegressor': 85.71, 'LGBMRegressor': 91.6}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mse_from_dask_dataframe(ddf):\n",
    "    mses = {}\n",
    "    for model_name in ddf.columns.drop(['ds', 'y', 'cutoff']):\n",
    "        mses[model_name] = (ddf['y'] - ddf[model_name]).pow(2).mean()\n",
    "    return client.gather(client.compute(mses))\n",
    "\n",
    "{k: round(v, 2) for k, v in mse_from_dask_dataframe(backtest_results).items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecb8e54-d964-42b2-9f24-33e7439f5436",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20:14:29] task [xgboost.dask]:tcp://127.0.0.1:39163 got new rank 0\n",
      "[20:14:29] task [xgboost.dask]:tcp://127.0.0.1:41527 got new rank 1\n",
      "[20:14:30] task [xgboost.dask]:tcp://127.0.0.1:39163 got new rank 0\n",
      "[20:14:30] task [xgboost.dask]:tcp://127.0.0.1:41527 got new rank 1\n"
     ]
    }
   ],
   "source": [
    "#|hide\n",
    "fcst = DistributedForecast(XGBForecast(random_state=0), lags=[7, 14])\n",
    "backtest_results = fcst.cross_validation(\n",
    "    non_std_series,\n",
    "    n_windows,\n",
    "    window_size,\n",
    "    id_col='some_id',\n",
    "    time_col='time',\n",
    "    target_col='value',\n",
    "    static_features=['static_0', 'static_1'],    \n",
    ").compute()\n",
    "renamer = {'some_id': 'unique_id', 'time': 'ds', 'value': 'y'}\n",
    "backtest_results = backtest_results.rename(columns=renamer).set_index('unique_id')\n",
    "renamed = non_std_series.rename(columns=renamer).set_index('unique_id')\n",
    "cv_models = fcst.cv_models_\n",
    "manual_results = []\n",
    "for i, (cutoff, train, valid) in enumerate(backtest_splits(renamed, n_windows, window_size, 1)):\n",
    "    fcst.preprocess(train)\n",
    "    fcst.models_ = cv_models[i]\n",
    "    pred = fcst.predict(window_size).compute()\n",
    "    res = valid[['ds', 'y']].compute()\n",
    "    res['cutoff'] = cutoff\n",
    "    res = res.merge(pred, on=['unique_id', 'ds'], how='left')\n",
    "    manual_results.append(res)\n",
    "manual_results = pd.concat(manual_results)\n",
    "pd.testing.assert_frame_equal(backtest_results, manual_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0086c2a-c5b3-4a40-9901-b05139c3d6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
