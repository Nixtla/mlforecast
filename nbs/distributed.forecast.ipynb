{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf5b1d7-bc13-4ed3-af30-f15cacc861f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp distributed.forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503920c4-0e1f-4cef-a7ed-dea1005027ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2921fbf1-b7be-4f3d-b8e6-ce58b49fbd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "import warnings\n",
    "\n",
    "from fastcore.test import test_warns, test_eq, test_ne\n",
    "from nbdev import show_doc\n",
    "from sklearn import set_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bf1328-ca14-4169-8608-31dbb62107d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "warnings.simplefilter('ignore', FutureWarning)\n",
    "set_config(display='text')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc9a984-a442-41d2-8132-471d73e64d96",
   "metadata": {},
   "source": [
    "# DistributedMLForecast\n",
    "\n",
    "> Distributed pipeline encapsulation\n",
    "\n",
    "**This interface is only tested on Linux**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8108c14-6d27-4dfd-b3e8-a2b315eb5f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import copy\n",
    "from collections import namedtuple\n",
    "from typing import Any, Callable, Iterable, List, Optional\n",
    "\n",
    "import cloudpickle\n",
    "try:\n",
    "    import dask.dataframe as dd\n",
    "    DASK_INSTALLED = True\n",
    "except ModuleNotFoundError:\n",
    "    DASK_INSTALLED = False\n",
    "import fugue\n",
    "import fugue.api as fa\n",
    "import pandas as pd\n",
    "try:\n",
    "    from pyspark.ml.feature import VectorAssembler\n",
    "    from pyspark.sql import DataFrame as SparkDataFrame\n",
    "    SPARK_INSTALLED = True\n",
    "except ModuleNotFoundError:\n",
    "    SPARK_INSTALLED = False\n",
    "from sklearn.base import clone\n",
    "\n",
    "from mlforecast.core import (\n",
    "    DateFeature,\n",
    "    Differences,\n",
    "    Freq,\n",
    "    LagTransforms,\n",
    "    Lags,\n",
    "    TimeSeries,\n",
    "    _name_models,\n",
    ")\n",
    "from mlforecast.utils import single_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c505b5b8-2c00-456b-8d61-2301516bc347",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "WindowInfo = namedtuple('WindowInfo', ['n_windows', 'window_size', 'step_size', 'i_window', 'input_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227f19b4-4582-4c51-94cd-c7c220d35dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class DistributedMLForecast:\n",
    "    \"\"\"Multi backend distributed pipeline\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        models,\n",
    "        freq: Optional[Freq] = None,\n",
    "        lags: Optional[Lags] = None,\n",
    "        lag_transforms: Optional[LagTransforms] = None,\n",
    "        date_features: Optional[Iterable[DateFeature]] = None,\n",
    "        differences: Optional[Differences] = None,\n",
    "        num_threads: int = 1,\n",
    "        engine = None,\n",
    "    ):\n",
    "        \"\"\"Create distributed forecast object\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        models : regressor or list of regressors\n",
    "            Models that will be trained and used to compute the forecasts.\n",
    "        freq : str or int, optional (default=None)\n",
    "            Pandas offset alias, e.g. 'D', 'W-THU' or integer denoting the frequency of the series.\n",
    "        lags : list of int, optional (default=None)\n",
    "            Lags of the target to use as features.\n",
    "        lag_transforms : dict of int to list of functions, optional (default=None)\n",
    "            Mapping of target lags to their transformations.\n",
    "        date_features : list of str or callable, optional (default=None)\n",
    "            Features computed from the dates. Can be pandas date attributes or functions that will take the dates as input.\n",
    "        differences : list of int, optional (default=None)\n",
    "            Differences to take of the target before computing the features. These are restored at the forecasting step.\n",
    "        num_threads : int (default=1)\n",
    "            Number of threads to use when computing the features.\n",
    "        engine : fugue execution engine, optional (default=None)\n",
    "            Dask Client, Spark Session, etc to use for the distributed computation.\n",
    "            If None will infer depending on the input type.\n",
    "        \"\"\"        \n",
    "        if not isinstance(models, dict) and not isinstance(models, list):\n",
    "            models = [models]\n",
    "        if isinstance(models, list):\n",
    "            model_names = _name_models([m.__class__.__name__ for m in models])\n",
    "            models_with_names = dict(zip(model_names, models))\n",
    "        else:\n",
    "            models_with_names = models\n",
    "        self.models = models_with_names\n",
    "        self._base_ts = TimeSeries(\n",
    "            freq, lags, lag_transforms, date_features, differences, num_threads\n",
    "        )\n",
    "        self.engine = engine\n",
    "        \n",
    "    def __repr__(self) -> str:\n",
    "        return (\n",
    "            f'{self.__class__.__name__}(models=[{\", \".join(self.models.keys())}], '\n",
    "            f\"freq={self._base_ts.freq}, \"\n",
    "            f\"lag_features={list(self._base_ts.transforms.keys())}, \"\n",
    "            f\"date_features={self._base_ts.date_features}, \"\n",
    "            f\"num_threads={self._base_ts.num_threads}, \"\n",
    "            f\"engine={self.engine})\"\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _preprocess_partition(\n",
    "        part: pd.DataFrame,\n",
    "        base_ts: TimeSeries,        \n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "        window_info: Optional[WindowInfo] = None,\n",
    "        fit_ts_only: bool = False,\n",
    "    ) -> List[List[Any]]:\n",
    "        ts = copy.deepcopy(base_ts)\n",
    "        if fit_ts_only:\n",
    "            ts._fit(\n",
    "                part,\n",
    "                id_col=id_col,\n",
    "                time_col=time_col,\n",
    "                target_col=target_col,\n",
    "                static_features=static_features,\n",
    "                keep_last_n=keep_last_n,                \n",
    "            )\n",
    "            return [[cloudpickle.dumps(ts), cloudpickle.dumps(None), cloudpickle.dumps(None)]]        \n",
    "        if window_info is None:\n",
    "            train = part\n",
    "            valid = None\n",
    "        else:\n",
    "            part = part.set_index(id_col)\n",
    "            max_dates = part.groupby(level=0, observed=True)[time_col].transform('max')\n",
    "            cutoffs, train_mask, valid_mask = single_split(\n",
    "                part,\n",
    "                i_window=window_info.i_window,\n",
    "                n_windows=window_info.n_windows,\n",
    "                window_size=window_info.window_size,\n",
    "                time_col=time_col,\n",
    "                freq=base_ts.freq,\n",
    "                max_dates=max_dates,\n",
    "                step_size=window_info.step_size,\n",
    "                input_size=window_info.input_size,\n",
    "            )\n",
    "            train = part[train_mask]\n",
    "            valid_keep_cols = part.columns\n",
    "            if static_features is not None:\n",
    "                valid_keep_cols.drop(static_features)\n",
    "            valid = part.loc[valid_mask, valid_keep_cols].join(cutoffs)\n",
    "            train = train.reset_index()\n",
    "            valid = valid.reset_index()\n",
    "        transformed = ts.fit_transform(\n",
    "            train,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            static_features=static_features,\n",
    "            dropna=dropna,\n",
    "            keep_last_n=keep_last_n,\n",
    "        )\n",
    "        return [[cloudpickle.dumps(ts), cloudpickle.dumps(transformed), cloudpickle.dumps(valid)]]\n",
    "\n",
    "    @staticmethod\n",
    "    def _retrieve_df(items: List[List[Any]]) -> Iterable[pd.DataFrame]:\n",
    "        for _, serialized_train, _ in items:\n",
    "            yield cloudpickle.loads(serialized_train)\n",
    "            \n",
    "    def _preprocess_partitions(\n",
    "        self,\n",
    "        data: fugue.AnyDataFrame,\n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "        window_info: Optional[WindowInfo] = None,\n",
    "        fit_ts_only: bool = False,\n",
    "    ) -> List[Any]:\n",
    "        return fa.transform(\n",
    "            data,\n",
    "            DistributedMLForecast._preprocess_partition,\n",
    "            params={\n",
    "                'base_ts': self._base_ts,\n",
    "                'id_col': id_col,\n",
    "                'time_col': time_col,\n",
    "                'target_col': target_col,\n",
    "                'static_features': static_features,\n",
    "                'dropna': dropna,\n",
    "                'keep_last_n': keep_last_n,\n",
    "                'window_info': window_info,\n",
    "                'fit_ts_only': fit_ts_only,\n",
    "            },\n",
    "            schema='ts:binary,train:binary,valid:binary',\n",
    "            engine=self.engine,\n",
    "            as_fugue=True,\n",
    "        )        \n",
    "\n",
    "    def _preprocess(\n",
    "        self,\n",
    "        data: fugue.AnyDataFrame,\n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "        window_info: Optional[WindowInfo] = None,\n",
    "    ) -> fugue.AnyDataFrame:\n",
    "        self.id_col = id_col\n",
    "        self.time_col = time_col\n",
    "        self.target_col = target_col\n",
    "        self.static_features = static_features\n",
    "        self.dropna = dropna\n",
    "        self.keep_last_n = keep_last_n\n",
    "        self.partition_results = self._preprocess_partitions(\n",
    "            data=data,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            static_features=static_features,\n",
    "            dropna=dropna,\n",
    "            keep_last_n=keep_last_n,\n",
    "            window_info=window_info,\n",
    "        )\n",
    "        base_schema = str(fa.get_schema(data))\n",
    "        features_schema = ','.join(f'{feat}:double' for feat in self._base_ts.features)\n",
    "        res = fa.transform(\n",
    "            self.partition_results,\n",
    "            DistributedMLForecast._retrieve_df,\n",
    "            schema=f'{base_schema},{features_schema}',\n",
    "            engine=self.engine,\n",
    "        )\n",
    "        return fa.get_native_as_df(res)\n",
    "    \n",
    "    def preprocess(\n",
    "        self,\n",
    "        data: fugue.AnyDataFrame,\n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "    ) -> fugue.AnyDataFrame:\n",
    "        \"\"\"Add the features to `data`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : dask or spark DataFrame.\n",
    "            Series data in long format.\n",
    "        id_col : str\n",
    "            Column that identifies each serie. If 'index' then the index is used.\n",
    "        time_col : str\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str\n",
    "            Column that contains the target.\n",
    "        static_features : list of str, optional (default=None)\n",
    "            Names of the features that are static and will be repeated when forecasting.\n",
    "        dropna : bool (default=True)\n",
    "            Drop rows with missing values produced by the transformations.\n",
    "        keep_last_n : int, optional (default=None)\n",
    "            Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        result : same type as input\n",
    "            data with added features.\n",
    "        \"\"\"        \n",
    "        return self._preprocess(\n",
    "            data,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            static_features=static_features,\n",
    "            dropna=dropna,\n",
    "            keep_last_n=keep_last_n,\n",
    "        )\n",
    "    \n",
    "    def _fit(\n",
    "        self,\n",
    "        data: fugue.AnyDataFrame,\n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "        window_info: Optional[WindowInfo] = None,\n",
    "    ) -> 'DistributedMLForecast':\n",
    "        prep = self._preprocess(\n",
    "            data,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            static_features=static_features,\n",
    "            dropna=dropna,\n",
    "            keep_last_n=keep_last_n,\n",
    "            window_info=window_info,\n",
    "        )\n",
    "        features = [x for x in prep.columns if x not in {id_col, time_col, target_col}]\n",
    "        self.models_ = {}\n",
    "        if SPARK_INSTALLED and isinstance(data, SparkDataFrame):\n",
    "            featurizer = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "            train_data = featurizer.transform(prep)[target_col, \"features\"]\n",
    "            for name, model in self.models.items():\n",
    "                trained_model = model._pre_fit(target_col).fit(train_data)\n",
    "                self.models_[name] = model.extract_local_model(trained_model)\n",
    "        elif DASK_INSTALLED and isinstance(data, dd.DataFrame):\n",
    "            X, y = prep[features], prep[target_col]\n",
    "            for name, model in self.models.items():\n",
    "                trained_model = clone(model).fit(X, y)\n",
    "                self.models_[name] = trained_model.model_\n",
    "        else:\n",
    "            raise NotImplementedError('Only spark and dask engines are supported.')\n",
    "        return self\n",
    "    \n",
    "    def fit(\n",
    "        self,\n",
    "        data: fugue.AnyDataFrame,\n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,        \n",
    "    ) -> 'DistributedMLForecast':\n",
    "        \"\"\"Apply the feature engineering and train the models.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : dask or spark DataFrame\n",
    "            Series data in long format.\n",
    "        id_col : str\n",
    "            Column that identifies each serie. If 'index' then the index is used.\n",
    "        time_col : str\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str\n",
    "            Column that contains the target.\n",
    "        static_features : list of str, optional (default=None)\n",
    "            Names of the features that are static and will be repeated when forecasting.\n",
    "        dropna : bool (default=True)\n",
    "            Drop rows with missing values produced by the transformations.\n",
    "        keep_last_n : int, optional (default=None)\n",
    "            Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : DistributedMLForecast\n",
    "            Forecast object with series values and trained models.\n",
    "        \"\"\"        \n",
    "        return self._fit(\n",
    "            data,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            static_features=static_features,\n",
    "            dropna=dropna,\n",
    "            keep_last_n=keep_last_n,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _predict(\n",
    "        items: List[List[Any]],\n",
    "        models,\n",
    "        horizon,\n",
    "        dynamic_dfs=None,\n",
    "        before_predict_callback=None,\n",
    "        after_predict_callback=None,\n",
    "    ) -> Iterable[pd.DataFrame]:\n",
    "        for serialized_ts, _, serialized_valid in items:\n",
    "            valid = cloudpickle.loads(serialized_valid)\n",
    "            ts = cloudpickle.loads(serialized_ts)\n",
    "            if valid is not None:\n",
    "                dynamic_features = valid.columns.drop(\n",
    "                    [ts.id_col, ts.time_col, ts.target_col]\n",
    "                )\n",
    "                if not dynamic_features.empty:\n",
    "                    dynamic_dfs = [valid.drop(columns=ts.target_col)]\n",
    "            res = ts.predict(\n",
    "                models=models,\n",
    "                horizon=horizon,\n",
    "                dynamic_dfs=dynamic_dfs,\n",
    "                before_predict_callback=before_predict_callback,\n",
    "                after_predict_callback=after_predict_callback,\n",
    "            ).reset_index()\n",
    "            if valid is not None:\n",
    "                res = res.merge(valid, how='left')\n",
    "            yield res\n",
    "            \n",
    "    def _get_predict_schema(self) -> str:\n",
    "        model_names = self.models.keys()\n",
    "        models_schema = ','.join(f'{model_name}:double' for model_name in model_names)\n",
    "        schema = f'{self.id_col}:string,{self.time_col}:datetime,' + models_schema\n",
    "        return schema\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        horizon: int,\n",
    "        dynamic_dfs: Optional[List[pd.DataFrame]] = None,\n",
    "        before_predict_callback: Optional[Callable] = None,\n",
    "        after_predict_callback: Optional[Callable] = None,\n",
    "        new_data: Optional[fugue.AnyDataFrame] = None,\n",
    "    ) -> fugue.AnyDataFrame:\n",
    "        \"\"\"Compute the predictions for the next `horizon` steps.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        horizon : int\n",
    "            Number of periods to predict.\n",
    "        dynamic_dfs : list of pandas DataFrame, optional (default=None)\n",
    "            Future values of the dynamic features, e.g. prices.\n",
    "        before_predict_callback : callable, optional (default=None)\n",
    "            Function to call on the features before computing the predictions.\n",
    "                This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure.\n",
    "                The series identifier is on the index.\n",
    "        after_predict_callback : callable, optional (default=None)\n",
    "            Function to call on the predictions before updating the targets.\n",
    "                This function will take a pandas Series with the predictions and should return another one with the same structure.\n",
    "                The series identifier is on the index.\n",
    "        new_data : dask or spark DataFrame, optional (default=None)\n",
    "            Series data of new observations for which forecasts are to be generated.\n",
    "                This dataframe should have the same structure as the one used to fit the model, including any features and time series data.\n",
    "                If `new_data` is not None, the method will generate forecasts for the new observations.                \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        result : dask or spark DataFrame\n",
    "            Predictions for each serie and timestep, with one column per model.\n",
    "        \"\"\"        \n",
    "        if new_data is not None:\n",
    "            partition_results = self._preprocess_partitions(\n",
    "                data=new_data,\n",
    "                id_col=self.id_col,\n",
    "                time_col=self.time_col,\n",
    "                target_col=self.target_col,\n",
    "                static_features=self.static_features,\n",
    "                dropna=self.dropna,\n",
    "                keep_last_n=self.keep_last_n,\n",
    "                fit_ts_only=True,\n",
    "            )\n",
    "        else:\n",
    "            partition_results = self.partition_results\n",
    "        schema = self._get_predict_schema()\n",
    "        res = fa.transform(\n",
    "            partition_results,\n",
    "            DistributedMLForecast._predict,\n",
    "            params={\n",
    "                'models': self.models_,\n",
    "                'horizon': horizon,\n",
    "                'dynamic_dfs': dynamic_dfs,\n",
    "                'before_predict_callback': before_predict_callback,\n",
    "                'after_predict_callback': after_predict_callback,\n",
    "            },\n",
    "            schema=schema,\n",
    "            engine=self.engine,\n",
    "        )\n",
    "        return fa.get_native_as_df(res)\n",
    "\n",
    "    def cross_validation(\n",
    "        self,\n",
    "        data: fugue.AnyDataFrame,\n",
    "        n_windows: int,\n",
    "        window_size: int,\n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,  \n",
    "        step_size: Optional[int] = None,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "        refit: bool = True,\n",
    "        before_predict_callback: Optional[Callable] = None,\n",
    "        after_predict_callback: Optional[Callable] = None,\n",
    "        input_size: Optional[int] = None,        \n",
    "    ) -> fugue.AnyDataFrame:\n",
    "        \"\"\"Perform time series cross validation.\n",
    "        Creates `n_windows` splits where each window has `window_size` test periods,\n",
    "        trains the models, computes the predictions and merges the actuals.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : dask DataFrame\n",
    "            Series data in long format.\n",
    "        n_windows : int\n",
    "            Number of windows to evaluate.\n",
    "        window_size : int\n",
    "            Number of test periods in each window.\n",
    "        id_col : str\n",
    "            Column that identifies each serie. If 'index' then the index is used.\n",
    "        time_col : str\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str\n",
    "            Column that contains the target.        \n",
    "        step_size : int, optional (default=None)\n",
    "            Step size between each cross validation window. If None it will be equal to `window_size`.\n",
    "        static_features : list of str, optional (default=None)\n",
    "            Names of the features that are static and will be repeated when forecasting.\n",
    "        dropna : bool (default=True)\n",
    "            Drop rows with missing values produced by the transformations.\n",
    "        keep_last_n : int, optional (default=None)\n",
    "            Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it.\n",
    "        refit : bool (default=True)\n",
    "            Retrain model for each cross validation window.\n",
    "            If False, the models are trained at the beginning and then used to predict each window.            \n",
    "        before_predict_callback : callable, optional (default=None)\n",
    "            Function to call on the features before computing the predictions.\n",
    "                This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure.\n",
    "                The series identifier is on the index.\n",
    "        after_predict_callback : callable, optional (default=None)\n",
    "            Function to call on the predictions before updating the targets.\n",
    "                This function will take a pandas Series with the predictions and should return another one with the same structure.\n",
    "                The series identifier is on the index.\n",
    "        input_size : int, optional (default=None)\n",
    "            Maximum training samples per serie in each window. If None, will use an expanding window.                \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        result : dask or spark DataFrame\n",
    "            Predictions for each window with the series id, timestamp, target value and predictions from each model.\n",
    "        \"\"\"            \n",
    "        self.cv_models_ = []\n",
    "        results = []\n",
    "        for i in range(n_windows):\n",
    "            window_info = WindowInfo(n_windows, window_size, step_size, i, input_size)            \n",
    "            if refit or i == 0:\n",
    "                self._fit(\n",
    "                    data,\n",
    "                    id_col=id_col,\n",
    "                    time_col=time_col,\n",
    "                    target_col=target_col,\n",
    "                    static_features=static_features,\n",
    "                    dropna=dropna,\n",
    "                    keep_last_n=keep_last_n,\n",
    "                    window_info=window_info,\n",
    "                )\n",
    "                self.cv_models_.append(self.models_)\n",
    "                partition_results = self.partition_results\n",
    "            elif not refit:\n",
    "                partition_results = self._preprocess_partitions(\n",
    "                    data=data,\n",
    "                    id_col=id_col,\n",
    "                    time_col=time_col,\n",
    "                    target_col=target_col,\n",
    "                    static_features=static_features,\n",
    "                    dropna=dropna,\n",
    "                    keep_last_n=keep_last_n,\n",
    "                    window_info=window_info,\n",
    "                )\n",
    "            schema = self._get_predict_schema() + f',cutoff:datetime,{self.target_col}:double'\n",
    "            preds = fa.transform(\n",
    "                partition_results,\n",
    "                DistributedMLForecast._predict,\n",
    "                params={\n",
    "                    'models': self.models_,\n",
    "                    'horizon': window_size,\n",
    "                    'before_predict_callback': before_predict_callback,\n",
    "                    'after_predict_callback': after_predict_callback,\n",
    "                },\n",
    "                schema=schema,\n",
    "                engine=self.engine,\n",
    "            )\n",
    "            results.append(fa.get_native_as_df(preds))\n",
    "        if len(results) == 1:\n",
    "            return results[0]\n",
    "        if len(results) == 2:\n",
    "            return fa.union(results[0], results[1])\n",
    "        return fa.union(results[0], results[1], results[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18b1388-3c79-4d1c-98cf-d748d64119e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### DistributedMLForecast\n",
       "\n",
       ">      DistributedMLForecast (models,\n",
       ">                             freq:Union[int,str,pandas._libs.tslibs.offsets.Bas\n",
       ">                             eOffset,NoneType]=None,\n",
       ">                             lags:Optional[Iterable[int]]=None, lag_transforms:\n",
       ">                             Optional[Dict[int,List[Union[Callable,Tuple[Callab\n",
       ">                             le,Any]]]]]=None, date_features:Optional[Iterable[\n",
       ">                             Union[str,Callable]]]=None,\n",
       ">                             differences:Optional[Iterable[int]]=None,\n",
       ">                             num_threads:int=1, engine=None)\n",
       "\n",
       "Multi backend distributed pipeline"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### DistributedMLForecast\n",
       "\n",
       ">      DistributedMLForecast (models,\n",
       ">                             freq:Union[int,str,pandas._libs.tslibs.offsets.Bas\n",
       ">                             eOffset,NoneType]=None,\n",
       ">                             lags:Optional[Iterable[int]]=None, lag_transforms:\n",
       ">                             Optional[Dict[int,List[Union[Callable,Tuple[Callab\n",
       ">                             le,Any]]]]]=None, date_features:Optional[Iterable[\n",
       ">                             Union[str,Callable]]]=None,\n",
       ">                             differences:Optional[Iterable[int]]=None,\n",
       ">                             num_threads:int=1, engine=None)\n",
       "\n",
       "Multi backend distributed pipeline"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DistributedMLForecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ec88ed-1306-4a4b-bca4-0a254ada08a5",
   "metadata": {},
   "source": [
    "The `DistributedMLForecast` class is a high level abstraction that encapsulates all the steps in the pipeline (preprocessing, fitting the model and computing predictions) and applies them in a distributed way.\n",
    "\n",
    "The different things that you need to use `DistributedMLForecast` (as opposed to `MLForecast`) are:\n",
    "\n",
    "1. You need to set up a cluster. We currently support dask and spark (ray is on the roadmap).\n",
    "2. Your data needs to be a distributed collection. We currently support dask and spark dataframes.\n",
    "3. You need to use a model that implements distributed training in your framework of choice, e.g. SynapseML for LightGBM in spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbda5c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dask.distributed import Client\n",
    "from window_ops.expanding import expanding_mean\n",
    "from window_ops.rolling import rolling_mean\n",
    "\n",
    "from mlforecast.utils import backtest_splits, generate_daily_series, generate_prices_for_series\n",
    "from mlforecast.distributed.models.dask.lgb import DaskLGBMForecast\n",
    "from mlforecast.distributed.models.dask.xgb import DaskXGBForecast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73eb8f10-1b95-453a-8f1c-91f9c1813af1",
   "metadata": {},
   "source": [
    "## Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92b13a2-f6bd-4d76-840d-cadb1d672147",
   "metadata": {},
   "source": [
    "### Client setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9fe50e-1b4a-4f58-8bbf-d1c087fb7d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(n_workers=2, threads_per_worker=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e838b0d-a0f3-4672-a794-75301a4aced3",
   "metadata": {},
   "source": [
    "Here we define a client that connects to a `dask.distributed.LocalCluster`, however it could be any other kind of cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f15a4a-79d2-4b72-8b9b-290edefe340f",
   "metadata": {},
   "source": [
    "### Data setup\n",
    "\n",
    "For dask, the data must be a `dask.dataframe.DataFrame`. You need to make sure that each time serie is only in one partition and it is recommended that you have as many partitions as you have workers. If you have more partitions than workers make sure to set `num_threads=1` to avoid having nested parallelism.\n",
    "\n",
    "The required input format is the same as for `MLForecast`, except that it's a `dask.dataframe.DataFrame` instead of a `pandas.Dataframe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552cea5f-b826-4c9b-ae9f-2532f92f31bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>static_0</th>\n",
       "      <th>static_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=10</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>object</td>\n",
       "      <td>datetime64[ns]</td>\n",
       "      <td>float64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_10</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_89</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_99</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: assign, 5 graph layers</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "               unique_id              ds        y static_0 static_1\n",
       "npartitions=10                                                     \n",
       "id_00             object  datetime64[ns]  float64    int64    int64\n",
       "id_10                ...             ...      ...      ...      ...\n",
       "...                  ...             ...      ...      ...      ...\n",
       "id_89                ...             ...      ...      ...      ...\n",
       "id_99                ...             ...      ...      ...      ...\n",
       "Dask Name: assign, 5 graph layers"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series = generate_daily_series(100, n_static_features=2, equal_ends=True, static_as_categorical=False)\n",
    "partitioned_series = dd.from_pandas(series, npartitions=10).map_partitions(lambda df: df.reset_index())\n",
    "partitioned_series['unique_id'] = partitioned_series['unique_id'].astype(str)\n",
    "partitioned_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf40d477-eddd-4c4d-b6b1-57fd4bd7c58f",
   "metadata": {},
   "source": [
    "### Models\n",
    "In order to perform distributed forecasting, we need to use a model that is able to train in a distributed way using `dask`. The current implementations are in `DaskLGBMForecast` and `DaskXGBForecast` which are just wrappers around the native implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e3eabd-f9d3-4d7e-b1e8-edccf8a27f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [DaskXGBForecast(random_state=0), DaskLGBMForecast(random_state=0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfb3968-0ee4-4141-ad7a-ee202dd8c044",
   "metadata": {},
   "source": [
    "### Training\n",
    "Once we have our models we instantiate a `DistributedMLForecast` object defining our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f56da13-0e3e-43d8-897d-af44fb89b037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistributedMLForecast(models=[DaskXGBForecast, DaskLGBMForecast], freq=<Day>, lag_features=['lag7', 'expanding_mean_lag1', 'rolling_mean_lag7_window_size14'], date_features=['dayofweek', 'month'], num_threads=1, engine=<Client: 'tcp://127.0.0.1:37041' processes=2 threads=2, memory=15.50 GiB>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcst = DistributedMLForecast(\n",
    "    models=models,\n",
    "    freq='D',\n",
    "    lags=[7],\n",
    "    lag_transforms={\n",
    "        1: [expanding_mean],\n",
    "        7: [(rolling_mean, 14)]\n",
    "    },\n",
    "    date_features=['dayofweek', 'month'],\n",
    "    num_threads=1,\n",
    "    engine=client,\n",
    ")\n",
    "fcst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67af6771-3484-4b54-82b1-ef6f284bb289",
   "metadata": {},
   "source": [
    "Here where we say that:\n",
    "\n",
    "* Our series have daily frequency.\n",
    "* We want to use lag 7 as a feature\n",
    "* We want the lag transformations to be:\n",
    "   * expanding mean of the lag 1\n",
    "   * rolling mean of the lag 7 over a window of size 14\n",
    "* We want to use dayofweek and month as date features.\n",
    "* We want to perform the preprocessing and the forecasting steps using 1 thread, because we have 10 partitions and 2 workers.\n",
    "\n",
    "From this point we have two options:\n",
    "\n",
    "1. Compute the features and fit our models.\n",
    "2. Compute the features and get them back as a dataframe to do some custom splitting or adding additional features, then training the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ea6591-d9bd-4273-8920-c40f678e2f2c",
   "metadata": {},
   "source": [
    "### 1. Using all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3201af6c-4915-4447-8bca-01fe9a1f7cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "## DistributedMLForecast.fit\n",
       "\n",
       ">      DistributedMLForecast.fit (data:~AnyDataFrame, id_col:str, time_col:str,\n",
       ">                                 target_col:str,\n",
       ">                                 static_features:Optional[List[str]]=None,\n",
       ">                                 dropna:bool=True,\n",
       ">                                 keep_last_n:Optional[int]=None)\n",
       "\n",
       "Apply the feature engineering and train the models.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| data | AnyDataFrame |  | Series data in long format. |\n",
       "| id_col | str |  | Column that identifies each serie. If 'index' then the index is used. |\n",
       "| time_col | str |  | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str |  | Column that contains the target. |\n",
       "| static_features | Optional | None | Names of the features that are static and will be repeated when forecasting. |\n",
       "| dropna | bool | True | Drop rows with missing values produced by the transformations. |\n",
       "| keep_last_n | Optional | None | Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it. |\n",
       "| **Returns** | **DistributedMLForecast** |  | **Forecast object with series values and trained models.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "## DistributedMLForecast.fit\n",
       "\n",
       ">      DistributedMLForecast.fit (data:~AnyDataFrame, id_col:str, time_col:str,\n",
       ">                                 target_col:str,\n",
       ">                                 static_features:Optional[List[str]]=None,\n",
       ">                                 dropna:bool=True,\n",
       ">                                 keep_last_n:Optional[int]=None)\n",
       "\n",
       "Apply the feature engineering and train the models.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| data | AnyDataFrame |  | Series data in long format. |\n",
       "| id_col | str |  | Column that identifies each serie. If 'index' then the index is used. |\n",
       "| time_col | str |  | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str |  | Column that contains the target. |\n",
       "| static_features | Optional | None | Names of the features that are static and will be repeated when forecasting. |\n",
       "| dropna | bool | True | Drop rows with missing values produced by the transformations. |\n",
       "| keep_last_n | Optional | None | Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it. |\n",
       "| **Returns** | **DistributedMLForecast** |  | **Forecast object with series values and trained models.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DistributedMLForecast.fit, title_level=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa359fc-e73b-4178-9421-848742b027ae",
   "metadata": {},
   "source": [
    "Calling `fit` on our data computes the features independently for each partition and performs distributed training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bf69ca-1723-43ef-a527-239aed06bd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst.fit(partitioned_series, id_col='unique_id', time_col='ds', target_col='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06df485-8496-45da-87e0-e45cf9b1f9ea",
   "metadata": {},
   "source": [
    "### Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc8f139-ae37-4927-8449-ed949dc254db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "## DistributedMLForecast.predict\n",
       "\n",
       ">      DistributedMLForecast.predict (horizon:int,\n",
       ">                                     dynamic_dfs:Optional[List[pandas.core.fram\n",
       ">                                     e.DataFrame]]=None, before_predict_callbac\n",
       ">                                     k:Optional[Callable]=None, after_predict_c\n",
       ">                                     allback:Optional[Callable]=None,\n",
       ">                                     new_data:Optional[~AnyDataFrame]=None)\n",
       "\n",
       "Compute the predictions for the next `horizon` steps.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| horizon | int |  | Number of periods to predict. |\n",
       "| dynamic_dfs | Optional | None | Future values of the dynamic features, e.g. prices. |\n",
       "| before_predict_callback | Optional | None | Function to call on the features before computing the predictions.<br>    This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure.<br>    The series identifier is on the index. |\n",
       "| after_predict_callback | Optional | None | Function to call on the predictions before updating the targets.<br>    This function will take a pandas Series with the predictions and should return another one with the same structure.<br>    The series identifier is on the index. |\n",
       "| new_data | Optional | None | Series data of new observations for which forecasts are to be generated.<br>    This dataframe should have the same structure as the one used to fit the model, including any features and time series data.<br>    If `new_data` is not None, the method will generate forecasts for the new observations.                 |\n",
       "| **Returns** | **AnyDataFrame** |  | **Predictions for each serie and timestep, with one column per model.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "## DistributedMLForecast.predict\n",
       "\n",
       ">      DistributedMLForecast.predict (horizon:int,\n",
       ">                                     dynamic_dfs:Optional[List[pandas.core.fram\n",
       ">                                     e.DataFrame]]=None, before_predict_callbac\n",
       ">                                     k:Optional[Callable]=None, after_predict_c\n",
       ">                                     allback:Optional[Callable]=None,\n",
       ">                                     new_data:Optional[~AnyDataFrame]=None)\n",
       "\n",
       "Compute the predictions for the next `horizon` steps.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| horizon | int |  | Number of periods to predict. |\n",
       "| dynamic_dfs | Optional | None | Future values of the dynamic features, e.g. prices. |\n",
       "| before_predict_callback | Optional | None | Function to call on the features before computing the predictions.<br>    This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure.<br>    The series identifier is on the index. |\n",
       "| after_predict_callback | Optional | None | Function to call on the predictions before updating the targets.<br>    This function will take a pandas Series with the predictions and should return another one with the same structure.<br>    The series identifier is on the index. |\n",
       "| new_data | Optional | None | Series data of new observations for which forecasts are to be generated.<br>    This dataframe should have the same structure as the one used to fit the model, including any features and time series data.<br>    If `new_data` is not None, the method will generate forecasts for the new observations.                 |\n",
       "| **Returns** | **AnyDataFrame** |  | **Predictions for each serie and timestep, with one column per model.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DistributedMLForecast.predict, title_level=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8162b4-4dc7-48cd-8a0e-f59f36825f1e",
   "metadata": {},
   "source": [
    "Once we have our fitted models we can compute the predictions for the next 7 timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694a1c30-40d0-4552-ab79-73eaf6179b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>DaskXGBForecast</th>\n",
       "      <th>DaskLGBMForecast</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=10</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>object</td>\n",
       "      <td>datetime64[ns]</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_10</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_89</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_99</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: map, 17 graph layers</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "               unique_id              ds DaskXGBForecast DaskLGBMForecast\n",
       "npartitions=10                                                           \n",
       "id_00             object  datetime64[ns]         float64          float64\n",
       "id_10                ...             ...             ...              ...\n",
       "...                  ...             ...             ...              ...\n",
       "id_89                ...             ...             ...              ...\n",
       "id_99                ...             ...             ...              ...\n",
       "Dask Name: map, 17 graph layers"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = fcst.predict(7)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba35d44-0916-4fb0-9a2d-ee98efbbe931",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "preds = preds.compute()\n",
    "preds2 = fcst.predict(7).compute()\n",
    "preds3 = fcst.predict(7, new_data=partitioned_series).compute()\n",
    "pd.testing.assert_frame_equal(preds, preds2)\n",
    "pd.testing.assert_frame_equal(preds, preds3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d85e0d-b167-43a2-a480-708e20aba44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "non_std_series = partitioned_series.copy()\n",
    "non_std_series['ds'] = non_std_series.map_partitions(lambda part: part.groupby('unique_id').cumcount())\n",
    "non_std_series = non_std_series.rename(columns={'ds': 'time', 'y': 'value', 'unique_id': 'some_id'})\n",
    "flow_params = dict(\n",
    "    models=[DaskXGBForecast(random_state=0)],\n",
    "    lags=[7],\n",
    "    lag_transforms={\n",
    "        1: [expanding_mean],\n",
    "        7: [(rolling_mean, 14)]\n",
    "    },\n",
    "    num_threads=1,\n",
    ")\n",
    "fcst = DistributedMLForecast(freq='D', **flow_params)\n",
    "fcst.fit(partitioned_series, id_col='unique_id', time_col='ds', target_col='y')\n",
    "preds = fcst.predict(7).compute()\n",
    "fcst2 = DistributedMLForecast(**flow_params)\n",
    "fcst2.preprocess(non_std_series, id_col='some_id', time_col='time', target_col='value')\n",
    "fcst2.models_ = fcst.models_  # distributed training can end up with different fits\n",
    "non_std_preds = fcst2.predict(7).compute()\n",
    "pd.testing.assert_frame_equal(\n",
    "    preds.drop(columns='ds'),\n",
    "    non_std_preds.drop(columns='time').rename(columns={'some_id': 'unique_id'})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6438efd-365c-444f-a447-03f44f1cc61a",
   "metadata": {},
   "source": [
    "### 2. Preprocess and train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b989f8-945e-4864-854d-92e846fe240d",
   "metadata": {},
   "source": [
    "If we only want to perform the preprocessing step we call `preprocess` with our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d825fee7-6b8f-4613-93b0-e5769a7abe39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "## DistributedMLForecast.preprocess\n",
       "\n",
       ">      DistributedMLForecast.preprocess (data:~AnyDataFrame, id_col:str,\n",
       ">                                        time_col:str, target_col:str, static_fe\n",
       ">                                        atures:Optional[List[str]]=None,\n",
       ">                                        dropna:bool=True,\n",
       ">                                        keep_last_n:Optional[int]=None)\n",
       "\n",
       "Add the features to `data`.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| data | AnyDataFrame |  | Series data in long format. |\n",
       "| id_col | str |  | Column that identifies each serie. If 'index' then the index is used. |\n",
       "| time_col | str |  | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str |  | Column that contains the target. |\n",
       "| static_features | Optional | None | Names of the features that are static and will be repeated when forecasting. |\n",
       "| dropna | bool | True | Drop rows with missing values produced by the transformations. |\n",
       "| keep_last_n | Optional | None | Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it. |\n",
       "| **Returns** | **AnyDataFrame** |  | **data with added features.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "## DistributedMLForecast.preprocess\n",
       "\n",
       ">      DistributedMLForecast.preprocess (data:~AnyDataFrame, id_col:str,\n",
       ">                                        time_col:str, target_col:str, static_fe\n",
       ">                                        atures:Optional[List[str]]=None,\n",
       ">                                        dropna:bool=True,\n",
       ">                                        keep_last_n:Optional[int]=None)\n",
       "\n",
       "Add the features to `data`.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| data | AnyDataFrame |  | Series data in long format. |\n",
       "| id_col | str |  | Column that identifies each serie. If 'index' then the index is used. |\n",
       "| time_col | str |  | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str |  | Column that contains the target. |\n",
       "| static_features | Optional | None | Names of the features that are static and will be repeated when forecasting. |\n",
       "| dropna | bool | True | Drop rows with missing values produced by the transformations. |\n",
       "| keep_last_n | Optional | None | Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it. |\n",
       "| **Returns** | **AnyDataFrame** |  | **data with added features.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DistributedMLForecast.preprocess, title_level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbd1a34-f3ad-4aca-8d94-62a2bc13299e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>static_0</th>\n",
       "      <th>static_1</th>\n",
       "      <th>lag7</th>\n",
       "      <th>expanding_mean_lag1</th>\n",
       "      <th>rolling_mean_lag7_window_size14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-10-25</td>\n",
       "      <td>49.766844</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>50.694639</td>\n",
       "      <td>25.001367</td>\n",
       "      <td>26.320060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-10-26</td>\n",
       "      <td>3.918347</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>3.887780</td>\n",
       "      <td>26.180675</td>\n",
       "      <td>26.313387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-10-27</td>\n",
       "      <td>9.437778</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>11.512774</td>\n",
       "      <td>25.168751</td>\n",
       "      <td>26.398056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-10-28</td>\n",
       "      <td>17.923574</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>18.038498</td>\n",
       "      <td>24.484796</td>\n",
       "      <td>26.425272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-10-29</td>\n",
       "      <td>26.754645</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>24.222859</td>\n",
       "      <td>24.211411</td>\n",
       "      <td>26.305563</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   unique_id         ds          y  static_0  static_1       lag7  \\\n",
       "20     id_00 2000-10-25  49.766844        79        45  50.694639   \n",
       "21     id_00 2000-10-26   3.918347        79        45   3.887780   \n",
       "22     id_00 2000-10-27   9.437778        79        45  11.512774   \n",
       "23     id_00 2000-10-28  17.923574        79        45  18.038498   \n",
       "24     id_00 2000-10-29  26.754645        79        45  24.222859   \n",
       "\n",
       "    expanding_mean_lag1  rolling_mean_lag7_window_size14  \n",
       "20            25.001367                        26.320060  \n",
       "21            26.180675                        26.313387  \n",
       "22            25.168751                        26.398056  \n",
       "23            24.484796                        26.425272  \n",
       "24            24.211411                        26.305563  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_ddf = fcst.preprocess(partitioned_series, id_col='unique_id', time_col='ds', target_col='y')\n",
    "features_ddf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15a97f1-7285-48fc-a016-6b8076c154ec",
   "metadata": {},
   "source": [
    "This is useful if we want to inspect the data the model will be trained. If we do this we must manually train our models and add a local version of them to the `models_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece0fb49-c02c-4336-af7c-e007bd19aaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = features_ddf.drop(columns=['unique_id', 'ds', 'y']), features_ddf['y']\n",
    "model = DaskXGBForecast(random_state=0).fit(X, y)\n",
    "fcst.models_ = {'DaskXGBForecast': model.model_}\n",
    "fcst.predict(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0380885-e914-4caf-ad85-7d9d1e6c0b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "fcst.models_ = fcst2.models_\n",
    "preds2 = fcst.predict(7).compute()\n",
    "pd.testing.assert_frame_equal(preds, preds2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e6f46c-8222-4cef-8955-a2b9a80a4735",
   "metadata": {},
   "source": [
    "### Dynamic features\n",
    "By default the predict method repeats the static features and updates the transformations and the date features. If you have dynamic features like prices or a calendar with holidays you can pass them as a list to the `dynamic_dfs` argument of `DistributedMLForecast.predict`, which will call `pd.DataFrame.merge` on each of them in order.\n",
    "\n",
    "Here's an example:\n",
    "\n",
    "Suppose that we have a `product_id` column and we have a catalog for prices based on that `product_id` and the date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a3b1df-530e-4673-b500-e26ffd378914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>product_id</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-06-09</td>\n",
       "      <td>1</td>\n",
       "      <td>0.548814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-06-10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.715189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-06-11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.602763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-06-12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.544883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-06-13</td>\n",
       "      <td>1</td>\n",
       "      <td>0.423655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20180</th>\n",
       "      <td>2001-05-17</td>\n",
       "      <td>99</td>\n",
       "      <td>0.223520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20181</th>\n",
       "      <td>2001-05-18</td>\n",
       "      <td>99</td>\n",
       "      <td>0.446104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20182</th>\n",
       "      <td>2001-05-19</td>\n",
       "      <td>99</td>\n",
       "      <td>0.044783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20183</th>\n",
       "      <td>2001-05-20</td>\n",
       "      <td>99</td>\n",
       "      <td>0.483216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20184</th>\n",
       "      <td>2001-05-21</td>\n",
       "      <td>99</td>\n",
       "      <td>0.799660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20185 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ds  product_id     price\n",
       "0     2000-06-09           1  0.548814\n",
       "1     2000-06-10           1  0.715189\n",
       "2     2000-06-11           1  0.602763\n",
       "3     2000-06-12           1  0.544883\n",
       "4     2000-06-13           1  0.423655\n",
       "...          ...         ...       ...\n",
       "20180 2001-05-17          99  0.223520\n",
       "20181 2001-05-18          99  0.446104\n",
       "20182 2001-05-19          99  0.044783\n",
       "20183 2001-05-20          99  0.483216\n",
       "20184 2001-05-21          99  0.799660\n",
       "\n",
       "[20185 rows x 3 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynamic_series = series.rename(columns={'static_1': 'product_id'})\n",
    "prices_catalog = generate_prices_for_series(dynamic_series)\n",
    "prices_catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5aec529-856e-4028-b8aa-4347eefd422f",
   "metadata": {},
   "source": [
    "And you have already merged these prices into your series dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0557a5a8-bfad-4c6e-a0a5-a3a2181d6c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>static_0</th>\n",
       "      <th>product_id</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-10-05</td>\n",
       "      <td>3.981198</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>0.570826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-10-06</td>\n",
       "      <td>10.327401</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>0.260562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-10-07</td>\n",
       "      <td>17.657474</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>0.274048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-10-08</td>\n",
       "      <td>25.898790</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>0.433878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-10-09</td>\n",
       "      <td>34.494040</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>0.653738</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  unique_id         ds          y  static_0  product_id     price\n",
       "0     id_00 2000-10-05   3.981198        79          45  0.570826\n",
       "1     id_00 2000-10-06  10.327401        79          45  0.260562\n",
       "2     id_00 2000-10-07  17.657474        79          45  0.274048\n",
       "3     id_00 2000-10-08  25.898790        79          45  0.433878\n",
       "4     id_00 2000-10-09  34.494040        79          45  0.653738"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynamic_series = partitioned_series.rename(columns={'static_1': 'product_id'})\n",
    "dynamic_series = dynamic_series\n",
    "series_with_prices = dynamic_series.merge(prices_catalog, how='left')\n",
    "series_with_prices.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951f5066-3685-43d0-91a6-09867790ab35",
   "metadata": {},
   "source": [
    "This dataframe will be passed to `DistributedMLForecast.fit` (or `DistributedMLForecast.preprocess`), however since the price is dynamic we have to tell that method that only `static_0` and `product_id` are static and we'll have to update `price` in every timestep, which basically involves merging the updated features with the prices catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ca112d-cbdb-41dc-88b4-fad613cb8b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst = DistributedMLForecast(\n",
    "    models,\n",
    "    freq='D',\n",
    "    lags=[7],\n",
    "    lag_transforms={\n",
    "        1: [expanding_mean],\n",
    "        7: [(rolling_mean, 14)]\n",
    "    },\n",
    "    date_features=['dayofweek', 'month'],\n",
    "    num_threads=1,\n",
    ")\n",
    "series_with_prices = series_with_prices\n",
    "fcst.fit(\n",
    "    series_with_prices,\n",
    "    id_col='unique_id',\n",
    "    time_col='ds',\n",
    "    target_col='y',\n",
    "    static_features=['static_0', 'product_id'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4191f9c1-d2cd-4896-9ff2-18c078cfc503",
   "metadata": {},
   "source": [
    "So in order to update the price in each timestep we just call `DistributedForecast.predict` with our forecast horizon and pass the prices catalog as a dynamic dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c4e580-73af-4b0f-943a-32bc3838cbb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>DaskXGBForecast</th>\n",
       "      <th>DaskLGBMForecast</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-15</td>\n",
       "      <td>42.592770</td>\n",
       "      <td>42.697789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-16</td>\n",
       "      <td>49.931698</td>\n",
       "      <td>50.436595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-17</td>\n",
       "      <td>1.919432</td>\n",
       "      <td>1.905254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-18</td>\n",
       "      <td>10.217001</td>\n",
       "      <td>10.293288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-19</td>\n",
       "      <td>18.384094</td>\n",
       "      <td>18.445365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-17</td>\n",
       "      <td>43.538052</td>\n",
       "      <td>44.447302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-18</td>\n",
       "      <td>2.083438</td>\n",
       "      <td>1.968955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-19</td>\n",
       "      <td>9.000432</td>\n",
       "      <td>9.276742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-20</td>\n",
       "      <td>15.026851</td>\n",
       "      <td>15.237301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-21</td>\n",
       "      <td>22.900578</td>\n",
       "      <td>23.007710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>700 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   unique_id         ds  DaskXGBForecast  DaskLGBMForecast\n",
       "0      id_00 2001-05-15        42.592770         42.697789\n",
       "1      id_00 2001-05-16        49.931698         50.436595\n",
       "2      id_00 2001-05-17         1.919432          1.905254\n",
       "3      id_00 2001-05-18        10.217001         10.293288\n",
       "4      id_00 2001-05-19        18.384094         18.445365\n",
       "..       ...        ...              ...               ...\n",
       "72     id_99 2001-05-17        43.538052         44.447302\n",
       "73     id_99 2001-05-18         2.083438          1.968955\n",
       "74     id_99 2001-05-19         9.000432          9.276742\n",
       "75     id_99 2001-05-20        15.026851         15.237301\n",
       "76     id_99 2001-05-21        22.900578         23.007710\n",
       "\n",
       "[700 rows x 4 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = fcst.predict(7, dynamic_dfs=[prices_catalog])\n",
    "preds.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06edbe8-27e1-4662-bf5a-60a67c7f6713",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test we can compute cross validation with\n",
    "# exougenous variables without adding extra information\n",
    "# later a more robust test is performed\n",
    "cv_with_ex = fcst.cross_validation(\n",
    "    series_with_prices,\n",
    "    window_size=7,\n",
    "    n_windows=2,\n",
    "    id_col='unique_id',\n",
    "    time_col='ds',\n",
    "    target_col='y',\n",
    "    static_features=['static_0', 'product_id'],\n",
    ").compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c37303b-d2a9-4041-8cbe-f7921e5a7316",
   "metadata": {},
   "source": [
    "### Custom predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3063a465-11bb-4e85-be4e-a3f4978e6fc7",
   "metadata": {},
   "source": [
    "If you want to do something like scaling the predictions you can define a function and pass it to `DistributedMLForecast.predict` as described in <a href=\"/forecast.html#custom-predictions\">Custom predictions</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9461e8-2891-4c52-ab1c-0b1417c9ce60",
   "metadata": {},
   "source": [
    "#### Cross validation\n",
    "Refer to `MLForecast.cross_validation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3faef3-89d4-45dd-bcda-78aba4414a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "## DistributedMLForecast.cross_validation\n",
       "\n",
       ">      DistributedMLForecast.cross_validation (data:~AnyDataFrame,\n",
       ">                                              n_windows:int, window_size:int,\n",
       ">                                              id_col:str, time_col:str,\n",
       ">                                              target_col:str,\n",
       ">                                              step_size:Optional[int]=None, sta\n",
       ">                                              tic_features:Optional[List[str]]=\n",
       ">                                              None, dropna:bool=True,\n",
       ">                                              keep_last_n:Optional[int]=None,\n",
       ">                                              refit:bool=True, before_predict_c\n",
       ">                                              allback:Optional[Callable]=None, \n",
       ">                                              after_predict_callback:Optional[C\n",
       ">                                              allable]=None,\n",
       ">                                              input_size:Optional[int]=None)\n",
       "\n",
       "Perform time series cross validation.\n",
       "Creates `n_windows` splits where each window has `window_size` test periods,\n",
       "trains the models, computes the predictions and merges the actuals.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| data | AnyDataFrame |  | Series data in long format. |\n",
       "| n_windows | int |  | Number of windows to evaluate. |\n",
       "| window_size | int |  | Number of test periods in each window. |\n",
       "| id_col | str |  | Column that identifies each serie. If 'index' then the index is used. |\n",
       "| time_col | str |  | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str |  | Column that contains the target.         |\n",
       "| step_size | Optional | None | Step size between each cross validation window. If None it will be equal to `window_size`. |\n",
       "| static_features | Optional | None | Names of the features that are static and will be repeated when forecasting. |\n",
       "| dropna | bool | True | Drop rows with missing values produced by the transformations. |\n",
       "| keep_last_n | Optional | None | Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it. |\n",
       "| refit | bool | True | Retrain model for each cross validation window.<br>If False, the models are trained at the beginning and then used to predict each window.             |\n",
       "| before_predict_callback | Optional | None | Function to call on the features before computing the predictions.<br>    This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure.<br>    The series identifier is on the index. |\n",
       "| after_predict_callback | Optional | None | Function to call on the predictions before updating the targets.<br>    This function will take a pandas Series with the predictions and should return another one with the same structure.<br>    The series identifier is on the index. |\n",
       "| input_size | Optional | None | Maximum training samples per serie in each window. If None, will use an expanding window.                 |\n",
       "| **Returns** | **AnyDataFrame** |  | **Predictions for each window with the series id, timestamp, target value and predictions from each model.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "## DistributedMLForecast.cross_validation\n",
       "\n",
       ">      DistributedMLForecast.cross_validation (data:~AnyDataFrame,\n",
       ">                                              n_windows:int, window_size:int,\n",
       ">                                              id_col:str, time_col:str,\n",
       ">                                              target_col:str,\n",
       ">                                              step_size:Optional[int]=None, sta\n",
       ">                                              tic_features:Optional[List[str]]=\n",
       ">                                              None, dropna:bool=True,\n",
       ">                                              keep_last_n:Optional[int]=None,\n",
       ">                                              refit:bool=True, before_predict_c\n",
       ">                                              allback:Optional[Callable]=None, \n",
       ">                                              after_predict_callback:Optional[C\n",
       ">                                              allable]=None,\n",
       ">                                              input_size:Optional[int]=None)\n",
       "\n",
       "Perform time series cross validation.\n",
       "Creates `n_windows` splits where each window has `window_size` test periods,\n",
       "trains the models, computes the predictions and merges the actuals.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| data | AnyDataFrame |  | Series data in long format. |\n",
       "| n_windows | int |  | Number of windows to evaluate. |\n",
       "| window_size | int |  | Number of test periods in each window. |\n",
       "| id_col | str |  | Column that identifies each serie. If 'index' then the index is used. |\n",
       "| time_col | str |  | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str |  | Column that contains the target.         |\n",
       "| step_size | Optional | None | Step size between each cross validation window. If None it will be equal to `window_size`. |\n",
       "| static_features | Optional | None | Names of the features that are static and will be repeated when forecasting. |\n",
       "| dropna | bool | True | Drop rows with missing values produced by the transformations. |\n",
       "| keep_last_n | Optional | None | Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it. |\n",
       "| refit | bool | True | Retrain model for each cross validation window.<br>If False, the models are trained at the beginning and then used to predict each window.             |\n",
       "| before_predict_callback | Optional | None | Function to call on the features before computing the predictions.<br>    This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure.<br>    The series identifier is on the index. |\n",
       "| after_predict_callback | Optional | None | Function to call on the predictions before updating the targets.<br>    This function will take a pandas Series with the predictions and should return another one with the same structure.<br>    The series identifier is on the index. |\n",
       "| input_size | Optional | None | Maximum training samples per serie in each window. If None, will use an expanding window.                 |\n",
       "| **Returns** | **AnyDataFrame** |  | **Predictions for each window with the series id, timestamp, target value and predictions from each model.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DistributedMLForecast.cross_validation, title_level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e006da05-b056-4859-8005-9ece21dd3c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst = DistributedMLForecast(\n",
    "    models=[DaskLGBMForecast(), DaskXGBForecast()],\n",
    "    freq='D',\n",
    "    lags=[7],\n",
    "    lag_transforms={\n",
    "        1: [expanding_mean],\n",
    "        7: [(rolling_mean, 14)]\n",
    "    },\n",
    "    date_features=['dayofweek', 'month'],\n",
    "    num_threads=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8eb4ac-1622-4eed-8892-e1272e131925",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_windows = 2\n",
    "window_size = 14\n",
    "\n",
    "cv_results = fcst.cross_validation(\n",
    "    partitioned_series,\n",
    "    n_windows,\n",
    "    window_size,\n",
    "    id_col='unique_id',\n",
    "    time_col='ds',\n",
    "    target_col='y',\n",
    ")\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320dbd3d-87fe-4aaa-900a-2a06227262eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# input_size\n",
    "input_size = 100\n",
    "reduced_train = fcst._preprocess(\n",
    "    partitioned_series,\n",
    "    id_col='unique_id',\n",
    "    time_col='ds',\n",
    "    target_col='y',\n",
    "    dropna=False,\n",
    "    window_info=WindowInfo(\n",
    "        n_windows=1,\n",
    "        window_size=10,\n",
    "        step_size=None,\n",
    "        i_window=0,\n",
    "        input_size=input_size,\n",
    "    ),\n",
    ")\n",
    "assert reduced_train.groupby('unique_id').size().compute().max() == input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a173f86-eeca-4e66-9df3-9282bf4afcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "cv_results_no_refit = fcst.cross_validation(\n",
    "    partitioned_series,\n",
    "    n_windows,\n",
    "    window_size,\n",
    "    id_col='unique_id',\n",
    "    time_col='ds',\n",
    "    target_col='y',\n",
    "    refit=False\n",
    ")\n",
    "cv_results_df = cv_results.compute()\n",
    "cv_results_no_refit_df = cv_results_no_refit.compute()\n",
    "# test we recover the same \"metadata\"\n",
    "models = ['DaskXGBForecast', 'DaskLGBMForecast']\n",
    "test_eq(\n",
    "    cv_results_no_refit_df.drop(columns=models),\n",
    "    cv_results_df.drop(columns=models)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df86990-35ff-4b44-b96a-cabd22b03adb",
   "metadata": {},
   "source": [
    "We can aggregate these by date to get a rough estimate of how our model is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1776f7a5-996d-4598-90d6-c5217b6da9e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DaskLGBMForecast</th>\n",
       "      <th>DaskXGBForecast</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ds</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2001-04-17</th>\n",
       "      <td>16.195230</td>\n",
       "      <td>16.168709</td>\n",
       "      <td>16.123231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-04-18</th>\n",
       "      <td>15.145318</td>\n",
       "      <td>15.135734</td>\n",
       "      <td>15.213920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-04-19</th>\n",
       "      <td>17.149119</td>\n",
       "      <td>17.087150</td>\n",
       "      <td>16.985699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-04-20</th>\n",
       "      <td>18.002781</td>\n",
       "      <td>18.045092</td>\n",
       "      <td>18.068340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-04-21</th>\n",
       "      <td>18.136612</td>\n",
       "      <td>18.142144</td>\n",
       "      <td>18.200609</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            DaskLGBMForecast  DaskXGBForecast          y\n",
       "ds                                                      \n",
       "2001-04-17         16.195230        16.168709  16.123231\n",
       "2001-04-18         15.145318        15.135734  15.213920\n",
       "2001-04-19         17.149119        17.087150  16.985699\n",
       "2001-04-20         18.002781        18.045092  18.068340\n",
       "2001-04-21         18.136612        18.142144  18.200609"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_results = cv_results_df.drop(columns='cutoff').groupby('ds').mean()\n",
    "agg_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccb8da2-6009-4143-8c3b-3425921f52de",
   "metadata": {},
   "source": [
    "We can also compute the error for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c4a38f-70a7-4606-a902-bd3944228cc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>DaskLGBMForecast</th>\n",
       "      <th>DaskXGBForecast</th>\n",
       "      <th>cutoff</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>object</td>\n",
       "      <td>datetime64[ns]</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>datetime64[ns]</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: drop-duplicates-agg, 33 graph layers</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "              unique_id              ds DaskLGBMForecast DaskXGBForecast          cutoff        y\n",
       "npartitions=1                                                                                    \n",
       "                 object  datetime64[ns]          float64         float64  datetime64[ns]  float64\n",
       "                    ...             ...              ...             ...             ...      ...\n",
       "Dask Name: drop-duplicates-agg, 33 graph layers"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a449b9e-7b68-443a-b3c2-4b0977d32143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DaskLGBMForecast': 0.91, 'DaskXGBForecast': 0.87}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mse_from_dask_dataframe(ddf):\n",
    "    mses = {}\n",
    "    for model_name in ddf.columns.drop(['unique_id', 'ds', 'y', 'cutoff']):\n",
    "        mses[model_name] = (ddf['y'] - ddf[model_name]).pow(2).mean()\n",
    "    return client.gather(client.compute(mses))\n",
    "\n",
    "{k: round(v, 2) for k, v in mse_from_dask_dataframe(cv_results).items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa70236c-dfd0-4cc7-9a0a-46a9769567ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bd1c31-9270-4176-b9b1-eaaaeae3cf28",
   "metadata": {},
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e7f9e1-4bac-4998-b0b7-a9897e5a3ef3",
   "metadata": {},
   "source": [
    "### Session setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81348566-8200-419f-9921-c6b5b655652d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9713172-2263-48a5-99fd-f66b13117cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"MyApp\")\n",
    "    .config(\"spark.jars.packages\", \"com.microsoft.azure:synapseml_2.12:0.10.2\")\n",
    "    .config(\"spark.jars.repositories\", \"https://mmlspark.azureedge.net/maven\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadaef9e-29a6-4423-8f32-573ea7b4edc7",
   "metadata": {},
   "source": [
    "### Data setup\n",
    "For spark, the data must be a `pyspark DataFrame`. You need to make sure that each time serie is only in one partition (which you can do using `repartitionByRange`, for example) and it is recommended that you have as many partitions as you have workers. If you have more partitions than workers make sure to set `num_threads=1` to avoid having nested parallelism.\n",
    "\n",
    "The required input format is the same as for `MLForecast`, i.e. it should have at least an id column, a time column and a target column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9efe100-a3b8-4b23-93c5-8235cbe4e6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "series = generate_daily_series(100, n_static_features=2, equal_ends=True, static_as_categorical=False)\n",
    "spark_series = spark.createDataFrame(series.reset_index()).repartitionByRange(4, 'unique_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609ef79b-259d-4939-bb9f-b9bec54e326e",
   "metadata": {},
   "source": [
    "### Models\n",
    "In order to perform distributed forecasting, we need to use a model that is able to train in a distributed way using `spark`. The current implementations are in `SparkLGBMForecast` and `SparkXGBForecast` which are just wrappers around the native implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071e6bb6-7cfe-4208-bae9-8e8b4cc0cfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlforecast.distributed.models.spark.lgb import SparkLGBMForecast\n",
    "\n",
    "models = [SparkLGBMForecast()]\n",
    "try:\n",
    "    from xgboost.spark import SparkXGBRegressor\n",
    "    from mlforecast.distributed.models.spark.xgb import SparkXGBForecast\n",
    "    models.append(SparkXGBForecast())\n",
    "except ModuleNotFoundError:  # py < 38\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff83d8e-a96c-4bba-9305-9c8a89153ac4",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91acad4-55fd-43c6-838d-3102816664fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst = DistributedMLForecast(\n",
    "    models,\n",
    "    freq='D',\n",
    "    lags=[1],\n",
    "    lag_transforms={\n",
    "        1: [expanding_mean]\n",
    "    },\n",
    "    date_features=['dayofweek'],\n",
    ")\n",
    "fcst.fit(\n",
    "    spark_series,\n",
    "    id_col='unique_id',\n",
    "    time_col='ds',\n",
    "    target_col='y',\n",
    "    static_features=['static_0', 'static_1'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473f10d8-6e41-4096-a239-cd8bc8febc4e",
   "metadata": {},
   "source": [
    "### Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d003e12c-ffa4-42b7-8387-5fe78baa1b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = fcst.predict(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f2fd37-39d1-4025-82b6-7cc9b76a1e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>SparkLGBMForecast</th>\n",
       "      <th>SparkXGBForecast</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-15</td>\n",
       "      <td>42.213984</td>\n",
       "      <td>42.305004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-16</td>\n",
       "      <td>49.718021</td>\n",
       "      <td>50.262386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-17</td>\n",
       "      <td>1.306248</td>\n",
       "      <td>1.912686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-18</td>\n",
       "      <td>10.060104</td>\n",
       "      <td>10.240939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-19</td>\n",
       "      <td>18.070785</td>\n",
       "      <td>18.265749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-24</td>\n",
       "      <td>43.426901</td>\n",
       "      <td>43.780163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-25</td>\n",
       "      <td>1.361680</td>\n",
       "      <td>2.097803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-26</td>\n",
       "      <td>8.787283</td>\n",
       "      <td>8.593580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-27</td>\n",
       "      <td>15.551965</td>\n",
       "      <td>15.622238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-28</td>\n",
       "      <td>22.518518</td>\n",
       "      <td>22.943216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1400 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     unique_id         ds  SparkLGBMForecast  SparkXGBForecast\n",
       "0        id_00 2001-05-15          42.213984         42.305004\n",
       "1        id_00 2001-05-16          49.718021         50.262386\n",
       "2        id_00 2001-05-17           1.306248          1.912686\n",
       "3        id_00 2001-05-18          10.060104         10.240939\n",
       "4        id_00 2001-05-19          18.070785         18.265749\n",
       "...        ...        ...                ...               ...\n",
       "1395     id_99 2001-05-24          43.426901         43.780163\n",
       "1396     id_99 2001-05-25           1.361680          2.097803\n",
       "1397     id_99 2001-05-26           8.787283          8.593580\n",
       "1398     id_99 2001-05-27          15.551965         15.622238\n",
       "1399     id_99 2001-05-28          22.518518         22.943216\n",
       "\n",
       "[1400 rows x 4 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7006bea-6774-49dd-8a34-b3f07320aee6",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df462c44-f309-45bc-97c5-39b946030d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_res = fcst.cross_validation(\n",
    "    spark_series,\n",
    "    n_windows=2,\n",
    "    window_size=14,\n",
    "    id_col='unique_id',\n",
    "    time_col='ds',\n",
    "    target_col='y',\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d9d284-4be4-45e1-9ba1-d285257d6607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>SparkLGBMForecast</th>\n",
       "      <th>SparkXGBForecast</th>\n",
       "      <th>cutoff</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_17</td>\n",
       "      <td>2001-04-30</td>\n",
       "      <td>31.467849</td>\n",
       "      <td>31.676336</td>\n",
       "      <td>2001-04-16</td>\n",
       "      <td>30.832464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_07</td>\n",
       "      <td>2001-04-17</td>\n",
       "      <td>1.015429</td>\n",
       "      <td>1.039312</td>\n",
       "      <td>2001-04-16</td>\n",
       "      <td>1.034871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_06</td>\n",
       "      <td>2001-04-29</td>\n",
       "      <td>21.133919</td>\n",
       "      <td>1.368022</td>\n",
       "      <td>2001-04-16</td>\n",
       "      <td>0.944155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_11</td>\n",
       "      <td>2001-04-17</td>\n",
       "      <td>57.069013</td>\n",
       "      <td>57.591526</td>\n",
       "      <td>2001-04-16</td>\n",
       "      <td>57.406090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_12</td>\n",
       "      <td>2001-04-27</td>\n",
       "      <td>7.965585</td>\n",
       "      <td>7.741258</td>\n",
       "      <td>2001-04-16</td>\n",
       "      <td>8.498222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2795</th>\n",
       "      <td>id_96</td>\n",
       "      <td>2001-05-12</td>\n",
       "      <td>9.069598</td>\n",
       "      <td>8.925149</td>\n",
       "      <td>2001-04-30</td>\n",
       "      <td>7.983343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2796</th>\n",
       "      <td>id_84</td>\n",
       "      <td>2001-05-04</td>\n",
       "      <td>10.474623</td>\n",
       "      <td>9.959846</td>\n",
       "      <td>2001-04-30</td>\n",
       "      <td>10.683266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2797</th>\n",
       "      <td>id_87</td>\n",
       "      <td>2001-05-07</td>\n",
       "      <td>2.162316</td>\n",
       "      <td>2.065432</td>\n",
       "      <td>2001-04-30</td>\n",
       "      <td>1.277810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2798</th>\n",
       "      <td>id_80</td>\n",
       "      <td>2001-05-11</td>\n",
       "      <td>22.679552</td>\n",
       "      <td>20.547785</td>\n",
       "      <td>2001-04-30</td>\n",
       "      <td>19.823192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2799</th>\n",
       "      <td>id_90</td>\n",
       "      <td>2001-05-08</td>\n",
       "      <td>40.225448</td>\n",
       "      <td>40.293419</td>\n",
       "      <td>2001-04-30</td>\n",
       "      <td>39.215204</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2800 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     unique_id         ds  SparkLGBMForecast  SparkXGBForecast     cutoff  \\\n",
       "0        id_17 2001-04-30          31.467849         31.676336 2001-04-16   \n",
       "1        id_07 2001-04-17           1.015429          1.039312 2001-04-16   \n",
       "2        id_06 2001-04-29          21.133919          1.368022 2001-04-16   \n",
       "3        id_11 2001-04-17          57.069013         57.591526 2001-04-16   \n",
       "4        id_12 2001-04-27           7.965585          7.741258 2001-04-16   \n",
       "...        ...        ...                ...               ...        ...   \n",
       "2795     id_96 2001-05-12           9.069598          8.925149 2001-04-30   \n",
       "2796     id_84 2001-05-04          10.474623          9.959846 2001-04-30   \n",
       "2797     id_87 2001-05-07           2.162316          2.065432 2001-04-30   \n",
       "2798     id_80 2001-05-11          22.679552         20.547785 2001-04-30   \n",
       "2799     id_90 2001-05-08          40.225448         40.293419 2001-04-30   \n",
       "\n",
       "              y  \n",
       "0     30.832464  \n",
       "1      1.034871  \n",
       "2      0.944155  \n",
       "3     57.406090  \n",
       "4      8.498222  \n",
       "...         ...  \n",
       "2795   7.983343  \n",
       "2796  10.683266  \n",
       "2797   1.277810  \n",
       "2798  19.823192  \n",
       "2799  39.215204  \n",
       "\n",
       "[2800 rows x 6 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
