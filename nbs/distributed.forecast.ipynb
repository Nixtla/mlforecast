{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71e077d-b6cc-4913-b206-77b652053251",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp distributed.forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a7d947-ce42-40ec-8b54-623ec2189dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c82054e-db3d-43ca-a4fe-c397e351ad1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastcore.test import test_warns\n",
    "from nbdev import show_doc\n",
    "from sklearn import set_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933ec908-b746-4b80-8da3-fcb0039145d4",
   "metadata": {},
   "source": [
    "# DistributedMLForecast\n",
    "\n",
    "> Distributed pipeline encapsulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6d15cd-5734-4585-aaca-de860e07f9ab",
   "metadata": {},
   "source": [
    "**This interface is only tested on Linux**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3e3690-1951-487a-8c66-ba1b2cc01756",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import warnings\n",
    "from typing import Callable, Iterable, List, Optional\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dask.distributed import Client, default_client\n",
    "from sklearn.base import clone\n",
    "\n",
    "from mlforecast.core import (\n",
    "    DateFeature,\n",
    "    Differences,\n",
    "    Freq,\n",
    "    LagTransforms,\n",
    "    Lags,\n",
    "    Models,\n",
    "    TimeSeries,\n",
    "    _name_models,\n",
    ")\n",
    "from mlforecast.distributed.core import DistributedTimeSeries\n",
    "from mlforecast.utils import backtest_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55dee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "set_config(display='text')\n",
    "warnings.simplefilter('ignore', UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c468d380-0a57-4e14-97d4-e9e665121360",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class DistributedMLForecast:\n",
    "    \"\"\"Distributed pipeline encapsulation.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        models: Models,\n",
    "        freq: Optional[Freq] = None,\n",
    "        lags: Optional[Lags] = None,\n",
    "        lag_transforms: Optional[LagTransforms] = None,\n",
    "        date_features: Optional[Iterable[DateFeature]] = None,\n",
    "        differences: Optional[Differences] = None,\n",
    "        num_threads: int = 1,\n",
    "        client: Optional[Client] = None,\n",
    "    ):\n",
    "        \"\"\"Create distributed forecast object\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        models : regressor or list of regressors\n",
    "            Models that will be trained and used to compute the forecasts.\n",
    "        freq : str or int, optional (default=None)\n",
    "            Pandas offset alias, e.g. 'D', 'W-THU' or integer denoting the frequency of the series.\n",
    "        lags : list of int, optional (default=None)\n",
    "            Lags of the target to use as features.\n",
    "        lag_transforms : dict of int to list of functions, optional (default=None)\n",
    "            Mapping of target lags to their transformations.\n",
    "        date_features : list of str or callable, optional (default=None)\n",
    "            Features computed from the dates. Can be pandas date attributes or functions that will take the dates as input.\n",
    "        differences : list of int, optional (default=None)\n",
    "            Differences to take of the target before computing the features. These are restored at the forecasting step.\n",
    "        num_threads : int (default=1)\n",
    "            Number of threads to use when computing the features.\n",
    "        client : dask distributed client\n",
    "            Client to use for computing data and training the models.        \n",
    "        \"\"\"        \n",
    "        if not isinstance(models, dict) and not isinstance(models, list):\n",
    "            models = [models]\n",
    "        if isinstance(models, list):\n",
    "            model_names = _name_models([m.__class__.__name__ for m in models])            \n",
    "            models_with_names = dict(zip(model_names, models))\n",
    "        else:\n",
    "            models_with_names = models\n",
    "        self.models = models_with_names\n",
    "        self.client = client or default_client()\n",
    "        self.dts = DistributedTimeSeries(\n",
    "            TimeSeries(\n",
    "                freq, lags, lag_transforms, date_features, differences, num_threads\n",
    "            ),\n",
    "            self.client,\n",
    "        )\n",
    "        \n",
    "    def __repr__(self) -> str:\n",
    "        return (\n",
    "            f'{self.__class__.__name__}(models=[{\", \".join(self.models.keys())}], '\n",
    "            f\"freq={self.freq}, \"\n",
    "            f\"lag_features={list(self.dts._base_ts.transforms.keys())}, \"\n",
    "            f\"date_features={self.dts._base_ts.date_features}, \"\n",
    "            f\"num_threads={self.dts._base_ts.num_threads}, \"\n",
    "            f\"client={self.client})\"\n",
    "        )\n",
    "    \n",
    "    @property\n",
    "    def freq(self):\n",
    "        return self.dts._base_ts.freq\n",
    "\n",
    "    def preprocess(\n",
    "        self,\n",
    "        data: dd.DataFrame,\n",
    "        id_col: str = 'index',\n",
    "        time_col: str = 'ds',\n",
    "        target_col: str = 'y',\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "    ) -> dd.DataFrame:\n",
    "        \"\"\"Add the features to `data`.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data : dask DataFrame\n",
    "            Series data in long format.\n",
    "        id_col : str\n",
    "            Column that identifies each serie. If 'index' then the index is used.\n",
    "        time_col : str\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str\n",
    "            Column that contains the target.\n",
    "        static_features : list of str, optional (default=None)\n",
    "            Names of the features that are static and will be repeated when forecasting.\n",
    "        dropna : bool (default=True)\n",
    "            Drop rows with missing values produced by the transformations.\n",
    "        keep_last_n : int, optional (default=None)\n",
    "            Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        result : dask DataFrame.\n",
    "            `data` plus added features.\n",
    "        \"\"\"\n",
    "        if id_col in data:\n",
    "            warnings.warn('It is recommended to have id_col as the index, since setting the index is a slow operation.')\n",
    "            data = data.set_index(id_col)\n",
    "            id_col = 'index'\n",
    "        return self.dts.fit_transform(data, id_col, time_col, target_col, static_features, dropna, keep_last_n)\n",
    "    \n",
    "    def fit_models(\n",
    "        self,\n",
    "        X: dd.DataFrame,\n",
    "        y: dd.Series,\n",
    "    ) -> 'DistributedMLForecast':\n",
    "        \"\"\"Manually train models. Use this if you called `Forecast.preprocess` beforehand.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : dask DataFrame\n",
    "            Features.\n",
    "        y : dask Series.\n",
    "            Target.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self : DistributedForecast\n",
    "            Forecast object with trained models.\n",
    "        \"\"\"\n",
    "        self.models_ = {}\n",
    "        for name, model in self.models.items():\n",
    "            self.models_[name] = clone(model).fit(X, y).model_\n",
    "        return self\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        data: dd.DataFrame,\n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "    ) -> 'DistributedMLForecast':\n",
    "        \"\"\"Apply the feature engineering and train the models.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data : dask DataFrame\n",
    "            Series data in long format.\n",
    "        id_col : str\n",
    "            Column that identifies each serie. If 'index' then the index is used.\n",
    "        time_col : str\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str\n",
    "            Column that contains the target.\n",
    "        static_features : list of str, optional (default=None)\n",
    "            Names of the features that are static and will be repeated when forecasting.\n",
    "        dropna : bool (default=True)\n",
    "            Drop rows with missing values produced by the transformations.\n",
    "        keep_last_n : int, optional (default=None)\n",
    "            Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : DistributedForecast\n",
    "            Forecast object with series values and trained models.\n",
    "        \"\"\"\n",
    "        train_ddf = self.preprocess(data, id_col, time_col, target_col, static_features, dropna, keep_last_n)\n",
    "        X, y = train_ddf.drop(columns=[time_col, target_col]), train_ddf[target_col]        \n",
    "        self.fit_models(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        horizon: int,\n",
    "        dynamic_dfs: Optional[List[pd.DataFrame]] = None,\n",
    "        before_predict_callback: Optional[Callable] = None,\n",
    "        after_predict_callback: Optional[Callable] = None,\n",
    "        new_data: Optional[pd.DataFrame] = None,\n",
    "    ) -> dd.DataFrame:\n",
    "        \"\"\"Compute the predictions for the next `horizon` steps.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        horizon : int\n",
    "            Number of periods to predict.\n",
    "        dynamic_dfs : list of pandas DataFrame, optional (default=None)\n",
    "            Future values of the dynamic features, e.g. prices.\n",
    "        before_predict_callback : callable, optional (default=None)\n",
    "            Function to call on the features before computing the predictions.\n",
    "                This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure.\n",
    "                The series identifier is on the index.\n",
    "        after_predict_callback : callable, optional (default=None)\n",
    "            Function to call on the predictions before updating the targets.\n",
    "                This function will take a pandas Series with the predictions and should return another one with the same structure.\n",
    "                The series identifier is on the index.\n",
    "        new_data : pandas DataFrame, optional (default=None)\n",
    "            Series data of new observations for which forecasts are to be generated. \n",
    "                This dataframe should have the same structure as the one used to fit the model, including any features and time series data. \n",
    "                If `new_data` is not None, the method will generate forecasts for the new observations.\n",
    "\n",
    "                    \n",
    "        Returns\n",
    "        -------\n",
    "        result : dask DataFrame\n",
    "            Predictions for each serie and timestep, with one column per model.\n",
    "        \"\"\"\n",
    "        if new_data is not None:\n",
    "            new_dts = DistributedTimeSeries(\n",
    "                TimeSeries(\n",
    "                    self.ts.freq, self.ts.lags, \n",
    "                    self.ts.lag_transforms, self.ts.date_features, \n",
    "                    self.ts.differences, self.ts.num_threads\n",
    "                ),\n",
    "                self.client\n",
    "            )\n",
    "            new_dts.fit_transform(\n",
    "                new_data, \n",
    "                self.ts.id_col, self.ts.time_col, self.ts.target_col, self.ts.static_features, \n",
    "                self.ts.dropna, self.ts.keep_last_n\n",
    "            )\n",
    "            dts = new_dts\n",
    "        else:\n",
    "            dts = self.dts\n",
    "            \n",
    "        return dts.predict(\n",
    "            self.models_, horizon, dynamic_dfs, before_predict_callback, after_predict_callback\n",
    "        )\n",
    "    \n",
    "    def cross_validation(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        n_windows: int,\n",
    "        window_size: int,\n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "        dynamic_dfs: Optional[List[pd.DataFrame]] = None,\n",
    "        before_predict_callback: Optional[Callable] = None,\n",
    "        after_predict_callback: Optional[Callable] = None,\n",
    "    ):\n",
    "        \"\"\"Perform time series cross validation.\n",
    "        Creates `n_windows` splits where each window has `window_size` test periods, \n",
    "        trains the models, computes the predictions and merges the actuals.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data : dask DataFrame\n",
    "            Series data in long format.\n",
    "        n_windows : int\n",
    "            Number of windows to evaluate.\n",
    "        window_size : int\n",
    "            Number of test periods in each window.\n",
    "        id_col : str\n",
    "            Column that identifies each serie. If 'index' then the index is used.\n",
    "        time_col : str\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str\n",
    "            Column that contains the target.\n",
    "        static_features : list of str, optional (default=None)\n",
    "            Names of the features that are static and will be repeated when forecasting.\n",
    "        dropna : bool (default=True)\n",
    "            Drop rows with missing values produced by the transformations.\n",
    "        keep_last_n : int, optional (default=None)\n",
    "            Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it.\n",
    "        dynamic_dfs : list of pandas DataFrame, optional (default=None)\n",
    "            Future values of the dynamic features, e.g. prices.\n",
    "        before_predict_callback : callable, optional (default=None)\n",
    "            Function to call on the features before computing the predictions.\n",
    "                This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure.\n",
    "                The series identifier is on the index.\n",
    "        after_predict_callback : callable, optional (default=None)\n",
    "            Function to call on the predictions before updating the targets.\n",
    "                This function will take a pandas Series with the predictions and should return another one with the same structure.\n",
    "                The series identifier is on the index.               \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        result : dask DataFrame\n",
    "            Predictions for each window with the series id, timestamp, last train date, target value and predictions from each model.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        self.cv_models_ = []\n",
    "        if id_col != 'index':\n",
    "            data = data.set_index(id_col)\n",
    "\n",
    "        def renames(df):\n",
    "            mapper = {time_col: 'ds', target_col: 'y'}\n",
    "            df = df.rename(columns=mapper, copy=False)\n",
    "            df.index.name = 'unique_id'\n",
    "            return df\n",
    "        data = data.map_partitions(renames)\n",
    "\n",
    "        if np.issubdtype(data['ds'].dtype.type, np.integer):\n",
    "            freq = 1\n",
    "        else:\n",
    "            freq = self.freq\n",
    "        for train_end, train, valid in backtest_splits(data, n_windows, window_size, freq):\n",
    "            self.fit(train, 'index', 'ds', 'y', static_features, dropna, keep_last_n)\n",
    "            self.cv_models_.append(self.models_)\n",
    "            y_pred = self.predict(\n",
    "                window_size, dynamic_dfs, before_predict_callback, after_predict_callback,\n",
    "            )\n",
    "            result = valid[['ds', 'y']].copy()\n",
    "            result['cutoff'] = train_end\n",
    "            \n",
    "            def merge_fn(res, pred):\n",
    "                return res.merge(pred, on=['unique_id', 'ds'], how='left')\n",
    "            meta = {**result.dtypes.to_dict(), **y_pred.dtypes.to_dict()}\n",
    "            result = result.map_partitions(merge_fn, y_pred, align_dataframes=False, meta=meta)\n",
    "            if id_col != 'index':\n",
    "                result = result.reset_index()\n",
    "            result = result.rename(columns={'ds': time_col, 'y': target_col, 'unique_id': id_col})\n",
    "            results.append(result)\n",
    "\n",
    "        return dd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d26ed3d-378d-4f5a-8c07-276c44e3278d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export\n",
    "class DistributedForecast(DistributedMLForecast):\n",
    "    def __init__(\n",
    "        self,\n",
    "        models: Models,\n",
    "        freq: Optional[Freq] = None,\n",
    "        lags: Optional[Lags] = None,\n",
    "        lag_transforms: Optional[LagTransforms] = None,\n",
    "        date_features: Optional[Iterable[DateFeature]] = None,\n",
    "        differences: Optional[Differences] = None,\n",
    "        num_threads: int = 1,\n",
    "        client: Optional[Client] = None,\n",
    "    ):\n",
    "        warning_msg = (\n",
    "            'The DistributedForecast class is deprecated and will be removed in a future version, '\n",
    "            'please use the DistributedMLForecast class instead.'\n",
    "        )\n",
    "        warnings.warn(warning_msg, DeprecationWarning)\n",
    "        super().__init__(models, freq, lags, lag_transforms, date_features, differences, num_threads, client)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2825a2a-02c1-47ae-98c5-e9d35690d291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### DistributedMLForecast\n",
       "\n",
       ">      DistributedMLForecast (models:Union[sklearn.base.BaseEstimator,List[sklea\n",
       ">                             rn.base.BaseEstimator],Dict[str,sklearn.base.BaseE\n",
       ">                             stimator]], freq:Union[int,str,NoneType]=None,\n",
       ">                             lags:Optional[Iterable[int]]=None, lag_transforms:\n",
       ">                             Optional[Dict[int,List[Union[Callable,Tuple[Callab\n",
       ">                             le,Any]]]]]=None, date_features:Optional[Iterable[\n",
       ">                             Union[str,Callable]]]=None,\n",
       ">                             differences:Optional[Iterable[int]]=None,\n",
       ">                             num_threads:int=1,\n",
       ">                             client:Optional[distributed.client.Client]=None)\n",
       "\n",
       "Distributed pipeline encapsulation."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### DistributedMLForecast\n",
       "\n",
       ">      DistributedMLForecast (models:Union[sklearn.base.BaseEstimator,List[sklea\n",
       ">                             rn.base.BaseEstimator],Dict[str,sklearn.base.BaseE\n",
       ">                             stimator]], freq:Union[int,str,NoneType]=None,\n",
       ">                             lags:Optional[Iterable[int]]=None, lag_transforms:\n",
       ">                             Optional[Dict[int,List[Union[Callable,Tuple[Callab\n",
       ">                             le,Any]]]]]=None, date_features:Optional[Iterable[\n",
       ">                             Union[str,Callable]]]=None,\n",
       ">                             differences:Optional[Iterable[int]]=None,\n",
       ">                             num_threads:int=1,\n",
       ">                             client:Optional[distributed.client.Client]=None)\n",
       "\n",
       "Distributed pipeline encapsulation."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DistributedMLForecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2b21c7-23fa-4628-be0d-4da2448fa382",
   "metadata": {},
   "source": [
    "The `DistributedMLForecast` class is a high level abstraction that encapsulates all the steps in the pipeline (preprocessing, fitting the model and computing predictions) and applies them in a distributed way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3009c1fe-caba-4412-be41-69588a512bbf",
   "metadata": {},
   "source": [
    "## Example\n",
    "This shows an example with simulated data, for a real world example in a remote cluster you can check the [M5 distributed example](https://www.kaggle.com/lemuz90/m5-mlforecast-distributed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bc621d-43b8-4160-a694-08d3259e202d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from window_ops.expanding import expanding_mean\n",
    "from window_ops.rolling import rolling_mean\n",
    "\n",
    "from mlforecast.utils import backtest_splits, generate_daily_series, generate_prices_for_series\n",
    "from mlforecast.distributed.models.lgb import LGBMForecast\n",
    "from mlforecast.distributed.models.xgb import XGBForecast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a9b623-af02-4acb-9d09-cda1debead4e",
   "metadata": {},
   "source": [
    "The different things that you need to use `DistributedMLForecast` (as opposed to `MLForecast`) are:\n",
    "\n",
    "1. You need to set up a `dask.distributed.Client`. If this client is connected to a remote cluster then the process will run there.\n",
    "2. Your data needs to be a `dask.dataframe.DataFrame`.\n",
    "3. You need to use a model that implements distributed training (either XGBForecast or LGBMForecast)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6479a531-60db-4c1e-ac28-189b8628d3ef",
   "metadata": {},
   "source": [
    "### Client setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb8d3ef-d1f3-48da-b2dc-0579ea058626",
   "metadata": {},
   "source": [
    "Here we define a client that connects to a `dask.distributed.LocalCluster`, however it could be any other kind of cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fea1b0f-f8d3-4be5-89fe-3dcbe00dff77",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(n_workers=2, threads_per_worker=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9976225-e465-4a83-a948-347ef709b8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_warns(lambda: DistributedForecast([]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62cfccf-5898-4046-a6c8-091fec26f0e4",
   "metadata": {},
   "source": [
    "### Data setup\n",
    "\n",
    "The data is given as a `dask.dataframe.DataFrame`, you need to make sure that each time serie is only in one partition and it is recommended that you have as many partitions as you have workers. If you have more partitions than workers make sure to set `num_threads=1` to avoid having nested parallelism.\n",
    "\n",
    "The required input format is the same as for `MLForecast`, except that it's a `dask.dataframe.DataFrame` instead of a `pandas.Dataframe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1388b1cf-78cb-4cf9-b146-6d2043d4d3ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>static_0</th>\n",
       "      <th>static_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=10</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>datetime64[ns]</td>\n",
       "      <td>float64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_10</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_89</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_99</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: from_pandas, 1 graph layer</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                            ds        y static_0 static_1\n",
       "npartitions=10                                           \n",
       "id_00           datetime64[ns]  float64    int64    int64\n",
       "id_10                      ...      ...      ...      ...\n",
       "...                        ...      ...      ...      ...\n",
       "id_89                      ...      ...      ...      ...\n",
       "id_99                      ...      ...      ...      ...\n",
       "Dask Name: from_pandas, 1 graph layer"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series = generate_daily_series(100, n_static_features=2, equal_ends=True, static_as_categorical=False)\n",
    "partitioned_series = dd.from_pandas(series, npartitions=10)\n",
    "partitioned_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b92d81-3a70-424d-81dc-8a995af1fb9c",
   "metadata": {},
   "source": [
    "### Models\n",
    "In order to perform distributed forecasting, we need to use a model that is able to train in a distributed way using `dask`. The current implementations are in `LGBMForecast` and `XGBForecast` which are just wrappers around `lightgbm.dask.DaskLGBMRegressor` and `xgboost.dask.DaskXGBRegressor` that add a `model_` property to get the trained model from them and send it to every worker to perform the predictions step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8012f8-3325-41a2-8fb0-7eaa07a24b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [XGBForecast(random_state=0), LGBMForecast(random_state=0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078e1983-7235-4930-9b44-36c76e35be69",
   "metadata": {},
   "source": [
    "### Training\n",
    "Once we have our models we instantiate a `DistributedForecast` object defining our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334d20c9-5ad7-42e8-a6b2-40d8005467bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistributedMLForecast(models=[XGBForecast, LGBMForecast], freq=<Day>, lag_features=['lag7', 'expanding_mean_lag1', 'rolling_mean_lag7_window_size14'], date_features=['dayofweek', 'month'], num_threads=1, client=<Client: 'tcp://127.0.0.1:58569' processes=2 threads=2, memory=8.00 GiB>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcst = DistributedMLForecast(\n",
    "    models=models,\n",
    "    freq='D',\n",
    "    lags=[7],\n",
    "    lag_transforms={\n",
    "        1: [expanding_mean],\n",
    "        7: [(rolling_mean, 14)]\n",
    "    },\n",
    "    date_features=['dayofweek', 'month'],\n",
    "    num_threads=1,\n",
    ")\n",
    "fcst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234a2f97-ea31-4359-b57b-bb5db5cdcfc2",
   "metadata": {},
   "source": [
    "Here where we say that:\n",
    "\n",
    "* Our series have daily frequency.\n",
    "* We want to use lag 7 as a feature\n",
    "* We want the lag transformations to be:\n",
    "   * expanding mean of the lag 1\n",
    "   * rolling mean of the lag 7 over a window of size 14\n",
    "* We want to use dayofweek and month as date features.\n",
    "* We want to perform the preprocessing and the forecasting steps using 1 thread, because we have 10 partitions and 2 workers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5713b6a-1436-4554-878f-cc47bf87b2d0",
   "metadata": {},
   "source": [
    "From this point we have two options:\n",
    "\n",
    "1. Compute the features and fit our models.\n",
    "2. Compute the features and get them back as a dataframe to do some custom splitting or adding additional features, then training the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bff858-7e8a-49dd-963d-3acd6214e7a8",
   "metadata": {},
   "source": [
    "#### 1. Using all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2882214c-877f-4f5c-af51-3b509fafc3fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### DistributedMLForecast.fit\n",
       "\n",
       ">      DistributedMLForecast.fit (data:dask.dataframe.core.DataFrame,\n",
       ">                                 id_col:str, time_col:str, target_col:str,\n",
       ">                                 static_features:Optional[List[str]]=None,\n",
       ">                                 dropna:bool=True,\n",
       ">                                 keep_last_n:Optional[int]=None)\n",
       "\n",
       "Apply the feature engineering and train the models.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| data | DataFrame |  | Series data in long format. |\n",
       "| id_col | str |  | Column that identifies each serie. If 'index' then the index is used. |\n",
       "| time_col | str |  | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str |  | Column that contains the target. |\n",
       "| static_features | Optional | None | Names of the features that are static and will be repeated when forecasting. |\n",
       "| dropna | bool | True | Drop rows with missing values produced by the transformations. |\n",
       "| keep_last_n | Optional | None | Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it. |\n",
       "| **Returns** | **DistributedMLForecast** |  | **Forecast object with series values and trained models.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### DistributedMLForecast.fit\n",
       "\n",
       ">      DistributedMLForecast.fit (data:dask.dataframe.core.DataFrame,\n",
       ">                                 id_col:str, time_col:str, target_col:str,\n",
       ">                                 static_features:Optional[List[str]]=None,\n",
       ">                                 dropna:bool=True,\n",
       ">                                 keep_last_n:Optional[int]=None)\n",
       "\n",
       "Apply the feature engineering and train the models.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| data | DataFrame |  | Series data in long format. |\n",
       "| id_col | str |  | Column that identifies each serie. If 'index' then the index is used. |\n",
       "| time_col | str |  | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str |  | Column that contains the target. |\n",
       "| static_features | Optional | None | Names of the features that are static and will be repeated when forecasting. |\n",
       "| dropna | bool | True | Drop rows with missing values produced by the transformations. |\n",
       "| keep_last_n | Optional | None | Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it. |\n",
       "| **Returns** | **DistributedMLForecast** |  | **Forecast object with series values and trained models.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DistributedMLForecast.fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8545b806-d828-42dc-be00-d71dab8499b6",
   "metadata": {},
   "source": [
    "Calling `fit` on our data computes the features independently for each partition and performs distributed training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8472ae1e-4b26-4414-8c58-9cc9a9e4d65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fedex/miniconda3/envs/mlforecast/lib/python3.10/site-packages/xgboost/dask.py:856: RuntimeWarning: coroutine 'Client._wait_for_workers' was never awaited\n",
      "  client.wait_for_workers(n_workers)\n",
      "INFO:distributed.scheduler:Receive client connection: Client-worker-004a6226-8252-11ed-81ff-fe3a9be67f31\n",
      "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:58588\n",
      "INFO:distributed.scheduler:Receive client connection: Client-worker-004a62bc-8252-11ed-8200-fe3a9be67f31\n",
      "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:58589\n",
      "[17:40:22] task [xgboost.dask-0]:tcp://127.0.0.1:58576 got new rank 0\n",
      "[17:40:22] task [xgboost.dask-1]:tcp://127.0.0.1:58577 got new rank 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding random open ports for workers\n",
      "[LightGBM] [Info] Trying to bind port 58598...\n",
      "[LightGBM] [Info] Trying to bind port 58599...\n",
      "[LightGBM] [Info] Binding port 58598 succeeded\n",
      "[LightGBM] [Info] Binding port 58599 succeeded\n",
      "[LightGBM] [Info] Listening...\n",
      "[LightGBM] [Info] Listening...\n",
      "[LightGBM] [Info] Connected to rank 1\n",
      "[LightGBM] [Info] Connected to rank 0\n",
      "[LightGBM] [Info] Local rank: 1, total number of machines: 2\n",
      "[LightGBM] [Info] Local rank: 0, total number of machines: 2\n",
      "[LightGBM] [Warning] num_threads is set=1, n_jobs=-1 will be ignored. Current value: num_threads=1\n",
      "[LightGBM] [Warning] num_threads is set=1, n_jobs=-1 will be ignored. Current value: num_threads=1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistributedMLForecast(models=[XGBForecast, LGBMForecast], freq=<Day>, lag_features=['lag7', 'expanding_mean_lag1', 'rolling_mean_lag7_window_size14'], date_features=['dayofweek', 'month'], num_threads=1, client=<Client: 'tcp://127.0.0.1:58569' processes=2 threads=2, memory=8.00 GiB>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcst.fit(partitioned_series, id_col='index', time_col='ds', target_col='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0b5ec6-bee0-4f68-8c91-b99c54576774",
   "metadata": {},
   "source": [
    "### Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e95a58-0fee-41ee-900b-d2b61825042f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### DistributedMLForecast.predict\n",
       "\n",
       ">      DistributedMLForecast.predict (horizon:int,\n",
       ">                                     dynamic_dfs:Optional[List[pandas.core.fram\n",
       ">                                     e.DataFrame]]=None, before_predict_callbac\n",
       ">                                     k:Optional[Callable]=None, after_predict_c\n",
       ">                                     allback:Optional[Callable]=None, new_data:\n",
       ">                                     Optional[pandas.core.frame.DataFrame]=None\n",
       ">                                     )\n",
       "\n",
       "Compute the predictions for the next `horizon` steps.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| horizon | int |  | Number of periods to predict. |\n",
       "| dynamic_dfs | Optional | None | Future values of the dynamic features, e.g. prices. |\n",
       "| before_predict_callback | Optional | None | Function to call on the features before computing the predictions.<br>    This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure.<br>    The series identifier is on the index. |\n",
       "| after_predict_callback | Optional | None | Function to call on the predictions before updating the targets.<br>    This function will take a pandas Series with the predictions and should return another one with the same structure.<br>    The series identifier is on the index. |\n",
       "| new_data | Optional | None | Series data of new observations for which forecasts are to be generated. <br>    This dataframe should have the same structure as the one used to fit the model, including any features and time series data. <br>    If `new_data` is not None, the method will generate forecasts for the new observations. |\n",
       "| **Returns** | **DataFrame** |  | **Predictions for each serie and timestep, with one column per model.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### DistributedMLForecast.predict\n",
       "\n",
       ">      DistributedMLForecast.predict (horizon:int,\n",
       ">                                     dynamic_dfs:Optional[List[pandas.core.fram\n",
       ">                                     e.DataFrame]]=None, before_predict_callbac\n",
       ">                                     k:Optional[Callable]=None, after_predict_c\n",
       ">                                     allback:Optional[Callable]=None, new_data:\n",
       ">                                     Optional[pandas.core.frame.DataFrame]=None\n",
       ">                                     )\n",
       "\n",
       "Compute the predictions for the next `horizon` steps.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| horizon | int |  | Number of periods to predict. |\n",
       "| dynamic_dfs | Optional | None | Future values of the dynamic features, e.g. prices. |\n",
       "| before_predict_callback | Optional | None | Function to call on the features before computing the predictions.<br>    This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure.<br>    The series identifier is on the index. |\n",
       "| after_predict_callback | Optional | None | Function to call on the predictions before updating the targets.<br>    This function will take a pandas Series with the predictions and should return another one with the same structure.<br>    The series identifier is on the index. |\n",
       "| new_data | Optional | None | Series data of new observations for which forecasts are to be generated. <br>    This dataframe should have the same structure as the one used to fit the model, including any features and time series data. <br>    If `new_data` is not None, the method will generate forecasts for the new observations. |\n",
       "| **Returns** | **DataFrame** |  | **Predictions for each serie and timestep, with one column per model.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DistributedMLForecast.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7417aa8d-e724-4ffa-a902-2ab822574030",
   "metadata": {},
   "source": [
    "Once we have our fitted models we can compute the predictions for the next 7 timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8a439d-e0d0-4f2f-925c-fccd92361c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>XGBForecast</th>\n",
       "      <th>LGBMForecast</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=10</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>datetime64[ns]</td>\n",
       "      <td>float32</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_10</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_89</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_99</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: from-delayed, 11 graph layers</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                            ds XGBForecast LGBMForecast\n",
       "npartitions=10                                         \n",
       "id_00           datetime64[ns]     float32      float64\n",
       "id_10                      ...         ...          ...\n",
       "...                        ...         ...          ...\n",
       "id_89                      ...         ...          ...\n",
       "id_99                      ...         ...          ...\n",
       "Dask Name: from-delayed, 11 graph layers"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = fcst.predict(7)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7a8518-0e2a-4699-8da0-e63f29a2f7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "preds = preds.compute()\n",
    "preds2 = fcst.predict(7).compute()\n",
    "pd.testing.assert_frame_equal(preds, preds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1facc224-5a5d-4416-9672-8ece981eb3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fedex/miniconda3/envs/mlforecast/lib/python3.10/site-packages/xgboost/dask.py:856: RuntimeWarning: coroutine 'Client._wait_for_workers' was never awaited\n",
      "  client.wait_for_workers(n_workers)\n",
      "INFO:distributed.worker:Run out-of-band function '_start_tracker'\n",
      "[17:40:57] task [xgboost.dask-0]:tcp://127.0.0.1:58576 got new rank 0\n",
      "[17:40:57] task [xgboost.dask-1]:tcp://127.0.0.1:58577 got new rank 1\n"
     ]
    }
   ],
   "source": [
    "##|hide\n",
    "non_std_series = partitioned_series.copy()\n",
    "non_std_series['ds'] = non_std_series.map_partitions(lambda part: part.groupby('unique_id').cumcount())\n",
    "non_std_series = non_std_series.reset_index().rename(columns={'ds': 'time', 'y': 'value', 'unique_id': 'some_id'})\n",
    "flow_params = dict(\n",
    "    models=[XGBForecast(random_state=0)],\n",
    "    lags=[7],\n",
    "    lag_transforms={\n",
    "        1: [expanding_mean],\n",
    "        7: [(rolling_mean, 14)]\n",
    "    },\n",
    "    num_threads=1,\n",
    ")\n",
    "fcst = DistributedMLForecast(freq='D', **flow_params)\n",
    "fcst.fit(partitioned_series, id_col='index', time_col='ds', target_col='y')\n",
    "preds = fcst.predict(7).compute()\n",
    "fcst2 = DistributedMLForecast(**flow_params)\n",
    "fcst2.preprocess(non_std_series, id_col='some_id', time_col='time', target_col='value')\n",
    "fcst2.models_ = fcst.models_  # distributed training can end up with different fits\n",
    "non_std_preds = fcst2.predict(7).compute()\n",
    "non_std_preds.index.name = 'unique_id'\n",
    "pd.testing.assert_frame_equal(preds.drop(columns='ds'), non_std_preds.drop(columns='time'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f27b9a-ebc3-4673-8459-ad5843a43dac",
   "metadata": {},
   "source": [
    "#### 2. Preprocess and train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0b1833-a63a-45da-b3dc-8e1e5bfeb589",
   "metadata": {},
   "source": [
    "If we only want to perform the preprocessing step we call `preprocess` with our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5c34f3-faa9-44a8-b070-d28225d73cae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### DistributedMLForecast.preprocess\n",
       "\n",
       ">      DistributedMLForecast.preprocess (data:dask.dataframe.core.DataFrame,\n",
       ">                                        id_col:str='index', time_col:str='ds',\n",
       ">                                        target_col:str='y', static_features:Opt\n",
       ">                                        ional[List[str]]=None,\n",
       ">                                        dropna:bool=True,\n",
       ">                                        keep_last_n:Optional[int]=None)\n",
       "\n",
       "Add the features to `data`.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| data | DataFrame |  | Series data in long format. |\n",
       "| id_col | str | index | Column that identifies each serie. If 'index' then the index is used. |\n",
       "| time_col | str | ds | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str | y | Column that contains the target. |\n",
       "| static_features | Optional | None | Names of the features that are static and will be repeated when forecasting. |\n",
       "| dropna | bool | True | Drop rows with missing values produced by the transformations. |\n",
       "| keep_last_n | Optional | None | Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it. |\n",
       "| **Returns** | **DataFrame** |  | **`data` plus added features.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### DistributedMLForecast.preprocess\n",
       "\n",
       ">      DistributedMLForecast.preprocess (data:dask.dataframe.core.DataFrame,\n",
       ">                                        id_col:str='index', time_col:str='ds',\n",
       ">                                        target_col:str='y', static_features:Opt\n",
       ">                                        ional[List[str]]=None,\n",
       ">                                        dropna:bool=True,\n",
       ">                                        keep_last_n:Optional[int]=None)\n",
       "\n",
       "Add the features to `data`.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| data | DataFrame |  | Series data in long format. |\n",
       "| id_col | str | index | Column that identifies each serie. If 'index' then the index is used. |\n",
       "| time_col | str | ds | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str | y | Column that contains the target. |\n",
       "| static_features | Optional | None | Names of the features that are static and will be repeated when forecasting. |\n",
       "| dropna | bool | True | Drop rows with missing values produced by the transformations. |\n",
       "| keep_last_n | Optional | None | Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it. |\n",
       "| **Returns** | **DataFrame** |  | **`data` plus added features.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DistributedMLForecast.preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a9c699-8768-4485-bc22-4da64bea46d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>static_0</th>\n",
       "      <th>static_1</th>\n",
       "      <th>lag7</th>\n",
       "      <th>expanding_mean_lag1</th>\n",
       "      <th>rolling_mean_lag7_window_size14</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2000-10-25</td>\n",
       "      <td>49.766844</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>50.694639</td>\n",
       "      <td>25.001367</td>\n",
       "      <td>26.320060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2000-10-26</td>\n",
       "      <td>3.918347</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>3.887780</td>\n",
       "      <td>26.180675</td>\n",
       "      <td>26.313387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2000-10-27</td>\n",
       "      <td>9.437778</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>11.512774</td>\n",
       "      <td>25.168751</td>\n",
       "      <td>26.398056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2000-10-28</td>\n",
       "      <td>17.923574</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>18.038498</td>\n",
       "      <td>24.484796</td>\n",
       "      <td>26.425272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2000-10-29</td>\n",
       "      <td>26.754645</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>24.222859</td>\n",
       "      <td>24.211411</td>\n",
       "      <td>26.305563</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ds          y  static_0  static_1       lag7  \\\n",
       "unique_id                                                        \n",
       "id_00     2000-10-25  49.766844        79        45  50.694639   \n",
       "id_00     2000-10-26   3.918347        79        45   3.887780   \n",
       "id_00     2000-10-27   9.437778        79        45  11.512774   \n",
       "id_00     2000-10-28  17.923574        79        45  18.038498   \n",
       "id_00     2000-10-29  26.754645        79        45  24.222859   \n",
       "\n",
       "           expanding_mean_lag1  rolling_mean_lag7_window_size14  \n",
       "unique_id                                                        \n",
       "id_00                25.001367                        26.320060  \n",
       "id_00                26.180675                        26.313387  \n",
       "id_00                25.168751                        26.398056  \n",
       "id_00                24.484796                        26.425272  \n",
       "id_00                24.211411                        26.305563  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_ddf = fcst.preprocess(partitioned_series)\n",
    "features_ddf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113b647b-7d05-42d7-b626-35c35fcf517b",
   "metadata": {},
   "source": [
    "This is useful if we want to inspect the data the model will be trained. If we do this we must call `fit_models` to train our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e1be6c-ea29-477a-83e4-4ae0f84d6869",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fedex/miniconda3/envs/mlforecast/lib/python3.10/site-packages/xgboost/dask.py:856: RuntimeWarning: coroutine 'Client._wait_for_workers' was never awaited\n",
      "  client.wait_for_workers(n_workers)\n",
      "INFO:distributed.worker:Run out-of-band function '_start_tracker'\n",
      "[17:40:59] task [xgboost.dask-0]:tcp://127.0.0.1:58576 got new rank 0\n",
      "[17:40:59] task [xgboost.dask-1]:tcp://127.0.0.1:58577 got new rank 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistributedMLForecast(models=[XGBForecast], freq=<Day>, lag_features=['lag7', 'expanding_mean_lag1', 'rolling_mean_lag7_window_size14'], date_features=[], num_threads=1, client=<Client: 'tcp://127.0.0.1:58569' processes=2 threads=2, memory=8.00 GiB>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = features_ddf.drop(columns=['ds', 'y']), features_ddf['y']\n",
    "fcst.fit_models(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5881f75-aece-4930-a9ac-d2096d89eb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "fcst.models_ = fcst2.models_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2fc513-383e-4469-95c0-380a2cc273f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds2 = fcst.predict(7).compute()\n",
    "pd.testing.assert_frame_equal(preds, preds2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7d1a41-a56c-4113-a7d9-a79f643021ab",
   "metadata": {},
   "source": [
    "#### Dynamic features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e66766-a8e9-4af0-96e3-9f5b847c4f0b",
   "metadata": {},
   "source": [
    "By default the predict method repeats the static features and updates the transformations and the date features. If you have dynamic features like prices or a calendar with holidays you can pass them as a list to the `dynamic_dfs` argument of `DistributedMLForecast.predict`, which will call `pd.DataFrame.merge` on each of them in order.\n",
    "\n",
    "Here's an example:\n",
    "\n",
    "Suppose that we have a `product_id` column and we have a catalog for prices based on that `product_id` and the date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee52992-0b18-43b0-833d-62f7246951c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>product_id</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-06-09</td>\n",
       "      <td>1</td>\n",
       "      <td>0.548814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-06-10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.715189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-06-11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.602763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-06-12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.544883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-06-13</td>\n",
       "      <td>1</td>\n",
       "      <td>0.423655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20180</th>\n",
       "      <td>2001-05-17</td>\n",
       "      <td>99</td>\n",
       "      <td>0.223520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20181</th>\n",
       "      <td>2001-05-18</td>\n",
       "      <td>99</td>\n",
       "      <td>0.446104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20182</th>\n",
       "      <td>2001-05-19</td>\n",
       "      <td>99</td>\n",
       "      <td>0.044783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20183</th>\n",
       "      <td>2001-05-20</td>\n",
       "      <td>99</td>\n",
       "      <td>0.483216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20184</th>\n",
       "      <td>2001-05-21</td>\n",
       "      <td>99</td>\n",
       "      <td>0.799660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20185 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ds  product_id     price\n",
       "0     2000-06-09           1  0.548814\n",
       "1     2000-06-10           1  0.715189\n",
       "2     2000-06-11           1  0.602763\n",
       "3     2000-06-12           1  0.544883\n",
       "4     2000-06-13           1  0.423655\n",
       "...          ...         ...       ...\n",
       "20180 2001-05-17          99  0.223520\n",
       "20181 2001-05-18          99  0.446104\n",
       "20182 2001-05-19          99  0.044783\n",
       "20183 2001-05-20          99  0.483216\n",
       "20184 2001-05-21          99  0.799660\n",
       "\n",
       "[20185 rows x 3 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynamic_series = series.rename(columns={'static_1': 'product_id'})\n",
    "prices_catalog = generate_prices_for_series(dynamic_series)\n",
    "prices_catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7730055-6bab-4b52-bc83-e8456358309e",
   "metadata": {},
   "source": [
    "And you have already merged these prices into your series dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2e34f7-3ee0-48a1-93d1-4cc77f1afdfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>static_0</th>\n",
       "      <th>product_id</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2000-10-05</td>\n",
       "      <td>3.981198</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>0.570826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2000-10-06</td>\n",
       "      <td>10.327401</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>0.260562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2000-10-07</td>\n",
       "      <td>17.657474</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>0.274048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2000-10-08</td>\n",
       "      <td>25.898790</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>0.433878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2000-10-09</td>\n",
       "      <td>34.494040</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>0.653738</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ds          y  static_0  product_id     price\n",
       "unique_id                                                      \n",
       "id_00     2000-10-05   3.981198        79          45  0.570826\n",
       "id_00     2000-10-06  10.327401        79          45  0.260562\n",
       "id_00     2000-10-07  17.657474        79          45  0.274048\n",
       "id_00     2000-10-08  25.898790        79          45  0.433878\n",
       "id_00     2000-10-09  34.494040        79          45  0.653738"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynamic_series = partitioned_series.rename(columns={'static_1': 'product_id'})\n",
    "dynamic_series = dynamic_series.reset_index()\n",
    "series_with_prices = dynamic_series.merge(prices_catalog, how='left')\n",
    "series_with_prices = series_with_prices.set_index('unique_id', sorted=True)\n",
    "series_with_prices.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e348d3-a449-4823-8d5c-1c8b13f9b133",
   "metadata": {},
   "source": [
    "This dataframe will be passed to `DistributedMLForecast.fit` (or `DistributedMLForecast.preprocess`), however since the price is dynamic we have to tell that method that only `static_0` and `product_id` are static and we'll have to update `price` in every timestep, which basically involves merging the updated features with the prices catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33ea277-66d3-4bf8-aeaa-cc117cfa8e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fedex/miniconda3/envs/mlforecast/lib/python3.10/site-packages/xgboost/dask.py:856: RuntimeWarning: coroutine 'Client._wait_for_workers' was never awaited\n",
      "  client.wait_for_workers(n_workers)\n",
      "INFO:distributed.worker:Run out-of-band function '_start_tracker'\n",
      "[17:41:02] task [xgboost.dask-0]:tcp://127.0.0.1:58576 got new rank 0\n",
      "[17:41:02] task [xgboost.dask-1]:tcp://127.0.0.1:58577 got new rank 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding random open ports for workers\n",
      "[LightGBM] [Info] Trying to bind port 58627...\n",
      "[LightGBM] [Info] Binding port 58627 succeeded\n",
      "[LightGBM] [Info] Listening...\n",
      "[LightGBM] [Warning] Connecting to rank 1 failed, waiting for 200 milliseconds\n",
      "[LightGBM] [Info] Trying to bind port 58628...\n",
      "[LightGBM] [Info] Binding port 58628 succeeded\n",
      "[LightGBM] [Info] Listening...\n",
      "[LightGBM] [Info] Connected to rank 1\n",
      "[LightGBM] [Info] Local rank: 0, total number of machines: 2\n",
      "[LightGBM] [Info] Connected to rank 0\n",
      "[LightGBM] [Info] Local rank: 1, total number of machines: 2\n",
      "[LightGBM] [Warning] num_threads is set=1, n_jobs=-1 will be ignored. Current value: num_threads=1\n",
      "[LightGBM] [Warning] num_threads is set=1, n_jobs=-1 will be ignored. Current value: num_threads=1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistributedMLForecast(models=[XGBForecast, LGBMForecast], freq=<Day>, lag_features=['lag7', 'expanding_mean_lag1', 'rolling_mean_lag7_window_size14'], date_features=['dayofweek', 'month'], num_threads=1, client=<Client: 'tcp://127.0.0.1:58569' processes=2 threads=2, memory=8.00 GiB>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcst = DistributedMLForecast(\n",
    "    models,\n",
    "    freq='D',\n",
    "    lags=[7],\n",
    "    lag_transforms={\n",
    "        1: [expanding_mean],\n",
    "        7: [(rolling_mean, 14)]\n",
    "    },\n",
    "    date_features=['dayofweek', 'month'],\n",
    "    num_threads=1,\n",
    ")\n",
    "series_with_prices = series_with_prices\n",
    "fcst.fit(\n",
    "    series_with_prices,\n",
    "    id_col='index',\n",
    "    time_col='ds',\n",
    "    target_col='y',\n",
    "    static_features=['static_0', 'product_id'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9bba71-8bb1-491b-bb0c-7ef8c6004f7b",
   "metadata": {},
   "source": [
    "So in order to update the price in each timestep we just call `DistributedForecast.predict` with our forecast horizon and pass the prices catalog as a dynamic dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3bc484-5e79-4139-acd5-c9ee3cf7270a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>XGBForecast</th>\n",
       "      <th>LGBMForecast</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2001-05-15</td>\n",
       "      <td>41.894302</td>\n",
       "      <td>42.632649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2001-05-16</td>\n",
       "      <td>50.457256</td>\n",
       "      <td>49.882593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2001-05-17</td>\n",
       "      <td>1.679064</td>\n",
       "      <td>2.032328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2001-05-18</td>\n",
       "      <td>10.395288</td>\n",
       "      <td>10.299019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2001-05-19</td>\n",
       "      <td>18.378986</td>\n",
       "      <td>18.576012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_99</th>\n",
       "      <td>2001-05-17</td>\n",
       "      <td>44.010822</td>\n",
       "      <td>44.152717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_99</th>\n",
       "      <td>2001-05-18</td>\n",
       "      <td>1.955814</td>\n",
       "      <td>2.112432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_99</th>\n",
       "      <td>2001-05-19</td>\n",
       "      <td>8.887053</td>\n",
       "      <td>9.113869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_99</th>\n",
       "      <td>2001-05-20</td>\n",
       "      <td>15.009413</td>\n",
       "      <td>15.525166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_99</th>\n",
       "      <td>2001-05-21</td>\n",
       "      <td>22.727417</td>\n",
       "      <td>22.887102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>700 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ds  XGBForecast  LGBMForecast\n",
       "unique_id                                      \n",
       "id_00     2001-05-15    41.894302     42.632649\n",
       "id_00     2001-05-16    50.457256     49.882593\n",
       "id_00     2001-05-17     1.679064      2.032328\n",
       "id_00     2001-05-18    10.395288     10.299019\n",
       "id_00     2001-05-19    18.378986     18.576012\n",
       "...              ...          ...           ...\n",
       "id_99     2001-05-17    44.010822     44.152717\n",
       "id_99     2001-05-18     1.955814      2.112432\n",
       "id_99     2001-05-19     8.887053      9.113869\n",
       "id_99     2001-05-20    15.009413     15.525166\n",
       "id_99     2001-05-21    22.727417     22.887102\n",
       "\n",
       "[700 rows x 3 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = fcst.predict(7, dynamic_dfs=[prices_catalog])\n",
    "preds.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b03634-d3e0-46e5-b05b-df7291fc8fdc",
   "metadata": {},
   "source": [
    "#### Custom predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa29c8f-65a6-4048-bf6a-0aa9874e2a2c",
   "metadata": {},
   "source": [
    "If you want to do something like scaling the predictions you can define a function and pass it to `DistributedMLForecast.predict` as described in <a href=\"/mlforecast/forecast.html#Custom-predictions\">Custom predictions</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec33846e-6d3c-4199-afe8-c5e064a53aac",
   "metadata": {},
   "source": [
    "### Cross validation\n",
    "Refer to `MLForecast.cross_validation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c97388-73f5-460d-9299-ab5cc6f31720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### DistributedMLForecast.cross_validation\n",
       "\n",
       ">      DistributedMLForecast.cross_validation (data:pandas.core.frame.DataFrame,\n",
       ">                                              n_windows:int, window_size:int,\n",
       ">                                              id_col:str, time_col:str,\n",
       ">                                              target_col:str, static_features:O\n",
       ">                                              ptional[List[str]]=None,\n",
       ">                                              dropna:bool=True,\n",
       ">                                              keep_last_n:Optional[int]=None, d\n",
       ">                                              ynamic_dfs:Optional[List[pandas.c\n",
       ">                                              ore.frame.DataFrame]]=None, befor\n",
       ">                                              e_predict_callback:Optional[Calla\n",
       ">                                              ble]=None, after_predict_callback\n",
       ">                                              :Optional[Callable]=None)\n",
       "\n",
       "Perform time series cross validation.\n",
       "Creates `n_windows` splits where each window has `window_size` test periods, \n",
       "trains the models, computes the predictions and merges the actuals.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| data | DataFrame |  | Series data in long format. |\n",
       "| n_windows | int |  | Number of windows to evaluate. |\n",
       "| window_size | int |  | Number of test periods in each window. |\n",
       "| id_col | str |  | Column that identifies each serie. If 'index' then the index is used. |\n",
       "| time_col | str |  | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str |  | Column that contains the target. |\n",
       "| static_features | Optional | None | Names of the features that are static and will be repeated when forecasting. |\n",
       "| dropna | bool | True | Drop rows with missing values produced by the transformations. |\n",
       "| keep_last_n | Optional | None | Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it. |\n",
       "| dynamic_dfs | Optional | None | Future values of the dynamic features, e.g. prices. |\n",
       "| before_predict_callback | Optional | None | Function to call on the features before computing the predictions.<br>    This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure.<br>    The series identifier is on the index. |\n",
       "| after_predict_callback | Optional | None | Function to call on the predictions before updating the targets.<br>    This function will take a pandas Series with the predictions and should return another one with the same structure.<br>    The series identifier is on the index.                |\n",
       "| **Returns** | **dask DataFrame** |  | **Predictions for each window with the series id, timestamp, last train date, target value and predictions from each model.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### DistributedMLForecast.cross_validation\n",
       "\n",
       ">      DistributedMLForecast.cross_validation (data:pandas.core.frame.DataFrame,\n",
       ">                                              n_windows:int, window_size:int,\n",
       ">                                              id_col:str, time_col:str,\n",
       ">                                              target_col:str, static_features:O\n",
       ">                                              ptional[List[str]]=None,\n",
       ">                                              dropna:bool=True,\n",
       ">                                              keep_last_n:Optional[int]=None, d\n",
       ">                                              ynamic_dfs:Optional[List[pandas.c\n",
       ">                                              ore.frame.DataFrame]]=None, befor\n",
       ">                                              e_predict_callback:Optional[Calla\n",
       ">                                              ble]=None, after_predict_callback\n",
       ">                                              :Optional[Callable]=None)\n",
       "\n",
       "Perform time series cross validation.\n",
       "Creates `n_windows` splits where each window has `window_size` test periods, \n",
       "trains the models, computes the predictions and merges the actuals.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| data | DataFrame |  | Series data in long format. |\n",
       "| n_windows | int |  | Number of windows to evaluate. |\n",
       "| window_size | int |  | Number of test periods in each window. |\n",
       "| id_col | str |  | Column that identifies each serie. If 'index' then the index is used. |\n",
       "| time_col | str |  | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str |  | Column that contains the target. |\n",
       "| static_features | Optional | None | Names of the features that are static and will be repeated when forecasting. |\n",
       "| dropna | bool | True | Drop rows with missing values produced by the transformations. |\n",
       "| keep_last_n | Optional | None | Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it. |\n",
       "| dynamic_dfs | Optional | None | Future values of the dynamic features, e.g. prices. |\n",
       "| before_predict_callback | Optional | None | Function to call on the features before computing the predictions.<br>    This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure.<br>    The series identifier is on the index. |\n",
       "| after_predict_callback | Optional | None | Function to call on the predictions before updating the targets.<br>    This function will take a pandas Series with the predictions and should return another one with the same structure.<br>    The series identifier is on the index.                |\n",
       "| **Returns** | **dask DataFrame** |  | **Predictions for each window with the series id, timestamp, last train date, target value and predictions from each model.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DistributedMLForecast.cross_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fab05e3-3d72-4afa-a74e-6961a3d9186a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fedex/miniconda3/envs/mlforecast/lib/python3.10/site-packages/xgboost/dask.py:856: RuntimeWarning: coroutine 'Client._wait_for_workers' was never awaited\n",
      "  client.wait_for_workers(n_workers)\n",
      "INFO:distributed.worker:Run out-of-band function '_start_tracker'\n",
      "[17:41:04] task [xgboost.dask-0]:tcp://127.0.0.1:58576 got new rank 0\n",
      "[17:41:04] task [xgboost.dask-1]:tcp://127.0.0.1:58577 got new rank 1\n",
      "[LightGBM] [Fatal] Socket recv error, Connection reset by peer (code: 54)\n",
      "INFO:distributed.nanny:Worker process 82431 was killed by signal 11\n",
      "INFO:distributed.core:Connection to tcp://127.0.0.1:58588 has been closed.\n",
      "INFO:distributed.scheduler:Remove client Client-worker-004a6226-8252-11ed-81ff-fe3a9be67f31\n",
      "INFO:distributed.core:Connection to tcp://127.0.0.1:58580 has been closed.\n",
      "INFO:distributed.scheduler:Remove worker <WorkerState 'tcp://127.0.0.1:58576', name: 0, status: running, memory: 45, processing: 1>\n",
      "INFO:distributed.core:Removing comms to tcp://127.0.0.1:58576\n",
      "INFO:distributed.scheduler:Close client connection: Client-worker-004a6226-8252-11ed-81ff-fe3a9be67f31\n",
      "2022-12-22 17:41:04,782 - distributed.nanny - WARNING - Restarting worker\n",
      "WARNING:distributed.nanny:Restarting worker\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding random open ports for workers\n",
      "[LightGBM] [Info] Trying to bind port 58640...\n",
      "[LightGBM] [Info] Binding port 58640 succeeded\n",
      "[LightGBM] [Info] Listening...\n",
      "[LightGBM] [Info] Trying to bind port 58639...\n",
      "[LightGBM] [Info] Binding port 58639 succeeded\n",
      "[LightGBM] [Info] Listening...\n",
      "[LightGBM] [Info] Connected to rank 1\n",
      "[LightGBM] [Info] Local rank: 0, total number of machines: 2\n",
      "[LightGBM] [Info] Connected to rank 0\n",
      "[LightGBM] [Info] Local rank: 1, total number of machines: 2\n",
      "[LightGBM] [Warning] num_threads is set=1, n_jobs=-1 will be ignored. Current value: num_threads=1\n",
      "[LightGBM] [Warning] num_threads is set=1, n_jobs=-1 will be ignored. Current value: num_threads=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-22 17:41:04,970 - distributed.worker - WARNING - Compute Failed\n",
      "Key:       _train_part-a860706a-23e3-4a5e-a7c2-962822bbead3\n",
      "Function:  _train_part\n",
      "args:      ()\n",
      "kwargs:    {'model_factory': <class 'lightgbm.sklearn.LGBMRegressor'>, 'params': {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'num_leaves': 31, 'objective': None, 'random_state': 0, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': 'warn', 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'tree_learner': 'data', 'num_threads': 1, 'machines': '127.0.0.1:58639,127.0.0.1:58640', 'local_listen_port': 58639, 'time_out': 120, 'num_machines': 2}, 'list_of_parts': [{'data':            static_0  static_1  ...  dayofweek  month\n",
      "unique_id                      ...                  \n",
      "id_89            51        17  ...          2      2\n",
      "id_89            51        17  ...          3      2\n",
      "id_89            51        17  ...          4      2\n",
      "id_89            51        17  ...          5      2\n",
      "id_89           \n",
      "Exception: \"LightGBMError('Socket recv error, Connection reset by peer (code: 54)')\"\n",
      "\n",
      "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:58642', name: 0, status: init, memory: 0, processing: 0>\n",
      "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:58642\n",
      "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:58644\n"
     ]
    },
    {
     "ename": "LightGBMError",
     "evalue": "Socket recv error, Connection reset by peer (code: 54)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLightGBMError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m n_windows \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m      2\u001b[0m window_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m14\u001b[39m\n\u001b[0;32m----> 4\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mfcst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_validation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartitioned_series\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_windows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mid_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mindex\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mds\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m cv_results\n",
      "Cell \u001b[0;32mIn[9], line 285\u001b[0m, in \u001b[0;36mDistributedMLForecast.cross_validation\u001b[0;34m(self, data, n_windows, window_size, id_col, time_col, target_col, static_features, dropna, keep_last_n, dynamic_dfs, before_predict_callback, after_predict_callback)\u001b[0m\n\u001b[1;32m    283\u001b[0m     freq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfreq\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_end, train, valid \u001b[38;5;129;01min\u001b[39;00m backtest_splits(data, n_windows, window_size, freq):\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mindex\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mds\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatic_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_last_n\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv_models_\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels_)\n\u001b[1;32m    287\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[1;32m    288\u001b[0m         window_size, dynamic_dfs, before_predict_callback, after_predict_callback,\n\u001b[1;32m    289\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[9], line 167\u001b[0m, in \u001b[0;36mDistributedMLForecast.fit\u001b[0;34m(self, data, id_col, time_col, target_col, static_features, dropna, keep_last_n)\u001b[0m\n\u001b[1;32m    165\u001b[0m train_ddf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(data, id_col, time_col, target_col, static_features, dropna, keep_last_n)\n\u001b[1;32m    166\u001b[0m X, y \u001b[38;5;241m=\u001b[39m train_ddf\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[time_col, target_col]), train_ddf[target_col]        \n\u001b[0;32m--> 167\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "Cell \u001b[0;32mIn[9], line 128\u001b[0m, in \u001b[0;36mDistributedMLForecast.fit_models\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels_ \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels_[name] \u001b[38;5;241m=\u001b[39m \u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmodel_\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlforecast/lib/python3.10/site-packages/lightgbm/dask.py:1354\u001b[0m, in \u001b[0;36mDaskLGBMRegressor.fit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, early_stopping_rounds, **kwargs)\u001b[0m\n\u001b[1;32m   1351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m early_stopping_rounds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1352\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mearly_stopping_rounds is not currently supported in lightgbm.dask\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1354\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lgb_dask_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_factory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLGBMRegressor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1357\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1358\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1360\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1361\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1362\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_init_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1364\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1366\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlforecast/lib/python3.10/site-packages/lightgbm/dask.py:1046\u001b[0m, in \u001b[0;36m_DaskLGBMModel._lgb_dask_fit\u001b[0;34m(self, model_factory, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, eval_at, early_stopping_rounds, **kwargs)\u001b[0m\n\u001b[1;32m   1043\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1044\u001b[0m params\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclient\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1046\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_get_dask_client\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_factory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1058\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_class_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_class_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1059\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_init_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1060\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_group\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_group\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1062\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_at\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_at\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1064\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel\u001b[38;5;241m.\u001b[39mget_params())\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lgb_dask_copy_extra_params(model, \u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlforecast/lib/python3.10/site-packages/lightgbm/dask.py:784\u001b[0m, in \u001b[0;36m_train\u001b[0;34m(client, data, label, params, model_factory, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, eval_at, **kwargs)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[38;5;66;03m# Tell each worker to train on the parts that it has locally\u001b[39;00m\n\u001b[1;32m    758\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;66;03m# This code treats ``_train_part()`` calls as not \"pure\" because:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[38;5;66;03m#        relies on global state (it and all the other LightGBM training processes\u001b[39;00m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;66;03m#        coordinate with each other)\u001b[39;00m\n\u001b[1;32m    765\u001b[0m futures_classifiers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    766\u001b[0m     client\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[1;32m    767\u001b[0m         _train_part,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m worker, list_of_parts \u001b[38;5;129;01min\u001b[39;00m worker_map\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    782\u001b[0m ]\n\u001b[0;32m--> 784\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures_classifiers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    785\u001b[0m results \u001b[38;5;241m=\u001b[39m [v \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m results \u001b[38;5;28;01mif\u001b[39;00m v]\n\u001b[1;32m    786\u001b[0m model \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/mlforecast/lib/python3.10/site-packages/distributed/client.py:2291\u001b[0m, in \u001b[0;36mClient.gather\u001b[0;34m(self, futures, errors, direct, asynchronous)\u001b[0m\n\u001b[1;32m   2289\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2290\u001b[0m     local_worker \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2291\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2292\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gather\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2294\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdirect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2296\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_worker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_worker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2297\u001b[0m \u001b[43m    \u001b[49m\u001b[43masynchronous\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43masynchronous\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2298\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlforecast/lib/python3.10/site-packages/distributed/utils.py:339\u001b[0m, in \u001b[0;36mSyncMethodMixin.sync\u001b[0;34m(self, func, asynchronous, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m future\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 339\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlforecast/lib/python3.10/site-packages/distributed/utils.py:406\u001b[0m, in \u001b[0;36msync\u001b[0;34m(loop, func, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error:\n\u001b[1;32m    405\u001b[0m     typ, exc, tb \u001b[38;5;241m=\u001b[39m error\n\u001b[0;32m--> 406\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/mlforecast/lib/python3.10/site-packages/distributed/utils.py:379\u001b[0m, in \u001b[0;36msync.<locals>.f\u001b[0;34m()\u001b[0m\n\u001b[1;32m    377\u001b[0m         future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mwait_for(future, callback_timeout)\n\u001b[1;32m    378\u001b[0m     future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(future)\n\u001b[0;32m--> 379\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m future\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m     error \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()\n",
      "File \u001b[0;32m~/miniconda3/envs/mlforecast/lib/python3.10/site-packages/tornado/gen.py:769\u001b[0m, in \u001b[0;36mRunner.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    766\u001b[0m exc_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    771\u001b[0m     exc_info \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()\n",
      "File \u001b[0;32m~/miniconda3/envs/mlforecast/lib/python3.10/site-packages/distributed/client.py:2154\u001b[0m, in \u001b[0;36mClient._gather\u001b[0;34m(self, futures, errors, direct, local_worker)\u001b[0m\n\u001b[1;32m   2152\u001b[0m         exc \u001b[38;5;241m=\u001b[39m CancelledError(key)\n\u001b[1;32m   2153\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2154\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exception\u001b[38;5;241m.\u001b[39mwith_traceback(traceback)\n\u001b[1;32m   2155\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m   2156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/mlforecast/lib/python3.10/site-packages/lightgbm/dask.py:319\u001b[0m, in \u001b[0;36m_train_part\u001b[0;34m()\u001b[0m\n\u001b[1;32m    305\u001b[0m         model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m    306\u001b[0m             data,\n\u001b[1;32m    307\u001b[0m             label,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    317\u001b[0m         )\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m         model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m    320\u001b[0m             data,\n\u001b[1;32m    321\u001b[0m             label,\n\u001b[1;32m    322\u001b[0m             sample_weight\u001b[38;5;241m=\u001b[39mweight,\n\u001b[1;32m    323\u001b[0m             init_score\u001b[38;5;241m=\u001b[39minit_score,\n\u001b[1;32m    324\u001b[0m             eval_set\u001b[38;5;241m=\u001b[39mlocal_eval_set,\n\u001b[1;32m    325\u001b[0m             eval_sample_weight\u001b[38;5;241m=\u001b[39mlocal_eval_sample_weight,\n\u001b[1;32m    326\u001b[0m             eval_init_score\u001b[38;5;241m=\u001b[39mlocal_eval_init_score,\n\u001b[1;32m    327\u001b[0m             eval_names\u001b[38;5;241m=\u001b[39mlocal_eval_names,\n\u001b[1;32m    328\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    329\u001b[0m         )\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    332\u001b[0m     _safe_call(_LIB\u001b[38;5;241m.\u001b[39mLGBM_NetworkFree())\n",
      "File \u001b[0;32m~/miniconda3/envs/mlforecast/lib/python3.10/site-packages/lightgbm/sklearn.py:895\u001b[0m, in \u001b[0;36mfit\u001b[0;34m()\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y,\n\u001b[1;32m    889\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, init_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    890\u001b[0m         eval_set\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, eval_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, eval_sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    891\u001b[0m         eval_init_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, early_stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    892\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m'\u001b[39m, feature_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m, categorical_feature\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    893\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, init_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;124;03m\"\"\"Docstring is inherited from the LGBMModel.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 895\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, init_score\u001b[38;5;241m=\u001b[39minit_score,\n\u001b[1;32m    896\u001b[0m                 eval_set\u001b[38;5;241m=\u001b[39meval_set, eval_names\u001b[38;5;241m=\u001b[39meval_names, eval_sample_weight\u001b[38;5;241m=\u001b[39meval_sample_weight,\n\u001b[1;32m    897\u001b[0m                 eval_init_score\u001b[38;5;241m=\u001b[39meval_init_score, eval_metric\u001b[38;5;241m=\u001b[39meval_metric,\n\u001b[1;32m    898\u001b[0m                 early_stopping_rounds\u001b[38;5;241m=\u001b[39mearly_stopping_rounds, verbose\u001b[38;5;241m=\u001b[39mverbose, feature_name\u001b[38;5;241m=\u001b[39mfeature_name,\n\u001b[1;32m    899\u001b[0m                 categorical_feature\u001b[38;5;241m=\u001b[39mcategorical_feature, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, init_model\u001b[38;5;241m=\u001b[39minit_model)\n\u001b[1;32m    900\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlforecast/lib/python3.10/site-packages/lightgbm/sklearn.py:748\u001b[0m, in \u001b[0;36mfit\u001b[0;34m()\u001b[0m\n\u001b[1;32m    745\u001b[0m evals_result \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    746\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mappend(record_evaluation(evals_result))\n\u001b[0;32m--> 748\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m train(\n\u001b[1;32m    749\u001b[0m     params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m    750\u001b[0m     train_set\u001b[38;5;241m=\u001b[39mtrain_set,\n\u001b[1;32m    751\u001b[0m     num_boost_round\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_estimators,\n\u001b[1;32m    752\u001b[0m     valid_sets\u001b[38;5;241m=\u001b[39mvalid_sets,\n\u001b[1;32m    753\u001b[0m     valid_names\u001b[38;5;241m=\u001b[39meval_names,\n\u001b[1;32m    754\u001b[0m     fobj\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fobj,\n\u001b[1;32m    755\u001b[0m     feval\u001b[38;5;241m=\u001b[39meval_metrics_callable,\n\u001b[1;32m    756\u001b[0m     init_model\u001b[38;5;241m=\u001b[39minit_model,\n\u001b[1;32m    757\u001b[0m     feature_name\u001b[38;5;241m=\u001b[39mfeature_name,\n\u001b[1;32m    758\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks\n\u001b[1;32m    759\u001b[0m )\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evals_result:\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evals_result \u001b[38;5;241m=\u001b[39m evals_result\n",
      "File \u001b[0;32m~/miniconda3/envs/mlforecast/lib/python3.10/site-packages/lightgbm/engine.py:292\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_before_iter:\n\u001b[1;32m    285\u001b[0m     cb(callback\u001b[38;5;241m.\u001b[39mCallbackEnv(model\u001b[38;5;241m=\u001b[39mbooster,\n\u001b[1;32m    286\u001b[0m                             params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m    287\u001b[0m                             iteration\u001b[38;5;241m=\u001b[39mi,\n\u001b[1;32m    288\u001b[0m                             begin_iteration\u001b[38;5;241m=\u001b[39minit_iteration,\n\u001b[1;32m    289\u001b[0m                             end_iteration\u001b[38;5;241m=\u001b[39minit_iteration \u001b[38;5;241m+\u001b[39m num_boost_round,\n\u001b[1;32m    290\u001b[0m                             evaluation_result_list\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m--> 292\u001b[0m booster\u001b[38;5;241m.\u001b[39mupdate(fobj\u001b[38;5;241m=\u001b[39mfobj)\n\u001b[1;32m    294\u001b[0m evaluation_result_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# check evaluation result.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlforecast/lib/python3.10/site-packages/lightgbm/basic.py:3021\u001b[0m, in \u001b[0;36mupdate\u001b[0;34m()\u001b[0m\n\u001b[1;32m   3019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_objective_to_none:\n\u001b[1;32m   3020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot update due to null objective function.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 3021\u001b[0m _safe_call(_LIB\u001b[38;5;241m.\u001b[39mLGBM_BoosterUpdateOneIter(\n\u001b[1;32m   3022\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[1;32m   3023\u001b[0m     ctypes\u001b[38;5;241m.\u001b[39mbyref(is_finished)))\n\u001b[1;32m   3024\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__is_predicted_cur_iter \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__num_dataset)]\n\u001b[1;32m   3025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m is_finished\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlforecast/lib/python3.10/site-packages/lightgbm/basic.py:125\u001b[0m, in \u001b[0;36m_safe_call\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m\"\"\"Check the return value from C API call.\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03m    The return value from C API calls.\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(_LIB\u001b[38;5;241m.\u001b[39mLGBM_GetLastError()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mLightGBMError\u001b[0m: Socket recv error, Connection reset by peer (code: 54)"
     ]
    }
   ],
   "source": [
    "n_windows = 2\n",
    "window_size = 14\n",
    "\n",
    "cv_results = fcst.cross_validation(\n",
    "    partitioned_series,\n",
    "    n_windows,\n",
    "    window_size,\n",
    "    id_col='index',\n",
    "    time_col='ds',\n",
    "    target_col='y',\n",
    ")\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fffbd79-f544-4cfc-8b62-ba009165c071",
   "metadata": {},
   "source": [
    "We can aggregate these by date to get a rough estimate of how our model is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a4a701-1779-4007-b9be-dbdee3454bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_results = cv_results.compute().groupby('ds').mean()\n",
    "agg_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da220459-5595-4bd4-901f-4c4f5ee8090c",
   "metadata": {},
   "source": [
    "We can also compute the error for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71efc3c1-f6ba-490f-8d19-9e4587c8e1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_from_dask_dataframe(ddf):\n",
    "    mses = {}\n",
    "    for model_name in ddf.columns.drop(['ds', 'y', 'cutoff']):\n",
    "        mses[model_name] = (ddf['y'] - ddf[model_name]).pow(2).mean()\n",
    "    return client.gather(client.compute(mses))\n",
    "\n",
    "{k: round(v, 2) for k, v in mse_from_dask_dataframe(cv_results).items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecb8e54-d964-42b2-9f24-33e7439f5436",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jose/mambaforge/envs/mlforecast/lib/python3.9/site-packages/xgboost/dask.py:856: RuntimeWarning: coroutine 'Client._wait_for_workers' was never awaited\n",
      "  client.wait_for_workers(n_workers)\n",
      "[01:58:31] task [xgboost.dask-0]:tcp://127.0.0.1:38361 got new rank 0\n",
      "[01:58:31] task [xgboost.dask-1]:tcp://127.0.0.1:39795 got new rank 1\n",
      "/home/jose/mambaforge/envs/mlforecast/lib/python3.9/site-packages/xgboost/dask.py:856: RuntimeWarning: coroutine 'Client._wait_for_workers' was never awaited\n",
      "  client.wait_for_workers(n_workers)\n",
      "[01:58:33] task [xgboost.dask-0]:tcp://127.0.0.1:38361 got new rank 0\n",
      "[01:58:33] task [xgboost.dask-1]:tcp://127.0.0.1:39795 got new rank 1\n"
     ]
    }
   ],
   "source": [
    "#|hide\n",
    "fcst = DistributedMLForecast(XGBForecast(random_state=0), lags=[7, 14])\n",
    "backtest_results = fcst.cross_validation(\n",
    "    non_std_series,\n",
    "    n_windows,\n",
    "    window_size,\n",
    "    id_col='some_id',\n",
    "    time_col='time',\n",
    "    target_col='value',\n",
    "    static_features=['static_0', 'static_1'],    \n",
    ").compute()\n",
    "renamer = {'some_id': 'unique_id', 'time': 'ds', 'value': 'y'}\n",
    "backtest_results = backtest_results.rename(columns=renamer).set_index('unique_id')\n",
    "renamed = non_std_series.rename(columns=renamer).set_index('unique_id')\n",
    "cv_models = fcst.cv_models_\n",
    "manual_results = []\n",
    "for i, (cutoff, train, valid) in enumerate(backtest_splits(renamed, n_windows, window_size, 1)):\n",
    "    fcst.preprocess(train)\n",
    "    fcst.models_ = cv_models[i]\n",
    "    pred = fcst.predict(window_size).compute()\n",
    "    res = valid[['ds', 'y']].compute()\n",
    "    res['cutoff'] = cutoff\n",
    "    res = res.merge(pred, on=['unique_id', 'ds'], how='left')\n",
    "    manual_results.append(res)\n",
    "manual_results = pd.concat(manual_results)\n",
    "pd.testing.assert_frame_equal(backtest_results, manual_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0086c2a-c5b3-4a40-9901-b05139c3d6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
