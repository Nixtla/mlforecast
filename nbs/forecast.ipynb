{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efa33fa-816f-463f-9215-9559b0ddd6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20376798-3d26-4c74-9e52-d5b657b7768a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccedaf1-56c9-4aaf-af7b-fe049df299ad",
   "metadata": {},
   "source": [
    "# Forecast\n",
    "\n",
    "> Full pipeline encapsulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b089a52-e06d-49b1-9328-793cffe56045",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from typing import Callable, Generator, List, Optional\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from mlforecast.core import TimeSeries, simple_predict\n",
    "from mlforecast.utils import backtest_splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07073d89-e33c-41d6-9bd3-a6daa07fef5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', UserWarning)\n",
    "\n",
    "from nbdev import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c823ab21-fabd-40aa-81fc-c8be0e7b12f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Forecast:\n",
    "    \"\"\"Full pipeline encapsulation. \n",
    "    \n",
    "    Takes a model (scikit-learn compatible regressor) and TimeSeries\n",
    "    and runs all the forecasting pipeline.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, ts: TimeSeries):\n",
    "        self.model = model\n",
    "        self.ts = ts\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f'Forecast(model={self.model}, ts={self.ts})'\n",
    "\n",
    "    def preprocess(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "    ) -> pd.DataFrame:\n",
    "        return self.ts.fit_transform(data, static_features, dropna, keep_last_n)\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,            \n",
    "        **fit_kwargs,\n",
    "    ) -> 'Forecast':\n",
    "        \"\"\"Preprocesses `data` and fits `model` to it.\"\"\"\n",
    "        series_df = self.preprocess(data, static_features, dropna, keep_last_n)\n",
    "        X, y = series_df.drop(columns=['ds', 'y']), series_df.y.values\n",
    "        self.train_features_ = X.columns\n",
    "        del series_df\n",
    "        self.model.fit(X, y, **fit_kwargs)\n",
    "        return self\n",
    "\n",
    "    def predict(\n",
    "        self, horizon: int, predict_fn: Callable = simple_predict, **predict_fn_kwargs\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Compute the predictions for the next `horizon` steps.\n",
    "        \n",
    "        `predict_fn(model, new_x, features_order, **predict_fn_kwargs)` is called in every timestep, where:\n",
    "        `model` is the trained model.\n",
    "        `new_x` is a dataframe with the same format as the input plus the computed features.\n",
    "        `features_order` is the list of column names that were used in the training step.\n",
    "        \"\"\"\n",
    "        return self.ts.predict(self.model, horizon, predict_fn, **predict_fn_kwargs)\n",
    "\n",
    "    def backtest(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        n_windows: int,\n",
    "        window_size: int,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,        \n",
    "        predict_fn: Callable = simple_predict,\n",
    "        **predict_fn_kwargs,\n",
    "    ) -> Generator[pd.DataFrame, None, None]:\n",
    "        \"\"\"Creates `n_windows` splits of `window_size` from `data`, trains the model\n",
    "        on the training set, predicts the window and merges the actuals and the predictions\n",
    "        in a dataframe.\n",
    "\n",
    "        Returns a generator to the dataframes containing the datestamps, actual values \n",
    "        and predictions.\"\"\"\n",
    "        for train, valid in backtest_splits(data, n_windows, window_size):\n",
    "            self.fit(train, static_features, dropna, keep_last_n)\n",
    "            y_pred = self.predict(window_size, predict_fn, **predict_fn_kwargs)\n",
    "            y_valid = valid[['ds', 'y']]\n",
    "            result = y_valid.merge(y_pred, on=['unique_id', 'ds'], how='left')\n",
    "            yield result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c7c92c-2ba9-4885-bbc1-8e4a4f67f0d9",
   "metadata": {},
   "source": [
    "The `Forecast` class is a high level abstraction that encapsulates all the steps in the pipeline (preprocessing, fitting the model and computing the predictions). It tries to mimic the scikit-learn API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5ec811-8876-4daa-84a2-2ebe0559a02b",
   "metadata": {},
   "source": [
    "## Example\n",
    "This shows an example with simulated data, for a real world example you can check the [M5 example](https://www.kaggle.com/lemuz90/m5-mlforecast)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3032775-f610-4091-a750-73219d904c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from window_ops.expanding import expanding_mean\n",
    "from window_ops.rolling import rolling_mean, rolling_std\n",
    "\n",
    "from mlforecast.utils import generate_daily_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb46457-6c08-420c-9a2e-ab0ae54dd41c",
   "metadata": {},
   "source": [
    "In order to forecast some time series you need a dataframe with `unique_id` as the index (which contains the identifier for each time serie), a `ds` column with the datestamps and a `y` column with the series values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c938e585-461e-464e-9a4b-4299a59f5272",
   "metadata": {},
   "source": [
    "### Data setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c776bb-b87b-4c01-a67a-ff95a055b5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "series = generate_daily_series(100, equal_ends=True, n_static_features=2)\n",
    "series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef89b3af-839e-43d4-9d09-ead84bedeed6",
   "metadata": {},
   "source": [
    "Whatever extra columns you have, like `static_0` and `static_1` here are considered to be static and are replicated when constructing the features for the next datestamp. You can disable this by passing `static_features` to `Forecast.preprocess` or `Forecast.fit` , which will only keep the columns you define there as static. Keep in mind that they will still be used for training, so you'll have to define a class that inherits from `TimeSeries` and override the `predict` method. This is shown in the [M5 example](https://www.kaggle.com/lemuz90/m5-mlforecast)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2929f996-45a0-4d48-9af3-9ba4607362e2",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1632163b-f049-4467-9635-7483982777b2",
   "metadata": {},
   "source": [
    "The model can be any scikit-learn compatible regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303dee3a-60ca-426b-a996-0e7a11efbbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4833ef5a-e31c-4ffb-9e62-f016e5b73c36",
   "metadata": {},
   "source": [
    "### TimeSeries\n",
    "The other component needed in `Forecast` is a `TimeSeries` object, which defines the features to be computed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfddfe2-7858-42b5-a10b-94a54ca0f529",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = TimeSeries(\n",
    "    freq='D',\n",
    "    lags=[7],\n",
    "    lag_transforms={\n",
    "        1: [expanding_mean],\n",
    "        7: [(rolling_mean, 14)]\n",
    "    },\n",
    "    date_features=['dayofweek', 'month'],\n",
    "    num_threads=2,    \n",
    ")\n",
    "ts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc538a8-97a1-4c5a-903e-9933188bdce0",
   "metadata": {},
   "source": [
    "Here where we say that:\n",
    "* Our series have daily frequency.\n",
    "* We want to use lag 7 as a feature\n",
    "* We want the lag transformations to be:\n",
    "   * expanding mean of the lag 1\n",
    "   * rolling mean of the lag 7 over a window of size 14\n",
    "* We want to use dayofweek and month as date features.\n",
    "* We want to perform the preprocessing and the forecasting steps using 2 threads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f06980a-e300-4855-ae8f-de4edb84627d",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6854f615-a7a1-45d9-9122-852478db2f8d",
   "metadata": {},
   "source": [
    "Once we have this setup we just instantiate a `Forecast` object with the model and the time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b62a31f-b470-49a8-bb05-73b3e6c02fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst = Forecast(model, ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b27ed8-c611-40ea-85cf-f97a2c56518f",
   "metadata": {},
   "source": [
    "From this point we have two options:\n",
    "\n",
    "1. Preprocess the data and fit our model using all of it.\n",
    "2. Preprocess the data and get it back as a dataframe to do some custom splitting or adding additional features, then training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00984e9a-3dbe-471f-98c7-a7ee82441575",
   "metadata": {},
   "source": [
    "#### 1. Using all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d42544-91a7-4c08-a190-925343e3c0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(Forecast.fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd38e85-4369-4711-b104-fd2ee4e6e8fb",
   "metadata": {},
   "source": [
    "Calling `.fit` on our data performs the preprocessing and uses all the data to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed6a43b-8bcf-4ea4-95fb-0f1460521f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst.fit(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36448fa6-4dd0-475e-90ec-2e804a7341b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst.model.fitted_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa1ea5b-73ed-4e5b-8e5d-7333b6c8ae14",
   "metadata": {},
   "source": [
    "#### 2. Preprocess and train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9662a990-8c9f-4d19-b42c-e03c97164296",
   "metadata": {},
   "source": [
    "If we only want to perform the preprocessing step we call `.preprocess` on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21260f3-bd8f-4143-bb60-deb9ca0d46e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(Forecast.preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e1ce17-4daa-413a-b30c-d989e681d33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = fcst.preprocess(series)\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ebc74f-3210-4b7e-b248-a0fbfdcfd1e8",
   "metadata": {},
   "source": [
    "This is useful if we want to inspect the data the model will be trained, adding additional features or performing some custom train-valid split. Here we perform a 80-20 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861f5c87-735d-4591-abbd-b8e9bc4256f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "train_mask = np.random.rand(features_df.shape[0]) < 0.8\n",
    "train, valid = features_df[train_mask], features_df[~train_mask]\n",
    "X_train, y_train = train.drop(columns=['ds', 'y']), train.y\n",
    "X_valid, y_valid = valid.drop(columns=['ds', 'y']), valid.y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f85d476-5096-4915-b267-13ff87f194bf",
   "metadata": {},
   "source": [
    "If we do this we must \"manually\" train our model calling `Forecast.model.fit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862ff93a-3c2f-4d2d-b318-b20330c6f617",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst.model.fit(X_train, y_train, \n",
    "               eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "               eval_metric='rmse',\n",
    "               verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3379e7-6fa9-4b6c-8909-9fc39d2f74ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lab in fcst.model.evals_result_.keys():\n",
    "    plt.plot(fcst.model.evals_result_[lab]['rmse'], label=lab)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41259cfa-6d22-4e57-b666-b50e3facf508",
   "metadata": {},
   "source": [
    "### Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdc813d-9ab9-4905-9864-442c09bba7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(Forecast.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907e47c2-eea4-4359-8557-7a1ae4c11752",
   "metadata": {},
   "source": [
    "Once we have this fitted model, we can compute the forecasts for the next 7 timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b224be-4f15-4b2b-8985-d8a590a384b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst.predict(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafe74b7-f0a3-4105-a11e-6da5b09ab7fe",
   "metadata": {},
   "source": [
    "This uses each prediction as the next value of the target and updates all features accordingly. The static features were propagated and the date features were computed using each new datestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab63b809-a70a-4067-891a-4fe3d6375069",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "preds = fcst.predict(7)\n",
    "preds2 = fcst.predict(7)\n",
    "\n",
    "np.testing.assert_equal(preds['y_pred'].values, preds2['y_pred'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50427c2-314d-4e8a-83a8-bc3099dbea7e",
   "metadata": {},
   "source": [
    "#### Custom predictions\n",
    "As you may have noticed `Forecast.predict` can take a `predict_fn` and `predict_fn_kwargs`. By default the predict method repeats the static features and updates the transformations and the date features. If you have dynamic features like prices or a calendar with holidays you can write a function that takes the trained model, the updated features and the features order and do custom stuff there, then pass it to `Forecast.predict`.\n",
    "\n",
    "Here's an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a42e014-1ee7-4935-8bc4-b76b97ca31ce",
   "metadata": {},
   "source": [
    "Suppose that we have a `product_id` column and we have a catalog for prices based on that `product_id` and the date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4d6354-e95e-444c-af11-18f273ee2d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "dynamic_series = series.rename(columns={'static_1': 'product_id'})\n",
    "day_offset = pd.tseries.frequencies.Day()\n",
    "starts_ends = dynamic_series.groupby('product_id')['ds'].agg([min, max])\n",
    "dfs = []\n",
    "for idx, (start, end) in starts_ends.iterrows():\n",
    "    product_df = pd.DataFrame(\n",
    "        {\n",
    "            'product_id': idx,\n",
    "            'price': np.random.rand((end - start).days + 8)\n",
    "        },\n",
    "        index=pd.date_range(start, end + 7 * day_offset, name='ds')\n",
    "    )\n",
    "    dfs.append(product_df)\n",
    "prices_catalog = pd.concat(dfs)\n",
    "series_with_prices = dynamic_series.reset_index().merge(prices_catalog, on=['ds', 'product_id'], how='left')\n",
    "series_with_prices = series_with_prices.set_index('unique_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29746ccf-8983-40bf-9872-9129ef300104",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c34b7c-9cb9-4f20-af01-6b0e18b7ded2",
   "metadata": {},
   "source": [
    "And you have already merged these prices into your series dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc453901-10d1-441f-af31-0ee52a3d58f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "series_with_prices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83710575-1276-4eeb-8cca-64165eee7154",
   "metadata": {},
   "source": [
    "This dataframe will be passed to `Forecast.fit` (or `Forecast.preprocess`), however since the price is dynamic we have to tell that method that only `static_0` and `product_id` are static and we'll have to update `price` in every timestep, which basically involves merging the updated features with the prices catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51961753-215f-4a0c-b5d5-a2678d63a4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst = Forecast(model, ts)\n",
    "fcst.fit(series_with_prices, static_features=['static_0', 'product_id'])\n",
    "fcst.ts.features_order_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3b2b4f-4ca7-47f6-bf7d-ab82deca1e21",
   "metadata": {},
   "source": [
    "The features used for training are stored in `Forecast.ts.features_order_`, as you can see `price` was used for training.\n",
    "\n",
    "In order to compute the predictions we'll need to update `price` in every timestep. `Forecast.predict` calls `predict_fn(model, new_x, features_order, **kwargs)` on every timestep, so we can pass a custom function there to do our join. `new_x` will have the same format as our input data plus the computed features, except it won't have `y` and the dynamic columns (`price` in this case).\n",
    "\n",
    "So what we have to do is take `new_x`, join with the prices catalog, sort by `unique_id` (in case the join modifies the row order) and then take only the columns that were used for training (this drops the `ds` and `unique_id` columns and arranges them in the correct order before passing the dataframe to the model). This can be achieved with the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b3f96f-4bea-4e29-84af-caa370b19c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_predict_fn(model, new_x, features_order):\n",
    "    new_x = new_x.reset_index('unique_id')  # to sort later\n",
    "    new_x = new_x.merge(prices_catalog, on=['ds', 'product_id'])\n",
    "    new_x = new_x.sort_values('unique_id')\n",
    "    new_x = new_x[features_order]  # features used for training\n",
    "    return model.predict(new_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472ced58-f746-4c2b-bf6f-9453d8344eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "class PredictPrice:\n",
    "    def fit(self, X, y=None):\n",
    "        self.features_ = X.columns\n",
    "        \n",
    "    def predict(self, X):\n",
    "        assert self.features_.equals(X.columns)\n",
    "        return X['price']\n",
    "\n",
    "dummy_model = PredictPrice()\n",
    "dummy_fcst = Forecast(dummy_model, ts)\n",
    "dummy_fcst.fit(series_with_prices, static_features=['static_0', 'product_id'])\n",
    "dummy_preds = dummy_fcst.predict(1, my_predict_fn)\n",
    "\n",
    "expected_prices = series_with_prices.reset_index()[['unique_id', 'product_id']].drop_duplicates()\n",
    "expected_prices['ds'] = series_with_prices['ds'].max() + fcst.ts.freq\n",
    "expected_prices = expected_prices.reset_index()\n",
    "expected_prices = expected_prices.merge(prices_catalog, on=['product_id', 'ds'], how='left')\n",
    "expected_prices = expected_prices.set_index('unique_id')[['ds', 'price']]\n",
    "\n",
    "assert dummy_preds.rename(columns={'y_pred': 'price'}).equals(expected_prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a252b2-b83c-49d0-bc59-6d4ae7f637fb",
   "metadata": {},
   "source": [
    "And now we just pass this function to `Forecast.predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d926bde-45db-4182-8488-48507af7a310",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = fcst.predict(7, my_predict_fn)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa7b0dd-d3e2-44c0-a25c-500e9ceaaa3d",
   "metadata": {},
   "source": [
    "### Backtesting\n",
    "\n",
    "If we would like to know how good our forecast will be for a specific model and set of features then we can perform backtesting. What backtesting does is take our data and split it in two parts, where the first part is used for training and the second one for validation. Since the data is time dependant we usually take the last *x* observations from our data as the validation set.\n",
    "\n",
    "This process is implemented in `Forecast.backtest`, which takes our data and performs the process described above for `n_windows` times where each window is of size `window_size`. For example, if we have 100 samples and we want to perform 2 backtests each of size 14, the splits will be as follows:\n",
    "\n",
    "1. Train: 1 to 72. Validation: 73 to 86.\n",
    "2. Train: 1 to 86. Validation: 87 to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e65bc38-7a1b-4f52-ace2-677fa9c3561d",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(Forecast.backtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307ea8a6-9245-4767-aab5-b31e90e98f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_windows = 2\n",
    "window_size = 14\n",
    "\n",
    "fcst = Forecast(model, ts)\n",
    "backtest_results = fcst.backtest(series, n_windows, window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d27945-e24c-40d1-8282-786d2c85866e",
   "metadata": {},
   "source": [
    "`Forecast.backtest` returns a generator that yields the results of each window one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33538cf6-877e-4068-a384-c1bf5176bd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "window1_result = next(backtest_results)\n",
    "window1_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf9f86b-d2e3-4311-88fb-8fff0ea676ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "window2_result = next(backtest_results)\n",
    "results = pd.concat([window1_result, window2_result])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc20827-e4db-4c14-aba5-66177d903614",
   "metadata": {},
   "source": [
    "We can aggregate these by date to get a rough estimate of how our model is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a0f301-474c-4abe-9e50-d52458fb19e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_results = results.groupby('ds').sum()\n",
    "agg_results.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c019933c-a6a7-4ac4-b9b8-f4668ed8ff1c",
   "metadata": {},
   "source": [
    "We can include some more context by using the values in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33410bb3-2c45-4003-840c-b042cde70511",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = series[series.ds < agg_results.index.min()]\n",
    "agg_history = history.groupby('ds')[['y']].sum().tail(50)\n",
    "agg_history.append(agg_results).plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cedc05-c9e8-4c9a-a6dd-a78118f635e8",
   "metadata": {},
   "source": [
    "Note that since the backtest results are returned as a generator we can also compute a single statistic on them and not keep the whole results in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a33a6f5-ab7c-4311-8c9b-c7185263f647",
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest_results = fcst.backtest(series, n_windows, window_size)\n",
    "\n",
    "losses = [mean_squared_error(res.y, res.y_pred) for res in backtest_results]\n",
    "np.round(losses, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
