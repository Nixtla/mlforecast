{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efa33fa-816f-463f-9215-9559b0ddd6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20376798-3d26-4c74-9e52-d5b657b7768a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccedaf1-56c9-4aaf-af7b-fe049df299ad",
   "metadata": {},
   "source": [
    "# Forecast\n",
    "\n",
    "> Full pipeline encapsulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b089a52-e06d-49b1-9328-793cffe56045",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import warnings\n",
    "from typing import Callable, Iterable, List, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import clone\n",
    "\n",
    "from mlforecast.core import (\n",
    "    DateFeature,\n",
    "    Differences,\n",
    "    Freq,\n",
    "    LagTransforms,\n",
    "    Lags,\n",
    "    Models,\n",
    "    TimeSeries,\n",
    ")\n",
    "from mlforecast.utils import backtest_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07073d89-e33c-41d6-9bd3-a6daa07fef5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastcore.test import test_warns\n",
    "from nbdev import show_doc\n",
    "from sklearn import set_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d9a4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "set_config(display='text')\n",
    "warnings.simplefilter('ignore', UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8303523-551f-48ed-8f81-3cfd222d6e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class MLForecast:\n",
    "    def __init__(\n",
    "        self,\n",
    "        models: Models,\n",
    "        freq: Optional[Freq] = None,\n",
    "        lags: Optional[Lags] = None,\n",
    "        lag_transforms: Optional[LagTransforms] = None,\n",
    "        date_features: Optional[Iterable[DateFeature]] = None,\n",
    "        differences: Optional[Differences] = None,\n",
    "        num_threads: int = 1,\n",
    "    ):\n",
    "        \"\"\"Create forecast object\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        models : regressor or list of regressors\n",
    "            Models that will be trained and used to compute the forecasts.\n",
    "        freq : str or int, optional (default=None)\n",
    "            Pandas offset alias, e.g. 'D', 'W-THU' or integer denoting the frequency of the series.\n",
    "        lags : list of int, optional (default=None)\n",
    "            Lags of the target to use as features.\n",
    "        lag_transforms : dict of int to list of functions, optional (default=None)\n",
    "            Mapping of target lags to their transformations.\n",
    "        date_features : list of str or callable, optional (default=None)\n",
    "            Features computed from the dates. Can be pandas date attributes or functions that will take the dates as input.\n",
    "        differences : list of int, optional (default=None)\n",
    "            Differences to take of the target before computing the features. These are restored at the forecasting step.\n",
    "        num_threads : int (default=1)\n",
    "            Number of threads to use when computing the features.\n",
    "        \"\"\"        \n",
    "        if not isinstance(models, list):\n",
    "            models = [clone(models)]\n",
    "        self.models = [clone(m) for m in models]\n",
    "        self.ts = TimeSeries(freq, lags, lag_transforms, date_features, differences, num_threads)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            f'{self.__class__.__name__}(models=[{\", \".join(m.__class__.__name__ for m in self.models)}], '\n",
    "            f'freq={self.freq}, '\n",
    "            f'lag_features={list(self.ts.transforms.keys())}, '\n",
    "            f'date_features={self.ts.date_features}, '\n",
    "            f'num_threads={self.ts.num_threads})'\n",
    "        )\n",
    "    \n",
    "    @property\n",
    "    def freq(self):\n",
    "        return self.ts.freq\n",
    "\n",
    "    def preprocess(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Add the features to `data`.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data : pandas DataFrame\n",
    "            Series data in long format.\n",
    "        id_col : str\n",
    "            Column that identifies each serie. If 'index' then the index is used.\n",
    "        time_col : str\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str\n",
    "            Column that contains the target.\n",
    "        static_features : list of str, optional (default=None)\n",
    "            Names of the features that are static and will be repeated when forecasting.\n",
    "        dropna : bool (default=True)\n",
    "            Drop rows with missing values produced by the transformations.\n",
    "        keep_last_n : int, optional (default=None)\n",
    "            Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        result : pandas DataFrame.\n",
    "            `data` plus added features.\n",
    "        \"\"\"\n",
    "        return self.ts.fit_transform(data, id_col, time_col, target_col, static_features, dropna, keep_last_n)\n",
    "    \n",
    "    def fit_models(\n",
    "        self,\n",
    "        X: pd.DataFrame,\n",
    "        y: Union[np.ndarray, pd.Series],\n",
    "    ) -> 'Forecast':\n",
    "        \"\"\"Manually train models. Use this if you called `Forecast.preprocess` beforehand.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas DataFrame\n",
    "            Features.\n",
    "        y : numpy array or pandas Series.\n",
    "            Target.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self : Forecast\n",
    "            Forecast object with trained models.\n",
    "        \"\"\"\n",
    "        self.models_ = []\n",
    "        for model in self.models:\n",
    "            self.models_.append(clone(model).fit(X, y))\n",
    "        return self\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "    ) -> 'Forecast':\n",
    "        \"\"\"Apply the feature engineering and train the models.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data : pandas DataFrame\n",
    "            Series data in long format.\n",
    "        id_col : str\n",
    "            Column that identifies each serie. If 'index' then the index is used.\n",
    "        time_col : str\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str\n",
    "            Column that contains the target.\n",
    "        static_features : list of str, optional (default=None)\n",
    "            Names of the features that are static and will be repeated when forecasting.\n",
    "        dropna : bool (default=True)\n",
    "            Drop rows with missing values produced by the transformations.\n",
    "        keep_last_n : int, optional (default=None)\n",
    "            Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : Forecast\n",
    "            Forecast object with series values and trained models.\n",
    "        \"\"\"\n",
    "        series_df = self.preprocess(data, id_col, time_col, target_col, static_features, dropna, keep_last_n)\n",
    "        X, y = series_df.drop(columns=[time_col, target_col]), series_df[target_col].values\n",
    "        del series_df\n",
    "        return self.fit_models(X, y)\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        horizon: int,\n",
    "        dynamic_dfs: Optional[List[pd.DataFrame]] = None,\n",
    "        predict_fn: Optional[Callable] = None,\n",
    "        **predict_fn_kwargs,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Compute the predictions for the next `horizon` steps.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        horizon : int\n",
    "            Number of periods to predict.\n",
    "        dynamic_dfs : list of pandas DataFrame, optional (default=None)\n",
    "            Future values of the dynamic features, e.g. prices.\n",
    "        predict_fn : callable, optional (default=None)\n",
    "            Custom function to compute predictions.\n",
    "            This function will recieve: model, new_x, dynamic_dfs, features_order and kwargs,\n",
    "            and should return an array with the predictions, where:\n",
    "                model : regressor\n",
    "                    Fitted model.\n",
    "                new_x : pandas DataFrame\n",
    "                    Current values of the features.\n",
    "                dynamic_dfs : list of pandas DataFrame\n",
    "                    Future values of the dynamic features\n",
    "                features_order : list of str\n",
    "                    Column names in the order in which they were used to train the model.\n",
    "                **kwargs\n",
    "                    Other keyword arguments passed to `Forecast.predict`.\n",
    "        **predict_fn_kwargs\n",
    "            Additional arguments passed to predict_fn\n",
    "                    \n",
    "        Returns\n",
    "        -------\n",
    "        result : pandas DataFrame\n",
    "            Predictions for each serie and timestep, with one column per model.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'models_'):\n",
    "            raise ValueError('No fitted models found. You have to call fit or preprocess + fit_models.')\n",
    "        return self.ts.predict(\n",
    "            self.models_, horizon, dynamic_dfs, predict_fn, **predict_fn_kwargs\n",
    "        )\n",
    "\n",
    "    def cross_validation(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        n_windows: int,\n",
    "        window_size: int,\n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "        dynamic_dfs: Optional[List[pd.DataFrame]] = None,\n",
    "        predict_fn: Optional[Callable] = None,\n",
    "        **predict_fn_kwargs,\n",
    "    ):\n",
    "        \"\"\"Perform time series cross validation.\n",
    "        Creates `n_windows` splits where each window has `window_size` test periods, \n",
    "        trains the models, computes the predictions and merges the actuals.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data : pandas DataFrame\n",
    "            Series data in long format.\n",
    "        n_windows : int\n",
    "            Number of windows to evaluate.\n",
    "        window_size : int\n",
    "            Number of test periods in each window.\n",
    "        id_col : str\n",
    "            Column that identifies each serie. If 'index' then the index is used.\n",
    "        time_col : str\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str\n",
    "            Column that contains the target.\n",
    "        static_features : list of str, optional (default=None)\n",
    "            Names of the features that are static and will be repeated when forecasting.\n",
    "        dropna : bool (default=True)\n",
    "            Drop rows with missing values produced by the transformations.\n",
    "        keep_last_n : int, optional (default=None)\n",
    "            Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it.\n",
    "        dynamic_dfs : list of pandas DataFrame, optional (default=None)\n",
    "            Future values of the dynamic features, e.g. prices.\n",
    "        predict_fn : callable, optional (default=None)\n",
    "            Custom function to compute predictions.\n",
    "            This function will recieve: model, new_x, dynamic_dfs, features_order and kwargs,\n",
    "            and should return an array with the predictions, where:\n",
    "                model : regressor\n",
    "                    Fitted model.\n",
    "                new_x : pandas DataFrame\n",
    "                    Current values of the features.\n",
    "                dynamic_dfs : list of pandas DataFrame\n",
    "                    Future values of the dynamic features\n",
    "                features_order : list of str\n",
    "                    Column names in the order in which they were used to train the model.\n",
    "                **kwargs\n",
    "                    Other keyword arguments passed to `Forecast.predict`.\n",
    "        **predict_fn_kwargs\n",
    "            Additional arguments passed to predict_fn                    \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        result : pandas DataFrame\n",
    "            Predictions for each window with the series id, timestamp, last train date, target value and predictions from each model.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        self.cv_models_ = []\n",
    "        if id_col != 'index':\n",
    "            data = data.set_index(id_col)\n",
    "        \n",
    "        if np.issubdtype(data[time_col].dtype.type, np.integer):\n",
    "            freq = 1\n",
    "        else:\n",
    "            freq = self.freq\n",
    "\n",
    "        for train_end, train, valid in backtest_splits(data, n_windows, window_size, freq, time_col):\n",
    "            self.fit(train, 'index', time_col, target_col, static_features, dropna, keep_last_n)\n",
    "            self.cv_models_.append(self.models_)\n",
    "            y_pred = self.predict(\n",
    "                window_size, dynamic_dfs, predict_fn, **predict_fn_kwargs\n",
    "            )\n",
    "            y_pred = y_pred.set_index(time_col, append=True)\n",
    "            result = valid.set_index(time_col, append=True)[[target_col]].copy()\n",
    "            result = result.join(y_pred).reset_index(time_col)\n",
    "            result['cutoff'] = train_end            \n",
    "            results.append(result)\n",
    "\n",
    "        out = pd.concat(results)\n",
    "        out = out[[time_col, 'cutoff', target_col, *y_pred.columns]]\n",
    "        if id_col != 'index':\n",
    "            out = out.reset_index()\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02b15e1-5ec8-493f-983b-7b6eedf6f9a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### MLForecast\n",
       "\n",
       ">      MLForecast (models:Union[sklearn.base.BaseEstimator,List[sklearn.base.Bas\n",
       ">                  eEstimator]], freq:Union[int,str,NoneType]=None,\n",
       ">                  lags:Optional[Iterable[int]]=None, lag_transforms:Optional[Di\n",
       ">                  ct[int,List[Union[Callable,Tuple[Callable,Any]]]]]=None,\n",
       ">                  date_features:Optional[Iterable[Union[str,Callable]]]=None,\n",
       ">                  differences:Optional[Iterable[int]]=None, num_threads:int=1)\n",
       "\n",
       "Create forecast object\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| models | typing.Union[sklearn.base.BaseEstimator, typing.List[sklearn.base.BaseEstimator]] |  | Models that will be trained and used to compute the forecasts. |\n",
       "| freq | typing.Union[int, str, NoneType] | None | Pandas offset alias, e.g. 'D', 'W-THU' or integer denoting the frequency of the series. |\n",
       "| lags | typing.Optional[typing.Iterable[int]] | None | Lags of the target to use as features. |\n",
       "| lag_transforms | typing.Optional[typing.Dict[int, typing.List[typing.Union[typing.Callable, typing.Tuple[typing.Callable, typing.Any]]]]] | None | Mapping of target lags to their transformations. |\n",
       "| date_features | typing.Optional[typing.Iterable[typing.Union[str, typing.Callable]]] | None | Features computed from the dates. Can be pandas date attributes or functions that will take the dates as input. |\n",
       "| differences | typing.Optional[typing.Iterable[int]] | None | Differences to take of the target before computing the features. These are restored at the forecasting step. |\n",
       "| num_threads | int | 1 | Number of threads to use when computing the features. |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### MLForecast\n",
       "\n",
       ">      MLForecast (models:Union[sklearn.base.BaseEstimator,List[sklearn.base.Bas\n",
       ">                  eEstimator]], freq:Union[int,str,NoneType]=None,\n",
       ">                  lags:Optional[Iterable[int]]=None, lag_transforms:Optional[Di\n",
       ">                  ct[int,List[Union[Callable,Tuple[Callable,Any]]]]]=None,\n",
       ">                  date_features:Optional[Iterable[Union[str,Callable]]]=None,\n",
       ">                  differences:Optional[Iterable[int]]=None, num_threads:int=1)\n",
       "\n",
       "Create forecast object\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| models | typing.Union[sklearn.base.BaseEstimator, typing.List[sklearn.base.BaseEstimator]] |  | Models that will be trained and used to compute the forecasts. |\n",
       "| freq | typing.Union[int, str, NoneType] | None | Pandas offset alias, e.g. 'D', 'W-THU' or integer denoting the frequency of the series. |\n",
       "| lags | typing.Optional[typing.Iterable[int]] | None | Lags of the target to use as features. |\n",
       "| lag_transforms | typing.Optional[typing.Dict[int, typing.List[typing.Union[typing.Callable, typing.Tuple[typing.Callable, typing.Any]]]]] | None | Mapping of target lags to their transformations. |\n",
       "| date_features | typing.Optional[typing.Iterable[typing.Union[str, typing.Callable]]] | None | Features computed from the dates. Can be pandas date attributes or functions that will take the dates as input. |\n",
       "| differences | typing.Optional[typing.Iterable[int]] | None | Differences to take of the target before computing the features. These are restored at the forecasting step. |\n",
       "| num_threads | int | 1 | Number of threads to use when computing the features. |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MLForecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c7c92c-2ba9-4885-bbc1-8e4a4f67f0d9",
   "metadata": {},
   "source": [
    "The `MLForecast` class is a high level abstraction that encapsulates all the steps in the pipeline (preprocessing, fitting the model and computing the predictions). It tries to mimic the scikit-learn API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1978a3-d98c-4a44-846a-ac6d8ffecb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Forecast(MLForecast):\n",
    "    def __init__(\n",
    "        self,\n",
    "        models: Models,\n",
    "        freq: Optional[Freq] = None,\n",
    "        lags: Optional[Lags] = None,\n",
    "        lag_transforms: Optional[LagTransforms] = None,\n",
    "        date_features: Optional[Iterable[DateFeature]] = None,\n",
    "        differences: Optional[Differences] = None,\n",
    "        num_threads: int = 1,\n",
    "    ):\n",
    "        warning_msg = (\n",
    "            'The Forecast class is deprecated and will be removed in a future version, '\n",
    "            'please use the MLForecast class instead.'\n",
    "        )\n",
    "        warnings.warn(warning_msg, DeprecationWarning)\n",
    "        super().__init__(models, freq, lags, lag_transforms, date_features, differences, num_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a9f64d-def2-45ad-879f-d191f231bfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_warns(lambda: Forecast([]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5ec811-8876-4daa-84a2-2ebe0559a02b",
   "metadata": {},
   "source": [
    "## Example\n",
    "This shows an example with just 4 series of the M4 dataset. If you want to run it yourself on all of them, you can refer to [this notebook](https://www.kaggle.com/code/lemuz90/m4-competition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3032775-f610-4091-a750-73219d904c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from datasetsforecast.m4 import M4, M4Info\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from window_ops.ewm import ewm_mean\n",
    "from window_ops.expanding import expanding_mean\n",
    "from window_ops.rolling import rolling_mean\n",
    "\n",
    "from mlforecast.utils import generate_daily_series, generate_prices_for_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c30ee9-38f3-4ef2-ab50-6ffc1e887f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "group = 'Hourly'\n",
    "await M4.async_download('data', group=group)\n",
    "df, *_ = M4.load(directory='data', group=group)\n",
    "df['ds'] = df['ds'].astype('int')\n",
    "ids = df['unique_id'].unique()\n",
    "random.seed(0)\n",
    "sample_ids = random.choices(ids, k=4)\n",
    "sample_df = df[df['unique_id'].isin(sample_ids)]\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaa4e47-4eee-43b6-8dba-1807bb00fda5",
   "metadata": {},
   "source": [
    "We now split this data into train and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91485631-d855-47f9-bdb4-94130b23c67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = M4Info[group]\n",
    "horizon = info.horizon\n",
    "valid = sample_df.groupby('unique_id').tail(horizon)\n",
    "train = sample_df.drop(valid.index)\n",
    "train.shape, valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c392f40-f181-4666-876e-1886e86eda5f",
   "metadata": {},
   "source": [
    "### Creating the Forecast object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc50c95b-359d-4e49-8f1f-40dab560c722",
   "metadata": {},
   "source": [
    "The forecast object encapsulates the feature engineering + training the models + forecasting. When we initialize it we define:\n",
    "\n",
    "* The models we want to train\n",
    "* The series frequency. This is added to the last dates seen in train for the forecast step, if the time column contains integer values we can leave it empty or set it to 1.\n",
    "* The feature engineering:\n",
    "    * Lags to use as features\n",
    "    * Transformations on the lags\n",
    "    * Date features\n",
    "    * Differences to apply to the target before computing the features, which are then restored when forecasting.\n",
    "* Number of threads to use when computing the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ebf59f-0fb7-45ff-b355-127952413245",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst = MLForecast(\n",
    "    models=lgb.LGBMRegressor(random_state=0),\n",
    "    differences=[24],\n",
    "    lags=[24 * (i+1) for i in range(7)],\n",
    "    lag_transforms={\n",
    "        48: [(ewm_mean, 0.3)],\n",
    "    },\n",
    "    num_threads=1,\n",
    ")\n",
    "fcst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10229c7-a69e-48fe-9ce8-f4754c58bec1",
   "metadata": {},
   "source": [
    "Once we have this setup we can compute the features and fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d42544-91a7-4c08-a190-925343e3c0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MLForecast.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53041a71-5d34-4c61-b28d-dcd0ccc3833b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst.fit(train, id_col='unique_id', time_col='ds', target_col='y');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5794fec5-fcc4-48c4-8f35-8b5add89a698",
   "metadata": {},
   "source": [
    "Once we've run this we're ready to compute our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdc813d-9ab9-4905-9864-442c09bba7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MLForecast.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c28426-e150-4fe8-b1f9-bcad401cbbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = fcst.predict(horizon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6cd880-ec12-46dc-9627-90d8521f9c38",
   "metadata": {},
   "source": [
    "We can see at a couple of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a953dd-e506-401b-bc94-c5f8373bb75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = valid.merge(predictions, on=['unique_id', 'ds']).set_index('unique_id')\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(16, 10))\n",
    "for uid, axi in zip(sample_ids, ax.flat):\n",
    "    results.loc[uid].set_index('ds').plot(ax=axi, title=uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b150efe0-bc84-483e-905f-c936e0aa0818",
   "metadata": {},
   "source": [
    "If you want to take a look at the data that will be used to train the models you can call `Forecast.preprocess`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21260f3-bd8f-4143-bb60-deb9ca0d46e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MLForecast.preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53214db1-05dd-4440-88b2-94763769e520",
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_df = fcst.preprocess(train, id_col='unique_id', time_col='ds', target_col='y')\n",
    "prep_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9959305d-8273-4994-8bab-4e1b5fd95966",
   "metadata": {},
   "source": [
    "If we do this we then have to call `Forecast.fit_models`, since this only stores the series information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6863d9e7-92ac-4302-8271-9e70cd5531aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MLForecast.fit_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69d243e-a271-4518-8a0d-432716fc8d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = prep_df.drop(columns=['ds', 'y']), prep_df['y']\n",
    "fcst.fit_models(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5eec03-60fa-4f35-b649-97ed5be35f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = fcst.predict(horizon)\n",
    "pd.testing.assert_frame_equal(predictions, predictions2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7d1b64-7b74-47b6-8bb7-e3ff63523eea",
   "metadata": {},
   "source": [
    "### Cross validation\n",
    "\n",
    "If we would like to know how good our forecast will be for a specific model and set of features then we can perform cross validation. What cross validation does is take our data and split it in two parts, where the first part is used for training and the second one for validation. Since the data is time dependant we usually take the last *x* observations from our data as the validation set.\n",
    "\n",
    "This process is implemented in `Forecast.cross_validation`, which takes our data and performs the process described above for `n_windows` times where each window has `window_size` validation samples in it. For example, if we have 100 samples and we want to perform 2 backtests each of size 14, the splits will be as follows:\n",
    "\n",
    "1. Train: 1 to 72. Validation: 73 to 86.\n",
    "2. Train: 1 to 86. Validation: 87 to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e65bc38-7a1b-4f52-ace2-677fa9c3561d",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MLForecast.cross_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541510fd-3fd5-49ab-baf3-9da4e9956087",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = fcst.cross_validation(\n",
    "    train,\n",
    "    n_windows=4,\n",
    "    window_size=horizon,\n",
    "    id_col='unique_id',\n",
    "    time_col='ds',\n",
    "    target_col='y',\n",
    ")\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e194aa-e261-400e-9264-1b54db9b163e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(16, 10))\n",
    "\n",
    "for uid, axi in zip(sample_ids, ax.flat):\n",
    "    subset = cv_results[cv_results['unique_id'].eq(uid)].drop(columns=['unique_id', 'cutoff'])\n",
    "    subset.set_index('ds').plot(ax=axi, title=uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1942738d-13a3-4641-8706-cd9578709ec0",
   "metadata": {},
   "source": [
    "### Dynamic features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7a5949-18a7-4e6a-a6f9-e812fb860c98",
   "metadata": {},
   "source": [
    "We're going to use a synthetic dataset from this point onwards to demonstrate some other functionalities regarding external regressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4861d6d5-2c7b-4f42-a383-70890512c630",
   "metadata": {},
   "outputs": [],
   "source": [
    "series = generate_daily_series(100, equal_ends=True, n_static_features=2, static_as_categorical=False)\n",
    "series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b5b920-853a-4fa0-91f2-625b0b6f5eec",
   "metadata": {},
   "source": [
    "As we saw in the previous example, the required columns are the series identifier, time and target. Whatever extra columns you have, like `static_0` and `static_1` here are considered to be static and are replicated when constructing the features for the next timestamp. You can disable this by passing `static_features` to `MLForecast.preprocess` or `MLForecast.fit` , which will only keep the columns you define there as static. Keep in mind that they will still be used for training, so you'll have to provide them to `MLForecast.predict` through the `dynamic_dfs` argument.\n",
    "\n",
    "By default the predict method repeats the static features and updates the transformations and the date features. If you have dynamic features like prices or a calendar with holidays you can pass them as a list to the `dynamic_dfs` argument of `MLForecast.predict`, which will call `pd.DataFrame.merge` on each of them in order.\n",
    "\n",
    "Here's an example:\n",
    "\n",
    "Suppose that we have a `product_id` column and we have a catalog for prices based on that `product_id` and the date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4d6354-e95e-444c-af11-18f273ee2d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_series = series.rename(columns={'static_1': 'product_id'})\n",
    "prices_catalog = generate_prices_for_series(dynamic_series)\n",
    "prices_catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61decbbd-91f6-4e90-a750-1eb97b880c90",
   "metadata": {},
   "source": [
    "And you have already merged these prices into your series dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc453901-10d1-441f-af31-0ee52a3d58f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "series_with_prices = dynamic_series.reset_index().merge(prices_catalog, how='left')\n",
    "series_with_prices.set_index('unique_id', inplace=True)\n",
    "series_with_prices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f373479-d37c-4f92-b12b-5c937898ef6b",
   "metadata": {},
   "source": [
    "This dataframe will be passed to `MLForecast.fit` (or `MLForecast.preprocess`), however since the price is dynamic we have to tell that method that only `static_0` and `product_id` are static and we'll have to update `price` in every timestep, which basically involves merging the updated features with the prices catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac4391d-5dd4-4eb9-91bb-e39fe67023c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def even_day(dates):\n",
    "    return dates.day % 2 == 0\n",
    "\n",
    "models = [\n",
    "    lgb.LGBMRegressor(n_jobs=1, random_state=0),\n",
    "    xgb.XGBRegressor(n_jobs=1, random_state=0),\n",
    "]\n",
    "fcst = MLForecast(\n",
    "    models,\n",
    "    freq='D',\n",
    "    lags=[7],\n",
    "    lag_transforms={\n",
    "        1: [expanding_mean],\n",
    "        7: [(rolling_mean, 14)]\n",
    "    },\n",
    "    date_features=['dayofweek', 'month', even_day],\n",
    "    num_threads=2,\n",
    ")\n",
    "fcst.fit(\n",
    "    series_with_prices,\n",
    "    id_col='index',\n",
    "    time_col='ds',\n",
    "    target_col='y',\n",
    "    static_features=['static_0', 'product_id'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c38fea-4868-4d34-9932-ce8af283d8ac",
   "metadata": {},
   "source": [
    "The features used for training are stored in `Forecast.ts.features_order_`, as you can see `price` was used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c160114-073e-483a-9c27-638487674b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst.ts.features_order_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f8d250-a5d8-4fb0-a41c-d5515b645c37",
   "metadata": {},
   "source": [
    "So in order to update the price in each timestep we just call `Forecast.predict` with our forecast horizon and pass the prices catalog as a dynamic dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e071906-3d80-4fa5-9b55-55c90f338985",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = fcst.predict(7, dynamic_dfs=[prices_catalog])\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab63b809-a70a-4067-891a-4fe3d6375069",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "preds2 = fcst.predict(7, dynamic_dfs=[prices_catalog])\n",
    "\n",
    "pd.testing.assert_frame_equal(preds, preds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ab2fd6-cc7f-47e3-8a87-e0ec21590385",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "non_std_series = series.copy()\n",
    "non_std_series['ds'] = non_std_series.groupby('unique_id').cumcount()\n",
    "non_std_series = non_std_series.reset_index().rename(columns={'unique_id': 'some_id', 'ds': 'time', 'y': 'value'})\n",
    "flow_params = dict(\n",
    "    models=models,\n",
    "    lags=[7],\n",
    "    lag_transforms={\n",
    "        1: [expanding_mean],\n",
    "        7: [(rolling_mean, 14)]\n",
    "    },\n",
    "    num_threads=2,\n",
    ")\n",
    "fcst = MLForecast(**flow_params)\n",
    "non_std_preds = fcst.fit(non_std_series, id_col='some_id', time_col='time', target_col='value').predict(7)\n",
    "non_std_preds.index.name = 'unique_id'\n",
    "fcst = MLForecast(freq='D', **flow_params)\n",
    "preds = fcst.fit(series, id_col='index', time_col='ds', target_col='y').predict(7)\n",
    "pd.testing.assert_frame_equal(preds.drop(columns='ds'), non_std_preds.drop(columns='time'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6fb8b4-f0a3-4012-a410-05972fe61ef1",
   "metadata": {},
   "source": [
    "### Custom predictions\n",
    "As you may have noticed `MLForecast.predict` can take a `predict_fn` and `predict_fn_kwargs`. By default the predict method repeats the static features and updates the transformations and the date features. If you have dynamic features you can pass them as a list to `MLForecast.predict` in the `dynamic_dfs` argument. However, if you want to do something else, you can define your own function which will take:\n",
    "\n",
    "* The trained model.\n",
    "* The updated features (static + transformations + date features).\n",
    "* A list of dataframes with the dynamic features.\n",
    "* The order of the features the model was trained on.\n",
    "* Additional keyword arguments passed to `MLForecast.predict`.\n",
    "\n",
    "Here's an example:\n",
    "\n",
    "Suppose that we want to scale our predictions so that our series are updated with these scaled values. We can achieve that with the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b3f96f-4bea-4e29-84af-caa370b19c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling_predict_fn(\n",
    "    model,\n",
    "    new_x,\n",
    "    dynamic_dfs,\n",
    "    features_order,\n",
    "    scale_factor,\n",
    ") -> np.ndarray:\n",
    "    new_x = new_x[features_order]\n",
    "    predictions = model.predict(new_x)\n",
    "    return scale_factor * predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a252b2-b83c-49d0-bc59-6d4ae7f637fb",
   "metadata": {},
   "source": [
    "And now we just pass this function to `MLForecast.predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395c58c7-2c30-4fba-89ae-d43a11ea6115",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst = MLForecast(models, freq='D', lags=[7])\n",
    "fcst.fit(series, id_col='index', time_col='ds', target_col='y')\n",
    "\n",
    "scale_factor = 1.1\n",
    "preds = fcst.predict(2, predict_fn=scaling_predict_fn, scale_factor=scale_factor)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f15e85-4cfe-4f04-b6bb-5f794dd12390",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "fcst.ts._predict_setup()\n",
    "\n",
    "for attr in ('head', 'tail'):\n",
    "    new_x = fcst.ts._update_features().drop(columns='ds')\n",
    "    original_preds = fcst.models_[0].predict(new_x)\n",
    "    \n",
    "    expected = scale_factor * original_preds\n",
    "    actual = getattr(preds.groupby('unique_id')[models[0].__class__.__name__], attr)(1).values\n",
    "    np.testing.assert_equal(expected, actual)\n",
    "    \n",
    "    fcst.ts._update_y(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383ca7ea-b275-4bc6-8ee7-3a41a46f3213",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "n_windows = 2\n",
    "window_size = 14\n",
    "fcst = MLForecast(lgb.LGBMRegressor(), freq='D', lags=[7, 14])\n",
    "backtest_results = fcst.cross_validation(\n",
    "    non_std_series,\n",
    "    n_windows,\n",
    "    window_size,\n",
    "    id_col='some_id',\n",
    "    time_col='time',\n",
    "    target_col='value',\n",
    "    static_features=['static_0', 'static_1'],\n",
    ")\n",
    "renamer = {'some_id': 'unique_id', 'time': 'ds', 'value': 'y'}\n",
    "backtest_results = backtest_results.rename(columns=renamer).set_index('unique_id')\n",
    "renamed = non_std_series.rename(columns=renamer).set_index('unique_id')\n",
    "manual_results = []\n",
    "for cutoff, train, valid in backtest_splits(renamed, n_windows, window_size, 1):\n",
    "    fcst.fit(\n",
    "        train,\n",
    "        id_col='index',\n",
    "        time_col='ds',\n",
    "        target_col='y',\n",
    "        static_features=['static_0', 'static_1'],\n",
    "    )\n",
    "    pred = fcst.predict(window_size)\n",
    "    res = valid[['ds', 'y']].copy()\n",
    "    res['cutoff'] = cutoff\n",
    "    res = res[['ds', 'cutoff', 'y']].copy()\n",
    "    manual_results.append(res.merge(pred, on=['unique_id', 'ds'], how='left'))\n",
    "manual_results = pd.concat(manual_results)\n",
    "pd.testing.assert_frame_equal(backtest_results, manual_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
