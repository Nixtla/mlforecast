{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efa33fa-816f-463f-9215-9559b0ddd6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20376798-3d26-4c74-9e52-d5b657b7768a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccedaf1-56c9-4aaf-af7b-fe049df299ad",
   "metadata": {},
   "source": [
    "# MLForecast\n",
    "\n",
    "> Full pipeline encapsulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b089a52-e06d-49b1-9328-793cffe56045",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import copy\n",
    "import warnings\n",
    "from typing import Callable, Dict, Iterable, List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, clone\n",
    "\n",
    "from mlforecast.core import (\n",
    "    DateFeature,\n",
    "    Differences,\n",
    "    Freq,\n",
    "    LagTransforms,\n",
    "    Lags,\n",
    "    Models,\n",
    "    TimeSeries,\n",
    "    _name_models,\n",
    ")\n",
    "from mlforecast.lgb_cv import LightGBMCV\n",
    "from mlforecast.utils import backtest_splits, _cotransform, PredictionIntervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07073d89-e33c-41d6-9bd3-a6daa07fef5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastcore.test import test_warns, test_eq, test_ne, test_fail\n",
    "from nbdev import show_doc\n",
    "from sklearn import set_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d9a4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "set_config(display='text')\n",
    "warnings.simplefilter('ignore', UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574354b3-0d44-44df-9004-79f5c28f9d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _add_conformal_intervals(\n",
    "        fcst_df: pd.DataFrame, \n",
    "        cs_df: pd.DataFrame, \n",
    "        model_names: List[str],\n",
    "        level: List[Union[int, float]],\n",
    "        cs_n_windows: int,\n",
    "        cs_window_size: int,\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds conformal intervals to a `fcst_df` based on conformal scores `cs_df`.\n",
    "    `level` should be already sorted.\n",
    "    \"\"\"\n",
    "    cuts = [lv / 100 for lv in level]\n",
    "    for model in model_names:\n",
    "        quantiles = np.quantile(\n",
    "            cs_df[model].values.reshape(cs_n_windows, cs_window_size), \n",
    "            cuts, \n",
    "            axis=0, \n",
    "        ).T\n",
    "        lo_cols = [f'{model}-lo-{lv}' for lv in reversed(level)]\n",
    "        hi_cols = [f'{model}-hi-{lv}' for lv in level]\n",
    "        mean = fcst_df[model].values.reshape(-1, 1)\n",
    "        fcst_df[lo_cols] = mean - quantiles[:, ::-1]\n",
    "        fcst_df[hi_cols] = mean + quantiles\n",
    "    return fcst_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9683ecd-615a-401f-83e2-39123b7236e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _schema_conformal_intervals(model_names, level, id_col, time_col, dtypes, fcst_df_columns):\n",
    "    \"\"\"Returns schema for conformal intervals.\"\"\"\n",
    "    models_schema = ','.join(f'{model_name}:double' for model_name in model_names)\n",
    "    level_schema = ''\n",
    "    for model in model_names:\n",
    "        level_schema += ','\n",
    "        lo_cols = [f'{model}-lo-{lv}:double' for lv in reversed(level)]\n",
    "        hi_cols = [f'{model}-hi-{lv}:double' for lv in level]\n",
    "        level_schema += ','.join(lo_cols) + ',' + ','.join(hi_cols)\n",
    "    id_col = id_col if id_col != 'index' else fcst_df_columns[0]\n",
    "    id_col_type = dtypes.loc[id_col] if id_col != 'index' else dtypes.loc[fcst_df_columns[0]]\n",
    "    if id_col_type == 'category':\n",
    "        raise NotImplementedError(\n",
    "            'Use of `category` type to identify each time series is not yet implemented. '\n",
    "            f'Please transform your {id_col} to string to continue.'\n",
    "        )\n",
    "    id_col_type = 'string' if id_col_type == 'object' else id_col_type\n",
    "    ts_col_type = f'{dtypes.loc[time_col]}'.replace('64[ns]', '')\n",
    "    schema = f'{id_col}:{id_col_type},{time_col}:{ts_col_type},' + models_schema + level_schema\n",
    "    return schema, id_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c9ae58-9a0e-4762-b182-0641f37281c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from mlforecast.utils import generate_daily_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac64dc78-b581-46e2-a5b8-ce4d353a6d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test for indexed data\n",
    "df = generate_daily_series(1)\n",
    "df.index = df.index.astype(str)\n",
    "schema_index, id_col_indx = _schema_conformal_intervals(\n",
    "    ['model1', 'model2'], \n",
    "    [80, 90], \n",
    "    'index', \n",
    "    'ds', \n",
    "    df.reset_index().dtypes, # inside predict, the index is reseted\n",
    "    df.reset_index().columns, # inside predict, the index is reseted\n",
    ")\n",
    "test_eq(\n",
    "    schema_index,\n",
    "    ('unique_id:string,ds:datetime,'\n",
    "     'model1:double,model2:double,'\n",
    "     'model1-lo-90:double,model1-lo-80:double,'\n",
    "     'model1-hi-80:double,model1-hi-90:double,'\n",
    "     'model2-lo-90:double,model2-lo-80:double,'\n",
    "     'model2-hi-80:double,model2-hi-90:double')\n",
    ")\n",
    "test_eq(\n",
    "    id_col_indx,\n",
    "    'unique_id'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de296868-16fb-440f-93d6-7f51b1d6c323",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test for not indexed data\n",
    "df = generate_daily_series(1)\n",
    "df.index = df.index.astype(str)\n",
    "df = df.reset_index()\n",
    "schema_index, id_col_indx = _schema_conformal_intervals(\n",
    "    ['model1', 'model2'], \n",
    "    [80, 90], \n",
    "    'unique_id', \n",
    "    'ds', \n",
    "    df.dtypes, # inside predict, the index is reseted\n",
    "    df.columns, # inside predict, the index is reseted\n",
    ")\n",
    "test_eq(\n",
    "    schema_index,\n",
    "    ('unique_id:string,ds:datetime,'\n",
    "     'model1:double,model2:double,'\n",
    "     'model1-lo-90:double,model1-lo-80:double,'\n",
    "     'model1-hi-80:double,model1-hi-90:double,'\n",
    "     'model2-lo-90:double,model2-lo-80:double,'\n",
    "     'model2-hi-80:double,model2-hi-90:double')\n",
    ")\n",
    "test_eq(\n",
    "    id_col_indx,\n",
    "    'unique_id'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d62cf0-6ab2-4664-b8fa-3c01cdbd9dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test category error\n",
    "df = generate_daily_series(1)\n",
    "test_fail(\n",
    "    lambda: _schema_conformal_intervals(\n",
    "        ['model1', 'model2'], \n",
    "        [80, 90], \n",
    "        'index', \n",
    "        'ds', \n",
    "        df.reset_index().dtypes, # inside predict, the index is reseted\n",
    "        df.reset_index().columns, # inside predict, the index is reseted\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8303523-551f-48ed-8f81-3cfd222d6e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class MLForecast:\n",
    "    def __init__(\n",
    "        self,\n",
    "        models: Models,\n",
    "        freq: Optional[Freq] = None,\n",
    "        lags: Optional[Lags] = None,\n",
    "        lag_transforms: Optional[LagTransforms] = None,\n",
    "        date_features: Optional[Iterable[DateFeature]] = None,\n",
    "        differences: Optional[Differences] = None,\n",
    "        num_threads: int = 1,\n",
    "    ):\n",
    "        \"\"\"Create forecast object\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        models : regressor or list of regressors\n",
    "            Models that will be trained and used to compute the forecasts.\n",
    "        freq : str or int or pd.offsets.BaseOffset, optional (default=None)\n",
    "            Pandas offset, pandas offset alias, e.g. 'D', 'W-THU' or integer denoting the frequency of the series.\n",
    "        lags : list of int, optional (default=None)\n",
    "            Lags of the target to use as features.\n",
    "        lag_transforms : dict of int to list of functions, optional (default=None)\n",
    "            Mapping of target lags to their transformations.\n",
    "        date_features : list of str or callable, optional (default=None)\n",
    "            Features computed from the dates. Can be pandas date attributes or functions that will take the dates as input.\n",
    "        differences : list of int, optional (default=None)\n",
    "            Differences to take of the target before computing the features. These are restored at the forecasting step.\n",
    "        num_threads : int (default=1)\n",
    "            Number of threads to use when computing the features.\n",
    "        \"\"\"\n",
    "        if not isinstance(models, dict) and not isinstance(models, list):\n",
    "            models = [models]\n",
    "        if isinstance(models, list):\n",
    "            model_names = _name_models([m.__class__.__name__ for m in models])            \n",
    "            models_with_names = dict(zip(model_names, models))\n",
    "        else:\n",
    "            models_with_names = models\n",
    "        self.models = models_with_names\n",
    "        self.ts = TimeSeries(freq, lags, lag_transforms, date_features, differences, num_threads)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            f'{self.__class__.__name__}(models=[{\", \".join(self.models.keys())}], '\n",
    "            f'freq={self.freq}, '\n",
    "            f'lag_features={list(self.ts.transforms.keys())}, '\n",
    "            f'date_features={self.ts.date_features}, '\n",
    "            f'num_threads={self.ts.num_threads})'\n",
    "        )\n",
    "    \n",
    "    @property\n",
    "    def freq(self):\n",
    "        return self.ts.freq\n",
    "    \n",
    "    @classmethod\n",
    "    def from_cv(cls, cv: LightGBMCV) -> 'MLForecast':\n",
    "        if not hasattr(cv, 'best_iteration_'):\n",
    "            raise ValueError('LightGBMCV object must be fitted first.')\n",
    "        import lightgbm as lgb\n",
    "        fcst = cls(lgb.LGBMRegressor(**{**cv.params, 'n_estimators': cv.best_iteration_}))\n",
    "        fcst.ts = copy.deepcopy(cv.ts)\n",
    "        return fcst\n",
    "\n",
    "    def preprocess(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "        max_horizon: Optional[int] = None,\n",
    "        return_X_y: bool = False,\n",
    "    ) -> Union[pd.DataFrame, Tuple[pd.DataFrame, Union[pd.Series, pd.DataFrame]]]:\n",
    "        \"\"\"Add the features to `data`.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data : pandas DataFrame\n",
    "            Series data in long format.\n",
    "        id_col : str\n",
    "            Column that identifies each serie. If 'index' then the index is used.\n",
    "        time_col : str\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str\n",
    "            Column that contains the target.\n",
    "        static_features : list of str, optional (default=None)\n",
    "            Names of the features that are static and will be repeated when forecasting.\n",
    "        dropna : bool (default=True)\n",
    "            Drop rows with missing values produced by the transformations.\n",
    "        keep_last_n : int, optional (default=None)\n",
    "            Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it.\n",
    "        max_horizon: int, optional (default=None)\n",
    "            Train this many models, where each model will predict a specific horizon.\n",
    "        return_X_y: bool (default=False)\n",
    "            Return a tuple with the features and the target. If False will return a single dataframe.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        result : pandas DataFrame or tuple of pandas Dataframe and either a pandas Series or a pandas Dataframe (for multi-output regression).\n",
    "            `data` plus added features and target(s).\n",
    "        \"\"\"\n",
    "        return self.ts.fit_transform(\n",
    "            data,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            static_features=static_features,\n",
    "            dropna=dropna,\n",
    "            keep_last_n=keep_last_n,\n",
    "            max_horizon=max_horizon,\n",
    "            return_X_y=return_X_y,\n",
    "        )\n",
    "    \n",
    "    def fit_models(\n",
    "        self,\n",
    "        X: pd.DataFrame,\n",
    "        y: Union[pd.Series, pd.DataFrame],\n",
    "    ) -> 'MLForecast':\n",
    "        \"\"\"Manually train models. Use this if you called `Forecast.preprocess` beforehand.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas DataFrame\n",
    "            Features.\n",
    "        y : pandas Series or pandas DataFrame (multi-output).\n",
    "            Target.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self : Forecast\n",
    "            Forecast object with trained models.\n",
    "        \"\"\"\n",
    "        self.models_: Dict[str, Union[BaseEstimator, List[BaseEstimator]]] = {}\n",
    "        for name, model in self.models.items():\n",
    "            if y.ndim == 2 and y.shape[1] > 1:\n",
    "                self.models_[name] = []                \n",
    "                for col in y:\n",
    "                    keep = y[col].notnull()\n",
    "                    self.models_[name].append(clone(model).fit(X.loc[keep], y.loc[keep, col]))\n",
    "            else:\n",
    "                self.models_[name] = clone(model).fit(X, y)\n",
    "        return self\n",
    "    \n",
    "    def _conformity_scores(\n",
    "        self,\n",
    "        data: pd.DataFrame, \n",
    "        id_col: str, \n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "        max_horizon: Optional[int] = None,\n",
    "        n_windows: int = 2,\n",
    "        window_size: int = 1,\n",
    "    ):\n",
    "        \"\"\"Compute conformity scores.\n",
    "        \n",
    "        We need at least two cross validation errors to compute\n",
    "        quantiles for prediction intervals (`n_windows=2`).\n",
    "        \n",
    "        The exception is raised by the PredictionIntervals data class.\n",
    "        \n",
    "        In this simplest case, we assume the width of the interval\n",
    "        is the same for all the forecasting horizon (`window_size=1`).\n",
    "        \"\"\"\n",
    "        cv_results = self.cross_validation(\n",
    "            data=data, \n",
    "            n_windows=n_windows,\n",
    "            window_size=window_size,\n",
    "            refit=False,\n",
    "            id_col=id_col, \n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            static_features=static_features,\n",
    "            dropna=dropna,\n",
    "            keep_last_n=keep_last_n,\n",
    "            max_horizon=max_horizon,\n",
    "            prediction_intervals=None,\n",
    "        )\n",
    "        # conformity score for each model\n",
    "        for model in self.models.keys():\n",
    "            # compute absolute error for each model\n",
    "            cv_results[model] = np.abs(cv_results[model] - cv_results[target_col])\n",
    "        if id_col == 'index':\n",
    "            cv_results = cv_results.reset_index()\n",
    "        return cv_results.drop('y', axis=1)\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "        max_horizon: Optional[int] = None,\n",
    "        prediction_intervals: Optional[PredictionIntervals] = None,\n",
    "    ) -> 'MLForecast':\n",
    "        \"\"\"Apply the feature engineering and train the models.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data : pandas DataFrame\n",
    "            Series data in long format.\n",
    "        id_col : str\n",
    "            Column that identifies each serie. If 'index' then the index is used.\n",
    "        time_col : str\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str\n",
    "            Column that contains the target.\n",
    "        static_features : list of str, optional (default=None)\n",
    "            Names of the features that are static and will be repeated when forecasting.\n",
    "        dropna : bool (default=True)\n",
    "            Drop rows with missing values produced by the transformations.\n",
    "        keep_last_n : int, optional (default=None)\n",
    "            Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it.\n",
    "        max_horizon: int, optional (default=None)\n",
    "            Train this many models, where each model will predict a specific horizon.\n",
    "        prediction_intervals : PredictionIntervals, optional (default=None)\n",
    "            Configuration to calibrate prediction intervals (Conformal Prediction).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : Forecast\n",
    "            Forecast object with series values and trained models.\n",
    "        \"\"\"\n",
    "        self._cs_df: Optional[pd.DataFrame] = None\n",
    "        if prediction_intervals is not None:\n",
    "            self.prediction_intervals = prediction_intervals\n",
    "            self._cs_df = self._conformity_scores(\n",
    "                data=data, \n",
    "                id_col=id_col, \n",
    "                time_col=time_col,\n",
    "                target_col=target_col,\n",
    "                static_features=static_features,\n",
    "                dropna=dropna,\n",
    "                keep_last_n=keep_last_n,\n",
    "                n_windows=prediction_intervals.n_windows,\n",
    "                window_size=prediction_intervals.window_size,\n",
    "            )\n",
    "        X, y = self.preprocess(\n",
    "            data,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            static_features=static_features,\n",
    "            dropna=dropna,\n",
    "            keep_last_n=keep_last_n,\n",
    "            max_horizon=max_horizon,\n",
    "            return_X_y=True,\n",
    "        )\n",
    "        features = X.columns.drop(time_col)\n",
    "        if id_col != 'index' and id_col not in self.ts.static_features:\n",
    "            features = features.drop(id_col)\n",
    "        X = X[features]\n",
    "        return self.fit_models(X, y)\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        horizon: int,\n",
    "        dynamic_dfs: Optional[List[pd.DataFrame]] = None,\n",
    "        before_predict_callback: Optional[Callable] = None,\n",
    "        after_predict_callback: Optional[Callable] = None,\n",
    "        new_data: Optional[pd.DataFrame] = None,\n",
    "        level: Optional[List[Union[int, float]]] = None,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Compute the predictions for the next `horizon` steps.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        horizon : int\n",
    "            Number of periods to predict.\n",
    "        dynamic_dfs : list of pandas DataFrame, optional (default=None)\n",
    "            Future values of the dynamic features, e.g. prices.\n",
    "        before_predict_callback : callable, optional (default=None)\n",
    "            Function to call on the features before computing the predictions.\n",
    "                This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure.\n",
    "                The series identifier is on the index.\n",
    "        after_predict_callback : callable, optional (default=None)\n",
    "            Function to call on the predictions before updating the targets.\n",
    "                This function will take a pandas Series with the predictions and should return another one with the same structure.\n",
    "                The series identifier is on the index.   \n",
    "        new_data : pandas DataFrame, optional (default=None)\n",
    "            Series data of new observations for which forecasts are to be generated. \n",
    "                This dataframe should have the same structure as the one used to fit the model, including any features and time series data. \n",
    "                If `new_data` is not None, the method will generate forecasts for the new observations.\n",
    "        level : list of ints or floats, optional (default=None)\n",
    "            Confidence levels between 0 and 100 for prediction intervals.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        result : pandas DataFrame\n",
    "            Predictions for each serie and timestep, with one column per model.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'models_'):\n",
    "            raise ValueError('No fitted models found. You have to call fit or preprocess + fit_models.')\n",
    "            \n",
    "        if new_data is not None:\n",
    "            new_ts = TimeSeries(\n",
    "                self.ts.freq, self.ts.lags, \n",
    "                self.ts.lag_transforms, self.ts.date_features, \n",
    "                self.ts.differences, self.ts.num_threads\n",
    "            )\n",
    "            new_ts._fit(\n",
    "                new_data, \n",
    "                self.ts.id_col, self.ts.time_col, self.ts.target_col, \n",
    "                self.ts.static_features.columns, \n",
    "                self.ts.keep_last_n\n",
    "            )\n",
    "            new_ts.max_horizon = self.ts.max_horizon\n",
    "            ts = new_ts\n",
    "        else:\n",
    "            ts = self.ts\n",
    "            \n",
    "        forecasts = ts.predict(\n",
    "            self.models_, horizon, dynamic_dfs, before_predict_callback, after_predict_callback\n",
    "        )\n",
    "        if level is not None:\n",
    "            if self._cs_df is None:\n",
    "                warn_msg = (\n",
    "                    'Please rerun the `fit` method passing a proper value '\n",
    "                    'to prediction intervals to compute them.'\n",
    "                )\n",
    "                warnings.warn(warn_msg, UserWarning)\n",
    "            else:\n",
    "                if self.prediction_intervals.window_size not in [1, horizon]:\n",
    "                    raise ValueError(\n",
    "                        'The `window_size` argument of PredictionIntervals '\n",
    "                        'should be equal to one or `horizon`. '\n",
    "                        'Please rerun the `fit` method passing a proper value '\n",
    "                        'to prediction intervals.'\n",
    "                    )\n",
    "                if self.prediction_intervals.window_size != horizon:\n",
    "                    warn_msg = (\n",
    "                        'Prediction intervals are calculated using 1-step ahead cross-validation, '\n",
    "                        'with a constant width for all horizons. To vary the error by horizon, '\n",
    "                        'pass PredictionIntervals(window_size=horizon) to the `prediction_intervals` '\n",
    "                        'argument when refitting the model.'\n",
    "                    )\n",
    "                    warnings.warn(warn_msg, UserWarning)\n",
    "                level_ = sorted(level)\n",
    "                model_names = self.models.keys()\n",
    "                if ts.id_col == 'index':\n",
    "                    forecasts = forecasts.reset_index()\n",
    "                dtypes = forecasts.dtypes\n",
    "                schema, id_col = _schema_conformal_intervals(\n",
    "                    model_names=model_names,\n",
    "                    level=level_,\n",
    "                    id_col=ts.id_col,\n",
    "                    time_col=ts.time_col,\n",
    "                    dtypes=dtypes,\n",
    "                    fcst_df_columns=forecasts.columns,\n",
    "                )\n",
    "                forecasts = _cotransform(\n",
    "                    forecasts, self._cs_df, \n",
    "                    using=_add_conformal_intervals, \n",
    "                    params=dict(\n",
    "                        model_names=list(model_names),\n",
    "                        level=level_,\n",
    "                        cs_window_size=self.prediction_intervals.window_size,\n",
    "                        cs_n_windows=self.prediction_intervals.n_windows,\n",
    "                    ),\n",
    "                    schema=schema,\n",
    "                    partition=id_col\n",
    "                )\n",
    "                if ts.id_col == 'index':\n",
    "                    forecasts = forecasts.set_index(forecasts.columns[0])\n",
    "        return forecasts\n",
    "\n",
    "    def cross_validation(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        n_windows: int,\n",
    "        window_size: int,\n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        step_size: Optional[int] = None,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "        refit: bool = True,\n",
    "        max_horizon: Optional[int] = None,\n",
    "        before_predict_callback: Optional[Callable] = None,\n",
    "        after_predict_callback: Optional[Callable] = None,\n",
    "        prediction_intervals: Optional[PredictionIntervals] = None,\n",
    "        level: Optional[List[Union[int, float]]] = None,\n",
    "    ):\n",
    "        \"\"\"Perform time series cross validation.\n",
    "        Creates `n_windows` splits where each window has `window_size` test periods, \n",
    "        trains the models, computes the predictions and merges the actuals.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data : pandas DataFrame\n",
    "            Series data in long format.\n",
    "        n_windows : int\n",
    "            Number of windows to evaluate.\n",
    "        window_size : int\n",
    "            Number of test periods in each window.\n",
    "        id_col : str\n",
    "            Column that identifies each serie. If 'index' then the index is used.\n",
    "        time_col : str\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str\n",
    "            Column that contains the target.\n",
    "        step_size : int, optional (default=None)\n",
    "            Step size between each cross validation window. If None it will be equal to `window_size`.\n",
    "        static_features : list of str, optional (default=None)\n",
    "            Names of the features that are static and will be repeated when forecasting.\n",
    "        dropna : bool (default=True)\n",
    "            Drop rows with missing values produced by the transformations.\n",
    "        keep_last_n : int, optional (default=None)\n",
    "            Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it.\n",
    "        max_horizon: int, optional (default=None)\n",
    "            Train this many models, where each model will predict a specific horizon.            \n",
    "        refit : bool (default=True)\n",
    "            Retrain model for each cross validation window.\n",
    "            If False, the models are trained at the beginning and then used to predict each window.\n",
    "        before_predict_callback : callable, optional (default=None)\n",
    "            Function to call on the features before computing the predictions.\n",
    "                This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure.\n",
    "                The series identifier is on the index.\n",
    "        after_predict_callback : callable, optional (default=None)\n",
    "            Function to call on the predictions before updating the targets.\n",
    "                This function will take a pandas Series with the predictions and should return another one with the same structure.\n",
    "                The series identifier is on the index.\n",
    "        prediction_intervals : PredictionIntervals, optional (default=None)\n",
    "            Configuration to calibrate prediction intervals (Conformal Prediction).\n",
    "        level : list of ints or floats, optional (default=None)\n",
    "            Confidence levels between 0 and 100 for prediction intervals.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        result : pandas DataFrame\n",
    "            Predictions for each window with the series id, timestamp, last train date, target value and predictions from each model.\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'models_'):\n",
    "            warnings.warn('Excuting `cross_validation` after `fit` can produce unexpected errors')\n",
    "        results = []\n",
    "        self.cv_models_ = []\n",
    "        if id_col != 'index':\n",
    "            data = data.set_index(id_col)\n",
    "        \n",
    "        if np.issubdtype(data[time_col].dtype.type, np.integer):\n",
    "            freq = 1\n",
    "        else:\n",
    "            freq = self.freq\n",
    "            \n",
    "        splits = backtest_splits(data, n_windows, window_size, freq, step_size, time_col)\n",
    "        ex_cols_to_drop = [target_col]\n",
    "        if static_features is not None:\n",
    "            ex_cols_to_drop.extend(static_features)\n",
    "        has_ex = data.shape[1] > len(ex_cols_to_drop) + 1 # +1 due to time_col\n",
    "        for i_window, (train_end, train, valid) in enumerate(splits):\n",
    "            if refit or i_window == 0:\n",
    "                self.fit(\n",
    "                    train,\n",
    "                    id_col='index',\n",
    "                    time_col=time_col,\n",
    "                    target_col=target_col,\n",
    "                    static_features=static_features,\n",
    "                    dropna=dropna,\n",
    "                    keep_last_n=keep_last_n,\n",
    "                    max_horizon=max_horizon,\n",
    "                    prediction_intervals=prediction_intervals,\n",
    "                )\n",
    "            self.cv_models_.append(self.models_)\n",
    "            # reset index of valid to be compatible\n",
    "            # with _get_features_for_next_step\n",
    "            dynamic_dfs = [valid.drop(columns=ex_cols_to_drop).reset_index()] if has_ex else None\n",
    "            y_pred = self.predict(\n",
    "                window_size, dynamic_dfs, before_predict_callback, after_predict_callback,\n",
    "                new_data=train if not refit else None,\n",
    "                level=level\n",
    "            )\n",
    "            y_pred = y_pred.set_index(time_col, append=True)\n",
    "            result = valid.set_index(time_col, append=True)[[target_col]].copy()\n",
    "            result = result.join(y_pred).reset_index(time_col)\n",
    "            result['cutoff'] = train_end            \n",
    "            results.append(result)\n",
    "\n",
    "        out = pd.concat(results)\n",
    "        out = out[[time_col, 'cutoff', target_col, *y_pred.columns]]\n",
    "        if id_col != 'index':\n",
    "            out = out.reset_index()\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02b15e1-5ec8-493f-983b-7b6eedf6f9a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### MLForecast\n",
       "\n",
       ">      MLForecast (models:Union[sklearn.base.BaseEstimator,List[sklearn.base.Bas\n",
       ">                  eEstimator],Dict[str,sklearn.base.BaseEstimator]], freq:Union\n",
       ">                  [int,str,pandas._libs.tslibs.offsets.BaseOffset,NoneType]=Non\n",
       ">                  e, lags:Optional[Iterable[int]]=None, lag_transforms:Optional\n",
       ">                  [Dict[int,List[Union[Callable,Tuple[Callable,Any]]]]]=None,\n",
       ">                  date_features:Optional[Iterable[Union[str,Callable]]]=None,\n",
       ">                  differences:Optional[Iterable[int]]=None, num_threads:int=1)\n",
       "\n",
       "Create forecast object\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| models | Union |  | Models that will be trained and used to compute the forecasts. |\n",
       "| freq | Union | None | Pandas offset, pandas offset alias, e.g. 'D', 'W-THU' or integer denoting the frequency of the series. |\n",
       "| lags | Optional | None | Lags of the target to use as features. |\n",
       "| lag_transforms | Optional | None | Mapping of target lags to their transformations. |\n",
       "| date_features | Optional | None | Features computed from the dates. Can be pandas date attributes or functions that will take the dates as input. |\n",
       "| differences | Optional | None | Differences to take of the target before computing the features. These are restored at the forecasting step. |\n",
       "| num_threads | int | 1 | Number of threads to use when computing the features. |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### MLForecast\n",
       "\n",
       ">      MLForecast (models:Union[sklearn.base.BaseEstimator,List[sklearn.base.Bas\n",
       ">                  eEstimator],Dict[str,sklearn.base.BaseEstimator]], freq:Union\n",
       ">                  [int,str,pandas._libs.tslibs.offsets.BaseOffset,NoneType]=Non\n",
       ">                  e, lags:Optional[Iterable[int]]=None, lag_transforms:Optional\n",
       ">                  [Dict[int,List[Union[Callable,Tuple[Callable,Any]]]]]=None,\n",
       ">                  date_features:Optional[Iterable[Union[str,Callable]]]=None,\n",
       ">                  differences:Optional[Iterable[int]]=None, num_threads:int=1)\n",
       "\n",
       "Create forecast object\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| models | Union |  | Models that will be trained and used to compute the forecasts. |\n",
       "| freq | Union | None | Pandas offset, pandas offset alias, e.g. 'D', 'W-THU' or integer denoting the frequency of the series. |\n",
       "| lags | Optional | None | Lags of the target to use as features. |\n",
       "| lag_transforms | Optional | None | Mapping of target lags to their transformations. |\n",
       "| date_features | Optional | None | Features computed from the dates. Can be pandas date attributes or functions that will take the dates as input. |\n",
       "| differences | Optional | None | Differences to take of the target before computing the features. These are restored at the forecasting step. |\n",
       "| num_threads | int | 1 | Number of threads to use when computing the features. |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MLForecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c7c92c-2ba9-4885-bbc1-8e4a4f67f0d9",
   "metadata": {},
   "source": [
    "The `MLForecast` class is a high level abstraction that encapsulates all the steps in the pipeline (preprocessing, fitting the model and computing the predictions). It tries to mimic the scikit-learn API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1978a3-d98c-4a44-846a-ac6d8ffecb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export\n",
    "class Forecast(MLForecast):\n",
    "    def __init__(\n",
    "        self,\n",
    "        models: Models,\n",
    "        freq: Optional[Freq] = None,\n",
    "        lags: Optional[Lags] = None,\n",
    "        lag_transforms: Optional[LagTransforms] = None,\n",
    "        date_features: Optional[Iterable[DateFeature]] = None,\n",
    "        differences: Optional[Differences] = None,\n",
    "        num_threads: int = 1,\n",
    "    ):\n",
    "        warning_msg = (\n",
    "            'The Forecast class is deprecated and will be removed in a future version, '\n",
    "            'please use the MLForecast class instead.'\n",
    "        )\n",
    "        warnings.warn(warning_msg, DeprecationWarning)\n",
    "        super().__init__(models, freq, lags, lag_transforms, date_features, differences, num_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a9f64d-def2-45ad-879f-d191f231bfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_warns(lambda: Forecast([]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5ec811-8876-4daa-84a2-2ebe0559a02b",
   "metadata": {},
   "source": [
    "## Example\n",
    "This shows an example with just 4 series of the M4 dataset. If you want to run it yourself on all of them, you can refer to [this notebook](https://www.kaggle.com/code/lemuz90/m4-competition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3032775-f610-4091-a750-73219d904c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from datasetsforecast.m4 import M4, M4Info\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from window_ops.ewm import ewm_mean\n",
    "from window_ops.expanding import expanding_mean\n",
    "from window_ops.rolling import rolling_mean\n",
    "\n",
    "from mlforecast.utils import generate_daily_series, generate_prices_for_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c30ee9-38f3-4ef2-ab50-6ffc1e887f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>86796</th>\n",
       "      <td>H196</td>\n",
       "      <td>1</td>\n",
       "      <td>11.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86797</th>\n",
       "      <td>H196</td>\n",
       "      <td>2</td>\n",
       "      <td>11.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86798</th>\n",
       "      <td>H196</td>\n",
       "      <td>3</td>\n",
       "      <td>11.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86799</th>\n",
       "      <td>H196</td>\n",
       "      <td>4</td>\n",
       "      <td>10.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86800</th>\n",
       "      <td>H196</td>\n",
       "      <td>5</td>\n",
       "      <td>10.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325235</th>\n",
       "      <td>H413</td>\n",
       "      <td>1004</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325236</th>\n",
       "      <td>H413</td>\n",
       "      <td>1005</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325237</th>\n",
       "      <td>H413</td>\n",
       "      <td>1006</td>\n",
       "      <td>47.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325238</th>\n",
       "      <td>H413</td>\n",
       "      <td>1007</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325239</th>\n",
       "      <td>H413</td>\n",
       "      <td>1008</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4032 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       unique_id    ds     y\n",
       "86796       H196     1  11.8\n",
       "86797       H196     2  11.4\n",
       "86798       H196     3  11.1\n",
       "86799       H196     4  10.8\n",
       "86800       H196     5  10.6\n",
       "...          ...   ...   ...\n",
       "325235      H413  1004  99.0\n",
       "325236      H413  1005  88.0\n",
       "325237      H413  1006  47.0\n",
       "325238      H413  1007  41.0\n",
       "325239      H413  1008  34.0\n",
       "\n",
       "[4032 rows x 3 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group = 'Hourly'\n",
    "await M4.async_download('data', group=group)\n",
    "df, *_ = M4.load(directory='data', group=group)\n",
    "df['ds'] = df['ds'].astype('int')\n",
    "ids = df['unique_id'].unique()\n",
    "random.seed(0)\n",
    "sample_ids = random.choices(ids, k=4)\n",
    "sample_df = df[df['unique_id'].isin(sample_ids)]\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaa4e47-4eee-43b6-8dba-1807bb00fda5",
   "metadata": {},
   "source": [
    "We now split this data into train and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91485631-d855-47f9-bdb4-94130b23c67a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3840, 3), (192, 3))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info = M4Info[group]\n",
    "horizon = info.horizon\n",
    "valid = sample_df.groupby('unique_id').tail(horizon)\n",
    "train = sample_df.drop(valid.index)\n",
    "train.shape, valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c392f40-f181-4666-876e-1886e86eda5f",
   "metadata": {},
   "source": [
    "### Creating the Forecast object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc50c95b-359d-4e49-8f1f-40dab560c722",
   "metadata": {},
   "source": [
    "The forecast object encapsulates the feature engineering + training the models + forecasting. When we initialize it we define:\n",
    "\n",
    "* The models we want to train\n",
    "* The series frequency. This is added to the last dates seen in train for the forecast step, if the time column contains integer values we can leave it empty or set it to 1.\n",
    "* The feature engineering:\n",
    "    * Lags to use as features\n",
    "    * Transformations on the lags\n",
    "    * Date features\n",
    "    * Differences to apply to the target before computing the features, which are then restored when forecasting.\n",
    "* Number of threads to use when computing the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ebf59f-0fb7-45ff-b355-127952413245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLForecast(models=[LGBMRegressor], freq=1, lag_features=['lag24', 'lag48', 'lag72', 'lag96', 'lag120', 'lag144', 'lag168', 'ewm_mean_lag48_alpha0.3'], date_features=[], num_threads=1)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcst = MLForecast(\n",
    "    models=lgb.LGBMRegressor(random_state=0),\n",
    "    differences=[24],\n",
    "    lags=[24 * (i+1) for i in range(7)],\n",
    "    lag_transforms={\n",
    "        48: [(ewm_mean, 0.3)],\n",
    "    },\n",
    "    num_threads=1,\n",
    ")\n",
    "fcst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10229c7-a69e-48fe-9ce8-f4754c58bec1",
   "metadata": {},
   "source": [
    "Once we have this setup we can compute the features and fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d42544-91a7-4c08-a190-925343e3c0b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### MLForecast.fit\n",
       "\n",
       ">      MLForecast.fit (data:pandas.core.frame.DataFrame, id_col:str,\n",
       ">                      time_col:str, target_col:str,\n",
       ">                      static_features:Optional[List[str]]=None,\n",
       ">                      dropna:bool=True, keep_last_n:Optional[int]=None,\n",
       ">                      max_horizon:Optional[int]=None, prediction_intervals:Opti\n",
       ">                      onal[mlforecast.utils.PredictionIntervals]=None)\n",
       "\n",
       "Apply the feature engineering and train the models.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| data | DataFrame |  | Series data in long format. |\n",
       "| id_col | str |  | Column that identifies each serie. If 'index' then the index is used. |\n",
       "| time_col | str |  | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str |  | Column that contains the target. |\n",
       "| static_features | Optional | None | Names of the features that are static and will be repeated when forecasting. |\n",
       "| dropna | bool | True | Drop rows with missing values produced by the transformations. |\n",
       "| keep_last_n | Optional | None | Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it. |\n",
       "| max_horizon | Optional | None |  |\n",
       "| prediction_intervals | Optional | None | Configuration to calibrate prediction intervals (Conformal Prediction). |\n",
       "| **Returns** | **MLForecast** |  | **Forecast object with series values and trained models.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### MLForecast.fit\n",
       "\n",
       ">      MLForecast.fit (data:pandas.core.frame.DataFrame, id_col:str,\n",
       ">                      time_col:str, target_col:str,\n",
       ">                      static_features:Optional[List[str]]=None,\n",
       ">                      dropna:bool=True, keep_last_n:Optional[int]=None,\n",
       ">                      max_horizon:Optional[int]=None, prediction_intervals:Opti\n",
       ">                      onal[mlforecast.utils.PredictionIntervals]=None)\n",
       "\n",
       "Apply the feature engineering and train the models.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| data | DataFrame |  | Series data in long format. |\n",
       "| id_col | str |  | Column that identifies each serie. If 'index' then the index is used. |\n",
       "| time_col | str |  | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str |  | Column that contains the target. |\n",
       "| static_features | Optional | None | Names of the features that are static and will be repeated when forecasting. |\n",
       "| dropna | bool | True | Drop rows with missing values produced by the transformations. |\n",
       "| keep_last_n | Optional | None | Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it. |\n",
       "| max_horizon | Optional | None |  |\n",
       "| prediction_intervals | Optional | None | Configuration to calibrate prediction intervals (Conformal Prediction). |\n",
       "| **Returns** | **MLForecast** |  | **Forecast object with series values and trained models.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MLForecast.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53041a71-5d34-4c61-b28d-dcd0ccc3833b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst.fit(train, id_col='unique_id', time_col='ds', target_col='y');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5794fec5-fcc4-48c4-8f35-8b5add89a698",
   "metadata": {},
   "source": [
    "Once we've run this we're ready to compute our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121e73f8-b88b-4c82-967e-9629587d7616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### MLForecast.predict\n",
       "\n",
       ">      MLForecast.predict (horizon:int,\n",
       ">                          dynamic_dfs:Optional[List[pandas.core.frame.DataFrame\n",
       ">                          ]]=None,\n",
       ">                          before_predict_callback:Optional[Callable]=None,\n",
       ">                          after_predict_callback:Optional[Callable]=None,\n",
       ">                          new_data:Optional[pandas.core.frame.DataFrame]=None,\n",
       ">                          level:Optional[List[Union[int,float]]]=None)\n",
       "\n",
       "Compute the predictions for the next `horizon` steps.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| horizon | int |  | Number of periods to predict. |\n",
       "| dynamic_dfs | Optional | None | Future values of the dynamic features, e.g. prices. |\n",
       "| before_predict_callback | Optional | None | Function to call on the features before computing the predictions.<br>    This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure.<br>    The series identifier is on the index. |\n",
       "| after_predict_callback | Optional | None | Function to call on the predictions before updating the targets.<br>    This function will take a pandas Series with the predictions and should return another one with the same structure.<br>    The series identifier is on the index.    |\n",
       "| new_data | Optional | None | Series data of new observations for which forecasts are to be generated. <br>    This dataframe should have the same structure as the one used to fit the model, including any features and time series data. <br>    If `new_data` is not None, the method will generate forecasts for the new observations. |\n",
       "| level | Optional | None | Confidence levels between 0 and 100 for prediction intervals. |\n",
       "| **Returns** | **DataFrame** |  | **Predictions for each serie and timestep, with one column per model.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### MLForecast.predict\n",
       "\n",
       ">      MLForecast.predict (horizon:int,\n",
       ">                          dynamic_dfs:Optional[List[pandas.core.frame.DataFrame\n",
       ">                          ]]=None,\n",
       ">                          before_predict_callback:Optional[Callable]=None,\n",
       ">                          after_predict_callback:Optional[Callable]=None,\n",
       ">                          new_data:Optional[pandas.core.frame.DataFrame]=None,\n",
       ">                          level:Optional[List[Union[int,float]]]=None)\n",
       "\n",
       "Compute the predictions for the next `horizon` steps.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| horizon | int |  | Number of periods to predict. |\n",
       "| dynamic_dfs | Optional | None | Future values of the dynamic features, e.g. prices. |\n",
       "| before_predict_callback | Optional | None | Function to call on the features before computing the predictions.<br>    This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure.<br>    The series identifier is on the index. |\n",
       "| after_predict_callback | Optional | None | Function to call on the predictions before updating the targets.<br>    This function will take a pandas Series with the predictions and should return another one with the same structure.<br>    The series identifier is on the index.    |\n",
       "| new_data | Optional | None | Series data of new observations for which forecasts are to be generated. <br>    This dataframe should have the same structure as the one used to fit the model, including any features and time series data. <br>    If `new_data` is not None, the method will generate forecasts for the new observations. |\n",
       "| level | Optional | None | Confidence levels between 0 and 100 for prediction intervals. |\n",
       "| **Returns** | **DataFrame** |  | **Predictions for each serie and timestep, with one column per model.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MLForecast.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c28426-e150-4fe8-b1f9-bcad401cbbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = fcst.predict(horizon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6cd880-ec12-46dc-9627-90d8521f9c38",
   "metadata": {},
   "source": [
    "We can see at a couple of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223287b5-70ec-419a-a28a-bb13f6fa005e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = valid.merge(predictions, on=['unique_id', 'ds']).set_index('unique_id')\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(16, 10))\n",
    "for uid, axi in zip(sample_ids, ax.flat):\n",
    "    results.loc[uid].set_index('ds').plot(ax=axi, title=uid)\n",
    "fig.savefig('figs/forecast__predict.png', bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a31d2b4-b9e5-427b-9cfa-30a1733969f1",
   "metadata": {},
   "source": [
    "![](figs/forecast__predict.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20724d12-d6b4-4c21-a38d-fd4c71461c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test new_data argument\n",
    "test_eq(\n",
    "    fcst.predict(horizon, new_data=train),\n",
    "    predictions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfb0f7f-ea89-44a6-b557-ac12a278f111",
   "metadata": {},
   "source": [
    "#### Prediction intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b367dee8-fab4-406a-8a9e-c4ec86003a7d",
   "metadata": {},
   "source": [
    "With `MLForecast`, you can generate prediction intervals using Conformal Prediction. To configure Conformal Prediction, you need to pass an instance of the `PredictionIntervals` class to the `prediction_intervals` argument of the `fit` method. The class takes two parameters: `n_windows` and `window_size`. `n_windows` represents the number of cross-validation windows used to calibrate the intervals and `window_size` is the forecast horizon. The strategy will adjust the intervals for each horizon step, resulting in different widths for each step. Please note that a minimum of 2 cross-validation windows must be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cd3346-f1e5-4ccb-b39c-a257d45d51fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst.fit(\n",
    "    train, \n",
    "    id_col='unique_id', \n",
    "    time_col='ds', \n",
    "    target_col='y', \n",
    "    prediction_intervals=PredictionIntervals(n_windows=3, window_size=48)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2273a783-1596-4d5e-b084-9252eb6c23af",
   "metadata": {},
   "source": [
    "After that, you just have to include your desired confidence levels to the `predict` method using the `level` argument. Levels must lie between 0 and 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02793e5f-cb32-4c61-8417-a89a531e1ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_w_intervals = fcst.predict(48, level=[50, 80, 95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef791c7d-aa1f-423b-a362-22919cac778a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_fail(lambda: fcst.predict(4, level=[68]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04359aaa-ac32-4fe6-ae30-e83df75bb4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test we can recover point forecasts\n",
    "test_eq(\n",
    "    predictions,\n",
    "    predictions_w_intervals[predictions.columns]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b94e8e-395f-4435-a03c-4749c1bea41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test monotonicity of intervals\n",
    "test_eq(\n",
    "    predictions_w_intervals.filter(regex='lo|hi').apply(\n",
    "        lambda x: x.is_monotonic_increasing,\n",
    "        axis=1\n",
    "    ).sum(),\n",
    "    len(predictions_w_intervals)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117a4080-fa17-4b13-a0b6-1d1afe70dfb8",
   "metadata": {},
   "source": [
    "Let's explore the generated intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0934d461-7b9c-4646-a614-f538df2752b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = valid.merge(predictions_w_intervals, on=['unique_id', 'ds']).set_index('unique_id')\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(16, 10))\n",
    "for uid, axi in zip(sample_ids, ax.flat):\n",
    "    uid_results = results.loc[uid].set_index('ds')\n",
    "    uid_results[['y', 'LGBMRegressor']].plot(ax=axi, title=uid)\n",
    "    for lv in [50, 80, 95]:\n",
    "        axi.fill_between(\n",
    "            uid_results.index, \n",
    "            uid_results[f'LGBMRegressor-lo-{lv}'].values, \n",
    "            uid_results[f'LGBMRegressor-hi-{lv}'].values,\n",
    "            label=f'LGBMRegressor-level-{lv}',\n",
    "            color='orange',\n",
    "            alpha=1 - lv / 100\n",
    "        )\n",
    "    axi.legend()\n",
    "fig.savefig('figs/forecast__predict_intervals.png', bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7b236b-b87a-4643-b674-5db42bfa4319",
   "metadata": {},
   "source": [
    "![](figs/forecast__predict_intervals.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6686093-f139-4241-8f7b-3a8bd0f2b47c",
   "metadata": {},
   "source": [
    "If you want to reduce the computational time and produce intervals with the same width for the whole forecast horizon, simple pass `window_size=1` to the `PredictionIntervals` class. The caveat of this strategy is that in some cases, variance of the absolute residuals maybe be small (even zero), so the intervals may be too narrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa76f77e-7341-4fc7-af11-80c3531d304b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst.fit(\n",
    "    train, \n",
    "    id_col='unique_id', \n",
    "    time_col='ds', \n",
    "    target_col='y', \n",
    "    prediction_intervals=PredictionIntervals(n_windows=3, window_size=1)\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ec6367-0fa3-4e40-b771-0b7182813cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_w_intervals_ws_1 = fcst.predict(48, level=[80, 90, 95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9905828c-21e4-4358-bb89-24d98dbb914c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>LGBMRegressor</th>\n",
       "      <th>LGBMRegressor-lo-95</th>\n",
       "      <th>LGBMRegressor-lo-90</th>\n",
       "      <th>LGBMRegressor-lo-80</th>\n",
       "      <th>LGBMRegressor-hi-80</th>\n",
       "      <th>LGBMRegressor-hi-90</th>\n",
       "      <th>LGBMRegressor-hi-95</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H196</td>\n",
       "      <td>961</td>\n",
       "      <td>16.071271</td>\n",
       "      <td>16.049062</td>\n",
       "      <td>16.049062</td>\n",
       "      <td>16.049062</td>\n",
       "      <td>16.093481</td>\n",
       "      <td>16.093481</td>\n",
       "      <td>16.093481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H196</td>\n",
       "      <td>962</td>\n",
       "      <td>15.671271</td>\n",
       "      <td>15.649062</td>\n",
       "      <td>15.649062</td>\n",
       "      <td>15.649062</td>\n",
       "      <td>15.693481</td>\n",
       "      <td>15.693481</td>\n",
       "      <td>15.693481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H196</td>\n",
       "      <td>963</td>\n",
       "      <td>15.271271</td>\n",
       "      <td>15.249062</td>\n",
       "      <td>15.249062</td>\n",
       "      <td>15.249062</td>\n",
       "      <td>15.293481</td>\n",
       "      <td>15.293481</td>\n",
       "      <td>15.293481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H196</td>\n",
       "      <td>964</td>\n",
       "      <td>14.971271</td>\n",
       "      <td>14.949062</td>\n",
       "      <td>14.949062</td>\n",
       "      <td>14.949062</td>\n",
       "      <td>14.993481</td>\n",
       "      <td>14.993481</td>\n",
       "      <td>14.993481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H196</td>\n",
       "      <td>965</td>\n",
       "      <td>14.671271</td>\n",
       "      <td>14.649062</td>\n",
       "      <td>14.649062</td>\n",
       "      <td>14.649062</td>\n",
       "      <td>14.693481</td>\n",
       "      <td>14.693481</td>\n",
       "      <td>14.693481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  unique_id   ds  LGBMRegressor  LGBMRegressor-lo-95  LGBMRegressor-lo-90  \\\n",
       "0      H196  961      16.071271            16.049062            16.049062   \n",
       "1      H196  962      15.671271            15.649062            15.649062   \n",
       "2      H196  963      15.271271            15.249062            15.249062   \n",
       "3      H196  964      14.971271            14.949062            14.949062   \n",
       "4      H196  965      14.671271            14.649062            14.649062   \n",
       "\n",
       "   LGBMRegressor-lo-80  LGBMRegressor-hi-80  LGBMRegressor-hi-90  \\\n",
       "0            16.049062            16.093481            16.093481   \n",
       "1            15.649062            15.693481            15.693481   \n",
       "2            15.249062            15.293481            15.293481   \n",
       "3            14.949062            14.993481            14.993481   \n",
       "4            14.649062            14.693481            14.693481   \n",
       "\n",
       "   LGBMRegressor-hi-95  \n",
       "0            16.093481  \n",
       "1            15.693481  \n",
       "2            15.293481  \n",
       "3            14.993481  \n",
       "4            14.693481  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_w_intervals_ws_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d754fa-a20c-4cd2-bf6d-6274cd9d126d",
   "metadata": {},
   "source": [
    "Let's explore the generated intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c8e1b3-04bc-4457-a714-dd5dbb06efb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = valid.merge(predictions_w_intervals_ws_1, on=['unique_id', 'ds']).set_index('unique_id')\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(16, 10))\n",
    "for uid, axi in zip(sample_ids, ax.flat):\n",
    "    uid_results = results.loc[uid].set_index('ds')\n",
    "    uid_results[['y', 'LGBMRegressor']].plot(ax=axi, title=uid)\n",
    "    axi.fill_between(\n",
    "        uid_results.index, \n",
    "        uid_results['LGBMRegressor-lo-90'].values, \n",
    "        uid_results['LGBMRegressor-hi-90'].values,\n",
    "        label='LGBMRegressor-level-90',\n",
    "        color='orange',\n",
    "        alpha=0.2\n",
    "    )\n",
    "    axi.legend()\n",
    "fig.savefig('figs/forecast__predict_intervals_window_size_1.png', bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0ab69f-948e-4f68-a905-309ca2213685",
   "metadata": {},
   "source": [
    "![](figs/forecast__predict_intervals_window_size_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73473351-3386-4959-9a41-4ce4ed64354f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test indexed data, datetime ds\n",
    "fcst_test = MLForecast(\n",
    "    models=lgb.LGBMRegressor(random_state=0),\n",
    "    lags=[1],\n",
    "    num_threads=1,\n",
    "    freq='D'\n",
    ")\n",
    "df_test = generate_daily_series(1)\n",
    "df_test.index = df_test.index.astype(str)\n",
    "fcst_test.fit(\n",
    "    df_test, \n",
    "    id_col='index', \n",
    "    time_col='ds', \n",
    "    target_col='y',\n",
    "    prediction_intervals=PredictionIntervals()\n",
    ")\n",
    "pred_test = fcst_test.predict(12)\n",
    "pred_int_test = fcst_test.predict(12, level=[80, 90])\n",
    "# test same structure\n",
    "test_eq(\n",
    "    pred_test,\n",
    "    pred_int_test[pred_test.columns]\n",
    ")\n",
    "# test monotonicity of intervals\n",
    "test_eq(\n",
    "    pred_int_test.filter(regex='lo|hi').apply(\n",
    "        lambda x: x.is_monotonic_increasing,\n",
    "        axis=1\n",
    "    ).sum(),\n",
    "    len(pred_int_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505aadfd-076d-42e6-8103-aeef4eef61b7",
   "metadata": {},
   "source": [
    "#### Forecast using a pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1bef71-d893-46ee-82dd-6819193f2d0e",
   "metadata": {},
   "source": [
    "MLForecast allows you to use a pretrained model to generate forecasts for a new dataset. Simply provide a pandas dataframe containing the new observations as the value for the `new_data` argument when calling the `predict` method. The dataframe should have the same structure as the one used to fit the model, including any features and time series data. The function will then use the pretrained model to generate forecasts for the new observations. This allows you to easily apply a pretrained model to a new dataset and generate forecasts without the need to retrain the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ade99c5-82f5-4047-bea6-2698f27872db",
   "metadata": {},
   "outputs": [],
   "source": [
    "ercot_df = pd.read_csv('https://datasets-nixtla.s3.amazonaws.com/ERCOT-clean.csv')\n",
    "# we have to convert the ds column to integers\n",
    "# since MLForecast was trained with that structure\n",
    "ercot_df['ds'] = np.arange(1, len(ercot_df) + 1)\n",
    "# use the `new_data` argument to pass the ercot dataset \n",
    "ercot_fcsts = fcst.predict(horizon, new_data=ercot_df)\n",
    "fig, ax = plt.subplots()\n",
    "ercot_df.tail(48 * 2).plot(x='ds', y='y', figsize=(20, 7), ax=ax)\n",
    "ercot_fcsts.plot(x='ds', y='LGBMRegressor', ax=ax, title='ERCOT forecasts trained on M4-Hourly dataset');\n",
    "plt.gcf().savefig('figs/forecast__ercot.png', bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60c5bab-453b-4ae8-a282-87233c4333e3",
   "metadata": {},
   "source": [
    "![](figs/forecast__ercot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b150efe0-bc84-483e-905f-c936e0aa0818",
   "metadata": {},
   "source": [
    "If you want to take a look at the data that will be used to train the models you can call `Forecast.preprocess`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21260f3-bd8f-4143-bb60-deb9ca0d46e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### MLForecast.preprocess\n",
       "\n",
       ">      MLForecast.preprocess (data:pandas.core.frame.DataFrame, id_col:str,\n",
       ">                             time_col:str, target_col:str,\n",
       ">                             static_features:Optional[List[str]]=None,\n",
       ">                             dropna:bool=True, keep_last_n:Optional[int]=None,\n",
       ">                             max_horizon:Optional[int]=None,\n",
       ">                             return_X_y:bool=False)\n",
       "\n",
       "Add the features to `data`.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| data | DataFrame |  | Series data in long format. |\n",
       "| id_col | str |  | Column that identifies each serie. If 'index' then the index is used. |\n",
       "| time_col | str |  | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str |  | Column that contains the target. |\n",
       "| static_features | Optional | None | Names of the features that are static and will be repeated when forecasting. |\n",
       "| dropna | bool | True | Drop rows with missing values produced by the transformations. |\n",
       "| keep_last_n | Optional | None | Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it. |\n",
       "| max_horizon | Optional | None |  |\n",
       "| return_X_y | bool | False |  |\n",
       "| **Returns** | **Union** |  | **`data` plus added features and target(s).** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### MLForecast.preprocess\n",
       "\n",
       ">      MLForecast.preprocess (data:pandas.core.frame.DataFrame, id_col:str,\n",
       ">                             time_col:str, target_col:str,\n",
       ">                             static_features:Optional[List[str]]=None,\n",
       ">                             dropna:bool=True, keep_last_n:Optional[int]=None,\n",
       ">                             max_horizon:Optional[int]=None,\n",
       ">                             return_X_y:bool=False)\n",
       "\n",
       "Add the features to `data`.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| data | DataFrame |  | Series data in long format. |\n",
       "| id_col | str |  | Column that identifies each serie. If 'index' then the index is used. |\n",
       "| time_col | str |  | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str |  | Column that contains the target. |\n",
       "| static_features | Optional | None | Names of the features that are static and will be repeated when forecasting. |\n",
       "| dropna | bool | True | Drop rows with missing values produced by the transformations. |\n",
       "| keep_last_n | Optional | None | Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it. |\n",
       "| max_horizon | Optional | None |  |\n",
       "| return_X_y | bool | False |  |\n",
       "| **Returns** | **Union** |  | **`data` plus added features and target(s).** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MLForecast.preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53214db1-05dd-4440-88b2-94763769e520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>lag24</th>\n",
       "      <th>lag48</th>\n",
       "      <th>lag72</th>\n",
       "      <th>lag96</th>\n",
       "      <th>lag120</th>\n",
       "      <th>lag144</th>\n",
       "      <th>lag168</th>\n",
       "      <th>ewm_mean_lag48_alpha0.3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>86988</th>\n",
       "      <td>H196</td>\n",
       "      <td>193</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.002810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86989</th>\n",
       "      <td>H196</td>\n",
       "      <td>194</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.031967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86990</th>\n",
       "      <td>H196</td>\n",
       "      <td>195</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.052377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86991</th>\n",
       "      <td>H196</td>\n",
       "      <td>196</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.036664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86992</th>\n",
       "      <td>H196</td>\n",
       "      <td>197</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.025665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325187</th>\n",
       "      <td>H413</td>\n",
       "      <td>956</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-53.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>-21.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7.963225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325188</th>\n",
       "      <td>H413</td>\n",
       "      <td>957</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>-46.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>-19.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>8.574257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325189</th>\n",
       "      <td>H413</td>\n",
       "      <td>958</td>\n",
       "      <td>16.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>-36.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>-13.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.501980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325190</th>\n",
       "      <td>H413</td>\n",
       "      <td>959</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-31.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>3.151386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325191</th>\n",
       "      <td>H413</td>\n",
       "      <td>960</td>\n",
       "      <td>15.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-17.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>-18.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.405970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3072 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       unique_id   ds     y  lag24  lag48  lag72  lag96  lag120  lag144  \\\n",
       "86988       H196  193   0.1    0.0    0.0    0.0    0.3     0.1     0.1   \n",
       "86989       H196  194   0.1   -0.1    0.1    0.0    0.3     0.1     0.1   \n",
       "86990       H196  195   0.1   -0.1    0.1    0.0    0.3     0.1     0.2   \n",
       "86991       H196  196   0.1    0.0    0.0    0.0    0.3     0.2     0.1   \n",
       "86992       H196  197   0.0    0.0    0.0    0.1    0.2     0.2     0.1   \n",
       "...          ...  ...   ...    ...    ...    ...    ...     ...     ...   \n",
       "325187      H413  956   0.0   10.0    1.0    6.0  -53.0    44.0   -21.0   \n",
       "325188      H413  957   9.0   10.0   10.0   -7.0  -46.0    27.0   -19.0   \n",
       "325189      H413  958  16.0    8.0    5.0   -9.0  -36.0    32.0   -13.0   \n",
       "325190      H413  959  -3.0   17.0   -7.0    2.0  -31.0    22.0     5.0   \n",
       "325191      H413  960  15.0   11.0   -6.0   -5.0  -17.0    22.0   -18.0   \n",
       "\n",
       "        lag168  ewm_mean_lag48_alpha0.3  \n",
       "86988      0.3                 0.002810  \n",
       "86989      0.3                 0.031967  \n",
       "86990      0.1                 0.052377  \n",
       "86991      0.2                 0.036664  \n",
       "86992      0.2                 0.025665  \n",
       "...        ...                      ...  \n",
       "325187    21.0                 7.963225  \n",
       "325188    24.0                 8.574257  \n",
       "325189     8.0                 7.501980  \n",
       "325190    -2.0                 3.151386  \n",
       "325191    10.0                 0.405970  \n",
       "\n",
       "[3072 rows x 11 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_df = fcst.preprocess(train, id_col='unique_id', time_col='ds', target_col='y')\n",
    "prep_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9959305d-8273-4994-8bab-4e1b5fd95966",
   "metadata": {},
   "source": [
    "If we do this we then have to call `Forecast.fit_models`, since this only stores the series information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6863d9e7-92ac-4302-8271-9e70cd5531aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### MLForecast.fit_models\n",
       "\n",
       ">      MLForecast.fit_models (X:pandas.core.frame.DataFrame,\n",
       ">                             y:Union[pandas.core.series.Series,pandas.core.fram\n",
       ">                             e.DataFrame])\n",
       "\n",
       "Manually train models. Use this if you called `Forecast.preprocess` beforehand.\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| X | DataFrame | Features. |\n",
       "| y | Union | Target. |\n",
       "| **Returns** | **MLForecast** | **Forecast object with trained models.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### MLForecast.fit_models\n",
       "\n",
       ">      MLForecast.fit_models (X:pandas.core.frame.DataFrame,\n",
       ">                             y:Union[pandas.core.series.Series,pandas.core.fram\n",
       ">                             e.DataFrame])\n",
       "\n",
       "Manually train models. Use this if you called `Forecast.preprocess` beforehand.\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| X | DataFrame | Features. |\n",
       "| y | Union | Target. |\n",
       "| **Returns** | **MLForecast** | **Forecast object with trained models.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MLForecast.fit_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69d243e-a271-4518-8a0d-432716fc8d2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLForecast(models=[LGBMRegressor], freq=1, lag_features=['lag24', 'lag48', 'lag72', 'lag96', 'lag120', 'lag144', 'lag168', 'ewm_mean_lag48_alpha0.3'], date_features=[], num_threads=1)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = prep_df.drop(columns=['unique_id', 'ds', 'y']), prep_df['y']\n",
    "fcst.fit_models(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5eec03-60fa-4f35-b649-97ed5be35f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = fcst.predict(horizon)\n",
    "pd.testing.assert_frame_equal(predictions, predictions2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681f4b39-28ee-4a9e-8470-1f0d8c8ef2e0",
   "metadata": {},
   "source": [
    "### Multi-output model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074fc9de-b5fb-40a4-806d-6d15b9b62bb9",
   "metadata": {},
   "source": [
    "By default mlforecast uses the recursive strategy, i.e. a model is trained to predict the next value and if we're predicting several values we do it one at a time and then use the model's predictions as the new target, recompute the features and predict the next step.\n",
    "\n",
    "There's another approach where if we want to predict 10 steps ahead we train 10 different models, where each model is trained to predict the value at each specific step, i.e. one model predicts the next value, another one predicts the value two steps ahead and so on. This can be very time consuming but can also provide better results. If you want to use this approach you can specify `max_horizon` in `MLForecast.fit`, which will train that many models and each model will predict its corresponding horizon when you call `MLForecast.predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6e796e-1bd6-4c65-a07a-68f2f9636ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_mape(df):\n",
    "    full = df.merge(valid)\n",
    "    return abs(full['LGBMRegressor'] - full['y']).div(full['y']).groupby(full['unique_id']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0754bfe7-9564-4884-97e0-48896c16472a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MAPE per method and serie\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>individual</th>\n",
       "      <th>recursive</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>H196</th>\n",
       "      <td>0.5%</td>\n",
       "      <td>0.6%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H256</th>\n",
       "      <td>0.7%</td>\n",
       "      <td>0.6%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H381</th>\n",
       "      <td>48.9%</td>\n",
       "      <td>20.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H413</th>\n",
       "      <td>26.9%</td>\n",
       "      <td>35.1%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          individual recursive\n",
       "unique_id                     \n",
       "H196            0.5%      0.6%\n",
       "H256            0.7%      0.6%\n",
       "H381           48.9%     20.3%\n",
       "H413           26.9%     35.1%"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcst = MLForecast(\n",
    "    models=lgb.LGBMRegressor(random_state=0),\n",
    "    differences=[24],\n",
    "    lags=[24 * (i+1) for i in range(7)],\n",
    "    lag_transforms={\n",
    "        1: [(rolling_mean, 24)],\n",
    "        24: [(rolling_mean, 24)],\n",
    "        48: [(ewm_mean, 0.3)],\n",
    "    },\n",
    "    num_threads=1,\n",
    ")\n",
    "max_horizon = 24\n",
    "# the following will train 24 models, one for each horizon\n",
    "individual_fcst = fcst.fit(train, id_col='unique_id', time_col='ds', target_col='y', max_horizon=max_horizon)\n",
    "individual_preds = individual_fcst.predict(max_horizon)\n",
    "avg_mape_individual = avg_mape(individual_preds).rename('individual')\n",
    "# the following will train a single model and use the recursive strategy\n",
    "recursive_fcst = fcst.fit(train, id_col='unique_id', time_col='ds', target_col='y')\n",
    "recursive_preds = recursive_fcst.predict(max_horizon)\n",
    "avg_mape_recursive = avg_mape(recursive_preds).rename('recursive')\n",
    "# results\n",
    "print('Average MAPE per method and serie')\n",
    "avg_mape_individual.to_frame().join(avg_mape_recursive).applymap('{:.1%}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94559ce-7814-4719-a9c7-ec1a2c2b14d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test intervals multioutput\n",
    "individual_fcst_intervals = fcst.fit(\n",
    "    train, id_col='unique_id', time_col='ds', target_col='y', max_horizon=max_horizon,\n",
    "    prediction_intervals=PredictionIntervals(window_size=max_horizon)\n",
    ")\n",
    "individual_preds_intervals = individual_fcst.predict(max_horizon, level=[90, 80])\n",
    "# test monotonicity of intervals\n",
    "test_eq(\n",
    "    individual_preds_intervals.filter(regex='lo|hi').apply(\n",
    "        lambda x: x.is_monotonic_increasing,\n",
    "        axis=1\n",
    "    ).sum(),\n",
    "    len(individual_preds_intervals)\n",
    ")\n",
    "# test we can recover point forecasts with intervals\n",
    "test_eq(\n",
    "    individual_preds,\n",
    "    individual_preds_intervals[individual_preds.columns]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1027bec7-720a-4f86-bfe2-601b0c7f63e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# we get the same prediction for the first timestep\n",
    "pd.testing.assert_frame_equal(\n",
    "    individual_preds.groupby('unique_id').head(1).astype({'ds': 'int64'}),\n",
    "    recursive_preds.groupby('unique_id').head(1).astype({'ds': 'int64'}),    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7d1b64-7b74-47b6-8bb7-e3ff63523eea",
   "metadata": {},
   "source": [
    "### Cross validation\n",
    "\n",
    "If we would like to know how good our forecast will be for a specific model and set of features then we can perform cross validation. What cross validation does is take our data and split it in two parts, where the first part is used for training and the second one for validation. Since the data is time dependant we usually take the last *x* observations from our data as the validation set.\n",
    "\n",
    "This process is implemented in `Forecast.cross_validation`, which takes our data and performs the process described above for `n_windows` times where each window has `window_size` validation samples in it. For example, if we have 100 samples and we want to perform 2 backtests each of size 14, the splits will be as follows:\n",
    "\n",
    "1. Train: 1 to 72. Validation: 73 to 86.\n",
    "2. Train: 1 to 86. Validation: 87 to 100.\n",
    "\n",
    "You can control the size between each cross validation window using the `step_size` argument. For example, if we have 100 samples and we want to perform 2 backtests each of size 14 and move one step ahead in each fold (`step_size=1`), the splits will be as follows:\n",
    "\n",
    "1. Train: 1 to 85. Validation: 86 to 99.\n",
    "2. Train: 1 to 86. Validation: 87 to 100.\n",
    "\n",
    "You can also perform cross validation without refitting your models for each window by setting `refit=False`. This allows you to evaluate the performance of your models using multiple window sizes without having to retrain them each time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e65bc38-7a1b-4f52-ace2-677fa9c3561d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### MLForecast.cross_validation\n",
       "\n",
       ">      MLForecast.cross_validation (data:pandas.core.frame.DataFrame,\n",
       ">                                   n_windows:int, window_size:int, id_col:str,\n",
       ">                                   time_col:str, target_col:str,\n",
       ">                                   step_size:Optional[int]=None,\n",
       ">                                   static_features:Optional[List[str]]=None,\n",
       ">                                   dropna:bool=True,\n",
       ">                                   keep_last_n:Optional[int]=None,\n",
       ">                                   refit:bool=True,\n",
       ">                                   max_horizon:Optional[int]=None, before_predi\n",
       ">                                   ct_callback:Optional[Callable]=None, after_p\n",
       ">                                   redict_callback:Optional[Callable]=None, pre\n",
       ">                                   diction_intervals:Optional[mlforecast.utils.\n",
       ">                                   PredictionIntervals]=None,\n",
       ">                                   level:Optional[List[Union[int,float]]]=None)\n",
       "\n",
       "Perform time series cross validation.\n",
       "Creates `n_windows` splits where each window has `window_size` test periods, \n",
       "trains the models, computes the predictions and merges the actuals.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| data | DataFrame |  | Series data in long format. |\n",
       "| n_windows | int |  | Number of windows to evaluate. |\n",
       "| window_size | int |  | Number of test periods in each window. |\n",
       "| id_col | str |  | Column that identifies each serie. If 'index' then the index is used. |\n",
       "| time_col | str |  | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str |  | Column that contains the target. |\n",
       "| step_size | Optional | None | Step size between each cross validation window. If None it will be equal to `window_size`. |\n",
       "| static_features | Optional | None | Names of the features that are static and will be repeated when forecasting. |\n",
       "| dropna | bool | True | Drop rows with missing values produced by the transformations. |\n",
       "| keep_last_n | Optional | None | Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it. |\n",
       "| refit | bool | True | Retrain model for each cross validation window.<br>If False, the models are trained at the beginning and then used to predict each window. |\n",
       "| max_horizon | Optional | None |  |\n",
       "| before_predict_callback | Optional | None | Function to call on the features before computing the predictions.<br>    This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure.<br>    The series identifier is on the index. |\n",
       "| after_predict_callback | Optional | None | Function to call on the predictions before updating the targets.<br>    This function will take a pandas Series with the predictions and should return another one with the same structure.<br>    The series identifier is on the index. |\n",
       "| prediction_intervals | Optional | None | Configuration to calibrate prediction intervals (Conformal Prediction). |\n",
       "| level | Optional | None | Confidence levels between 0 and 100 for prediction intervals. |\n",
       "| **Returns** | **pandas DataFrame** |  | **Predictions for each window with the series id, timestamp, last train date, target value and predictions from each model.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### MLForecast.cross_validation\n",
       "\n",
       ">      MLForecast.cross_validation (data:pandas.core.frame.DataFrame,\n",
       ">                                   n_windows:int, window_size:int, id_col:str,\n",
       ">                                   time_col:str, target_col:str,\n",
       ">                                   step_size:Optional[int]=None,\n",
       ">                                   static_features:Optional[List[str]]=None,\n",
       ">                                   dropna:bool=True,\n",
       ">                                   keep_last_n:Optional[int]=None,\n",
       ">                                   refit:bool=True,\n",
       ">                                   max_horizon:Optional[int]=None, before_predi\n",
       ">                                   ct_callback:Optional[Callable]=None, after_p\n",
       ">                                   redict_callback:Optional[Callable]=None, pre\n",
       ">                                   diction_intervals:Optional[mlforecast.utils.\n",
       ">                                   PredictionIntervals]=None,\n",
       ">                                   level:Optional[List[Union[int,float]]]=None)\n",
       "\n",
       "Perform time series cross validation.\n",
       "Creates `n_windows` splits where each window has `window_size` test periods, \n",
       "trains the models, computes the predictions and merges the actuals.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| data | DataFrame |  | Series data in long format. |\n",
       "| n_windows | int |  | Number of windows to evaluate. |\n",
       "| window_size | int |  | Number of test periods in each window. |\n",
       "| id_col | str |  | Column that identifies each serie. If 'index' then the index is used. |\n",
       "| time_col | str |  | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str |  | Column that contains the target. |\n",
       "| step_size | Optional | None | Step size between each cross validation window. If None it will be equal to `window_size`. |\n",
       "| static_features | Optional | None | Names of the features that are static and will be repeated when forecasting. |\n",
       "| dropna | bool | True | Drop rows with missing values produced by the transformations. |\n",
       "| keep_last_n | Optional | None | Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it. |\n",
       "| refit | bool | True | Retrain model for each cross validation window.<br>If False, the models are trained at the beginning and then used to predict each window. |\n",
       "| max_horizon | Optional | None |  |\n",
       "| before_predict_callback | Optional | None | Function to call on the features before computing the predictions.<br>    This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure.<br>    The series identifier is on the index. |\n",
       "| after_predict_callback | Optional | None | Function to call on the predictions before updating the targets.<br>    This function will take a pandas Series with the predictions and should return another one with the same structure.<br>    The series identifier is on the index. |\n",
       "| prediction_intervals | Optional | None | Configuration to calibrate prediction intervals (Conformal Prediction). |\n",
       "| level | Optional | None | Confidence levels between 0 and 100 for prediction intervals. |\n",
       "| **Returns** | **pandas DataFrame** |  | **Predictions for each window with the series id, timestamp, last train date, target value and predictions from each model.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MLForecast.cross_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541510fd-3fd5-49ab-baf3-9da4e9956087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>cutoff</th>\n",
       "      <th>y</th>\n",
       "      <th>LGBMRegressor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H196</td>\n",
       "      <td>769</td>\n",
       "      <td>768</td>\n",
       "      <td>15.2</td>\n",
       "      <td>15.167163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H196</td>\n",
       "      <td>770</td>\n",
       "      <td>768</td>\n",
       "      <td>14.8</td>\n",
       "      <td>14.767163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H196</td>\n",
       "      <td>771</td>\n",
       "      <td>768</td>\n",
       "      <td>14.4</td>\n",
       "      <td>14.467163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H196</td>\n",
       "      <td>772</td>\n",
       "      <td>768</td>\n",
       "      <td>14.1</td>\n",
       "      <td>14.167163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H196</td>\n",
       "      <td>773</td>\n",
       "      <td>768</td>\n",
       "      <td>13.8</td>\n",
       "      <td>13.867163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>H413</td>\n",
       "      <td>956</td>\n",
       "      <td>912</td>\n",
       "      <td>59.0</td>\n",
       "      <td>64.284167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>H413</td>\n",
       "      <td>957</td>\n",
       "      <td>912</td>\n",
       "      <td>58.0</td>\n",
       "      <td>64.830429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>H413</td>\n",
       "      <td>958</td>\n",
       "      <td>912</td>\n",
       "      <td>53.0</td>\n",
       "      <td>40.726851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>H413</td>\n",
       "      <td>959</td>\n",
       "      <td>912</td>\n",
       "      <td>38.0</td>\n",
       "      <td>42.739657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>H413</td>\n",
       "      <td>960</td>\n",
       "      <td>912</td>\n",
       "      <td>46.0</td>\n",
       "      <td>52.802769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    unique_id   ds  cutoff     y  LGBMRegressor\n",
       "0        H196  769     768  15.2      15.167163\n",
       "1        H196  770     768  14.8      14.767163\n",
       "2        H196  771     768  14.4      14.467163\n",
       "3        H196  772     768  14.1      14.167163\n",
       "4        H196  773     768  13.8      13.867163\n",
       "..        ...  ...     ...   ...            ...\n",
       "763      H413  956     912  59.0      64.284167\n",
       "764      H413  957     912  58.0      64.830429\n",
       "765      H413  958     912  53.0      40.726851\n",
       "766      H413  959     912  38.0      42.739657\n",
       "767      H413  960     912  46.0      52.802769\n",
       "\n",
       "[768 rows x 5 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results = fcst.cross_validation(\n",
    "    train,\n",
    "    n_windows=4,\n",
    "    window_size=horizon,\n",
    "    step_size=horizon,\n",
    "    id_col='unique_id',\n",
    "    time_col='ds',\n",
    "    target_col='y',\n",
    ")\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f940f245-4a5c-4da6-a6b9-297c99610d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>cutoff</th>\n",
       "      <th>y</th>\n",
       "      <th>LGBMRegressor</th>\n",
       "      <th>LGBMRegressor-lo-90</th>\n",
       "      <th>LGBMRegressor-lo-80</th>\n",
       "      <th>LGBMRegressor-hi-80</th>\n",
       "      <th>LGBMRegressor-hi-90</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H196</td>\n",
       "      <td>769</td>\n",
       "      <td>768</td>\n",
       "      <td>15.2</td>\n",
       "      <td>15.167163</td>\n",
       "      <td>15.141751</td>\n",
       "      <td>15.141751</td>\n",
       "      <td>15.192575</td>\n",
       "      <td>15.192575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H196</td>\n",
       "      <td>770</td>\n",
       "      <td>768</td>\n",
       "      <td>14.8</td>\n",
       "      <td>14.767163</td>\n",
       "      <td>14.741751</td>\n",
       "      <td>14.741751</td>\n",
       "      <td>14.792575</td>\n",
       "      <td>14.792575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H196</td>\n",
       "      <td>771</td>\n",
       "      <td>768</td>\n",
       "      <td>14.4</td>\n",
       "      <td>14.467163</td>\n",
       "      <td>14.397493</td>\n",
       "      <td>14.402410</td>\n",
       "      <td>14.531916</td>\n",
       "      <td>14.536833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H196</td>\n",
       "      <td>772</td>\n",
       "      <td>768</td>\n",
       "      <td>14.1</td>\n",
       "      <td>14.167163</td>\n",
       "      <td>14.092575</td>\n",
       "      <td>14.092575</td>\n",
       "      <td>14.241751</td>\n",
       "      <td>14.241751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H196</td>\n",
       "      <td>773</td>\n",
       "      <td>768</td>\n",
       "      <td>13.8</td>\n",
       "      <td>13.867163</td>\n",
       "      <td>13.792575</td>\n",
       "      <td>13.792575</td>\n",
       "      <td>13.941751</td>\n",
       "      <td>13.941751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>H413</td>\n",
       "      <td>956</td>\n",
       "      <td>912</td>\n",
       "      <td>59.0</td>\n",
       "      <td>64.284167</td>\n",
       "      <td>28.396284</td>\n",
       "      <td>31.383915</td>\n",
       "      <td>97.184419</td>\n",
       "      <td>100.172049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>H413</td>\n",
       "      <td>957</td>\n",
       "      <td>912</td>\n",
       "      <td>58.0</td>\n",
       "      <td>64.830429</td>\n",
       "      <td>56.556867</td>\n",
       "      <td>57.192278</td>\n",
       "      <td>72.468580</td>\n",
       "      <td>73.103991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>H413</td>\n",
       "      <td>958</td>\n",
       "      <td>912</td>\n",
       "      <td>53.0</td>\n",
       "      <td>40.726851</td>\n",
       "      <td>35.112858</td>\n",
       "      <td>35.479532</td>\n",
       "      <td>45.974169</td>\n",
       "      <td>46.340843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>H413</td>\n",
       "      <td>959</td>\n",
       "      <td>912</td>\n",
       "      <td>38.0</td>\n",
       "      <td>42.739657</td>\n",
       "      <td>35.120323</td>\n",
       "      <td>35.463982</td>\n",
       "      <td>50.015332</td>\n",
       "      <td>50.358990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>H413</td>\n",
       "      <td>960</td>\n",
       "      <td>912</td>\n",
       "      <td>46.0</td>\n",
       "      <td>52.802769</td>\n",
       "      <td>41.988907</td>\n",
       "      <td>42.942288</td>\n",
       "      <td>62.663250</td>\n",
       "      <td>63.616632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    unique_id   ds  cutoff     y  LGBMRegressor  LGBMRegressor-lo-90  \\\n",
       "0        H196  769     768  15.2      15.167163            15.141751   \n",
       "1        H196  770     768  14.8      14.767163            14.741751   \n",
       "2        H196  771     768  14.4      14.467163            14.397493   \n",
       "3        H196  772     768  14.1      14.167163            14.092575   \n",
       "4        H196  773     768  13.8      13.867163            13.792575   \n",
       "..        ...  ...     ...   ...            ...                  ...   \n",
       "763      H413  956     912  59.0      64.284167            28.396284   \n",
       "764      H413  957     912  58.0      64.830429            56.556867   \n",
       "765      H413  958     912  53.0      40.726851            35.112858   \n",
       "766      H413  959     912  38.0      42.739657            35.120323   \n",
       "767      H413  960     912  46.0      52.802769            41.988907   \n",
       "\n",
       "     LGBMRegressor-lo-80  LGBMRegressor-hi-80  LGBMRegressor-hi-90  \n",
       "0              15.141751            15.192575            15.192575  \n",
       "1              14.741751            14.792575            14.792575  \n",
       "2              14.402410            14.531916            14.536833  \n",
       "3              14.092575            14.241751            14.241751  \n",
       "4              13.792575            13.941751            13.941751  \n",
       "..                   ...                  ...                  ...  \n",
       "763            31.383915            97.184419           100.172049  \n",
       "764            57.192278            72.468580            73.103991  \n",
       "765            35.479532            45.974169            46.340843  \n",
       "766            35.463982            50.015332            50.358990  \n",
       "767            42.942288            62.663250            63.616632  \n",
       "\n",
       "[768 rows x 9 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results_intervals = fcst.cross_validation(\n",
    "    train,\n",
    "    n_windows=4,\n",
    "    window_size=horizon,\n",
    "    step_size=horizon,\n",
    "    id_col='unique_id',\n",
    "    time_col='ds',\n",
    "    target_col='y',\n",
    "    prediction_intervals=PredictionIntervals(window_size=horizon),\n",
    "    level=[80, 90]\n",
    ")\n",
    "cv_results_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24dc0ae-4847-4258-a2ab-ae56cf403a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "cv_results_no_refit = fcst.cross_validation(\n",
    "    train,\n",
    "    n_windows=4,\n",
    "    window_size=horizon,\n",
    "    step_size=horizon,\n",
    "    id_col='unique_id',\n",
    "    time_col='ds',\n",
    "    target_col='y',\n",
    "    refit=False\n",
    ")\n",
    "# test we recover the same \"metadata\"\n",
    "test_eq(\n",
    "    cv_results_no_refit.drop(columns='LGBMRegressor'),\n",
    "    cv_results.drop(columns='LGBMRegressor')\n",
    ")\n",
    "# test the first window has the same forecasts\n",
    "first_cutoff = cv_results['cutoff'].iloc[0]\n",
    "test_eq(\n",
    "    cv_results_no_refit.query('cutoff == @first_cutoff'),\n",
    "    cv_results.query('cutoff == @first_cutoff')\n",
    ")\n",
    "# test next windows have different forecasts\n",
    "test_ne(\n",
    "    cv_results_no_refit.query('cutoff != @first_cutoff'),\n",
    "    cv_results.query('cutoff != @first_cutoff')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8ef2aa-daad-4b08-82e7-bf75938a0fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# one model per horizon\n",
    "cv_results2 = fcst.cross_validation(\n",
    "    train,\n",
    "    n_windows=4,\n",
    "    window_size=horizon,\n",
    "    step_size=horizon,\n",
    "    id_col='unique_id',\n",
    "    time_col='ds',\n",
    "    target_col='y',\n",
    "    max_horizon=horizon,\n",
    ")\n",
    "# the first entry per id and window is the same\n",
    "pd.testing.assert_frame_equal(\n",
    "    cv_results.groupby(['unique_id', 'cutoff']).head(1),\n",
    "    cv_results2.groupby(['unique_id', 'cutoff']).head(1)\n",
    ")\n",
    "# the rest is different\n",
    "test_fail(lambda: pd.testing.assert_frame_equal(cv_results, cv_results2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5520ec36-e57c-49e0-a49c-7bf5fb8ff1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# one model per horizon with prediction intervals\n",
    "cv_results2_intervals = fcst.cross_validation(\n",
    "    train,\n",
    "    n_windows=4,\n",
    "    window_size=horizon,\n",
    "    step_size=horizon,\n",
    "    id_col='unique_id',\n",
    "    time_col='ds',\n",
    "    target_col='y',\n",
    "    max_horizon=horizon,\n",
    "    prediction_intervals=PredictionIntervals(n_windows=2, window_size=horizon),\n",
    "    level=[80, 90]\n",
    ")\n",
    "# the first entry per id and window is the same\n",
    "pd.testing.assert_frame_equal(\n",
    "    cv_results_intervals.groupby(['unique_id', 'cutoff']).head(1),\n",
    "    cv_results2_intervals.groupby(['unique_id', 'cutoff']).head(1)\n",
    ")\n",
    "# the rest is different\n",
    "test_fail(lambda: pd.testing.assert_frame_equal(cv_results_intervals, cv_results2_intervals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e194aa-e261-400e-9264-1b54db9b163e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(16, 10))\n",
    "\n",
    "for uid, axi in zip(sample_ids, ax.flat):\n",
    "    subset = cv_results[cv_results['unique_id'].eq(uid)].drop(columns=['unique_id', 'cutoff'])\n",
    "    subset.set_index('ds').plot(ax=axi, title=uid)\n",
    "fig.savefig('figs/forecast__cross_validation.png', bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e195029f-0de8-4425-8c91-8a3b4f2333c6",
   "metadata": {},
   "source": [
    "![](figs/forecast__cross_validation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110faf3c-71d9-4686-ad2e-09ff61daa317",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(16, 10))\n",
    "\n",
    "for uid, axi in zip(sample_ids, ax.flat):\n",
    "    subset = cv_results_intervals[cv_results_intervals['unique_id'].eq(uid)].drop(columns=['unique_id', 'cutoff']).set_index('ds')\n",
    "    subset[['y', 'LGBMRegressor']].plot(ax=axi, title=uid)\n",
    "    axi.fill_between(\n",
    "        subset.index, \n",
    "        subset['LGBMRegressor-lo-90'].values, \n",
    "        subset['LGBMRegressor-hi-90'].values,\n",
    "        label='LGBMRegressor-level-90',\n",
    "        color='orange',\n",
    "        alpha=0.2\n",
    "    )\n",
    "    axi.legend()\n",
    "fig.savefig('figs/forecast__cross_validation_intervals.png', bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2745474f-0dfe-4161-a466-36d6c425eb37",
   "metadata": {},
   "source": [
    "![](figs/forecast__cross_validation_intervals.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0aaa93-3ce6-42a5-87a1-5a9d6f69f289",
   "metadata": {},
   "source": [
    "### Create MLForecast from LightGBMCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0fe555-a79c-4347-bad6-00d4e6a4cf7c",
   "metadata": {},
   "source": [
    "Once you've found a set of features and parameters that work for your problem you can build a forecast object from it using `MLForecast.from_cv`, which takes the trained `LightGBMCV` object and builds an `MLForecast` object that will use the same features and parameters. Then you can call fit and predict as you normally would."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf6f8a1-a69c-44e6-b419-6f29fe7db2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Start training from score 0.084340\n",
      "[10] mape: 0.118569\n",
      "[20] mape: 0.111506\n",
      "[30] mape: 0.107314\n",
      "[40] mape: 0.106089\n",
      "[50] mape: 0.106630\n",
      "Early stopping at round 50\n",
      "Using best iteration: 40\n"
     ]
    }
   ],
   "source": [
    "cv = LightGBMCV(\n",
    "    freq=1,\n",
    "    differences=[24],\n",
    "    lags=[24 * (i+1) for i in range(7)],\n",
    "    lag_transforms={\n",
    "        48: [(ewm_mean, 0.3)],\n",
    "    },\n",
    "    num_threads=1,\n",
    ")\n",
    "hist = cv.fit(\n",
    "    train,\n",
    "    n_windows=2,\n",
    "    window_size=horizon,\n",
    "    id_col='unique_id',\n",
    "    time_col='ds',\n",
    "    target_col='y',\n",
    "    params={'verbosity': -1},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2533aaa2-b668-402c-b86b-50125cca4158",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst = MLForecast.from_cv(cv)\n",
    "assert cv.best_iteration_ == fcst.models['LGBMRegressor'].n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2082a5-5562-4816-9d53-cba4599e0196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>LGBMRegressor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H196</td>\n",
       "      <td>961</td>\n",
       "      <td>16.111079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H196</td>\n",
       "      <td>962</td>\n",
       "      <td>15.711079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H196</td>\n",
       "      <td>963</td>\n",
       "      <td>15.311079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H196</td>\n",
       "      <td>964</td>\n",
       "      <td>15.011079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H196</td>\n",
       "      <td>965</td>\n",
       "      <td>14.711079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>H413</td>\n",
       "      <td>1004</td>\n",
       "      <td>92.722032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>H413</td>\n",
       "      <td>1005</td>\n",
       "      <td>69.153603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>H413</td>\n",
       "      <td>1006</td>\n",
       "      <td>68.811675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>H413</td>\n",
       "      <td>1007</td>\n",
       "      <td>53.693346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>H413</td>\n",
       "      <td>1008</td>\n",
       "      <td>46.055481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    unique_id    ds  LGBMRegressor\n",
       "0        H196   961      16.111079\n",
       "1        H196   962      15.711079\n",
       "2        H196   963      15.311079\n",
       "3        H196   964      15.011079\n",
       "4        H196   965      14.711079\n",
       "..        ...   ...            ...\n",
       "187      H413  1004      92.722032\n",
       "188      H413  1005      69.153603\n",
       "189      H413  1006      68.811675\n",
       "190      H413  1007      53.693346\n",
       "191      H413  1008      46.055481\n",
       "\n",
       "[192 rows x 3 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcst.fit(\n",
    "    train,\n",
    "    id_col='unique_id',\n",
    "    time_col='ds',\n",
    "    target_col='y',\n",
    ")\n",
    "fcst.predict(horizon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1942738d-13a3-4641-8706-cd9578709ec0",
   "metadata": {},
   "source": [
    "### Dynamic features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7a5949-18a7-4e6a-a6f9-e812fb860c98",
   "metadata": {},
   "source": [
    "We're going to use a synthetic dataset from this point onwards to demonstrate some other functionalities regarding external regressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4861d6d5-2c7b-4f42-a383-70890512c630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>static_0</th>\n",
       "      <th>static_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2000-10-05</td>\n",
       "      <td>3.981198</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2000-10-06</td>\n",
       "      <td>10.327401</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2000-10-07</td>\n",
       "      <td>17.657474</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2000-10-08</td>\n",
       "      <td>25.898790</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2000-10-09</td>\n",
       "      <td>34.494040</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_99</th>\n",
       "      <td>2001-05-10</td>\n",
       "      <td>45.340051</td>\n",
       "      <td>69</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_99</th>\n",
       "      <td>2001-05-11</td>\n",
       "      <td>3.022948</td>\n",
       "      <td>69</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_99</th>\n",
       "      <td>2001-05-12</td>\n",
       "      <td>10.131371</td>\n",
       "      <td>69</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_99</th>\n",
       "      <td>2001-05-13</td>\n",
       "      <td>14.572434</td>\n",
       "      <td>69</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_99</th>\n",
       "      <td>2001-05-14</td>\n",
       "      <td>22.816357</td>\n",
       "      <td>69</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27003 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ds          y  static_0  static_1\n",
       "unique_id                                          \n",
       "id_00     2000-10-05   3.981198        79        45\n",
       "id_00     2000-10-06  10.327401        79        45\n",
       "id_00     2000-10-07  17.657474        79        45\n",
       "id_00     2000-10-08  25.898790        79        45\n",
       "id_00     2000-10-09  34.494040        79        45\n",
       "...              ...        ...       ...       ...\n",
       "id_99     2001-05-10  45.340051        69        35\n",
       "id_99     2001-05-11   3.022948        69        35\n",
       "id_99     2001-05-12  10.131371        69        35\n",
       "id_99     2001-05-13  14.572434        69        35\n",
       "id_99     2001-05-14  22.816357        69        35\n",
       "\n",
       "[27003 rows x 4 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series = generate_daily_series(100, equal_ends=True, n_static_features=2, static_as_categorical=False)\n",
    "series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b5b920-853a-4fa0-91f2-625b0b6f5eec",
   "metadata": {},
   "source": [
    "As we saw in the previous example, the required columns are the series identifier, time and target. Whatever extra columns you have, like `static_0` and `static_1` here are considered to be static and are replicated when constructing the features for the next timestamp. You can disable this by passing `static_features` to `MLForecast.preprocess` or `MLForecast.fit` , which will only keep the columns you define there as static. Keep in mind that they will still be used for training, so you'll have to provide them to `MLForecast.predict` through the `dynamic_dfs` argument.\n",
    "\n",
    "By default the predict method repeats the static features and updates the transformations and the date features. If you have dynamic features like prices or a calendar with holidays you can pass them as a list to the `dynamic_dfs` argument of `MLForecast.predict`, which will call `pd.DataFrame.merge` on each of them in order.\n",
    "\n",
    "Here's an example:\n",
    "\n",
    "Suppose that we have a `product_id` column and we have a catalog for prices based on that `product_id` and the date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4d6354-e95e-444c-af11-18f273ee2d07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>product_id</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-06-09</td>\n",
       "      <td>1</td>\n",
       "      <td>0.548814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-06-10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.715189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-06-11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.602763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-06-12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.544883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-06-13</td>\n",
       "      <td>1</td>\n",
       "      <td>0.423655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20180</th>\n",
       "      <td>2001-05-17</td>\n",
       "      <td>99</td>\n",
       "      <td>0.223520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20181</th>\n",
       "      <td>2001-05-18</td>\n",
       "      <td>99</td>\n",
       "      <td>0.446104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20182</th>\n",
       "      <td>2001-05-19</td>\n",
       "      <td>99</td>\n",
       "      <td>0.044783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20183</th>\n",
       "      <td>2001-05-20</td>\n",
       "      <td>99</td>\n",
       "      <td>0.483216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20184</th>\n",
       "      <td>2001-05-21</td>\n",
       "      <td>99</td>\n",
       "      <td>0.799660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20185 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ds  product_id     price\n",
       "0     2000-06-09           1  0.548814\n",
       "1     2000-06-10           1  0.715189\n",
       "2     2000-06-11           1  0.602763\n",
       "3     2000-06-12           1  0.544883\n",
       "4     2000-06-13           1  0.423655\n",
       "...          ...         ...       ...\n",
       "20180 2001-05-17          99  0.223520\n",
       "20181 2001-05-18          99  0.446104\n",
       "20182 2001-05-19          99  0.044783\n",
       "20183 2001-05-20          99  0.483216\n",
       "20184 2001-05-21          99  0.799660\n",
       "\n",
       "[20185 rows x 3 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynamic_series = series.rename(columns={'static_1': 'product_id'})\n",
    "prices_catalog = generate_prices_for_series(dynamic_series)\n",
    "prices_catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61decbbd-91f6-4e90-a750-1eb97b880c90",
   "metadata": {},
   "source": [
    "And you have already merged these prices into your series dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc453901-10d1-441f-af31-0ee52a3d58f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>static_0</th>\n",
       "      <th>product_id</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2000-10-05</td>\n",
       "      <td>3.981198</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>0.570826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2000-10-06</td>\n",
       "      <td>10.327401</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>0.260562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2000-10-07</td>\n",
       "      <td>17.657474</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>0.274048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2000-10-08</td>\n",
       "      <td>25.898790</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>0.433878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2000-10-09</td>\n",
       "      <td>34.494040</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>0.653738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_99</th>\n",
       "      <td>2001-05-10</td>\n",
       "      <td>45.340051</td>\n",
       "      <td>69</td>\n",
       "      <td>35</td>\n",
       "      <td>0.792152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_99</th>\n",
       "      <td>2001-05-11</td>\n",
       "      <td>3.022948</td>\n",
       "      <td>69</td>\n",
       "      <td>35</td>\n",
       "      <td>0.782687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_99</th>\n",
       "      <td>2001-05-12</td>\n",
       "      <td>10.131371</td>\n",
       "      <td>69</td>\n",
       "      <td>35</td>\n",
       "      <td>0.019463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_99</th>\n",
       "      <td>2001-05-13</td>\n",
       "      <td>14.572434</td>\n",
       "      <td>69</td>\n",
       "      <td>35</td>\n",
       "      <td>0.190413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_99</th>\n",
       "      <td>2001-05-14</td>\n",
       "      <td>22.816357</td>\n",
       "      <td>69</td>\n",
       "      <td>35</td>\n",
       "      <td>0.653394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27003 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ds          y  static_0  product_id     price\n",
       "unique_id                                                      \n",
       "id_00     2000-10-05   3.981198        79          45  0.570826\n",
       "id_00     2000-10-06  10.327401        79          45  0.260562\n",
       "id_00     2000-10-07  17.657474        79          45  0.274048\n",
       "id_00     2000-10-08  25.898790        79          45  0.433878\n",
       "id_00     2000-10-09  34.494040        79          45  0.653738\n",
       "...              ...        ...       ...         ...       ...\n",
       "id_99     2001-05-10  45.340051        69          35  0.792152\n",
       "id_99     2001-05-11   3.022948        69          35  0.782687\n",
       "id_99     2001-05-12  10.131371        69          35  0.019463\n",
       "id_99     2001-05-13  14.572434        69          35  0.190413\n",
       "id_99     2001-05-14  22.816357        69          35  0.653394\n",
       "\n",
       "[27003 rows x 5 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series_with_prices = dynamic_series.reset_index().merge(prices_catalog, how='left')\n",
    "series_with_prices.set_index('unique_id', inplace=True)\n",
    "series_with_prices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f373479-d37c-4f92-b12b-5c937898ef6b",
   "metadata": {},
   "source": [
    "This dataframe will be passed to `MLForecast.fit` (or `MLForecast.preprocess`), however since the price is dynamic we have to tell that method that only `static_0` and `product_id` are static and we'll have to update `price` in every timestep, which basically involves merging the updated features with the prices catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac4391d-5dd4-4eb9-91bb-e39fe67023c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLForecast(models=[LGBMRegressor, XGBRegressor], freq=<Day>, lag_features=['lag7', 'expanding_mean_lag1', 'rolling_mean_lag7_window_size14'], date_features=['dayofweek', 'month', <function even_day>], num_threads=2)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def even_day(dates):\n",
    "    return dates.day % 2 == 0\n",
    "\n",
    "models = [\n",
    "    lgb.LGBMRegressor(n_jobs=1, random_state=0),\n",
    "    xgb.XGBRegressor(n_jobs=1, random_state=0),\n",
    "]\n",
    "fcst = MLForecast(\n",
    "    models,\n",
    "    freq='D',\n",
    "    lags=[7],\n",
    "    lag_transforms={\n",
    "        1: [expanding_mean],\n",
    "        7: [(rolling_mean, 14)]\n",
    "    },\n",
    "    date_features=['dayofweek', 'month', even_day],\n",
    "    num_threads=2,\n",
    ")\n",
    "fcst.fit(\n",
    "    series_with_prices,\n",
    "    id_col='index',\n",
    "    time_col='ds',\n",
    "    target_col='y',\n",
    "    static_features=['static_0', 'product_id'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c38fea-4868-4d34-9932-ce8af283d8ac",
   "metadata": {},
   "source": [
    "The features used for training are stored in `Forecast.ts.features_order_`, as you can see `price` was used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c160114-073e-483a-9c27-638487674b25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['static_0',\n",
       " 'product_id',\n",
       " 'price',\n",
       " 'lag7',\n",
       " 'expanding_mean_lag1',\n",
       " 'rolling_mean_lag7_window_size14',\n",
       " 'dayofweek',\n",
       " 'month',\n",
       " 'even_day']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcst.ts.features_order_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f8d250-a5d8-4fb0-a41c-d5515b645c37",
   "metadata": {},
   "source": [
    "So in order to update the price in each timestep we just call `Forecast.predict` with our forecast horizon and pass the prices catalog as a dynamic dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e071906-3d80-4fa5-9b55-55c90f338985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>LGBMRegressor</th>\n",
       "      <th>XGBRegressor</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2001-05-15</td>\n",
       "      <td>42.184358</td>\n",
       "      <td>43.174004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2001-05-16</td>\n",
       "      <td>50.186606</td>\n",
       "      <td>50.842575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2001-05-17</td>\n",
       "      <td>1.940786</td>\n",
       "      <td>1.911936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2001-05-18</td>\n",
       "      <td>10.432289</td>\n",
       "      <td>9.788165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2001-05-19</td>\n",
       "      <td>18.701071</td>\n",
       "      <td>18.377850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_99</th>\n",
       "      <td>2001-05-17</td>\n",
       "      <td>44.311743</td>\n",
       "      <td>43.611797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_99</th>\n",
       "      <td>2001-05-18</td>\n",
       "      <td>1.909511</td>\n",
       "      <td>1.922798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_99</th>\n",
       "      <td>2001-05-19</td>\n",
       "      <td>9.067718</td>\n",
       "      <td>8.772107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_99</th>\n",
       "      <td>2001-05-20</td>\n",
       "      <td>14.967183</td>\n",
       "      <td>15.344975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_99</th>\n",
       "      <td>2001-05-21</td>\n",
       "      <td>22.917440</td>\n",
       "      <td>22.898575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>700 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ds  LGBMRegressor  XGBRegressor\n",
       "unique_id                                        \n",
       "id_00     2001-05-15      42.184358     43.174004\n",
       "id_00     2001-05-16      50.186606     50.842575\n",
       "id_00     2001-05-17       1.940786      1.911936\n",
       "id_00     2001-05-18      10.432289      9.788165\n",
       "id_00     2001-05-19      18.701071     18.377850\n",
       "...              ...            ...           ...\n",
       "id_99     2001-05-17      44.311743     43.611797\n",
       "id_99     2001-05-18       1.909511      1.922798\n",
       "id_99     2001-05-19       9.067718      8.772107\n",
       "id_99     2001-05-20      14.967183     15.344975\n",
       "id_99     2001-05-21      22.917440     22.898575\n",
       "\n",
       "[700 rows x 3 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = fcst.predict(7, dynamic_dfs=[prices_catalog])\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab63b809-a70a-4067-891a-4fe3d6375069",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "preds2 = fcst.predict(7, dynamic_dfs=[prices_catalog])\n",
    "preds3 = fcst.predict(7, new_data=series_with_prices, dynamic_dfs=[prices_catalog])\n",
    "\n",
    "pd.testing.assert_frame_equal(preds, preds2)\n",
    "pd.testing.assert_frame_equal(preds, preds3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f95689-3ff3-4588-9385-9e85ba6a2dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test we can compute cross validation with\n",
    "# exougenous variables without adding extra information\n",
    "# later a more robust test is performed\n",
    "_ = fcst.cross_validation(\n",
    "    series_with_prices,\n",
    "    window_size=7,\n",
    "    n_windows=2,\n",
    "    id_col='index',\n",
    "    time_col='ds',\n",
    "    target_col='y',\n",
    "    static_features=['static_0', 'product_id'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ab2fd6-cc7f-47e3-8a87-e0ec21590385",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "non_std_series = series.copy()\n",
    "non_std_series['ds'] = non_std_series.groupby('unique_id').cumcount()\n",
    "non_std_series = non_std_series.reset_index().rename(columns={'unique_id': 'some_id', 'ds': 'time', 'y': 'value'})\n",
    "flow_params = dict(\n",
    "    models=models,\n",
    "    lags=[7],\n",
    "    lag_transforms={\n",
    "        1: [expanding_mean],\n",
    "        7: [(rolling_mean, 14)]\n",
    "    },\n",
    "    num_threads=2,\n",
    ")\n",
    "fcst = MLForecast(**flow_params)\n",
    "non_std_preds = fcst.fit(non_std_series, id_col='some_id', time_col='time', target_col='value').predict(7)\n",
    "non_std_preds = non_std_preds.rename(columns={'some_id': 'unique_id'}).set_index('unique_id')\n",
    "fcst = MLForecast(freq='D', **flow_params)\n",
    "preds = fcst.fit(series, id_col='index', time_col='ds', target_col='y').predict(7)\n",
    "pd.testing.assert_frame_equal(preds.drop(columns='ds'), non_std_preds.drop(columns='time'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6fb8b4-f0a3-4012-a410-05972fe61ef1",
   "metadata": {},
   "source": [
    "### Custom predictions\n",
    "As you may have noticed `MLForecast.predict` can take a `before_predict_callback` and `after_predict_callback`. By default the predict method repeats the static features and updates the transformations and the date features. If you have dynamic features you can pass them as a list to `MLForecast.predict` in the `dynamic_dfs` argument. However, if you want to do something to the input before predicting or do something to the output before it gets used to update the target (and thus the next features that rely on lags), you can pass a function to run at any of these times. \n",
    "\n",
    "Suppose that we want to look at our inputs and scale our predictions so that our series are updated with these scaled values. We can achieve that with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b3f96f-4bea-4e29-84af-caa370b19c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "def inspect_input(new_x):\n",
    "    \"\"\"Displays the first row of our input to inspect it\"\"\"\n",
    "    print('Inputs:')\n",
    "    display(new_x.head(1))\n",
    "    return new_x\n",
    "\n",
    "def increase_predictions(predictions):\n",
    "    \"\"\"Prints the last prediction and increases all of them by 10%.\"\"\"\n",
    "    print(f'Prediction:\\n{predictions.tail(1)}\\n')\n",
    "    return 1.1 * predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a252b2-b83c-49d0-bc59-6d4ae7f637fb",
   "metadata": {},
   "source": [
    "And now we just pass these functions to `MLForecast.predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395c58c7-2c30-4fba-89ae-d43a11ea6115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>static_0</th>\n",
       "      <th>static_1</th>\n",
       "      <th>lag7</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>43.150169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           static_0  static_1       lag7\n",
       "unique_id                               \n",
       "id_00            79        45  43.150169"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\n",
      "unique_id\n",
      "id_99    29.638271\n",
      "dtype: float64\n",
      "\n",
      "Inputs:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>static_0</th>\n",
       "      <th>static_1</th>\n",
       "      <th>lag7</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>50.425902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           static_0  static_1       lag7\n",
       "unique_id                               \n",
       "id_00            79        45  50.425902"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\n",
      "unique_id\n",
      "id_99    37.138933\n",
      "dtype: float64\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>LGBMRegressor</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2001-05-15</td>\n",
       "      <td>46.298462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2001-05-16</td>\n",
       "      <td>54.772835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_01</th>\n",
       "      <td>2001-05-15</td>\n",
       "      <td>12.808338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_01</th>\n",
       "      <td>2001-05-16</td>\n",
       "      <td>16.426544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_02</th>\n",
       "      <td>2001-05-15</td>\n",
       "      <td>16.339826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_97</th>\n",
       "      <td>2001-05-16</td>\n",
       "      <td>38.865211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_98</th>\n",
       "      <td>2001-05-15</td>\n",
       "      <td>8.268328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_98</th>\n",
       "      <td>2001-05-16</td>\n",
       "      <td>10.476819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_99</th>\n",
       "      <td>2001-05-15</td>\n",
       "      <td>32.602098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_99</th>\n",
       "      <td>2001-05-16</td>\n",
       "      <td>40.852826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ds  LGBMRegressor\n",
       "unique_id                          \n",
       "id_00     2001-05-15      46.298462\n",
       "id_00     2001-05-16      54.772835\n",
       "id_01     2001-05-15      12.808338\n",
       "id_01     2001-05-16      16.426544\n",
       "id_02     2001-05-15      16.339826\n",
       "...              ...            ...\n",
       "id_97     2001-05-16      38.865211\n",
       "id_98     2001-05-15       8.268328\n",
       "id_98     2001-05-16      10.476819\n",
       "id_99     2001-05-15      32.602098\n",
       "id_99     2001-05-16      40.852826\n",
       "\n",
       "[200 rows x 2 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcst = MLForecast(lgb.LGBMRegressor(), freq='D', lags=[7])\n",
    "fcst.fit(series, id_col='index', time_col='ds', target_col='y')\n",
    "\n",
    "preds = fcst.predict(2, before_predict_callback=inspect_input, after_predict_callback=increase_predictions)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f15e85-4cfe-4f04-b6bb-5f794dd12390",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "fcst.ts._predict_setup()\n",
    "\n",
    "for attr in ('head', 'tail'):\n",
    "    new_x = fcst.ts._update_features().drop(columns='ds')\n",
    "    original_preds = fcst.models_['LGBMRegressor'].predict(new_x)\n",
    "    \n",
    "    expected = 1.1 * original_preds\n",
    "    actual = getattr(preds.groupby('unique_id')[models[0].__class__.__name__], attr)(1).values\n",
    "    np.testing.assert_equal(expected, actual)\n",
    "    \n",
    "    fcst.ts._update_y(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383ca7ea-b275-4bc6-8ee7-3a41a46f3213",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "def test_cross_validation(data=non_std_series, add_exogenous=False):\n",
    "    n_windows = 2\n",
    "    window_size = 14\n",
    "    fcst = MLForecast(lgb.LGBMRegressor(), freq='D', lags=[7, 14])\n",
    "    if add_exogenous:\n",
    "        data = data.assign(ex1 = lambda x: np.arange(0, len(x)))\n",
    "    backtest_results = fcst.cross_validation(\n",
    "        data,\n",
    "        n_windows,\n",
    "        window_size,\n",
    "        id_col='some_id',\n",
    "        time_col='time',\n",
    "        target_col='value',\n",
    "        static_features=['static_0', 'static_1'],\n",
    "    )\n",
    "    renamer = {'some_id': 'unique_id', 'time': 'ds', 'value': 'y'}\n",
    "    backtest_results = backtest_results.rename(columns=renamer).set_index('unique_id')\n",
    "    renamed = data.rename(columns=renamer).set_index('unique_id')\n",
    "    manual_results = []\n",
    "    for cutoff, train, valid in backtest_splits(renamed, n_windows, window_size, 1):\n",
    "        fcst.fit(\n",
    "            train,\n",
    "            id_col='index',\n",
    "            time_col='ds',\n",
    "            target_col='y',\n",
    "            static_features=['static_0', 'static_1'],\n",
    "        )\n",
    "        if add_exogenous:\n",
    "            dynamic_dfs = [valid.drop(columns=['y', 'static_0', 'static_1']).reset_index()]\n",
    "        else:\n",
    "            dynamic_dfs = None\n",
    "        pred = fcst.predict(window_size, dynamic_dfs=dynamic_dfs)\n",
    "        res = valid[['ds', 'y']].copy()\n",
    "        res['cutoff'] = cutoff\n",
    "        res = res[['ds', 'cutoff', 'y']].copy()\n",
    "        manual_results.append(res.merge(pred, on=['unique_id', 'ds'], how='left'))\n",
    "    manual_results = pd.concat(manual_results)\n",
    "    pd.testing.assert_frame_equal(backtest_results, manual_results)\n",
    "test_cross_validation()\n",
    "test_cross_validation(add_exogenous=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
