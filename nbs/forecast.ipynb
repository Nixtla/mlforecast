{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efa33fa-816f-463f-9215-9559b0ddd6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20376798-3d26-4c74-9e52-d5b657b7768a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccedaf1-56c9-4aaf-af7b-fe049df299ad",
   "metadata": {},
   "source": [
    "# MLForecast\n",
    "\n",
    "> Full pipeline encapsulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b089a52-e06d-49b1-9328-793cffe56045",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import copy\n",
    "import warnings\n",
    "from typing import TYPE_CHECKING, Callable, Dict, Iterable, List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, clone\n",
    "\n",
    "from mlforecast.core import (\n",
    "    DateFeature,\n",
    "    Freq,\n",
    "    LagTransforms,\n",
    "    Lags,\n",
    "    Models,\n",
    "    TimeSeries,\n",
    "    _name_models,\n",
    ")\n",
    "if TYPE_CHECKING:\n",
    "    from mlforecast.lgb_cv import LightGBMCV\n",
    "from mlforecast.target_transforms import BaseTargetTransform\n",
    "from mlforecast.utils import backtest_splits, PredictionIntervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07073d89-e33c-41d6-9bd3-a6daa07fef5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastcore.test import test_warns, test_eq, test_ne, test_fail, test_close\n",
    "from nbdev import show_doc\n",
    "from sklearn import set_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d9a4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "set_config(display='text')\n",
    "warnings.simplefilter('ignore', UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574354b3-0d44-44df-9004-79f5c28f9d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _add_conformal_distribution_intervals(\n",
    "        fcst_df: pd.DataFrame, \n",
    "        cs_df: pd.DataFrame, \n",
    "        model_names: List[str],\n",
    "        level: List[Union[int, float]],\n",
    "        cs_n_windows: int,\n",
    "        cs_window_size: int,\n",
    "        n_series: int,\n",
    "        horizon: int,\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds conformal intervals to a `fcst_df` based on conformal scores `cs_df`.\n",
    "    `level` should be already sorted. This strategy creates forecasts paths\n",
    "    based on errors and calculate quantiles using those paths.\n",
    "    \"\"\"\n",
    "    fcst_df = fcst_df.copy()\n",
    "    alphas = [100 - lv for lv in level]\n",
    "    cuts = [alpha / 200 for alpha in reversed(alphas)]\n",
    "    cuts.extend(1 - alpha / 200 for alpha in alphas)\n",
    "    for model in model_names:\n",
    "        scores = cs_df[model].values.reshape(cs_n_windows, n_series, cs_window_size)\n",
    "        # restrict scores to horizon\n",
    "        scores = scores[:,:,:horizon]\n",
    "        mean = fcst_df[model].values.reshape(1, n_series, -1)\n",
    "        scores = np.vstack([mean - scores, mean + scores])\n",
    "        quantiles = np.quantile(\n",
    "            scores,\n",
    "            cuts,\n",
    "            axis=0,\n",
    "        )\n",
    "        quantiles = quantiles.reshape(len(cuts), -1)\n",
    "        lo_cols = [f\"{model}-lo-{lv}\" for lv in reversed(level)]\n",
    "        hi_cols = [f\"{model}-hi-{lv}\" for lv in level]\n",
    "        out_cols = lo_cols + hi_cols\n",
    "        for i, col in enumerate(out_cols):\n",
    "            fcst_df[col] = quantiles[i]\n",
    "    return fcst_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba40be92-6287-4d29-86bc-3905f798e8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _add_conformal_error_intervals(\n",
    "        fcst_df: pd.DataFrame, \n",
    "        cs_df: pd.DataFrame, \n",
    "        model_names: List[str],\n",
    "        level: List[Union[int, float]],\n",
    "        cs_n_windows: int,\n",
    "        cs_window_size: int,\n",
    "        n_series: int,\n",
    "        horizon: int,\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds conformal intervals to a `fcst_df` based on conformal scores `cs_df`.\n",
    "    `level` should be already sorted. This startegy creates prediction intervals\n",
    "    based on the absolute errors.\n",
    "    \"\"\"\n",
    "    fcst_df = fcst_df.copy()\n",
    "    cuts = [lv / 100 for lv in level]\n",
    "    for model in model_names:\n",
    "        mean = fcst_df[model].values.ravel()\n",
    "        scores = cs_df[model].values.reshape(cs_n_windows, n_series, cs_window_size)\n",
    "        # restrict scores to horizon\n",
    "        scores = scores[:,:,:horizon]\n",
    "        quantiles = np.quantile(\n",
    "            scores,\n",
    "            cuts,\n",
    "            axis=0,\n",
    "        )\n",
    "        quantiles = quantiles.reshape(len(cuts), -1)\n",
    "        lo_cols = [f\"{model}-lo-{lv}\" for lv in reversed(level)]\n",
    "        hi_cols = [f\"{model}-hi-{lv}\" for lv in level]\n",
    "        for i, col in enumerate(lo_cols):\n",
    "            fcst_df[col] = mean - quantiles[len(level) - 1 - i]\n",
    "        for i, col in enumerate(hi_cols):\n",
    "            fcst_df[col] = mean + quantiles[i]\n",
    "    return fcst_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fdbe39-a0fe-47bb-b284-b36c2044aa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _get_conformal_method(method: str):\n",
    "    available_methods = {\n",
    "        'conformal_distribution': _add_conformal_distribution_intervals,\n",
    "        'conformal_error': _add_conformal_error_intervals, \n",
    "    }\n",
    "    if method not in available_methods.keys():\n",
    "        raise ValueError(\n",
    "            f'prediction intervals method {method} not supported '\n",
    "            f'please choose one of {\", \".join(available_methods.keys())}'\n",
    "        )\n",
    "    return available_methods[method]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7901585-90c4-4950-a2a0-ec45e0a665d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_fail(lambda: _get_conformal_method('my_method'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8303523-551f-48ed-8f81-3cfd222d6e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class MLForecast:\n",
    "    def __init__(\n",
    "        self,\n",
    "        models: Models,\n",
    "        freq: Optional[Freq] = None,\n",
    "        lags: Optional[Lags] = None,\n",
    "        lag_transforms: Optional[LagTransforms] = None,\n",
    "        date_features: Optional[Iterable[DateFeature]] = None,\n",
    "        differences: Optional[Iterable[int]] = None,\n",
    "        num_threads: int = 1,\n",
    "        target_transforms: Optional[List[BaseTargetTransform]] = None,\n",
    "    ):\n",
    "        \"\"\"Create forecast object\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        models : regressor or list of regressors\n",
    "            Models that will be trained and used to compute the forecasts.\n",
    "        freq : str or int or pd.offsets.BaseOffset, optional (default=None)\n",
    "            Pandas offset, pandas offset alias, e.g. 'D', 'W-THU' or integer denoting the frequency of the series.\n",
    "        lags : list of int, optional (default=None)\n",
    "            Lags of the target to use as features.\n",
    "        lag_transforms : dict of int to list of functions, optional (default=None)\n",
    "            Mapping of target lags to their transformations.\n",
    "        date_features : list of str or callable, optional (default=None)\n",
    "            Features computed from the dates. Can be pandas date attributes or functions that will take the dates as input.\n",
    "        differences : list of int, optional (default=None)\n",
    "            Differences to take of the target before computing the features. These are restored at the forecasting step.\n",
    "        num_threads : int (default=1)\n",
    "            Number of threads to use when computing the features.\n",
    "        target_transforms : list of transformers, optional(default=None)\n",
    "            Transformations that will be applied to the target before computing the features and restored after the forecasting step.\n",
    "        \"\"\"\n",
    "        if not isinstance(models, dict) and not isinstance(models, list):\n",
    "            models = [models]\n",
    "        if isinstance(models, list):\n",
    "            model_names = _name_models([m.__class__.__name__ for m in models])            \n",
    "            models_with_names = dict(zip(model_names, models))\n",
    "        else:\n",
    "            models_with_names = models\n",
    "        self.models = models_with_names\n",
    "        self.ts = TimeSeries(\n",
    "            freq=freq,\n",
    "            lags=lags,\n",
    "            lag_transforms=lag_transforms,\n",
    "            date_features=date_features,\n",
    "            differences=differences,\n",
    "            num_threads=num_threads,\n",
    "            target_transforms=target_transforms\n",
    "        )\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            f'{self.__class__.__name__}(models=[{\", \".join(self.models.keys())}], '\n",
    "            f'freq={self.freq}, '\n",
    "            f'lag_features={list(self.ts.transforms.keys())}, '\n",
    "            f'date_features={self.ts.date_features}, '\n",
    "            f'num_threads={self.ts.num_threads})'\n",
    "        )\n",
    "    \n",
    "    @property\n",
    "    def freq(self):\n",
    "        return self.ts.freq\n",
    "    \n",
    "    @classmethod\n",
    "    def from_cv(cls, cv: 'LightGBMCV') -> 'MLForecast':\n",
    "        if not hasattr(cv, 'best_iteration_'):\n",
    "            raise ValueError('LightGBMCV object must be fitted first.')\n",
    "        import lightgbm as lgb\n",
    "        fcst = cls(lgb.LGBMRegressor(**{**cv.params, 'n_estimators': cv.best_iteration_}))\n",
    "        fcst.ts = copy.deepcopy(cv.ts)\n",
    "        return fcst\n",
    "\n",
    "    def preprocess(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        target_col: str = 'y',\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "        max_horizon: Optional[int] = None,\n",
    "        return_X_y: bool = False,\n",
    "    ) -> Union[pd.DataFrame, Tuple[pd.DataFrame, Union[pd.Series, pd.DataFrame]]]:\n",
    "        \"\"\"Add the features to `data`.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data : pandas DataFrame\n",
    "            Series data in long format.\n",
    "        id_col : str (default='unique_id')\n",
    "            Column that identifies each serie.\n",
    "        time_col : str (default='ds')\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str (default='y')\n",
    "            Column that contains the target.\n",
    "        static_features : list of str, optional (default=None)\n",
    "            Names of the features that are static and will be repeated when forecasting.\n",
    "        dropna : bool (default=True)\n",
    "            Drop rows with missing values produced by the transformations.\n",
    "        keep_last_n : int, optional (default=None)\n",
    "            Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it.\n",
    "        max_horizon: int, optional (default=None)\n",
    "            Train this many models, where each model will predict a specific horizon.\n",
    "        return_X_y: bool (default=False)\n",
    "            Return a tuple with the features and the target. If False will return a single dataframe.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        result : pandas DataFrame or tuple of pandas Dataframe and either a pandas Series or a pandas Dataframe (for multi-output regression).\n",
    "            `data` plus added features and target(s).\n",
    "        \"\"\"\n",
    "        return self.ts.fit_transform(\n",
    "            data,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            static_features=static_features,\n",
    "            dropna=dropna,\n",
    "            keep_last_n=keep_last_n,\n",
    "            max_horizon=max_horizon,\n",
    "            return_X_y=return_X_y,\n",
    "        )\n",
    "    \n",
    "    def fit_models(\n",
    "        self,\n",
    "        X: pd.DataFrame,\n",
    "        y: Union[pd.Series, pd.DataFrame],\n",
    "    ) -> 'MLForecast':\n",
    "        \"\"\"Manually train models. Use this if you called `Forecast.preprocess` beforehand.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas DataFrame\n",
    "            Features.\n",
    "        y : pandas Series or pandas DataFrame (multi-output).\n",
    "            Target.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self : MLForecast\n",
    "            Forecast object with trained models.\n",
    "        \"\"\"\n",
    "        self.models_: Dict[str, Union[BaseEstimator, List[BaseEstimator]]] = {}\n",
    "        for name, model in self.models.items():\n",
    "            if y.ndim == 2 and y.shape[1] > 1:\n",
    "                self.models_[name] = []                \n",
    "                for col in y:\n",
    "                    keep = y[col].notnull()\n",
    "                    self.models_[name].append(clone(model).fit(X.loc[keep], y.loc[keep, col]))\n",
    "            else:\n",
    "                self.models_[name] = clone(model).fit(X, y)\n",
    "        return self\n",
    "    \n",
    "    def _conformity_scores(\n",
    "        self,\n",
    "        data: pd.DataFrame, \n",
    "        id_col: str, \n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "        max_horizon: Optional[int] = None,\n",
    "        n_windows: int = 2,\n",
    "        window_size: int = 1,\n",
    "    ):\n",
    "        \"\"\"Compute conformity scores.\n",
    "        \n",
    "        We need at least two cross validation errors to compute\n",
    "        quantiles for prediction intervals (`n_windows=2`).\n",
    "        \n",
    "        The exception is raised by the PredictionIntervals data class.\n",
    "        \n",
    "        In this simplest case, we assume the width of the interval\n",
    "        is the same for all the forecasting horizon (`window_size=1`).\n",
    "        \"\"\"\n",
    "        cv_results = self.cross_validation(\n",
    "            data=data, \n",
    "            n_windows=n_windows,\n",
    "            window_size=window_size,\n",
    "            refit=False,\n",
    "            id_col=id_col, \n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            static_features=static_features,\n",
    "            dropna=dropna,\n",
    "            keep_last_n=keep_last_n,\n",
    "            max_horizon=max_horizon,\n",
    "            prediction_intervals=None,\n",
    "        )\n",
    "        # conformity score for each model\n",
    "        for model in self.models.keys():\n",
    "            # compute absolute error for each model\n",
    "            cv_results[model] = np.abs(cv_results[model] - cv_results[target_col])\n",
    "        return cv_results.drop(columns=target_col)\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        target_col: str = 'y',\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "        max_horizon: Optional[int] = None,\n",
    "        prediction_intervals: Optional[PredictionIntervals] = None,\n",
    "    ) -> 'MLForecast':\n",
    "        \"\"\"Apply the feature engineering and train the models.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data : pandas DataFrame\n",
    "            Series data in long format.\n",
    "        id_col : str (default='unique_id')\n",
    "            Column that identifies each serie.\n",
    "        time_col : str (default='ds')\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str (default='y')\n",
    "            Column that contains the target.\n",
    "        static_features : list of str, optional (default=None)\n",
    "            Names of the features that are static and will be repeated when forecasting.\n",
    "        dropna : bool (default=True)\n",
    "            Drop rows with missing values produced by the transformations.\n",
    "        keep_last_n : int, optional (default=None)\n",
    "            Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it.\n",
    "        max_horizon: int, optional (default=None)\n",
    "            Train this many models, where each model will predict a specific horizon.\n",
    "        prediction_intervals : PredictionIntervals, optional (default=None)\n",
    "            Configuration to calibrate prediction intervals (Conformal Prediction).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : MLForecast\n",
    "            Forecast object with series values and trained models.\n",
    "        \"\"\"\n",
    "        self._cs_df: Optional[pd.DataFrame] = None\n",
    "        if prediction_intervals is not None:\n",
    "            self.prediction_intervals = prediction_intervals\n",
    "            self._cs_df = self._conformity_scores(\n",
    "                data=data, \n",
    "                id_col=id_col, \n",
    "                time_col=time_col,\n",
    "                target_col=target_col,\n",
    "                static_features=static_features,\n",
    "                dropna=dropna,\n",
    "                keep_last_n=keep_last_n,\n",
    "                n_windows=prediction_intervals.n_windows,\n",
    "                window_size=prediction_intervals.window_size,\n",
    "            )\n",
    "        X, y = self.preprocess(\n",
    "            data,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            static_features=static_features,\n",
    "            dropna=dropna,\n",
    "            keep_last_n=keep_last_n,\n",
    "            max_horizon=max_horizon,\n",
    "            return_X_y=True,\n",
    "        )\n",
    "        X = X[self.ts.features_order_]\n",
    "        return self.fit_models(X, y)\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        horizon: int,\n",
    "        dynamic_dfs: Optional[List[pd.DataFrame]] = None,\n",
    "        before_predict_callback: Optional[Callable] = None,\n",
    "        after_predict_callback: Optional[Callable] = None,\n",
    "        new_data: Optional[pd.DataFrame] = None,\n",
    "        level: Optional[List[Union[int, float]]] = None,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Compute the predictions for the next `horizon` steps.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        horizon : int\n",
    "            Number of periods to predict.\n",
    "        dynamic_dfs : list of pandas DataFrame, optional (default=None)\n",
    "            Future values of the dynamic features, e.g. prices.\n",
    "        before_predict_callback : callable, optional (default=None)\n",
    "            Function to call on the features before computing the predictions.\n",
    "                This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure.\n",
    "                The series identifier is on the index.\n",
    "        after_predict_callback : callable, optional (default=None)\n",
    "            Function to call on the predictions before updating the targets.\n",
    "                This function will take a pandas Series with the predictions and should return another one with the same structure.\n",
    "                The series identifier is on the index.   \n",
    "        new_data : pandas DataFrame, optional (default=None)\n",
    "            Series data of new observations for which forecasts are to be generated. \n",
    "                This dataframe should have the same structure as the one used to fit the model, including any features and time series data. \n",
    "                If `new_data` is not None, the method will generate forecasts for the new observations.\n",
    "        level : list of ints or floats, optional (default=None)\n",
    "            Confidence levels between 0 and 100 for prediction intervals.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        result : pandas DataFrame\n",
    "            Predictions for each serie and timestep, with one column per model.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'models_'):\n",
    "            raise ValueError('No fitted models found. You have to call fit or preprocess + fit_models.')\n",
    "            \n",
    "        if new_data is not None:\n",
    "            new_ts = TimeSeries(\n",
    "                freq=self.ts.freq,\n",
    "                lags=self.ts.lags, \n",
    "                lag_transforms=self.ts.lag_transforms,\n",
    "                date_features=self.ts.date_features, \n",
    "                num_threads=self.ts.num_threads,\n",
    "                target_transforms=self.ts.target_transforms,\n",
    "            )\n",
    "            new_ts._fit(\n",
    "                new_data,\n",
    "                id_col=self.ts.id_col,\n",
    "                time_col=self.ts.time_col,\n",
    "                target_col=self.ts.target_col, \n",
    "                static_features=self.ts.static_features,\n",
    "                keep_last_n=self.ts.keep_last_n,\n",
    "            )\n",
    "            new_ts.max_horizon = self.ts.max_horizon\n",
    "            ts = new_ts\n",
    "        else:\n",
    "            ts = self.ts\n",
    "            \n",
    "        forecasts = ts.predict(\n",
    "            self.models_, horizon, dynamic_dfs, before_predict_callback, after_predict_callback\n",
    "        )\n",
    "        if level is not None:\n",
    "            if self._cs_df is None:\n",
    "                warn_msg = (\n",
    "                    'Please rerun the `fit` method passing a proper value '\n",
    "                    'to prediction intervals to compute them.'\n",
    "                )\n",
    "                warnings.warn(warn_msg, UserWarning)\n",
    "            else:\n",
    "                if (self.prediction_intervals.window_size != 1) and (self.prediction_intervals.window_size < horizon):\n",
    "                    raise ValueError(\n",
    "                        'The `window_size` argument of PredictionIntervals '\n",
    "                        'should be equal to one or greater or equal to `horizon`. '\n",
    "                        'Please rerun the `fit` method passing a proper value '\n",
    "                        'to prediction intervals.'\n",
    "                    )\n",
    "                if self.prediction_intervals.window_size == 1 and horizon > 1:\n",
    "                    warn_msg = (\n",
    "                        'Prediction intervals are calculated using 1-step ahead cross-validation, '\n",
    "                        'with a constant width for all horizons. To vary the error by horizon, '\n",
    "                        'pass PredictionIntervals(window_size=horizon) to the `prediction_intervals` '\n",
    "                        'argument when refitting the model.'\n",
    "                    )\n",
    "                    warnings.warn(warn_msg, UserWarning)\n",
    "                level_ = sorted(level)\n",
    "                model_names = self.models.keys()\n",
    "                conformal_method = _get_conformal_method(self.prediction_intervals.method)\n",
    "                forecasts = conformal_method(\n",
    "                    forecasts,\n",
    "                    self._cs_df,\n",
    "                    model_names=list(model_names),\n",
    "                    level=level_,\n",
    "                    cs_window_size=self.prediction_intervals.window_size,\n",
    "                    cs_n_windows=self.prediction_intervals.n_windows,\n",
    "                    n_series=self.ts.ga.ngroups,\n",
    "                    horizon=horizon,\n",
    "                )\n",
    "        return forecasts\n",
    "\n",
    "    def cross_validation(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        n_windows: int,\n",
    "        window_size: int,\n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        target_col: str = 'y',\n",
    "        step_size: Optional[int] = None,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "        refit: bool = True,\n",
    "        max_horizon: Optional[int] = None,\n",
    "        before_predict_callback: Optional[Callable] = None,\n",
    "        after_predict_callback: Optional[Callable] = None,\n",
    "        prediction_intervals: Optional[PredictionIntervals] = None,\n",
    "        level: Optional[List[Union[int, float]]] = None,\n",
    "        input_size: Optional[int] = None,\n",
    "        fitted: bool = False,\n",
    "    ):\n",
    "        \"\"\"Perform time series cross validation.\n",
    "        Creates `n_windows` splits where each window has `window_size` test periods, \n",
    "        trains the models, computes the predictions and merges the actuals.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data : pandas DataFrame\n",
    "            Series data in long format.\n",
    "        n_windows : int\n",
    "            Number of windows to evaluate.\n",
    "        window_size : int\n",
    "            Number of test periods in each window.\n",
    "        id_col : str (default='unique_id')\n",
    "            Column that identifies each serie.\n",
    "        time_col : str (default='ds')\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str (default='y')\n",
    "            Column that contains the target.\n",
    "        step_size : int, optional (default=None)\n",
    "            Step size between each cross validation window. If None it will be equal to `window_size`.\n",
    "        static_features : list of str, optional (default=None)\n",
    "            Names of the features that are static and will be repeated when forecasting.\n",
    "        dropna : bool (default=True)\n",
    "            Drop rows with missing values produced by the transformations.\n",
    "        keep_last_n : int, optional (default=None)\n",
    "            Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it.\n",
    "        max_horizon: int, optional (default=None)\n",
    "            Train this many models, where each model will predict a specific horizon.            \n",
    "        refit : bool (default=True)\n",
    "            Retrain model for each cross validation window.\n",
    "            If False, the models are trained at the beginning and then used to predict each window.\n",
    "        before_predict_callback : callable, optional (default=None)\n",
    "            Function to call on the features before computing the predictions.\n",
    "                This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure.\n",
    "                The series identifier is on the index.\n",
    "        after_predict_callback : callable, optional (default=None)\n",
    "            Function to call on the predictions before updating the targets.\n",
    "                This function will take a pandas Series with the predictions and should return another one with the same structure.\n",
    "                The series identifier is on the index.\n",
    "        prediction_intervals : PredictionIntervals, optional (default=None)\n",
    "            Configuration to calibrate prediction intervals (Conformal Prediction).\n",
    "        level : list of ints or floats, optional (default=None)\n",
    "            Confidence levels between 0 and 100 for prediction intervals.\n",
    "        input_size : int, optional (default=None)\n",
    "            Maximum training samples per serie in each window. If None, will use an expanding window.\n",
    "        fitted : bool (default=False)\n",
    "            Store the in-sample predictions.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        result : pandas DataFrame\n",
    "            Predictions for each window with the series id, timestamp, last train date, target value and predictions from each model.\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'models_'):\n",
    "            warnings.warn('Excuting `cross_validation` after `fit` can produce unexpected errors')\n",
    "        results = []\n",
    "        self.cv_models_ = []\n",
    "        if np.issubdtype(data[time_col].dtype.type, np.integer):\n",
    "            freq = 1\n",
    "        else:\n",
    "            freq = self.freq\n",
    "            \n",
    "        splits = backtest_splits(\n",
    "            data,\n",
    "            n_windows=n_windows,\n",
    "            window_size=window_size,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            freq=freq,\n",
    "            step_size=step_size,\n",
    "            input_size=input_size,\n",
    "        )\n",
    "        ex_cols_to_drop = [id_col, time_col, target_col]\n",
    "        if static_features is not None:\n",
    "            ex_cols_to_drop.extend(static_features)\n",
    "        has_ex = not data.columns.drop(ex_cols_to_drop).empty\n",
    "        self.cv_fitted_values_ = []\n",
    "        for i_window, (cutoffs, train, valid) in enumerate(splits):\n",
    "            if refit or i_window == 0:\n",
    "                self.fit(\n",
    "                    train,\n",
    "                    id_col=id_col,\n",
    "                    time_col=time_col,\n",
    "                    target_col=target_col,\n",
    "                    static_features=static_features,\n",
    "                    dropna=dropna,\n",
    "                    keep_last_n=keep_last_n,\n",
    "                    max_horizon=max_horizon,\n",
    "                    prediction_intervals=prediction_intervals,\n",
    "                )\n",
    "            self.cv_models_.append(self.models_)\n",
    "            if fitted:\n",
    "                insample_results = train[[id_col, time_col]].copy()\n",
    "                trainX, _ = self.preprocess(\n",
    "                    train,\n",
    "                    id_col=id_col,\n",
    "                    time_col=time_col,\n",
    "                    target_col=target_col,\n",
    "                    static_features=static_features,\n",
    "                    dropna=False,\n",
    "                    keep_last_n=keep_last_n,\n",
    "                    max_horizon=max_horizon,\n",
    "                    return_X_y=True,\n",
    "                )\n",
    "                trainX = trainX[self.ts.features_order_]\n",
    "                for name, model in self.models_.items():\n",
    "                    insample_results[name] = model.predict(trainX)  # type: ignore[union-attr]\n",
    "                if self.ts.target_transforms is not None:\n",
    "                    for tfm in self.ts.target_transforms[::-1]:\n",
    "                        insample_results = tfm.inverse_transform(insample_results)\n",
    "                insample_results['fold'] = i_window                        \n",
    "                insample_results[target_col] = train[target_col].values\n",
    "                self.cv_fitted_values_.append(insample_results)\n",
    "            dynamic_dfs = [valid.drop(columns=[target_col])] if has_ex else None\n",
    "            y_pred = self.predict(\n",
    "                window_size,\n",
    "                dynamic_dfs,\n",
    "                before_predict_callback,\n",
    "                after_predict_callback,\n",
    "                new_data=train if not refit else None,\n",
    "                level=level,\n",
    "            )\n",
    "            y_pred = y_pred.merge(cutoffs, on=id_col, how='left')\n",
    "            result = valid[[id_col, time_col, target_col]].merge(y_pred, on=[id_col, time_col])\n",
    "            if result.shape[0] < valid.shape[0]:\n",
    "                raise ValueError(\n",
    "                    \"Cross validation result produced less results than expected. \"\n",
    "                    \"Please verify that the frequency set on the MLForecast constructor matches your series' \"\n",
    "                    \"and that there aren't any missing periods.\"\n",
    "                )\n",
    "            results.append(result)\n",
    "        out = pd.concat(results)\n",
    "        cols_order = [id_col, time_col, 'cutoff', target_col]\n",
    "        return out[cols_order + out.columns.drop(cols_order).tolist()]\n",
    "    \n",
    "    def cross_validation_fitted_values(self):\n",
    "        if not getattr(self, 'cv_fitted_values_', []):\n",
    "            raise ValueError('Please run cross_validation with fitted=True first.')\n",
    "        cols_order = [self.ts.id_col, self.ts.time_col, 'fold', self.ts.target_col]\n",
    "        out = pd.concat(self.cv_fitted_values_).reset_index(drop=True)\n",
    "        return out[cols_order + out.columns.drop(cols_order).tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02b15e1-5ec8-493f-983b-7b6eedf6f9a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### MLForecast\n",
       "\n",
       ">      MLForecast (models:Union[sklearn.base.BaseEstimator,List[sklearn.base.Bas\n",
       ">                  eEstimator],Dict[str,sklearn.base.BaseEstimator]], freq:Union\n",
       ">                  [int,str,pandas._libs.tslibs.offsets.BaseOffset,NoneType]=Non\n",
       ">                  e, lags:Optional[Iterable[int]]=None, lag_transforms:Optional\n",
       ">                  [Dict[int,List[Union[Callable,Tuple[Callable,Any]]]]]=None,\n",
       ">                  date_features:Optional[Iterable[Union[str,Callable]]]=None,\n",
       ">                  differences:Optional[Iterable[int]]=None, num_threads:int=1, \n",
       ">                  target_transforms:Optional[List[mlforecast.target_transforms.\n",
       ">                  BaseTargetTransform]]=None)\n",
       "\n",
       "Create forecast object\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| models | Union |  | Models that will be trained and used to compute the forecasts. |\n",
       "| freq | Union | None | Pandas offset, pandas offset alias, e.g. 'D', 'W-THU' or integer denoting the frequency of the series. |\n",
       "| lags | Optional | None | Lags of the target to use as features. |\n",
       "| lag_transforms | Optional | None | Mapping of target lags to their transformations. |\n",
       "| date_features | Optional | None | Features computed from the dates. Can be pandas date attributes or functions that will take the dates as input. |\n",
       "| differences | Optional | None | Differences to take of the target before computing the features. These are restored at the forecasting step. |\n",
       "| num_threads | int | 1 | Number of threads to use when computing the features. |\n",
       "| target_transforms | Optional | None | Transformations that will be applied to the target before computing the features and restored after the forecasting step. |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### MLForecast\n",
       "\n",
       ">      MLForecast (models:Union[sklearn.base.BaseEstimator,List[sklearn.base.Bas\n",
       ">                  eEstimator],Dict[str,sklearn.base.BaseEstimator]], freq:Union\n",
       ">                  [int,str,pandas._libs.tslibs.offsets.BaseOffset,NoneType]=Non\n",
       ">                  e, lags:Optional[Iterable[int]]=None, lag_transforms:Optional\n",
       ">                  [Dict[int,List[Union[Callable,Tuple[Callable,Any]]]]]=None,\n",
       ">                  date_features:Optional[Iterable[Union[str,Callable]]]=None,\n",
       ">                  differences:Optional[Iterable[int]]=None, num_threads:int=1, \n",
       ">                  target_transforms:Optional[List[mlforecast.target_transforms.\n",
       ">                  BaseTargetTransform]]=None)\n",
       "\n",
       "Create forecast object\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| models | Union |  | Models that will be trained and used to compute the forecasts. |\n",
       "| freq | Union | None | Pandas offset, pandas offset alias, e.g. 'D', 'W-THU' or integer denoting the frequency of the series. |\n",
       "| lags | Optional | None | Lags of the target to use as features. |\n",
       "| lag_transforms | Optional | None | Mapping of target lags to their transformations. |\n",
       "| date_features | Optional | None | Features computed from the dates. Can be pandas date attributes or functions that will take the dates as input. |\n",
       "| differences | Optional | None | Differences to take of the target before computing the features. These are restored at the forecasting step. |\n",
       "| num_threads | int | 1 | Number of threads to use when computing the features. |\n",
       "| target_transforms | Optional | None | Transformations that will be applied to the target before computing the features and restored after the forecasting step. |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MLForecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c7c92c-2ba9-4885-bbc1-8e4a4f67f0d9",
   "metadata": {},
   "source": [
    "The `MLForecast` class is a high level abstraction that encapsulates all the steps in the pipeline (preprocessing, fitting the model and computing the predictions). It tries to mimic the scikit-learn API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5ec811-8876-4daa-84a2-2ebe0559a02b",
   "metadata": {},
   "source": [
    "## Example\n",
    "This shows an example with just 4 series of the M4 dataset. If you want to run it yourself on all of them, you can refer to [this notebook](https://www.kaggle.com/code/lemuz90/m4-competition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3032775-f610-4091-a750-73219d904c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from datasetsforecast.m4 import M4, M4Info\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from window_ops.ewm import ewm_mean\n",
    "from window_ops.expanding import expanding_mean\n",
    "from window_ops.rolling import rolling_mean\n",
    "\n",
    "from mlforecast.lgb_cv import LightGBMCV\n",
    "from mlforecast.target_transforms import Differences\n",
    "from mlforecast.utils import generate_daily_series, generate_prices_for_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c30ee9-38f3-4ef2-ab50-6ffc1e887f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>86796</th>\n",
       "      <td>H196</td>\n",
       "      <td>1</td>\n",
       "      <td>11.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86797</th>\n",
       "      <td>H196</td>\n",
       "      <td>2</td>\n",
       "      <td>11.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86798</th>\n",
       "      <td>H196</td>\n",
       "      <td>3</td>\n",
       "      <td>11.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86799</th>\n",
       "      <td>H196</td>\n",
       "      <td>4</td>\n",
       "      <td>10.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86800</th>\n",
       "      <td>H196</td>\n",
       "      <td>5</td>\n",
       "      <td>10.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325235</th>\n",
       "      <td>H413</td>\n",
       "      <td>1004</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325236</th>\n",
       "      <td>H413</td>\n",
       "      <td>1005</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325237</th>\n",
       "      <td>H413</td>\n",
       "      <td>1006</td>\n",
       "      <td>47.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325238</th>\n",
       "      <td>H413</td>\n",
       "      <td>1007</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325239</th>\n",
       "      <td>H413</td>\n",
       "      <td>1008</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4032 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       unique_id    ds     y\n",
       "86796       H196     1  11.8\n",
       "86797       H196     2  11.4\n",
       "86798       H196     3  11.1\n",
       "86799       H196     4  10.8\n",
       "86800       H196     5  10.6\n",
       "...          ...   ...   ...\n",
       "325235      H413  1004  99.0\n",
       "325236      H413  1005  88.0\n",
       "325237      H413  1006  47.0\n",
       "325238      H413  1007  41.0\n",
       "325239      H413  1008  34.0\n",
       "\n",
       "[4032 rows x 3 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group = 'Hourly'\n",
    "await M4.async_download('data', group=group)\n",
    "df, *_ = M4.load(directory='data', group=group)\n",
    "df['ds'] = df['ds'].astype('int')\n",
    "ids = df['unique_id'].unique()\n",
    "random.seed(0)\n",
    "sample_ids = random.choices(ids, k=4)\n",
    "sample_df = df[df['unique_id'].isin(sample_ids)]\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaa4e47-4eee-43b6-8dba-1807bb00fda5",
   "metadata": {},
   "source": [
    "We now split this data into train and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91485631-d855-47f9-bdb4-94130b23c67a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3840, 3), (192, 3))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info = M4Info[group]\n",
    "horizon = info.horizon\n",
    "valid = sample_df.groupby('unique_id').tail(horizon)\n",
    "train = sample_df.drop(valid.index)\n",
    "train.shape, valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c392f40-f181-4666-876e-1886e86eda5f",
   "metadata": {},
   "source": [
    "### Creating the Forecast object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc50c95b-359d-4e49-8f1f-40dab560c722",
   "metadata": {},
   "source": [
    "The forecast object encapsulates the feature engineering + training the models + forecasting. When we initialize it we define:\n",
    "\n",
    "* The models we want to train\n",
    "* The series frequency. This is added to the last dates seen in train for the forecast step, if the time column contains integer values we can leave it empty or set it to 1.\n",
    "* The feature engineering:\n",
    "    * Lags to use as features\n",
    "    * Transformations on the lags\n",
    "    * Date features\n",
    "    * Differences to apply to the target before computing the features, which are then restored when forecasting.\n",
    "* Number of threads to use when computing the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ebf59f-0fb7-45ff-b355-127952413245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLForecast(models=[LGBMRegressor], freq=1, lag_features=['lag24', 'lag48', 'lag72', 'lag96', 'lag120', 'lag144', 'lag168', 'ewm_mean_lag48_alpha0.3'], date_features=[], num_threads=1)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcst = MLForecast(\n",
    "    models=lgb.LGBMRegressor(random_state=0),\n",
    "    lags=[24 * (i+1) for i in range(7)],\n",
    "    lag_transforms={\n",
    "        48: [(ewm_mean, 0.3)],\n",
    "    },\n",
    "    num_threads=1,\n",
    "    target_transforms=[Differences([24])],\n",
    ")\n",
    "fcst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10229c7-a69e-48fe-9ce8-f4754c58bec1",
   "metadata": {},
   "source": [
    "Once we have this setup we can compute the features and fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d42544-91a7-4c08-a190-925343e3c0b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### MLForecast.fit\n",
       "\n",
       ">      MLForecast.fit (data:pandas.core.frame.DataFrame, id_col:str='unique_id',\n",
       ">                      time_col:str='ds', target_col:str='y',\n",
       ">                      static_features:Optional[List[str]]=None,\n",
       ">                      dropna:bool=True, keep_last_n:Optional[int]=None,\n",
       ">                      max_horizon:Optional[int]=None, prediction_intervals:Opti\n",
       ">                      onal[mlforecast.utils.PredictionIntervals]=None)\n",
       "\n",
       "Apply the feature engineering and train the models.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| data | DataFrame |  | Series data in long format. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| time_col | str | ds | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str | y | Column that contains the target. |\n",
       "| static_features | Optional | None | Names of the features that are static and will be repeated when forecasting. |\n",
       "| dropna | bool | True | Drop rows with missing values produced by the transformations. |\n",
       "| keep_last_n | Optional | None | Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it. |\n",
       "| max_horizon | Optional | None |  |\n",
       "| prediction_intervals | Optional | None | Configuration to calibrate prediction intervals (Conformal Prediction). |\n",
       "| **Returns** | **MLForecast** |  | **Forecast object with series values and trained models.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### MLForecast.fit\n",
       "\n",
       ">      MLForecast.fit (data:pandas.core.frame.DataFrame, id_col:str='unique_id',\n",
       ">                      time_col:str='ds', target_col:str='y',\n",
       ">                      static_features:Optional[List[str]]=None,\n",
       ">                      dropna:bool=True, keep_last_n:Optional[int]=None,\n",
       ">                      max_horizon:Optional[int]=None, prediction_intervals:Opti\n",
       ">                      onal[mlforecast.utils.PredictionIntervals]=None)\n",
       "\n",
       "Apply the feature engineering and train the models.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| data | DataFrame |  | Series data in long format. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| time_col | str | ds | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str | y | Column that contains the target. |\n",
       "| static_features | Optional | None | Names of the features that are static and will be repeated when forecasting. |\n",
       "| dropna | bool | True | Drop rows with missing values produced by the transformations. |\n",
       "| keep_last_n | Optional | None | Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it. |\n",
       "| max_horizon | Optional | None |  |\n",
       "| prediction_intervals | Optional | None | Configuration to calibrate prediction intervals (Conformal Prediction). |\n",
       "| **Returns** | **MLForecast** |  | **Forecast object with series values and trained models.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MLForecast.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53041a71-5d34-4c61-b28d-dcd0ccc3833b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst.fit(train);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5794fec5-fcc4-48c4-8f35-8b5add89a698",
   "metadata": {},
   "source": [
    "Once we've run this we're ready to compute our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121e73f8-b88b-4c82-967e-9629587d7616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### MLForecast.predict\n",
       "\n",
       ">      MLForecast.predict (horizon:int,\n",
       ">                          dynamic_dfs:Optional[List[pandas.core.frame.DataFrame\n",
       ">                          ]]=None,\n",
       ">                          before_predict_callback:Optional[Callable]=None,\n",
       ">                          after_predict_callback:Optional[Callable]=None,\n",
       ">                          new_data:Optional[pandas.core.frame.DataFrame]=None,\n",
       ">                          level:Optional[List[Union[int,float]]]=None)\n",
       "\n",
       "Compute the predictions for the next `horizon` steps.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| horizon | int |  | Number of periods to predict. |\n",
       "| dynamic_dfs | Optional | None | Future values of the dynamic features, e.g. prices. |\n",
       "| before_predict_callback | Optional | None | Function to call on the features before computing the predictions.<br>    This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure.<br>    The series identifier is on the index. |\n",
       "| after_predict_callback | Optional | None | Function to call on the predictions before updating the targets.<br>    This function will take a pandas Series with the predictions and should return another one with the same structure.<br>    The series identifier is on the index.    |\n",
       "| new_data | Optional | None | Series data of new observations for which forecasts are to be generated. <br>    This dataframe should have the same structure as the one used to fit the model, including any features and time series data. <br>    If `new_data` is not None, the method will generate forecasts for the new observations. |\n",
       "| level | Optional | None | Confidence levels between 0 and 100 for prediction intervals. |\n",
       "| **Returns** | **DataFrame** |  | **Predictions for each serie and timestep, with one column per model.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### MLForecast.predict\n",
       "\n",
       ">      MLForecast.predict (horizon:int,\n",
       ">                          dynamic_dfs:Optional[List[pandas.core.frame.DataFrame\n",
       ">                          ]]=None,\n",
       ">                          before_predict_callback:Optional[Callable]=None,\n",
       ">                          after_predict_callback:Optional[Callable]=None,\n",
       ">                          new_data:Optional[pandas.core.frame.DataFrame]=None,\n",
       ">                          level:Optional[List[Union[int,float]]]=None)\n",
       "\n",
       "Compute the predictions for the next `horizon` steps.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| horizon | int |  | Number of periods to predict. |\n",
       "| dynamic_dfs | Optional | None | Future values of the dynamic features, e.g. prices. |\n",
       "| before_predict_callback | Optional | None | Function to call on the features before computing the predictions.<br>    This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure.<br>    The series identifier is on the index. |\n",
       "| after_predict_callback | Optional | None | Function to call on the predictions before updating the targets.<br>    This function will take a pandas Series with the predictions and should return another one with the same structure.<br>    The series identifier is on the index.    |\n",
       "| new_data | Optional | None | Series data of new observations for which forecasts are to be generated. <br>    This dataframe should have the same structure as the one used to fit the model, including any features and time series data. <br>    If `new_data` is not None, the method will generate forecasts for the new observations. |\n",
       "| level | Optional | None | Confidence levels between 0 and 100 for prediction intervals. |\n",
       "| **Returns** | **DataFrame** |  | **Predictions for each serie and timestep, with one column per model.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MLForecast.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c28426-e150-4fe8-b1f9-bcad401cbbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = fcst.predict(horizon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6cd880-ec12-46dc-9627-90d8521f9c38",
   "metadata": {},
   "source": [
    "We can see at a couple of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223287b5-70ec-419a-a28a-bb13f6fa005e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = valid.merge(predictions, on=['unique_id', 'ds']).set_index('unique_id')\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(16, 10))\n",
    "for uid, axi in zip(sample_ids, ax.flat):\n",
    "    results.loc[uid].set_index('ds').plot(ax=axi, title=uid)\n",
    "fig.savefig('figs/forecast__predict.png', bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a31d2b4-b9e5-427b-9cfa-30a1733969f1",
   "metadata": {},
   "source": [
    "![](figs/forecast__predict.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20724d12-d6b4-4c21-a38d-fd4c71461c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test new_data argument\n",
    "pd.testing.assert_frame_equal(\n",
    "    fcst.predict(horizon, new_data=train),\n",
    "    predictions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfb0f7f-ea89-44a6-b557-ac12a278f111",
   "metadata": {},
   "source": [
    "#### Prediction intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b367dee8-fab4-406a-8a9e-c4ec86003a7d",
   "metadata": {},
   "source": [
    "With `MLForecast`, you can generate prediction intervals using Conformal Prediction. To configure Conformal Prediction, you need to pass an instance of the `PredictionIntervals` class to the `prediction_intervals` argument of the `fit` method. The class takes three parameters: `n_windows`, `window_size` and `method`. `n_windows` represents the number of cross-validation windows used to calibrate the intervals, `window_size` is the forecast horizon, and `method` can be `conformal_distribution` or `conformal_error`; `conformal_distribution` (default) creates forecasts paths based on the cross-validation errors and calculate quantiles using those paths, on the other hand `conformal_error` calculates the error quantiles to produce prediction intervals. The strategy will adjust the intervals for each horizon step, resulting in different widths for each step. Please note that a minimum of 2 cross-validation windows must be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cd3346-f1e5-4ccb-b39c-a257d45d51fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst.fit(\n",
    "    train, \n",
    "    prediction_intervals=PredictionIntervals(n_windows=3, window_size=48)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2273a783-1596-4d5e-b084-9252eb6c23af",
   "metadata": {},
   "source": [
    "After that, you just have to include your desired confidence levels to the `predict` method using the `level` argument. Levels must lie between 0 and 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02793e5f-cb32-4c61-8417-a89a531e1ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_w_intervals = fcst.predict(48, level=[50, 80, 95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba4b53a-c49a-4846-aafb-a604a3b158e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>LGBMRegressor</th>\n",
       "      <th>LGBMRegressor-lo-95</th>\n",
       "      <th>LGBMRegressor-lo-80</th>\n",
       "      <th>LGBMRegressor-lo-50</th>\n",
       "      <th>LGBMRegressor-hi-50</th>\n",
       "      <th>LGBMRegressor-hi-80</th>\n",
       "      <th>LGBMRegressor-hi-95</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H196</td>\n",
       "      <td>961</td>\n",
       "      <td>16.071271</td>\n",
       "      <td>15.958042</td>\n",
       "      <td>15.971271</td>\n",
       "      <td>16.005091</td>\n",
       "      <td>16.137452</td>\n",
       "      <td>16.171271</td>\n",
       "      <td>16.184501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H196</td>\n",
       "      <td>962</td>\n",
       "      <td>15.671271</td>\n",
       "      <td>15.553632</td>\n",
       "      <td>15.553632</td>\n",
       "      <td>15.578632</td>\n",
       "      <td>15.763911</td>\n",
       "      <td>15.788911</td>\n",
       "      <td>15.788911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H196</td>\n",
       "      <td>963</td>\n",
       "      <td>15.271271</td>\n",
       "      <td>15.153632</td>\n",
       "      <td>15.153632</td>\n",
       "      <td>15.162452</td>\n",
       "      <td>15.380091</td>\n",
       "      <td>15.388911</td>\n",
       "      <td>15.388911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H196</td>\n",
       "      <td>964</td>\n",
       "      <td>14.971271</td>\n",
       "      <td>14.858042</td>\n",
       "      <td>14.871271</td>\n",
       "      <td>14.905091</td>\n",
       "      <td>15.037452</td>\n",
       "      <td>15.071271</td>\n",
       "      <td>15.084501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H196</td>\n",
       "      <td>965</td>\n",
       "      <td>14.671271</td>\n",
       "      <td>14.553632</td>\n",
       "      <td>14.553632</td>\n",
       "      <td>14.562452</td>\n",
       "      <td>14.780091</td>\n",
       "      <td>14.788911</td>\n",
       "      <td>14.788911</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  unique_id   ds  LGBMRegressor  LGBMRegressor-lo-95  LGBMRegressor-lo-80  \\\n",
       "0      H196  961      16.071271            15.958042            15.971271   \n",
       "1      H196  962      15.671271            15.553632            15.553632   \n",
       "2      H196  963      15.271271            15.153632            15.153632   \n",
       "3      H196  964      14.971271            14.858042            14.871271   \n",
       "4      H196  965      14.671271            14.553632            14.553632   \n",
       "\n",
       "   LGBMRegressor-lo-50  LGBMRegressor-hi-50  LGBMRegressor-hi-80  \\\n",
       "0            16.005091            16.137452            16.171271   \n",
       "1            15.578632            15.763911            15.788911   \n",
       "2            15.162452            15.380091            15.388911   \n",
       "3            14.905091            15.037452            15.071271   \n",
       "4            14.562452            14.780091            14.788911   \n",
       "\n",
       "   LGBMRegressor-hi-95  \n",
       "0            16.184501  \n",
       "1            15.788911  \n",
       "2            15.388911  \n",
       "3            15.084501  \n",
       "4            14.788911  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_w_intervals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1381372c-3571-4db9-a76e-355d7e0ef118",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| head\n",
    "# test we can forecast horizon lower than window_size \n",
    "# with prediction intervals\n",
    "for method in ['conformal_distribution', 'conformal_errors']:\n",
    "    fcst.fit(\n",
    "        train, \n",
    "        prediction_intervals=PredictionIntervals(n_windows=3, window_size=48)\n",
    "    )\n",
    "\n",
    "    preds_h_lower_window_size = fcst.predict(1, level=[50, 80, 95])\n",
    "    preds_h_lower_window_size = fcst.predict(30, level=[50, 80, 95])\n",
    "\n",
    "    # test monotonicity of intervals\n",
    "    test_eq(\n",
    "        preds_h_lower_window_size.filter(regex='lo|hi').apply(\n",
    "            lambda x: x.is_monotonic_increasing,\n",
    "            axis=1\n",
    "        ).sum(),\n",
    "        len(preds_h_lower_window_size)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef791c7d-aa1f-423b-a362-22919cac778a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_fail(lambda: fcst.predict(49, level=[68]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04359aaa-ac32-4fe6-ae30-e83df75bb4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test we can recover point forecasts\n",
    "test_eq(\n",
    "    predictions,\n",
    "    predictions_w_intervals[predictions.columns]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0772c6f9-469e-4ce2-9e41-0e027edb4fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test we can recover mean forecasts with level 0\n",
    "test_close(\n",
    "    predictions['LGBMRegressor'].values,\n",
    "    fcst.predict(48, level=[0])['LGBMRegressor-lo-0'].values,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b94e8e-395f-4435-a03c-4749c1bea41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test monotonicity of intervals\n",
    "test_eq(\n",
    "    predictions_w_intervals.filter(regex='lo|hi').apply(\n",
    "        lambda x: x.is_monotonic_increasing,\n",
    "        axis=1\n",
    "    ).sum(),\n",
    "    len(predictions_w_intervals)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117a4080-fa17-4b13-a0b6-1d1afe70dfb8",
   "metadata": {},
   "source": [
    "Let's explore the generated intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0934d461-7b9c-4646-a614-f538df2752b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = valid.merge(predictions_w_intervals, on=['unique_id', 'ds']).set_index('unique_id')\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(16, 10))\n",
    "for uid, axi in zip(sample_ids, ax.flat):\n",
    "    uid_results = results.loc[uid].set_index('ds')\n",
    "    uid_results[['y', 'LGBMRegressor']].plot(ax=axi, title=uid)\n",
    "    for lv in [50, 80, 95]:\n",
    "        axi.fill_between(\n",
    "            uid_results.index, \n",
    "            uid_results[f'LGBMRegressor-lo-{lv}'].values, \n",
    "            uid_results[f'LGBMRegressor-hi-{lv}'].values,\n",
    "            label=f'LGBMRegressor-level-{lv}',\n",
    "            color='orange',\n",
    "            alpha=1 - lv / 100\n",
    "        )\n",
    "    axi.legend()\n",
    "fig.savefig('figs/forecast__predict_intervals.png', bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058a8cf6-5b32-464b-b6f9-60905eb82781",
   "metadata": {},
   "source": [
    "![](figs/forecast__predict_intervals.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6686093-f139-4241-8f7b-3a8bd0f2b47c",
   "metadata": {},
   "source": [
    "If you want to reduce the computational time and produce intervals with the same width for the whole forecast horizon, simple pass `window_size=1` to the `PredictionIntervals` class. The caveat of this strategy is that in some cases, variance of the absolute residuals maybe be small (even zero), so the intervals may be too narrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa76f77e-7341-4fc7-af11-80c3531d304b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst.fit(\n",
    "    train,  \n",
    "    prediction_intervals=PredictionIntervals(n_windows=3, window_size=1)\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ec6367-0fa3-4e40-b771-0b7182813cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_w_intervals_ws_1 = fcst.predict(48, level=[80, 90, 95])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d754fa-a20c-4cd2-bf6d-6274cd9d126d",
   "metadata": {},
   "source": [
    "Let's explore the generated intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c8e1b3-04bc-4457-a714-dd5dbb06efb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = valid.merge(predictions_w_intervals_ws_1, on=['unique_id', 'ds']).set_index('unique_id')\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(16, 10))\n",
    "for uid, axi in zip(sample_ids, ax.flat):\n",
    "    uid_results = results.loc[uid].set_index('ds')\n",
    "    uid_results[['y', 'LGBMRegressor']].plot(ax=axi, title=uid)\n",
    "    axi.fill_between(\n",
    "        uid_results.index, \n",
    "        uid_results['LGBMRegressor-lo-90'].values, \n",
    "        uid_results['LGBMRegressor-hi-90'].values,\n",
    "        label='LGBMRegressor-level-90',\n",
    "        color='orange',\n",
    "        alpha=0.2\n",
    "    )\n",
    "    axi.legend()\n",
    "fig.savefig('figs/forecast__predict_intervals_window_size_1.png', bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511f7426-dfa8-4988-80d3-6092e65211cb",
   "metadata": {},
   "source": [
    "![](figs/forecast__predict_intervals_window_size_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73473351-3386-4959-9a41-4ce4ed64354f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test indexed data, datetime ds\n",
    "fcst_test = MLForecast(\n",
    "    models=lgb.LGBMRegressor(random_state=0),\n",
    "    lags=[1],\n",
    "    num_threads=1,\n",
    "    freq='D'\n",
    ")\n",
    "df_test = generate_daily_series(1)\n",
    "fcst_test.fit(\n",
    "    df_test,\n",
    "    prediction_intervals=PredictionIntervals()\n",
    ")\n",
    "pred_test = fcst_test.predict(12)\n",
    "pred_int_test = fcst_test.predict(12, level=[80, 90])\n",
    "# test same structure\n",
    "test_eq(\n",
    "    pred_test,\n",
    "    pred_int_test[pred_test.columns]\n",
    ")\n",
    "# test monotonicity of intervals\n",
    "test_eq(\n",
    "    pred_int_test.filter(regex='lo|hi').apply(\n",
    "        lambda x: x.is_monotonic_increasing,\n",
    "        axis=1\n",
    "    ).sum(),\n",
    "    len(pred_int_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505aadfd-076d-42e6-8103-aeef4eef61b7",
   "metadata": {},
   "source": [
    "#### Forecast using a pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1bef71-d893-46ee-82dd-6819193f2d0e",
   "metadata": {},
   "source": [
    "MLForecast allows you to use a pretrained model to generate forecasts for a new dataset. Simply provide a pandas dataframe containing the new observations as the value for the `new_data` argument when calling the `predict` method. The dataframe should have the same structure as the one used to fit the model, including any features and time series data. The function will then use the pretrained model to generate forecasts for the new observations. This allows you to easily apply a pretrained model to a new dataset and generate forecasts without the need to retrain the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ade99c5-82f5-4047-bea6-2698f27872db",
   "metadata": {},
   "outputs": [],
   "source": [
    "ercot_df = pd.read_csv('https://datasets-nixtla.s3.amazonaws.com/ERCOT-clean.csv')\n",
    "# we have to convert the ds column to integers\n",
    "# since MLForecast was trained with that structure\n",
    "ercot_df['ds'] = np.arange(1, len(ercot_df) + 1)\n",
    "# use the `new_data` argument to pass the ercot dataset \n",
    "ercot_fcsts = fcst.predict(horizon, new_data=ercot_df)\n",
    "fig, ax = plt.subplots()\n",
    "ercot_df.tail(48 * 2).plot(x='ds', y='y', figsize=(20, 7), ax=ax)\n",
    "ercot_fcsts.plot(x='ds', y='LGBMRegressor', ax=ax, title='ERCOT forecasts trained on M4-Hourly dataset');\n",
    "plt.gcf().savefig('figs/forecast__ercot.png', bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60c5bab-453b-4ae8-a282-87233c4333e3",
   "metadata": {},
   "source": [
    "![](figs/forecast__ercot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b150efe0-bc84-483e-905f-c936e0aa0818",
   "metadata": {},
   "source": [
    "If you want to take a look at the data that will be used to train the models you can call `Forecast.preprocess`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21260f3-bd8f-4143-bb60-deb9ca0d46e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### MLForecast.preprocess\n",
       "\n",
       ">      MLForecast.preprocess (data:pandas.core.frame.DataFrame,\n",
       ">                             id_col:str='unique_id', time_col:str='ds',\n",
       ">                             target_col:str='y',\n",
       ">                             static_features:Optional[List[str]]=None,\n",
       ">                             dropna:bool=True, keep_last_n:Optional[int]=None,\n",
       ">                             max_horizon:Optional[int]=None,\n",
       ">                             return_X_y:bool=False)\n",
       "\n",
       "Add the features to `data`.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| data | DataFrame |  | Series data in long format. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| time_col | str | ds | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str | y | Column that contains the target. |\n",
       "| static_features | Optional | None | Names of the features that are static and will be repeated when forecasting. |\n",
       "| dropna | bool | True | Drop rows with missing values produced by the transformations. |\n",
       "| keep_last_n | Optional | None | Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it. |\n",
       "| max_horizon | Optional | None |  |\n",
       "| return_X_y | bool | False |  |\n",
       "| **Returns** | **Union** |  | **`data` plus added features and target(s).** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### MLForecast.preprocess\n",
       "\n",
       ">      MLForecast.preprocess (data:pandas.core.frame.DataFrame,\n",
       ">                             id_col:str='unique_id', time_col:str='ds',\n",
       ">                             target_col:str='y',\n",
       ">                             static_features:Optional[List[str]]=None,\n",
       ">                             dropna:bool=True, keep_last_n:Optional[int]=None,\n",
       ">                             max_horizon:Optional[int]=None,\n",
       ">                             return_X_y:bool=False)\n",
       "\n",
       "Add the features to `data`.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| data | DataFrame |  | Series data in long format. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| time_col | str | ds | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str | y | Column that contains the target. |\n",
       "| static_features | Optional | None | Names of the features that are static and will be repeated when forecasting. |\n",
       "| dropna | bool | True | Drop rows with missing values produced by the transformations. |\n",
       "| keep_last_n | Optional | None | Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it. |\n",
       "| max_horizon | Optional | None |  |\n",
       "| return_X_y | bool | False |  |\n",
       "| **Returns** | **Union** |  | **`data` plus added features and target(s).** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MLForecast.preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53214db1-05dd-4440-88b2-94763769e520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>lag24</th>\n",
       "      <th>lag48</th>\n",
       "      <th>lag72</th>\n",
       "      <th>lag96</th>\n",
       "      <th>lag120</th>\n",
       "      <th>lag144</th>\n",
       "      <th>lag168</th>\n",
       "      <th>ewm_mean_lag48_alpha0.3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>86988</th>\n",
       "      <td>H196</td>\n",
       "      <td>193</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.002810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86989</th>\n",
       "      <td>H196</td>\n",
       "      <td>194</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.031967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86990</th>\n",
       "      <td>H196</td>\n",
       "      <td>195</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.052377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86991</th>\n",
       "      <td>H196</td>\n",
       "      <td>196</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.036664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86992</th>\n",
       "      <td>H196</td>\n",
       "      <td>197</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.025665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325187</th>\n",
       "      <td>H413</td>\n",
       "      <td>956</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-53.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>-21.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7.963225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325188</th>\n",
       "      <td>H413</td>\n",
       "      <td>957</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>-46.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>-19.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>8.574257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325189</th>\n",
       "      <td>H413</td>\n",
       "      <td>958</td>\n",
       "      <td>16.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>-36.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>-13.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.501980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325190</th>\n",
       "      <td>H413</td>\n",
       "      <td>959</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-31.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>3.151386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325191</th>\n",
       "      <td>H413</td>\n",
       "      <td>960</td>\n",
       "      <td>15.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-17.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>-18.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.405970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3072 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       unique_id   ds     y  lag24  lag48  lag72  lag96  lag120  lag144  \\\n",
       "86988       H196  193   0.1    0.0    0.0    0.0    0.3     0.1     0.1   \n",
       "86989       H196  194   0.1   -0.1    0.1    0.0    0.3     0.1     0.1   \n",
       "86990       H196  195   0.1   -0.1    0.1    0.0    0.3     0.1     0.2   \n",
       "86991       H196  196   0.1    0.0    0.0    0.0    0.3     0.2     0.1   \n",
       "86992       H196  197   0.0    0.0    0.0    0.1    0.2     0.2     0.1   \n",
       "...          ...  ...   ...    ...    ...    ...    ...     ...     ...   \n",
       "325187      H413  956   0.0   10.0    1.0    6.0  -53.0    44.0   -21.0   \n",
       "325188      H413  957   9.0   10.0   10.0   -7.0  -46.0    27.0   -19.0   \n",
       "325189      H413  958  16.0    8.0    5.0   -9.0  -36.0    32.0   -13.0   \n",
       "325190      H413  959  -3.0   17.0   -7.0    2.0  -31.0    22.0     5.0   \n",
       "325191      H413  960  15.0   11.0   -6.0   -5.0  -17.0    22.0   -18.0   \n",
       "\n",
       "        lag168  ewm_mean_lag48_alpha0.3  \n",
       "86988      0.3                 0.002810  \n",
       "86989      0.3                 0.031967  \n",
       "86990      0.1                 0.052377  \n",
       "86991      0.2                 0.036664  \n",
       "86992      0.2                 0.025665  \n",
       "...        ...                      ...  \n",
       "325187    21.0                 7.963225  \n",
       "325188    24.0                 8.574257  \n",
       "325189     8.0                 7.501980  \n",
       "325190    -2.0                 3.151386  \n",
       "325191    10.0                 0.405970  \n",
       "\n",
       "[3072 rows x 11 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_df = fcst.preprocess(train)\n",
    "prep_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9959305d-8273-4994-8bab-4e1b5fd95966",
   "metadata": {},
   "source": [
    "If we do this we then have to call `Forecast.fit_models`, since this only stores the series information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6863d9e7-92ac-4302-8271-9e70cd5531aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### MLForecast.fit_models\n",
       "\n",
       ">      MLForecast.fit_models (X:pandas.core.frame.DataFrame,\n",
       ">                             y:Union[pandas.core.series.Series,pandas.core.fram\n",
       ">                             e.DataFrame])\n",
       "\n",
       "Manually train models. Use this if you called `Forecast.preprocess` beforehand.\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| X | DataFrame | Features. |\n",
       "| y | Union | Target. |\n",
       "| **Returns** | **MLForecast** | **Forecast object with trained models.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### MLForecast.fit_models\n",
       "\n",
       ">      MLForecast.fit_models (X:pandas.core.frame.DataFrame,\n",
       ">                             y:Union[pandas.core.series.Series,pandas.core.fram\n",
       ">                             e.DataFrame])\n",
       "\n",
       "Manually train models. Use this if you called `Forecast.preprocess` beforehand.\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| X | DataFrame | Features. |\n",
       "| y | Union | Target. |\n",
       "| **Returns** | **MLForecast** | **Forecast object with trained models.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MLForecast.fit_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69d243e-a271-4518-8a0d-432716fc8d2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLForecast(models=[LGBMRegressor], freq=1, lag_features=['lag24', 'lag48', 'lag72', 'lag96', 'lag120', 'lag144', 'lag168', 'ewm_mean_lag48_alpha0.3'], date_features=[], num_threads=1)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = prep_df.drop(columns=['unique_id', 'ds', 'y']), prep_df['y']\n",
    "fcst.fit_models(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5eec03-60fa-4f35-b649-97ed5be35f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = fcst.predict(horizon)\n",
    "pd.testing.assert_frame_equal(predictions, predictions2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681f4b39-28ee-4a9e-8470-1f0d8c8ef2e0",
   "metadata": {},
   "source": [
    "### Multi-output model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074fc9de-b5fb-40a4-806d-6d15b9b62bb9",
   "metadata": {},
   "source": [
    "By default mlforecast uses the recursive strategy, i.e. a model is trained to predict the next value and if we're predicting several values we do it one at a time and then use the model's predictions as the new target, recompute the features and predict the next step.\n",
    "\n",
    "There's another approach where if we want to predict 10 steps ahead we train 10 different models, where each model is trained to predict the value at each specific step, i.e. one model predicts the next value, another one predicts the value two steps ahead and so on. This can be very time consuming but can also provide better results. If you want to use this approach you can specify `max_horizon` in `MLForecast.fit`, which will train that many models and each model will predict its corresponding horizon when you call `MLForecast.predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6e796e-1bd6-4c65-a07a-68f2f9636ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_mape(df):\n",
    "    full = df.merge(valid)\n",
    "    return abs(full['LGBMRegressor'] - full['y']).div(full['y']).groupby(full['unique_id']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb58020-6ee6-4b5d-a1d4-95810db5f55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst = MLForecast(\n",
    "    models=lgb.LGBMRegressor(random_state=0),\n",
    "    lags=[24 * (i+1) for i in range(7)],\n",
    "    lag_transforms={\n",
    "        1: [(rolling_mean, 24)],\n",
    "        24: [(rolling_mean, 24)],\n",
    "        48: [(ewm_mean, 0.3)],\n",
    "    },\n",
    "    num_threads=1,\n",
    "    target_transforms=[Differences([24])],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4f5142-1282-4931-8243-2c03a5240b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MAPE per method and serie\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>individual</th>\n",
       "      <th>recursive</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>H196</th>\n",
       "      <td>0.5%</td>\n",
       "      <td>0.6%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H256</th>\n",
       "      <td>0.7%</td>\n",
       "      <td>0.6%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H381</th>\n",
       "      <td>48.9%</td>\n",
       "      <td>20.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H413</th>\n",
       "      <td>26.9%</td>\n",
       "      <td>35.1%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          individual recursive\n",
       "unique_id                     \n",
       "H196            0.5%      0.6%\n",
       "H256            0.7%      0.6%\n",
       "H381           48.9%     20.3%\n",
       "H413           26.9%     35.1%"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_horizon = 24\n",
    "# the following will train 24 models, one for each horizon\n",
    "individual_fcst = fcst.fit(train, max_horizon=max_horizon)\n",
    "individual_preds = individual_fcst.predict(max_horizon)\n",
    "avg_mape_individual = avg_mape(individual_preds).rename('individual')\n",
    "# the following will train a single model and use the recursive strategy\n",
    "recursive_fcst = fcst.fit(train)\n",
    "recursive_preds = recursive_fcst.predict(max_horizon)\n",
    "avg_mape_recursive = avg_mape(recursive_preds).rename('recursive')\n",
    "# results\n",
    "print('Average MAPE per method and serie')\n",
    "avg_mape_individual.to_frame().join(avg_mape_recursive).applymap('{:.1%}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1027bec7-720a-4f86-bfe2-601b0c7f63e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# we get the same prediction for the first timestep\n",
    "pd.testing.assert_frame_equal(\n",
    "    individual_preds.groupby('unique_id').head(1).astype({'ds': 'int64'}),\n",
    "    recursive_preds.groupby('unique_id').head(1).astype({'ds': 'int64'}),    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94559ce-7814-4719-a9c7-ec1a2c2b14d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test intervals multioutput\n",
    "individual_fcst_intervals = fcst.fit(\n",
    "    train,\n",
    "    max_horizon=max_horizon,\n",
    "    prediction_intervals=PredictionIntervals(window_size=max_horizon)\n",
    ")\n",
    "individual_preds_intervals = individual_fcst.predict(max_horizon, level=[90, 80])\n",
    "# test monotonicity of intervals\n",
    "test_eq(\n",
    "    individual_preds_intervals.filter(regex='lo|hi').apply(\n",
    "        lambda x: x.is_monotonic_increasing,\n",
    "        axis=1\n",
    "    ).sum(),\n",
    "    len(individual_preds_intervals)\n",
    ")\n",
    "# test we can recover point forecasts with intervals\n",
    "test_eq(\n",
    "    individual_preds,\n",
    "    individual_preds_intervals[individual_preds.columns]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7d1b64-7b74-47b6-8bb7-e3ff63523eea",
   "metadata": {},
   "source": [
    "### Cross validation\n",
    "\n",
    "If we would like to know how good our forecast will be for a specific model and set of features then we can perform cross validation. What cross validation does is take our data and split it in two parts, where the first part is used for training and the second one for validation. Since the data is time dependant we usually take the last *x* observations from our data as the validation set.\n",
    "\n",
    "This process is implemented in `MLForecast.cross_validation`, which takes our data and performs the process described above for `n_windows` times where each window has `window_size` validation samples in it. For example, if we have 100 samples and we want to perform 2 backtests each of size 14, the splits will be as follows:\n",
    "\n",
    "1. Train: 1 to 72. Validation: 73 to 86.\n",
    "2. Train: 1 to 86. Validation: 87 to 100.\n",
    "\n",
    "You can control the size between each cross validation window using the `step_size` argument. For example, if we have 100 samples and we want to perform 2 backtests each of size 14 and move one step ahead in each fold (`step_size=1`), the splits will be as follows:\n",
    "\n",
    "1. Train: 1 to 85. Validation: 86 to 99.\n",
    "2. Train: 1 to 86. Validation: 87 to 100.\n",
    "\n",
    "You can also perform cross validation without refitting your models for each window by setting `refit=False`. This allows you to evaluate the performance of your models using multiple window sizes without having to retrain them each time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e65bc38-7a1b-4f52-ace2-677fa9c3561d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### MLForecast.cross_validation\n",
       "\n",
       ">      MLForecast.cross_validation (data:pandas.core.frame.DataFrame,\n",
       ">                                   n_windows:int, window_size:int,\n",
       ">                                   id_col:str='unique_id', time_col:str='ds',\n",
       ">                                   target_col:str='y',\n",
       ">                                   step_size:Optional[int]=None,\n",
       ">                                   static_features:Optional[List[str]]=None,\n",
       ">                                   dropna:bool=True,\n",
       ">                                   keep_last_n:Optional[int]=None,\n",
       ">                                   refit:bool=True,\n",
       ">                                   max_horizon:Optional[int]=None, before_predi\n",
       ">                                   ct_callback:Optional[Callable]=None, after_p\n",
       ">                                   redict_callback:Optional[Callable]=None, pre\n",
       ">                                   diction_intervals:Optional[mlforecast.utils.\n",
       ">                                   PredictionIntervals]=None,\n",
       ">                                   level:Optional[List[Union[int,float]]]=None,\n",
       ">                                   input_size:Optional[int]=None,\n",
       ">                                   fitted:bool=False)\n",
       "\n",
       "Perform time series cross validation.\n",
       "Creates `n_windows` splits where each window has `window_size` test periods, \n",
       "trains the models, computes the predictions and merges the actuals.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| data | DataFrame |  | Series data in long format. |\n",
       "| n_windows | int |  | Number of windows to evaluate. |\n",
       "| window_size | int |  | Number of test periods in each window. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| time_col | str | ds | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str | y | Column that contains the target. |\n",
       "| step_size | Optional | None | Step size between each cross validation window. If None it will be equal to `window_size`. |\n",
       "| static_features | Optional | None | Names of the features that are static and will be repeated when forecasting. |\n",
       "| dropna | bool | True | Drop rows with missing values produced by the transformations. |\n",
       "| keep_last_n | Optional | None | Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it. |\n",
       "| refit | bool | True | Retrain model for each cross validation window.<br>If False, the models are trained at the beginning and then used to predict each window. |\n",
       "| max_horizon | Optional | None |  |\n",
       "| before_predict_callback | Optional | None | Function to call on the features before computing the predictions.<br>    This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure.<br>    The series identifier is on the index. |\n",
       "| after_predict_callback | Optional | None | Function to call on the predictions before updating the targets.<br>    This function will take a pandas Series with the predictions and should return another one with the same structure.<br>    The series identifier is on the index. |\n",
       "| prediction_intervals | Optional | None | Configuration to calibrate prediction intervals (Conformal Prediction). |\n",
       "| level | Optional | None | Confidence levels between 0 and 100 for prediction intervals. |\n",
       "| input_size | Optional | None | Maximum training samples per serie in each window. If None, will use an expanding window. |\n",
       "| fitted | bool | False | Store the in-sample predictions. |\n",
       "| **Returns** | **pandas DataFrame** |  | **Predictions for each window with the series id, timestamp, last train date, target value and predictions from each model.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### MLForecast.cross_validation\n",
       "\n",
       ">      MLForecast.cross_validation (data:pandas.core.frame.DataFrame,\n",
       ">                                   n_windows:int, window_size:int,\n",
       ">                                   id_col:str='unique_id', time_col:str='ds',\n",
       ">                                   target_col:str='y',\n",
       ">                                   step_size:Optional[int]=None,\n",
       ">                                   static_features:Optional[List[str]]=None,\n",
       ">                                   dropna:bool=True,\n",
       ">                                   keep_last_n:Optional[int]=None,\n",
       ">                                   refit:bool=True,\n",
       ">                                   max_horizon:Optional[int]=None, before_predi\n",
       ">                                   ct_callback:Optional[Callable]=None, after_p\n",
       ">                                   redict_callback:Optional[Callable]=None, pre\n",
       ">                                   diction_intervals:Optional[mlforecast.utils.\n",
       ">                                   PredictionIntervals]=None,\n",
       ">                                   level:Optional[List[Union[int,float]]]=None,\n",
       ">                                   input_size:Optional[int]=None,\n",
       ">                                   fitted:bool=False)\n",
       "\n",
       "Perform time series cross validation.\n",
       "Creates `n_windows` splits where each window has `window_size` test periods, \n",
       "trains the models, computes the predictions and merges the actuals.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| data | DataFrame |  | Series data in long format. |\n",
       "| n_windows | int |  | Number of windows to evaluate. |\n",
       "| window_size | int |  | Number of test periods in each window. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| time_col | str | ds | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str | y | Column that contains the target. |\n",
       "| step_size | Optional | None | Step size between each cross validation window. If None it will be equal to `window_size`. |\n",
       "| static_features | Optional | None | Names of the features that are static and will be repeated when forecasting. |\n",
       "| dropna | bool | True | Drop rows with missing values produced by the transformations. |\n",
       "| keep_last_n | Optional | None | Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it. |\n",
       "| refit | bool | True | Retrain model for each cross validation window.<br>If False, the models are trained at the beginning and then used to predict each window. |\n",
       "| max_horizon | Optional | None |  |\n",
       "| before_predict_callback | Optional | None | Function to call on the features before computing the predictions.<br>    This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure.<br>    The series identifier is on the index. |\n",
       "| after_predict_callback | Optional | None | Function to call on the predictions before updating the targets.<br>    This function will take a pandas Series with the predictions and should return another one with the same structure.<br>    The series identifier is on the index. |\n",
       "| prediction_intervals | Optional | None | Configuration to calibrate prediction intervals (Conformal Prediction). |\n",
       "| level | Optional | None | Confidence levels between 0 and 100 for prediction intervals. |\n",
       "| input_size | Optional | None | Maximum training samples per serie in each window. If None, will use an expanding window. |\n",
       "| fitted | bool | False | Store the in-sample predictions. |\n",
       "| **Returns** | **pandas DataFrame** |  | **Predictions for each window with the series id, timestamp, last train date, target value and predictions from each model.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MLForecast.cross_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9667452-b4b4-4d8c-a952-b7e70a7e92f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>cutoff</th>\n",
       "      <th>y</th>\n",
       "      <th>LGBMRegressor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H196</td>\n",
       "      <td>769</td>\n",
       "      <td>768</td>\n",
       "      <td>15.2</td>\n",
       "      <td>15.167163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H196</td>\n",
       "      <td>770</td>\n",
       "      <td>768</td>\n",
       "      <td>14.8</td>\n",
       "      <td>14.767163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H196</td>\n",
       "      <td>771</td>\n",
       "      <td>768</td>\n",
       "      <td>14.4</td>\n",
       "      <td>14.467163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H196</td>\n",
       "      <td>772</td>\n",
       "      <td>768</td>\n",
       "      <td>14.1</td>\n",
       "      <td>14.167163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H196</td>\n",
       "      <td>773</td>\n",
       "      <td>768</td>\n",
       "      <td>13.8</td>\n",
       "      <td>13.867163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>H413</td>\n",
       "      <td>956</td>\n",
       "      <td>912</td>\n",
       "      <td>59.0</td>\n",
       "      <td>64.284167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>H413</td>\n",
       "      <td>957</td>\n",
       "      <td>912</td>\n",
       "      <td>58.0</td>\n",
       "      <td>64.830429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>H413</td>\n",
       "      <td>958</td>\n",
       "      <td>912</td>\n",
       "      <td>53.0</td>\n",
       "      <td>40.726851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>H413</td>\n",
       "      <td>959</td>\n",
       "      <td>912</td>\n",
       "      <td>38.0</td>\n",
       "      <td>42.739657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>H413</td>\n",
       "      <td>960</td>\n",
       "      <td>912</td>\n",
       "      <td>46.0</td>\n",
       "      <td>52.802769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    unique_id   ds  cutoff     y  LGBMRegressor\n",
       "0        H196  769     768  15.2      15.167163\n",
       "1        H196  770     768  14.8      14.767163\n",
       "2        H196  771     768  14.4      14.467163\n",
       "3        H196  772     768  14.1      14.167163\n",
       "4        H196  773     768  13.8      13.867163\n",
       "..        ...  ...     ...   ...            ...\n",
       "187      H413  956     912  59.0      64.284167\n",
       "188      H413  957     912  58.0      64.830429\n",
       "189      H413  958     912  53.0      40.726851\n",
       "190      H413  959     912  38.0      42.739657\n",
       "191      H413  960     912  46.0      52.802769\n",
       "\n",
       "[768 rows x 5 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcst = MLForecast(\n",
    "    models=lgb.LGBMRegressor(random_state=0),\n",
    "    lags=[24 * (i+1) for i in range(7)],\n",
    "    lag_transforms={\n",
    "        1: [(rolling_mean, 24)],\n",
    "        24: [(rolling_mean, 24)],\n",
    "        48: [(ewm_mean, 0.3)],\n",
    "    },\n",
    "    num_threads=1,\n",
    "    target_transforms=[Differences([24])],\n",
    ")\n",
    "cv_results = fcst.cross_validation(\n",
    "    train,\n",
    "    n_windows=4,\n",
    "    window_size=horizon,\n",
    "    step_size=horizon,\n",
    "    fitted=True,\n",
    ")\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a71926a-fbfe-4d2f-99c3-804fe6e2908f",
   "metadata": {},
   "source": [
    "Since we set `fitted=True` we can access the predictions for the training sets as well with the `cross_validation_fitted_values` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c295f09-1416-4e55-8701-7a5ef8e2200d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>fold</th>\n",
       "      <th>y</th>\n",
       "      <th>LGBMRegressor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H196</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11.8</td>\n",
       "      <td>15.167163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H196</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>11.4</td>\n",
       "      <td>14.767163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H196</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>11.1</td>\n",
       "      <td>14.467163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H196</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>10.8</td>\n",
       "      <td>14.167163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H196</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>10.6</td>\n",
       "      <td>13.867163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13435</th>\n",
       "      <td>H413</td>\n",
       "      <td>908</td>\n",
       "      <td>3</td>\n",
       "      <td>49.0</td>\n",
       "      <td>40.262691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13436</th>\n",
       "      <td>H413</td>\n",
       "      <td>909</td>\n",
       "      <td>3</td>\n",
       "      <td>39.0</td>\n",
       "      <td>26.603123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13437</th>\n",
       "      <td>H413</td>\n",
       "      <td>910</td>\n",
       "      <td>3</td>\n",
       "      <td>29.0</td>\n",
       "      <td>42.545732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13438</th>\n",
       "      <td>H413</td>\n",
       "      <td>911</td>\n",
       "      <td>3</td>\n",
       "      <td>24.0</td>\n",
       "      <td>30.053714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13439</th>\n",
       "      <td>H413</td>\n",
       "      <td>912</td>\n",
       "      <td>3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-13.589900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13440 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      unique_id   ds  fold     y  LGBMRegressor\n",
       "0          H196    1     0  11.8      15.167163\n",
       "1          H196    2     0  11.4      14.767163\n",
       "2          H196    3     0  11.1      14.467163\n",
       "3          H196    4     0  10.8      14.167163\n",
       "4          H196    5     0  10.6      13.867163\n",
       "...         ...  ...   ...   ...            ...\n",
       "13435      H413  908     3  49.0      40.262691\n",
       "13436      H413  909     3  39.0      26.603123\n",
       "13437      H413  910     3  29.0      42.545732\n",
       "13438      H413  911     3  24.0      30.053714\n",
       "13439      H413  912     3  20.0     -13.589900\n",
       "\n",
       "[13440 rows x 5 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcst.cross_validation_fitted_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9193261-df9c-40f4-a2ea-27cf39b90cd5",
   "metadata": {},
   "source": [
    "We can also compute prediction intervals by passing a configuration to `prediction_intervals` as well as values for the width through `levels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f940f245-4a5c-4da6-a6b9-297c99610d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>cutoff</th>\n",
       "      <th>y</th>\n",
       "      <th>LGBMRegressor</th>\n",
       "      <th>LGBMRegressor-lo-90</th>\n",
       "      <th>LGBMRegressor-lo-80</th>\n",
       "      <th>LGBMRegressor-hi-80</th>\n",
       "      <th>LGBMRegressor-hi-90</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H196</td>\n",
       "      <td>769</td>\n",
       "      <td>768</td>\n",
       "      <td>15.2</td>\n",
       "      <td>15.167163</td>\n",
       "      <td>15.141751</td>\n",
       "      <td>15.141751</td>\n",
       "      <td>15.192575</td>\n",
       "      <td>15.192575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H196</td>\n",
       "      <td>770</td>\n",
       "      <td>768</td>\n",
       "      <td>14.8</td>\n",
       "      <td>14.767163</td>\n",
       "      <td>14.741751</td>\n",
       "      <td>14.741751</td>\n",
       "      <td>14.792575</td>\n",
       "      <td>14.792575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H196</td>\n",
       "      <td>771</td>\n",
       "      <td>768</td>\n",
       "      <td>14.4</td>\n",
       "      <td>14.467163</td>\n",
       "      <td>14.399951</td>\n",
       "      <td>14.407328</td>\n",
       "      <td>14.526998</td>\n",
       "      <td>14.534374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H196</td>\n",
       "      <td>772</td>\n",
       "      <td>768</td>\n",
       "      <td>14.1</td>\n",
       "      <td>14.167163</td>\n",
       "      <td>14.092575</td>\n",
       "      <td>14.092575</td>\n",
       "      <td>14.241751</td>\n",
       "      <td>14.241751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H196</td>\n",
       "      <td>773</td>\n",
       "      <td>768</td>\n",
       "      <td>13.8</td>\n",
       "      <td>13.867163</td>\n",
       "      <td>13.792575</td>\n",
       "      <td>13.792575</td>\n",
       "      <td>13.941751</td>\n",
       "      <td>13.941751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>H413</td>\n",
       "      <td>956</td>\n",
       "      <td>912</td>\n",
       "      <td>59.0</td>\n",
       "      <td>64.284167</td>\n",
       "      <td>29.890099</td>\n",
       "      <td>34.371545</td>\n",
       "      <td>94.196788</td>\n",
       "      <td>98.678234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>H413</td>\n",
       "      <td>957</td>\n",
       "      <td>912</td>\n",
       "      <td>58.0</td>\n",
       "      <td>64.830429</td>\n",
       "      <td>56.874572</td>\n",
       "      <td>57.827689</td>\n",
       "      <td>71.833169</td>\n",
       "      <td>72.786285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>H413</td>\n",
       "      <td>958</td>\n",
       "      <td>912</td>\n",
       "      <td>53.0</td>\n",
       "      <td>40.726851</td>\n",
       "      <td>35.296195</td>\n",
       "      <td>35.846206</td>\n",
       "      <td>45.607495</td>\n",
       "      <td>46.157506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>H413</td>\n",
       "      <td>959</td>\n",
       "      <td>912</td>\n",
       "      <td>38.0</td>\n",
       "      <td>42.739657</td>\n",
       "      <td>35.292153</td>\n",
       "      <td>35.807640</td>\n",
       "      <td>49.671674</td>\n",
       "      <td>50.187161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>H413</td>\n",
       "      <td>960</td>\n",
       "      <td>912</td>\n",
       "      <td>46.0</td>\n",
       "      <td>52.802769</td>\n",
       "      <td>42.465597</td>\n",
       "      <td>43.895670</td>\n",
       "      <td>61.709869</td>\n",
       "      <td>63.139941</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    unique_id   ds  cutoff     y  LGBMRegressor  LGBMRegressor-lo-90  \\\n",
       "0        H196  769     768  15.2      15.167163            15.141751   \n",
       "1        H196  770     768  14.8      14.767163            14.741751   \n",
       "2        H196  771     768  14.4      14.467163            14.399951   \n",
       "3        H196  772     768  14.1      14.167163            14.092575   \n",
       "4        H196  773     768  13.8      13.867163            13.792575   \n",
       "..        ...  ...     ...   ...            ...                  ...   \n",
       "187      H413  956     912  59.0      64.284167            29.890099   \n",
       "188      H413  957     912  58.0      64.830429            56.874572   \n",
       "189      H413  958     912  53.0      40.726851            35.296195   \n",
       "190      H413  959     912  38.0      42.739657            35.292153   \n",
       "191      H413  960     912  46.0      52.802769            42.465597   \n",
       "\n",
       "     LGBMRegressor-lo-80  LGBMRegressor-hi-80  LGBMRegressor-hi-90  \n",
       "0              15.141751            15.192575            15.192575  \n",
       "1              14.741751            14.792575            14.792575  \n",
       "2              14.407328            14.526998            14.534374  \n",
       "3              14.092575            14.241751            14.241751  \n",
       "4              13.792575            13.941751            13.941751  \n",
       "..                   ...                  ...                  ...  \n",
       "187            34.371545            94.196788            98.678234  \n",
       "188            57.827689            71.833169            72.786285  \n",
       "189            35.846206            45.607495            46.157506  \n",
       "190            35.807640            49.671674            50.187161  \n",
       "191            43.895670            61.709869            63.139941  \n",
       "\n",
       "[768 rows x 9 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results_intervals = fcst.cross_validation(\n",
    "    train,\n",
    "    n_windows=4,\n",
    "    window_size=horizon,\n",
    "    step_size=horizon,\n",
    "    prediction_intervals=PredictionIntervals(window_size=horizon),\n",
    "    level=[80, 90]\n",
    ")\n",
    "cv_results_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24dc0ae-4847-4258-a2ab-ae56cf403a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "cv_results_no_refit = fcst.cross_validation(\n",
    "    train,\n",
    "    n_windows=4,\n",
    "    window_size=horizon,\n",
    "    step_size=horizon,\n",
    "    refit=False\n",
    ")\n",
    "# test we recover the same \"metadata\"\n",
    "test_eq(\n",
    "    cv_results_no_refit.drop(columns='LGBMRegressor'),\n",
    "    cv_results.drop(columns='LGBMRegressor')\n",
    ")\n",
    "# test the first window has the same forecasts\n",
    "first_cutoff = cv_results['cutoff'].iloc[0]\n",
    "test_eq(\n",
    "    cv_results_no_refit.query('cutoff == @first_cutoff'),\n",
    "    cv_results.query('cutoff == @first_cutoff')\n",
    ")\n",
    "# test next windows have different forecasts\n",
    "test_ne(\n",
    "    cv_results_no_refit.query('cutoff != @first_cutoff'),\n",
    "    cv_results.query('cutoff != @first_cutoff')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa13128-57be-4f72-94da-3287eae36713",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# cv with input_size\n",
    "input_size = 300\n",
    "cv_results_input_size = fcst.cross_validation(\n",
    "    train,\n",
    "    n_windows=4,\n",
    "    window_size=horizon,\n",
    "    step_size=horizon,\n",
    "    input_size=input_size,\n",
    ")\n",
    "series_lengths = np.diff(fcst.ts.ga.indptr)\n",
    "unique_lengths = np.unique(series_lengths)\n",
    "assert unique_lengths.size == 1\n",
    "assert unique_lengths[0] == input_size + horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8ef2aa-daad-4b08-82e7-bf75938a0fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# one model per horizon\n",
    "cv_results2 = fcst.cross_validation(\n",
    "    train,\n",
    "    n_windows=4,\n",
    "    window_size=horizon,\n",
    "    step_size=horizon,\n",
    "    max_horizon=horizon,\n",
    ")\n",
    "# the first entry per id and window is the same\n",
    "pd.testing.assert_frame_equal(\n",
    "    cv_results.groupby(['unique_id', 'cutoff']).head(1),\n",
    "    cv_results2.groupby(['unique_id', 'cutoff']).head(1)\n",
    ")\n",
    "# the rest is different\n",
    "test_fail(lambda: pd.testing.assert_frame_equal(cv_results, cv_results2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5520ec36-e57c-49e0-a49c-7bf5fb8ff1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# one model per horizon with prediction intervals\n",
    "cv_results2_intervals = fcst.cross_validation(\n",
    "    train,\n",
    "    n_windows=4,\n",
    "    window_size=horizon,\n",
    "    step_size=horizon,\n",
    "    max_horizon=horizon,\n",
    "    prediction_intervals=PredictionIntervals(n_windows=2, window_size=horizon),\n",
    "    level=[80, 90]\n",
    ")\n",
    "# the first entry per id and window is the same\n",
    "pd.testing.assert_frame_equal(\n",
    "    cv_results_intervals.groupby(['unique_id', 'cutoff']).head(1),\n",
    "    cv_results2_intervals.groupby(['unique_id', 'cutoff']).head(1)\n",
    ")\n",
    "# the rest is different\n",
    "test_fail(lambda: pd.testing.assert_frame_equal(cv_results_intervals, cv_results2_intervals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6234482e-1e43-4168-88d2-0c06ba06b7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# wrong frequency raises error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "df_wrong_freq = pd.DataFrame({'ds': pd.to_datetime(['2020-01-02', '2020-02-02', '2020-03-02', '2020-04-02'])})\n",
    "df_wrong_freq['unique_id'] = 'id1'\n",
    "df_wrong_freq['y'] = 1\n",
    "fcst_wrong_freq = MLForecast(\n",
    "    models=[LinearRegression()],\n",
    "    freq='MS',\n",
    "    lags=[1],\n",
    ")\n",
    "test_fail(\n",
    "    lambda: fcst_wrong_freq.cross_validation(df_wrong_freq, n_windows=1, window_size=1),\n",
    "    contains='Cross validation result produced less results than expected',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e194aa-e261-400e-9264-1b54db9b163e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(16, 10))\n",
    "\n",
    "for uid, axi in zip(sample_ids, ax.flat):\n",
    "    subset = cv_results[cv_results['unique_id'].eq(uid)].drop(columns=['unique_id', 'cutoff'])\n",
    "    subset.set_index('ds').plot(ax=axi, title=uid)\n",
    "fig.savefig('figs/forecast__cross_validation.png', bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e195029f-0de8-4425-8c91-8a3b4f2333c6",
   "metadata": {},
   "source": [
    "![](figs/forecast__cross_validation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110faf3c-71d9-4686-ad2e-09ff61daa317",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(16, 10))\n",
    "\n",
    "for uid, axi in zip(sample_ids, ax.flat):\n",
    "    subset = cv_results_intervals[cv_results_intervals['unique_id'].eq(uid)].drop(columns=['unique_id', 'cutoff']).set_index('ds')\n",
    "    subset[['y', 'LGBMRegressor']].plot(ax=axi, title=uid)\n",
    "    axi.fill_between(\n",
    "        subset.index, \n",
    "        subset['LGBMRegressor-lo-90'].values, \n",
    "        subset['LGBMRegressor-hi-90'].values,\n",
    "        label='LGBMRegressor-level-90',\n",
    "        color='orange',\n",
    "        alpha=0.2\n",
    "    )\n",
    "    axi.legend()\n",
    "fig.savefig('figs/forecast__cross_validation_intervals.png', bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d7b641-19a9-4cff-97f0-45dc06161cc2",
   "metadata": {},
   "source": [
    "![](figs/forecast__cross_validation_intervals.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0aaa93-3ce6-42a5-87a1-5a9d6f69f289",
   "metadata": {},
   "source": [
    "### Create MLForecast from LightGBMCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0fe555-a79c-4347-bad6-00d4e6a4cf7c",
   "metadata": {},
   "source": [
    "Once you've found a set of features and parameters that work for your problem you can build a forecast object from it using `MLForecast.from_cv`, which takes the trained `LightGBMCV` object and builds an `MLForecast` object that will use the same features and parameters. Then you can call fit and predict as you normally would."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf6f8a1-a69c-44e6-b419-6f29fe7db2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Start training from score 0.084340\n",
      "[10] mape: 0.118569\n",
      "[20] mape: 0.111506\n",
      "[30] mape: 0.107314\n",
      "[40] mape: 0.106089\n",
      "[50] mape: 0.106630\n",
      "Early stopping at round 50\n",
      "Using best iteration: 40\n"
     ]
    }
   ],
   "source": [
    "cv = LightGBMCV(\n",
    "    freq=1,\n",
    "    lags=[24 * (i+1) for i in range(7)],\n",
    "    lag_transforms={\n",
    "        48: [(ewm_mean, 0.3)],\n",
    "    },\n",
    "    num_threads=1,\n",
    "    target_transforms=[Differences([24])]\n",
    ")\n",
    "hist = cv.fit(\n",
    "    train,\n",
    "    n_windows=2,\n",
    "    window_size=horizon,\n",
    "    params={'verbosity': -1},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2533aaa2-b668-402c-b86b-50125cca4158",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst = MLForecast.from_cv(cv)\n",
    "assert cv.best_iteration_ == fcst.models['LGBMRegressor'].n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2082a5-5562-4816-9d53-cba4599e0196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>LGBMRegressor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H196</td>\n",
       "      <td>961</td>\n",
       "      <td>16.111079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H196</td>\n",
       "      <td>962</td>\n",
       "      <td>15.711079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H196</td>\n",
       "      <td>963</td>\n",
       "      <td>15.311079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H196</td>\n",
       "      <td>964</td>\n",
       "      <td>15.011079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H196</td>\n",
       "      <td>965</td>\n",
       "      <td>14.711079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>H413</td>\n",
       "      <td>1004</td>\n",
       "      <td>92.722032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>H413</td>\n",
       "      <td>1005</td>\n",
       "      <td>69.153603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>H413</td>\n",
       "      <td>1006</td>\n",
       "      <td>68.811675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>H413</td>\n",
       "      <td>1007</td>\n",
       "      <td>53.693346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>H413</td>\n",
       "      <td>1008</td>\n",
       "      <td>46.055481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    unique_id    ds  LGBMRegressor\n",
       "0        H196   961      16.111079\n",
       "1        H196   962      15.711079\n",
       "2        H196   963      15.311079\n",
       "3        H196   964      15.011079\n",
       "4        H196   965      14.711079\n",
       "..        ...   ...            ...\n",
       "187      H413  1004      92.722032\n",
       "188      H413  1005      69.153603\n",
       "189      H413  1006      68.811675\n",
       "190      H413  1007      53.693346\n",
       "191      H413  1008      46.055481\n",
       "\n",
       "[192 rows x 3 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcst.fit(train)\n",
    "fcst.predict(horizon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1942738d-13a3-4641-8706-cd9578709ec0",
   "metadata": {},
   "source": [
    "### Dynamic features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7a5949-18a7-4e6a-a6f9-e812fb860c98",
   "metadata": {},
   "source": [
    "We're going to use a synthetic dataset from this point onwards to demonstrate some other functionalities regarding external regressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4861d6d5-2c7b-4f42-a383-70890512c630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>static_0</th>\n",
       "      <th>static_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-10-05</td>\n",
       "      <td>3.981198</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-10-06</td>\n",
       "      <td>10.327401</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-10-07</td>\n",
       "      <td>17.657474</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-10-08</td>\n",
       "      <td>25.898790</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-10-09</td>\n",
       "      <td>34.494040</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26998</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-10</td>\n",
       "      <td>45.340051</td>\n",
       "      <td>69</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26999</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-11</td>\n",
       "      <td>3.022948</td>\n",
       "      <td>69</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27000</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-12</td>\n",
       "      <td>10.131371</td>\n",
       "      <td>69</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27001</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-13</td>\n",
       "      <td>14.572434</td>\n",
       "      <td>69</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27002</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-14</td>\n",
       "      <td>22.816357</td>\n",
       "      <td>69</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27003 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      unique_id         ds          y  static_0  static_1\n",
       "0         id_00 2000-10-05   3.981198        79        45\n",
       "1         id_00 2000-10-06  10.327401        79        45\n",
       "2         id_00 2000-10-07  17.657474        79        45\n",
       "3         id_00 2000-10-08  25.898790        79        45\n",
       "4         id_00 2000-10-09  34.494040        79        45\n",
       "...         ...        ...        ...       ...       ...\n",
       "26998     id_99 2001-05-10  45.340051        69        35\n",
       "26999     id_99 2001-05-11   3.022948        69        35\n",
       "27000     id_99 2001-05-12  10.131371        69        35\n",
       "27001     id_99 2001-05-13  14.572434        69        35\n",
       "27002     id_99 2001-05-14  22.816357        69        35\n",
       "\n",
       "[27003 rows x 5 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series = generate_daily_series(100, equal_ends=True, n_static_features=2, static_as_categorical=False)\n",
    "series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b5b920-853a-4fa0-91f2-625b0b6f5eec",
   "metadata": {},
   "source": [
    "As we saw in the previous example, the required columns are the series identifier, time and target. Whatever extra columns you have, like `static_0` and `static_1` here are considered to be static and are replicated when constructing the features for the next timestamp. You can disable this by passing `static_features` to `MLForecast.preprocess` or `MLForecast.fit` , which will only keep the columns you define there as static. Keep in mind that they will still be used for training, so you'll have to provide them to `MLForecast.predict` through the `dynamic_dfs` argument.\n",
    "\n",
    "By default the predict method repeats the static features and updates the transformations and the date features. If you have dynamic features like prices or a calendar with holidays you can pass them as a list to the `dynamic_dfs` argument of `MLForecast.predict`, which will call `pd.DataFrame.merge` on each of them in order.\n",
    "\n",
    "Here's an example:\n",
    "\n",
    "Suppose that we have a `product_id` column and we have a catalog for prices based on that `product_id` and the date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4d6354-e95e-444c-af11-18f273ee2d07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>product_id</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-06-09</td>\n",
       "      <td>1</td>\n",
       "      <td>0.548814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-06-10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.715189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-06-11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.602763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-06-12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.544883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-06-13</td>\n",
       "      <td>1</td>\n",
       "      <td>0.423655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20180</th>\n",
       "      <td>2001-05-17</td>\n",
       "      <td>99</td>\n",
       "      <td>0.223520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20181</th>\n",
       "      <td>2001-05-18</td>\n",
       "      <td>99</td>\n",
       "      <td>0.446104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20182</th>\n",
       "      <td>2001-05-19</td>\n",
       "      <td>99</td>\n",
       "      <td>0.044783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20183</th>\n",
       "      <td>2001-05-20</td>\n",
       "      <td>99</td>\n",
       "      <td>0.483216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20184</th>\n",
       "      <td>2001-05-21</td>\n",
       "      <td>99</td>\n",
       "      <td>0.799660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20185 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ds  product_id     price\n",
       "0     2000-06-09           1  0.548814\n",
       "1     2000-06-10           1  0.715189\n",
       "2     2000-06-11           1  0.602763\n",
       "3     2000-06-12           1  0.544883\n",
       "4     2000-06-13           1  0.423655\n",
       "...          ...         ...       ...\n",
       "20180 2001-05-17          99  0.223520\n",
       "20181 2001-05-18          99  0.446104\n",
       "20182 2001-05-19          99  0.044783\n",
       "20183 2001-05-20          99  0.483216\n",
       "20184 2001-05-21          99  0.799660\n",
       "\n",
       "[20185 rows x 3 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynamic_series = series.rename(columns={'static_1': 'product_id'})\n",
    "prices_catalog = generate_prices_for_series(dynamic_series)\n",
    "prices_catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61decbbd-91f6-4e90-a750-1eb97b880c90",
   "metadata": {},
   "source": [
    "And you have already merged these prices into your series dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc453901-10d1-441f-af31-0ee52a3d58f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>static_0</th>\n",
       "      <th>product_id</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-10-05</td>\n",
       "      <td>3.981198</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>0.570826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-10-06</td>\n",
       "      <td>10.327401</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>0.260562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-10-07</td>\n",
       "      <td>17.657474</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>0.274048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-10-08</td>\n",
       "      <td>25.898790</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>0.433878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-10-09</td>\n",
       "      <td>34.494040</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>0.653738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26998</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-10</td>\n",
       "      <td>45.340051</td>\n",
       "      <td>69</td>\n",
       "      <td>35</td>\n",
       "      <td>0.792152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26999</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-11</td>\n",
       "      <td>3.022948</td>\n",
       "      <td>69</td>\n",
       "      <td>35</td>\n",
       "      <td>0.782687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27000</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-12</td>\n",
       "      <td>10.131371</td>\n",
       "      <td>69</td>\n",
       "      <td>35</td>\n",
       "      <td>0.019463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27001</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-13</td>\n",
       "      <td>14.572434</td>\n",
       "      <td>69</td>\n",
       "      <td>35</td>\n",
       "      <td>0.190413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27002</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-14</td>\n",
       "      <td>22.816357</td>\n",
       "      <td>69</td>\n",
       "      <td>35</td>\n",
       "      <td>0.653394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27003 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      unique_id         ds          y  static_0  product_id     price\n",
       "0         id_00 2000-10-05   3.981198        79          45  0.570826\n",
       "1         id_00 2000-10-06  10.327401        79          45  0.260562\n",
       "2         id_00 2000-10-07  17.657474        79          45  0.274048\n",
       "3         id_00 2000-10-08  25.898790        79          45  0.433878\n",
       "4         id_00 2000-10-09  34.494040        79          45  0.653738\n",
       "...         ...        ...        ...       ...         ...       ...\n",
       "26998     id_99 2001-05-10  45.340051        69          35  0.792152\n",
       "26999     id_99 2001-05-11   3.022948        69          35  0.782687\n",
       "27000     id_99 2001-05-12  10.131371        69          35  0.019463\n",
       "27001     id_99 2001-05-13  14.572434        69          35  0.190413\n",
       "27002     id_99 2001-05-14  22.816357        69          35  0.653394\n",
       "\n",
       "[27003 rows x 6 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series_with_prices = dynamic_series.merge(prices_catalog, how='left')\n",
    "series_with_prices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f373479-d37c-4f92-b12b-5c937898ef6b",
   "metadata": {},
   "source": [
    "This dataframe will be passed to `MLForecast.fit` (or `MLForecast.preprocess`), however since the price is dynamic we have to tell that method that only `static_0` and `product_id` are static and we'll have to update `price` in every timestep, which basically involves merging the updated features with the prices catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac4391d-5dd4-4eb9-91bb-e39fe67023c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLForecast(models=[LGBMRegressor], freq=<Day>, lag_features=['lag7', 'expanding_mean_lag1', 'rolling_mean_lag7_window_size14'], date_features=['dayofweek', 'month', <function even_day>], num_threads=2)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def even_day(dates):\n",
    "    return dates.day % 2 == 0\n",
    "\n",
    "fcst = MLForecast(\n",
    "    models=lgb.LGBMRegressor(n_jobs=1, random_state=0),\n",
    "    freq='D',\n",
    "    lags=[7],\n",
    "    lag_transforms={\n",
    "        1: [expanding_mean],\n",
    "        7: [(rolling_mean, 14)]\n",
    "    },\n",
    "    date_features=['dayofweek', 'month', even_day],\n",
    "    num_threads=2,\n",
    ")\n",
    "fcst.fit(series_with_prices, static_features=['unique_id', 'static_0', 'product_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c38fea-4868-4d34-9932-ce8af283d8ac",
   "metadata": {},
   "source": [
    "The features used for training are stored in `MLForecast.ts.features_order_`, as you can see `price` was used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c160114-073e-483a-9c27-638487674b25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['unique_id',\n",
       " 'static_0',\n",
       " 'product_id',\n",
       " 'price',\n",
       " 'lag7',\n",
       " 'expanding_mean_lag1',\n",
       " 'rolling_mean_lag7_window_size14',\n",
       " 'dayofweek',\n",
       " 'month',\n",
       " 'even_day']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcst.ts.features_order_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f8d250-a5d8-4fb0-a41c-d5515b645c37",
   "metadata": {},
   "source": [
    "So in order to update the price in each timestep we just call `MLForecast.predict` with our forecast horizon and pass the prices catalog as a dynamic dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e071906-3d80-4fa5-9b55-55c90f338985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>LGBMRegressor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-15</td>\n",
       "      <td>42.406978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-16</td>\n",
       "      <td>50.076236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-17</td>\n",
       "      <td>1.904567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-18</td>\n",
       "      <td>10.259930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-19</td>\n",
       "      <td>18.727878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-17</td>\n",
       "      <td>44.266018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-18</td>\n",
       "      <td>1.936728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-19</td>\n",
       "      <td>9.091219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-20</td>\n",
       "      <td>15.262409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-21</td>\n",
       "      <td>22.840666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>700 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    unique_id         ds  LGBMRegressor\n",
       "0       id_00 2001-05-15      42.406978\n",
       "1       id_00 2001-05-16      50.076236\n",
       "2       id_00 2001-05-17       1.904567\n",
       "3       id_00 2001-05-18      10.259930\n",
       "4       id_00 2001-05-19      18.727878\n",
       "..        ...        ...            ...\n",
       "695     id_99 2001-05-17      44.266018\n",
       "696     id_99 2001-05-18       1.936728\n",
       "697     id_99 2001-05-19       9.091219\n",
       "698     id_99 2001-05-20      15.262409\n",
       "699     id_99 2001-05-21      22.840666\n",
       "\n",
       "[700 rows x 3 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = fcst.predict(7, dynamic_dfs=[prices_catalog])\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab63b809-a70a-4067-891a-4fe3d6375069",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "preds2 = fcst.predict(7, dynamic_dfs=[prices_catalog])\n",
    "preds3 = fcst.predict(7, new_data=series_with_prices, dynamic_dfs=[prices_catalog])\n",
    "\n",
    "pd.testing.assert_frame_equal(preds, preds2)\n",
    "pd.testing.assert_frame_equal(preds, preds3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f95689-3ff3-4588-9385-9e85ba6a2dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test we can compute cross validation with\n",
    "# exougenous variables without adding extra information\n",
    "# later a more robust test is performed\n",
    "_ = fcst.cross_validation(\n",
    "    series_with_prices,\n",
    "    window_size=7,\n",
    "    n_windows=2,\n",
    "    static_features=['static_0', 'product_id'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9188fd-8264-41d1-a4d7-89fa51d915d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "non_std_series = series.copy()\n",
    "non_std_series['ds'] = non_std_series.groupby('unique_id').cumcount()\n",
    "non_std_series = non_std_series.rename(columns={'unique_id': 'some_id', 'ds': 'time', 'y': 'value'})\n",
    "models = [\n",
    "    lgb.LGBMRegressor(n_jobs=1, random_state=0),\n",
    "    xgb.XGBRegressor(n_jobs=1, random_state=0),\n",
    "]\n",
    "flow_params = dict(\n",
    "    models=models,\n",
    "    lags=[7],\n",
    "    lag_transforms={\n",
    "        1: [expanding_mean],\n",
    "        7: [(rolling_mean, 14)]\n",
    "    },\n",
    "    num_threads=2,\n",
    ")\n",
    "fcst = MLForecast(**flow_params)\n",
    "non_std_preds = fcst.fit(non_std_series, id_col='some_id', time_col='time', target_col='value').predict(7)\n",
    "non_std_preds = non_std_preds.rename(columns={'some_id': 'unique_id'})\n",
    "fcst = MLForecast(freq='D', **flow_params)\n",
    "preds = fcst.fit(series).predict(7)\n",
    "pd.testing.assert_frame_equal(preds.drop(columns='ds'), non_std_preds.drop(columns='time'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6fb8b4-f0a3-4012-a410-05972fe61ef1",
   "metadata": {},
   "source": [
    "### Custom predictions\n",
    "As you may have noticed `MLForecast.predict` can take a `before_predict_callback` and `after_predict_callback`. By default the predict method repeats the static features and updates the transformations and the date features. If you have dynamic features you can pass them as a list to `MLForecast.predict` in the `dynamic_dfs` argument. However, if you want to do something to the input before predicting or do something to the output before it gets used to update the target (and thus the next features that rely on lags), you can pass a function to run at any of these times. \n",
    "\n",
    "Suppose that we want to look at our inputs and scale our predictions so that our series are updated with these scaled values. We can achieve that with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0ff10b-028f-4034-aef9-a503cc244cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5a9eb6-7e70-4a77-9c2b-a75d01e3b165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_input(new_x):\n",
    "    \"\"\"Displays the first row of our input to inspect it\"\"\"\n",
    "    print('Inputs:')\n",
    "    display(new_x.head(1))\n",
    "    return new_x\n",
    "\n",
    "def increase_predictions(predictions):\n",
    "    \"\"\"Prints the last prediction and increases all of them by 10%.\"\"\"\n",
    "    print(f'Prediction:\\n{predictions.tail(1)}\\n')\n",
    "    return 1.1 * predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a252b2-b83c-49d0-bc59-6d4ae7f637fb",
   "metadata": {},
   "source": [
    "And now we just pass these functions to `MLForecast.predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395c58c7-2c30-4fba-89ae-d43a11ea6115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>static_0</th>\n",
       "      <th>static_1</th>\n",
       "      <th>lag1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>34.862245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   static_0  static_1       lag1\n",
       "0        79        45  34.862245"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\n",
      "unique_id\n",
      "id_99    30.643253\n",
      "dtype: float64\n",
      "\n",
      "Inputs:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>static_0</th>\n",
       "      <th>static_1</th>\n",
       "      <th>lag1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>46.396346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   static_0  static_1       lag1\n",
       "0        79        45  46.396346"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\n",
      "unique_id\n",
      "id_99    41.024064\n",
      "dtype: float64\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>LGBMRegressor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-15</td>\n",
       "      <td>46.396346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-16</td>\n",
       "      <td>23.651944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_01</td>\n",
       "      <td>2001-05-15</td>\n",
       "      <td>14.388954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_01</td>\n",
       "      <td>2001-05-16</td>\n",
       "      <td>17.796990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_02</td>\n",
       "      <td>2001-05-15</td>\n",
       "      <td>15.640528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>id_97</td>\n",
       "      <td>2001-05-16</td>\n",
       "      <td>39.849693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>id_98</td>\n",
       "      <td>2001-05-15</td>\n",
       "      <td>8.408627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>id_98</td>\n",
       "      <td>2001-05-16</td>\n",
       "      <td>4.290933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-15</td>\n",
       "      <td>33.707579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-16</td>\n",
       "      <td>45.126470</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    unique_id         ds  LGBMRegressor\n",
       "0       id_00 2001-05-15      46.396346\n",
       "1       id_00 2001-05-16      23.651944\n",
       "2       id_01 2001-05-15      14.388954\n",
       "3       id_01 2001-05-16      17.796990\n",
       "4       id_02 2001-05-15      15.640528\n",
       "..        ...        ...            ...\n",
       "195     id_97 2001-05-16      39.849693\n",
       "196     id_98 2001-05-15       8.408627\n",
       "197     id_98 2001-05-16       4.290933\n",
       "198     id_99 2001-05-15      33.707579\n",
       "199     id_99 2001-05-16      45.126470\n",
       "\n",
       "[200 rows x 3 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcst = MLForecast(lgb.LGBMRegressor(), freq='D', lags=[1])\n",
    "fcst.fit(series)\n",
    "\n",
    "preds = fcst.predict(2, before_predict_callback=inspect_input, after_predict_callback=increase_predictions)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f15e85-4cfe-4f04-b6bb-5f794dd12390",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "fcst.ts._predict_setup()\n",
    "\n",
    "for attr in ('head', 'tail'):\n",
    "    new_x = fcst.ts._get_features_for_next_step(None)\n",
    "    original_preds = fcst.models_['LGBMRegressor'].predict(new_x)\n",
    "\n",
    "    expected = 1.1 * original_preds\n",
    "    actual = getattr(preds.groupby('unique_id')['LGBMRegressor'], attr)(1).values\n",
    "    np.testing.assert_equal(expected, actual)\n",
    "\n",
    "    fcst.ts._update_y(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383ca7ea-b275-4bc6-8ee7-3a41a46f3213",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "def test_cross_validation(data=non_std_series, add_exogenous=False):\n",
    "    n_windows = 2\n",
    "    window_size = 14\n",
    "    fcst = MLForecast(lgb.LGBMRegressor(), freq='D', lags=[7, 14])\n",
    "    if add_exogenous:\n",
    "        data = data.assign(ex1 = lambda x: np.arange(0, len(x)))\n",
    "    backtest_results = fcst.cross_validation(\n",
    "        data,\n",
    "        n_windows,\n",
    "        window_size,\n",
    "        id_col='some_id',\n",
    "        time_col='time',\n",
    "        target_col='value',\n",
    "        static_features=['some_id', 'static_0', 'static_1'],\n",
    "    )\n",
    "    renamer = {'some_id': 'unique_id', 'time': 'ds', 'value': 'y'}\n",
    "    backtest_results = backtest_results.rename(columns=renamer)\n",
    "    renamed = data.rename(columns=renamer)\n",
    "    manual_results = []\n",
    "    for cutoff, train, valid in backtest_splits(renamed, n_windows, window_size, 'unique_id', 'ds', 1):\n",
    "        fcst.fit(train, static_features=['unique_id', 'static_0', 'static_1'])\n",
    "        if add_exogenous:\n",
    "            dynamic_dfs = [valid.drop(columns=['y', 'static_0', 'static_1']).reset_index()]\n",
    "        else:\n",
    "            dynamic_dfs = None\n",
    "        pred = fcst.predict(window_size, dynamic_dfs=dynamic_dfs)\n",
    "        res = valid[['unique_id', 'ds', 'y']].copy()\n",
    "        res = res.merge(cutoff, on='unique_id')\n",
    "        res = res[['unique_id', 'ds', 'cutoff', 'y']].copy()\n",
    "        manual_results.append(res.merge(pred, on=['unique_id', 'ds'], how='left'))\n",
    "    manual_results = pd.concat(manual_results)\n",
    "    pd.testing.assert_frame_equal(backtest_results, manual_results)\n",
    "test_cross_validation()\n",
    "test_cross_validation(add_exogenous=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
