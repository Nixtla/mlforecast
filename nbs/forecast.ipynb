{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efa33fa-816f-463f-9215-9559b0ddd6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20376798-3d26-4c74-9e52-d5b657b7768a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccedaf1-56c9-4aaf-af7b-fe049df299ad",
   "metadata": {},
   "source": [
    "# MLForecast\n",
    "\n",
    "> Full pipeline encapsulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b089a52-e06d-49b1-9328-793cffe56045",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import copy\n",
    "import re\n",
    "import warnings\n",
    "from typing import TYPE_CHECKING, Callable, Dict, Iterable, List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, clone\n",
    "from utilsforecast.compat import DataFrame\n",
    "from utilsforecast.processing import (\n",
    "    assign_columns,\n",
    "    backtest_splits,\n",
    "    copy_if_pandas,\n",
    "    counts_by_id,\n",
    "    drop_index_if_pandas,\n",
    "    filter_with_mask,\n",
    "    is_in,\n",
    "    is_nan,\n",
    "    join,\n",
    "    maybe_compute_sort_indices,\n",
    "    take_rows,\n",
    "    to_numpy,\n",
    "    vertical_concat,\n",
    ")\n",
    "\n",
    "from mlforecast.core import (\n",
    "    DateFeature,\n",
    "    Freq,\n",
    "    LagTransforms,\n",
    "    Lags,\n",
    "    Models,\n",
    "    TargetTransform,\n",
    "    TimeSeries,\n",
    "    _name_models,\n",
    ")\n",
    "from mlforecast.grouped_array import GroupedArray\n",
    "if TYPE_CHECKING:\n",
    "    from mlforecast.lgb_cv import LightGBMCV\n",
    "from mlforecast.target_transforms import BaseGroupedArrayTargetTransform\n",
    "from mlforecast.utils import PredictionIntervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07073d89-e33c-41d6-9bd3-a6daa07fef5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastcore.test import test_warns, test_eq, test_ne, test_fail, test_close\n",
    "from nbdev import show_doc\n",
    "from sklearn import set_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d9a4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "set_config(display='text')\n",
    "warnings.simplefilter('ignore', UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574354b3-0d44-44df-9004-79f5c28f9d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _add_conformal_distribution_intervals(\n",
    "        fcst_df: DataFrame, \n",
    "        cs_df: DataFrame,\n",
    "        model_names: List[str],\n",
    "        level: List[Union[int, float]],\n",
    "        cs_n_windows: int,\n",
    "        cs_h: int,\n",
    "        n_series: int,\n",
    "        horizon: int,\n",
    "    ) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Adds conformal intervals to a `fcst_df` based on conformal scores `cs_df`.\n",
    "    `level` should be already sorted. This strategy creates forecasts paths\n",
    "    based on errors and calculate quantiles using those paths.\n",
    "    \"\"\"\n",
    "    fcst_df = copy_if_pandas(fcst_df, deep=False)\n",
    "    alphas = [100 - lv for lv in level]\n",
    "    cuts = [alpha / 200 for alpha in reversed(alphas)]\n",
    "    cuts.extend(1 - alpha / 200 for alpha in alphas)\n",
    "    for model in model_names:\n",
    "        scores = cs_df[model].to_numpy().reshape(cs_n_windows, n_series, cs_h)\n",
    "        # restrict scores to horizon\n",
    "        scores = scores[:,:,:horizon]\n",
    "        mean = fcst_df[model].to_numpy().reshape(1, n_series, -1)\n",
    "        scores = np.vstack([mean - scores, mean + scores])\n",
    "        quantiles = np.quantile(\n",
    "            scores,\n",
    "            cuts,\n",
    "            axis=0,\n",
    "        )\n",
    "        quantiles = quantiles.reshape(len(cuts), -1).T\n",
    "        lo_cols = [f\"{model}-lo-{lv}\" for lv in reversed(level)]\n",
    "        hi_cols = [f\"{model}-hi-{lv}\" for lv in level]\n",
    "        out_cols = lo_cols + hi_cols\n",
    "        fcst_df = assign_columns(fcst_df, out_cols, quantiles)\n",
    "    return fcst_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba40be92-6287-4d29-86bc-3905f798e8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _add_conformal_error_intervals(\n",
    "        fcst_df: DataFrame, \n",
    "        cs_df: DataFrame, \n",
    "        model_names: List[str],\n",
    "        level: List[Union[int, float]],\n",
    "        cs_n_windows: int,\n",
    "        cs_h: int,\n",
    "        n_series: int,\n",
    "        horizon: int,\n",
    "    ) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Adds conformal intervals to a `fcst_df` based on conformal scores `cs_df`.\n",
    "    `level` should be already sorted. This startegy creates prediction intervals\n",
    "    based on the absolute errors.\n",
    "    \"\"\"\n",
    "    fcst_df = copy_if_pandas(fcst_df, deep=False)\n",
    "    cuts = [lv / 100 for lv in level]\n",
    "    for model in model_names:\n",
    "        mean = fcst_df[model].to_numpy().ravel()\n",
    "        scores = cs_df[model].to_numpy().reshape(cs_n_windows, n_series, cs_h)\n",
    "        # restrict scores to horizon\n",
    "        scores = scores[:,:,:horizon]\n",
    "        quantiles = np.quantile(\n",
    "            scores,\n",
    "            cuts,\n",
    "            axis=0,\n",
    "        )\n",
    "        quantiles = quantiles.reshape(len(cuts), -1)\n",
    "        lo_cols = [f\"{model}-lo-{lv}\" for lv in reversed(level)]\n",
    "        hi_cols = [f\"{model}-hi-{lv}\" for lv in level]\n",
    "        quantiles = np.vstack([mean - quantiles[::-1], mean + quantiles]).T\n",
    "        columns = lo_cols + hi_cols\n",
    "        fcst_df = assign_columns(fcst_df, columns, quantiles)\n",
    "    return fcst_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fdbe39-a0fe-47bb-b284-b36c2044aa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _get_conformal_method(method: str):\n",
    "    available_methods = {\n",
    "        'conformal_distribution': _add_conformal_distribution_intervals,\n",
    "        'conformal_error': _add_conformal_error_intervals, \n",
    "    }\n",
    "    if method not in available_methods.keys():\n",
    "        raise ValueError(\n",
    "            f'prediction intervals method {method} not supported '\n",
    "            f'please choose one of {\", \".join(available_methods.keys())}'\n",
    "        )\n",
    "    return available_methods[method]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7901585-90c4-4950-a2a0-ec45e0a665d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_fail(lambda: _get_conformal_method('my_method'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8303523-551f-48ed-8f81-3cfd222d6e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class MLForecast:\n",
    "    def __init__(\n",
    "        self,\n",
    "        models: Models,\n",
    "        freq: Freq,\n",
    "        lags: Optional[Lags] = None,\n",
    "        lag_transforms: Optional[LagTransforms] = None,\n",
    "        date_features: Optional[Iterable[DateFeature]] = None,\n",
    "        num_threads: int = 1,\n",
    "        target_transforms: Optional[List[TargetTransform]] = None,\n",
    "        transforms_namer: Optional[Callable] = None,\n",
    "    ):\n",
    "        \"\"\"Forecasting pipeline\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        models : regressor or list of regressors\n",
    "            Models that will be trained and used to compute the forecasts.\n",
    "        freq : str or int or pd.offsets.BaseOffset\n",
    "            Pandas offset, pandas offset alias, e.g. 'D', 'W-THU' or integer denoting the frequency of the series.\n",
    "        lags : list of int, optional (default=None)\n",
    "            Lags of the target to use as features.\n",
    "        lag_transforms : dict of int to list of functions, optional (default=None)\n",
    "            Mapping of target lags to their transformations.\n",
    "        date_features : list of str or callable, optional (default=None)\n",
    "            Features computed from the dates. Can be pandas date attributes or functions that will take the dates as input.\n",
    "        num_threads : int (default=1)\n",
    "            Number of threads to use when computing the features.\n",
    "        target_transforms : list of transformers, optional(default=None)\n",
    "            Transformations that will be applied to the target before computing the features and restored after the forecasting step.\n",
    "        transforms_namer : callable, optional(default=None)\n",
    "            Function that takes a transformation (either function or class), a lag and extra arguments and produces a name.\n",
    "        \"\"\"\n",
    "        if not isinstance(models, dict) and not isinstance(models, list):\n",
    "            models = [models]\n",
    "        if isinstance(models, list):\n",
    "            model_names = _name_models([m.__class__.__name__ for m in models])            \n",
    "            models_with_names = dict(zip(model_names, models))\n",
    "        else:\n",
    "            models_with_names = models\n",
    "        self.models = models_with_names\n",
    "        self.ts = TimeSeries(\n",
    "            freq=freq,\n",
    "            lags=lags,\n",
    "            lag_transforms=lag_transforms,\n",
    "            date_features=date_features,\n",
    "            num_threads=num_threads,\n",
    "            target_transforms=target_transforms,\n",
    "            transforms_namer=transforms_namer,\n",
    "        )\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            f'{self.__class__.__name__}(models=[{\", \".join(self.models.keys())}], '\n",
    "            f'freq={self.freq}, '\n",
    "            f'lag_features={list(self.ts.transforms.keys())}, '\n",
    "            f'date_features={self.ts.date_features}, '\n",
    "            f'num_threads={self.ts.num_threads})'\n",
    "        )\n",
    "    \n",
    "    @property\n",
    "    def freq(self):\n",
    "        return self.ts.freq\n",
    "\n",
    "    @classmethod\n",
    "    def from_cv(cls, cv: 'LightGBMCV') -> 'MLForecast':\n",
    "        if not hasattr(cv, 'best_iteration_'):\n",
    "            raise ValueError('LightGBMCV object must be fitted first.')\n",
    "        import lightgbm as lgb\n",
    "        fcst = cls(\n",
    "            models=lgb.LGBMRegressor(**{**cv.params, 'n_estimators': cv.best_iteration_}),\n",
    "            freq=cv.ts.freq,\n",
    "        )\n",
    "        fcst.ts = copy.deepcopy(cv.ts)\n",
    "        return fcst\n",
    "\n",
    "    def preprocess(\n",
    "        self,\n",
    "        df: DataFrame,\n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        target_col: str = 'y',\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "        max_horizon: Optional[int] = None,\n",
    "        return_X_y: bool = False,\n",
    "        as_numpy: bool = False,\n",
    "    ) -> Union[DataFrame, Tuple[DataFrame, np.ndarray]]:\n",
    "        \"\"\"Add the features to `data`.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas DataFrame\n",
    "            Series data in long format.\n",
    "        id_col : str (default='unique_id')\n",
    "            Column that identifies each serie.\n",
    "        time_col : str (default='ds')\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str (default='y')\n",
    "            Column that contains the target.\n",
    "        static_features : list of str, optional (default=None)\n",
    "            Names of the features that are static and will be repeated when forecasting.\n",
    "        dropna : bool (default=True)\n",
    "            Drop rows with missing values produced by the transformations.\n",
    "        keep_last_n : int, optional (default=None)\n",
    "            Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it.\n",
    "        max_horizon : int, optional (default=None)\n",
    "            Train this many models, where each model will predict a specific horizon.\n",
    "        return_X_y : bool (default=False)\n",
    "            Return a tuple with the features and the target. If False will return a single dataframe.\n",
    "        as_numpy : bool (default = False)\n",
    "            Cast features to numpy array. Only works for `return_X_y=True`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        result : DataFrame or tuple of pandas Dataframe and a numpy array.\n",
    "            `df` plus added features and target(s).\n",
    "        \"\"\"     \n",
    "        return self.ts.fit_transform(\n",
    "            df,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            static_features=static_features,\n",
    "            dropna=dropna,\n",
    "            keep_last_n=keep_last_n,\n",
    "            max_horizon=max_horizon,\n",
    "            return_X_y=return_X_y,\n",
    "            as_numpy=as_numpy,\n",
    "        )\n",
    "\n",
    "    def fit_models(\n",
    "        self,\n",
    "        X: Union[DataFrame, np.ndarray],\n",
    "        y: np.ndarray,\n",
    "    ) -> 'MLForecast':\n",
    "        \"\"\"Manually train models. Use this if you called `MLForecast.preprocess` beforehand.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas or polars DataFrame or numpy array\n",
    "            Features.\n",
    "        y : numpy array.\n",
    "            Target.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : MLForecast\n",
    "            Forecast object with trained models.\n",
    "        \"\"\"\n",
    "        self.models_: Dict[str, Union[BaseEstimator, List[BaseEstimator]]] = {}\n",
    "        for name, model in self.models.items():\n",
    "            if y.ndim == 2 and y.shape[1] > 1:\n",
    "                self.models_[name] = []                \n",
    "                for col in range(y.shape[1]):\n",
    "                    keep = ~np.isnan(y[:, col])\n",
    "                    if isinstance(X, np.ndarray):\n",
    "                        # TODO: migrate to utils\n",
    "                        Xh = X[keep]\n",
    "                    else:\n",
    "                        Xh = filter_with_mask(X, keep)\n",
    "                    yh = y[keep, col]\n",
    "                    self.models_[name].append(clone(model).fit(Xh, yh))\n",
    "            else:\n",
    "                self.models_[name] = clone(model).fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def _conformity_scores(\n",
    "        self,\n",
    "        df: DataFrame,\n",
    "        id_col: str, \n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "        max_horizon: Optional[int] = None,\n",
    "        n_windows: int = 2,\n",
    "        h: int = 1,\n",
    "        as_numpy: bool = False,\n",
    "    ):\n",
    "        \"\"\"Compute conformity scores.\n",
    "        \n",
    "        We need at least two cross validation errors to compute\n",
    "        quantiles for prediction intervals (`n_windows=2`).\n",
    "        \n",
    "        The exception is raised by the PredictionIntervals data class.\n",
    "        \n",
    "        In this simplest case, we assume the width of the interval\n",
    "        is the same for all the forecasting horizon (`h=1`).\n",
    "        \"\"\"\n",
    "        cv_results = self.cross_validation(\n",
    "            df=df, \n",
    "            n_windows=n_windows,\n",
    "            h=h,\n",
    "            refit=False,\n",
    "            id_col=id_col, \n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            static_features=static_features,\n",
    "            dropna=dropna,\n",
    "            keep_last_n=keep_last_n,\n",
    "            max_horizon=max_horizon,\n",
    "            prediction_intervals=None,\n",
    "            as_numpy=as_numpy,\n",
    "        )\n",
    "        # conformity score for each model\n",
    "        for model in self.models.keys():\n",
    "            # compute absolute error for each model\n",
    "            abs_err = abs(cv_results[model] - cv_results[target_col])\n",
    "            cv_results = assign_columns(cv_results, model, abs_err)\n",
    "        return cv_results.drop(columns=target_col)\n",
    "\n",
    "    def _invert_transforms_fitted(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        if self.ts.target_transforms is None:\n",
    "            return df\n",
    "        if any(isinstance(tfm, BaseGroupedArrayTargetTransform) for tfm in self.ts.target_transforms):\n",
    "            model_cols = [c for c in df.columns if c not in (self.ts.id_col, self.ts.time_col)]\n",
    "            id_counts = counts_by_id(df, self.ts.id_col)\n",
    "            sizes = id_counts['counts'].to_numpy()\n",
    "            indptr = np.append(0, sizes.cumsum())\n",
    "        for tfm in self.ts.target_transforms[::-1]:\n",
    "            if isinstance(tfm, BaseGroupedArrayTargetTransform):\n",
    "                if self.ts._dropped_series is not None:\n",
    "                    tfm.idxs = np.delete(np.arange(self.ts.ga.n_groups), self.ts._dropped_series)\n",
    "                for col in model_cols:\n",
    "                    ga = GroupedArray(df[col].to_numpy(), indptr)\n",
    "                    ga = tfm.inverse_transform_fitted(ga)\n",
    "                    df = assign_columns(df, col, ga.data)\n",
    "                tfm.idxs = None\n",
    "            else:\n",
    "                df = tfm.inverse_transform(df)\n",
    "        return df\n",
    "\n",
    "    def _extract_X_y(\n",
    "        self,\n",
    "        prep: DataFrame,\n",
    "        target_col: str,\n",
    "    ) -> Tuple[Union[DataFrame, np.ndarray], np.ndarray]:\n",
    "        X = prep[self.ts.features_order_]\n",
    "        targets = [c for c in prep.columns if re.match(rf'^{target_col}\\d?$', c)]\n",
    "        if len(targets) == 1:\n",
    "            targets = targets[0]\n",
    "        y = prep[targets].to_numpy()        \n",
    "        return X, y\n",
    "\n",
    "    def _compute_fitted_values(\n",
    "        self,\n",
    "        base: DataFrame,\n",
    "        X: Union[DataFrame, np.ndarray],\n",
    "        y: np.ndarray,\n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        max_horizon: Optional[int],\n",
    "    ) -> DataFrame:\n",
    "        base = copy_if_pandas(base, deep=False)\n",
    "        sort_idxs = maybe_compute_sort_indices(base, id_col, time_col)\n",
    "        if sort_idxs is not None:\n",
    "            base = take_rows(base, sort_idxs)\n",
    "            X = take_rows(X, sort_idxs)\n",
    "            y = y[sort_idxs]\n",
    "        if max_horizon is None:\n",
    "            fitted_values = assign_columns(base, target_col, y)\n",
    "            for name, model in self.models_.items():\n",
    "                assert not isinstance(model, list)  # mypy\n",
    "                preds = model.predict(X)\n",
    "                fitted_values = assign_columns(fitted_values, name, preds)\n",
    "            fitted_values = self._invert_transforms_fitted(fitted_values)\n",
    "        else:\n",
    "            horizon_fitted_values = []\n",
    "            for horizon in range(max_horizon):\n",
    "                horizon_base = copy_if_pandas(base, deep=True)\n",
    "                horizon_base = assign_columns(horizon_base, target_col, y[:, horizon])\n",
    "                horizon_fitted_values.append(horizon_base)\n",
    "            for name, horizon_models in self.models_.items():\n",
    "                for horizon, model in enumerate(horizon_models):\n",
    "                    preds = model.predict(X)\n",
    "                    horizon_fitted_values[horizon] = assign_columns(\n",
    "                        horizon_fitted_values[horizon], name, preds\n",
    "                    )\n",
    "            for horizon, horizon_df in enumerate(horizon_fitted_values):\n",
    "                keep_mask = ~is_nan(horizon_df[target_col])\n",
    "                horizon_df = filter_with_mask(horizon_df, keep_mask)\n",
    "                horizon_df = copy_if_pandas(horizon_df, deep=True)\n",
    "                horizon_df = self._invert_transforms_fitted(horizon_df)\n",
    "                horizon_df = assign_columns(horizon_df, 'h', horizon + 1)\n",
    "                horizon_fitted_values[horizon] = horizon_df\n",
    "            fitted_values = vertical_concat(horizon_fitted_values, match_categories=False)\n",
    "        if self.ts.target_transforms is not None:\n",
    "            for tfm in self.ts.target_transforms[::-1]:            \n",
    "                if hasattr(tfm, 'store_fitted'):\n",
    "                    tfm.store_fitted = False\n",
    "                if hasattr(tfm, 'fitted_'):\n",
    "                    tfm.fitted_ = []            \n",
    "        return fitted_values\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        df: DataFrame,\n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        target_col: str = 'y',\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "        max_horizon: Optional[int] = None,\n",
    "        prediction_intervals: Optional[PredictionIntervals] = None,\n",
    "        fitted: bool = False,\n",
    "        as_numpy: bool = False,\n",
    "    ) -> 'MLForecast':\n",
    "        \"\"\"Apply the feature engineering and train the models.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas or polars DataFrame\n",
    "            Series data in long format.\n",
    "        id_col : str (default='unique_id')\n",
    "            Column that identifies each serie.\n",
    "        time_col : str (default='ds')\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str (default='y')\n",
    "            Column that contains the target.\n",
    "        static_features : list of str, optional (default=None)\n",
    "            Names of the features that are static and will be repeated when forecasting.\n",
    "                If `None`, will consider all columns (except id_col and time_col) as static.\n",
    "        dropna : bool (default=True)\n",
    "            Drop rows with missing values produced by the transformations.\n",
    "        keep_last_n : int, optional (default=None)\n",
    "            Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it.\n",
    "        max_horizon : int, optional (default=None)\n",
    "            Train this many models, where each model will predict a specific horizon.\n",
    "        prediction_intervals : PredictionIntervals, optional (default=None)\n",
    "            Configuration to calibrate prediction intervals (Conformal Prediction).\n",
    "        fitted : bool (default=False)\n",
    "            Save in-sample predictions.\n",
    "        as_numpy : bool (default = False)\n",
    "            Cast features to numpy array.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : MLForecast\n",
    "            Forecast object with series values and trained models.\n",
    "        \"\"\"\n",
    "        if fitted and self.ts.target_transforms is not None:\n",
    "            for tfm in self.ts.target_transforms:\n",
    "                if hasattr(tfm, 'store_fitted'):\n",
    "                    tfm.store_fitted = True\n",
    "        self._cs_df: Optional[DataFrame] = None\n",
    "        if prediction_intervals is not None:\n",
    "            self.prediction_intervals = prediction_intervals\n",
    "            self._cs_df = self._conformity_scores(\n",
    "                df=df, \n",
    "                id_col=id_col, \n",
    "                time_col=time_col,\n",
    "                target_col=target_col,\n",
    "                static_features=static_features,\n",
    "                dropna=dropna,\n",
    "                keep_last_n=keep_last_n,\n",
    "                n_windows=prediction_intervals.n_windows,\n",
    "                h=prediction_intervals.h,\n",
    "                as_numpy=as_numpy,\n",
    "            )\n",
    "        prep = self.preprocess(\n",
    "            df=df,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            static_features=static_features,\n",
    "            dropna=dropna,\n",
    "            keep_last_n=keep_last_n,\n",
    "            max_horizon=max_horizon,\n",
    "            return_X_y=not fitted,\n",
    "            as_numpy=as_numpy,\n",
    "        )\n",
    "        if isinstance(prep, tuple):\n",
    "            X, y = prep\n",
    "        else:\n",
    "            base = prep[[id_col, time_col]]\n",
    "            X, y = self._extract_X_y(prep, target_col)\n",
    "            if as_numpy:\n",
    "                X = to_numpy(X)\n",
    "            del prep\n",
    "        self.fit_models(X, y)\n",
    "        if fitted:\n",
    "            fitted_values = self._compute_fitted_values(\n",
    "                base=base,\n",
    "                X=X,\n",
    "                y=y,\n",
    "                id_col=id_col,\n",
    "                time_col=time_col,\n",
    "                target_col=target_col,\n",
    "                max_horizon=max_horizon,\n",
    "            )\n",
    "            fitted_values = drop_index_if_pandas(fitted_values)\n",
    "            self.fcst_fitted_values_ = fitted_values\n",
    "        return self\n",
    "\n",
    "    def forecast_fitted_values(self):\n",
    "        \"\"\"Access in-sample predictions.\"\"\"\n",
    "        if not hasattr(self, 'fcst_fitted_values_'):\n",
    "            raise Exception('Please run the `fit` method using `fitted=True`')\n",
    "        return self.fcst_fitted_values_\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        h: int,\n",
    "        before_predict_callback: Optional[Callable] = None,\n",
    "        after_predict_callback: Optional[Callable] = None,\n",
    "        new_df: Optional[DataFrame] = None,\n",
    "        level: Optional[List[Union[int, float]]] = None,\n",
    "        X_df: Optional[DataFrame] = None,\n",
    "        ids: Optional[List[str]] = None,\n",
    "    ) -> DataFrame:\n",
    "        \"\"\"Compute the predictions for the next `h` steps.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        h : int\n",
    "            Number of periods to predict.\n",
    "        before_predict_callback : callable, optional (default=None)\n",
    "            Function to call on the features before computing the predictions.\n",
    "                This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure.\n",
    "                The series identifier is on the index.\n",
    "        after_predict_callback : callable, optional (default=None)\n",
    "            Function to call on the predictions before updating the targets.\n",
    "                This function will take a pandas Series with the predictions and should return another one with the same structure.\n",
    "                The series identifier is on the index.   \n",
    "        new_df : pandas or polars DataFrame, optional (default=None)\n",
    "            Series data of new observations for which forecasts are to be generated. \n",
    "                This dataframe should have the same structure as the one used to fit the model, including any features and time series data. \n",
    "                If `new_df` is not None, the method will generate forecasts for the new observations.\n",
    "        level : list of ints or floats, optional (default=None)\n",
    "            Confidence levels between 0 and 100 for prediction intervals.\n",
    "        X_df : pandas or polars DataFrame, optional (default=None)\n",
    "            Dataframe with the future exogenous features. Should have the id column and the time column.\n",
    "        ids : list of str, optional (default=None)\n",
    "            List with subset of ids seen during training for which the forecasts should be computed.        \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        result : pandas or polars DataFrame\n",
    "            Predictions for each serie and timestep, with one column per model.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'models_'):\n",
    "            raise ValueError(\n",
    "                \"No fitted models found. You have to call fit or preprocess + fit_models. \"\n",
    "                \"If you used cross_validation before please fit again.\"\n",
    "            )\n",
    "        first_model_is_list = isinstance(next(iter(self.models_.values())), list)\n",
    "        max_horizon = self.ts.max_horizon\n",
    "        if first_model_is_list and max_horizon is None:\n",
    "            raise ValueError(\n",
    "                'Found one model per horizon but `max_horizon` is None. '\n",
    "                'If you ran preprocess after fit please run fit again.'\n",
    "            )\n",
    "        elif not first_model_is_list and max_horizon is not None:\n",
    "            raise ValueError(\n",
    "                'Found a single model for all horizons '\n",
    "                f'but `max_horizon` is {max_horizon}. '\n",
    "                'If you ran preprocess after fit please run fit again.'\n",
    "            )\n",
    "\n",
    "        if new_df is not None:\n",
    "            new_ts = TimeSeries(\n",
    "                freq=self.ts.freq,\n",
    "                lags=self.ts.lags, \n",
    "                lag_transforms=self.ts.lag_transforms,\n",
    "                date_features=self.ts.date_features, \n",
    "                num_threads=self.ts.num_threads,\n",
    "                target_transforms=self.ts.target_transforms,\n",
    "            )\n",
    "            new_ts._fit(\n",
    "                new_df,\n",
    "                id_col=self.ts.id_col,\n",
    "                time_col=self.ts.time_col,\n",
    "                target_col=self.ts.target_col, \n",
    "                static_features=self.ts.static_features,\n",
    "                keep_last_n=self.ts.keep_last_n,\n",
    "            )\n",
    "            new_ts.max_horizon = self.ts.max_horizon\n",
    "            new_ts.as_numpy = self.ts.as_numpy\n",
    "            ts = new_ts\n",
    "        else:\n",
    "            ts = self.ts\n",
    "            \n",
    "        forecasts = ts.predict(\n",
    "            models=self.models_,\n",
    "            horizon=h,\n",
    "            before_predict_callback=before_predict_callback,\n",
    "            after_predict_callback=after_predict_callback,\n",
    "            X_df=X_df,\n",
    "            ids=ids,\n",
    "        )\n",
    "        if level is not None:\n",
    "            if self._cs_df is None:\n",
    "                warn_msg = (\n",
    "                    'Please rerun the `fit` method passing a proper value '\n",
    "                    'to prediction intervals to compute them.'\n",
    "                )\n",
    "                warnings.warn(warn_msg, UserWarning)\n",
    "            else:\n",
    "                if (self.prediction_intervals.h != 1) and (self.prediction_intervals.h < h):\n",
    "                    raise ValueError(\n",
    "                        'The `h` argument of PredictionIntervals '\n",
    "                        'should be equal to one or greater or equal to `h`. '\n",
    "                        'Please rerun the `fit` method passing a proper value '\n",
    "                        'to prediction intervals.'\n",
    "                    )\n",
    "                if self.prediction_intervals.h == 1 and h > 1:\n",
    "                    warn_msg = (\n",
    "                        'Prediction intervals are calculated using 1-step ahead cross-validation, '\n",
    "                        'with a constant width for all horizons. To vary the error by horizon, '\n",
    "                        'pass PredictionIntervals(h=h) to the `prediction_intervals` '\n",
    "                        'argument when refitting the model.'\n",
    "                    )\n",
    "                    warnings.warn(warn_msg, UserWarning)\n",
    "                level_ = sorted(level)\n",
    "                model_names = self.models.keys()\n",
    "                conformal_method = _get_conformal_method(self.prediction_intervals.method)\n",
    "                if ids is not None:\n",
    "                    ids_mask = is_in(self._cs_df[self.ts.id_col], ids)\n",
    "                    cs_df = filter_with_mask(self._cs_df, ids_mask)\n",
    "                    n_series = len(ids)\n",
    "                else:\n",
    "                    cs_df = self._cs_df\n",
    "                    n_series = self.ts.ga.n_groups\n",
    "                forecasts = conformal_method(\n",
    "                    forecasts,\n",
    "                    cs_df,\n",
    "                    model_names=list(model_names),\n",
    "                    level=level_,\n",
    "                    cs_h=self.prediction_intervals.h,\n",
    "                    cs_n_windows=self.prediction_intervals.n_windows,\n",
    "                    n_series=n_series,\n",
    "                    horizon=h,\n",
    "                )\n",
    "        return forecasts\n",
    "\n",
    "    def cross_validation(\n",
    "        self,\n",
    "        df: DataFrame,\n",
    "        n_windows: int,\n",
    "        h: int,\n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        target_col: str = 'y',\n",
    "        step_size: Optional[int] = None,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "        refit: Union[bool, int] = True,\n",
    "        max_horizon: Optional[int] = None,\n",
    "        before_predict_callback: Optional[Callable] = None,\n",
    "        after_predict_callback: Optional[Callable] = None,\n",
    "        prediction_intervals: Optional[PredictionIntervals] = None,\n",
    "        level: Optional[List[Union[int, float]]] = None,\n",
    "        input_size: Optional[int] = None,\n",
    "        fitted: bool = False,\n",
    "        as_numpy: bool = False,\n",
    "    ) -> DataFrame:\n",
    "        \"\"\"Perform time series cross validation.\n",
    "        Creates `n_windows` splits where each window has `h` test periods, \n",
    "        trains the models, computes the predictions and merges the actuals.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas or polars DataFrame\n",
    "            Series data in long format.\n",
    "        n_windows : int\n",
    "            Number of windows to evaluate.\n",
    "        h : int\n",
    "            Forecast horizon.\n",
    "        id_col : str (default='unique_id')\n",
    "            Column that identifies each serie.\n",
    "        time_col : str (default='ds')\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str (default='y')\n",
    "            Column that contains the target.\n",
    "        step_size : int, optional (default=None)\n",
    "            Step size between each cross validation window. If None it will be equal to `h`.\n",
    "        static_features : list of str, optional (default=None)\n",
    "            Names of the features that are static and will be repeated when forecasting.\n",
    "        dropna : bool (default=True)\n",
    "            Drop rows with missing values produced by the transformations.\n",
    "        keep_last_n : int, optional (default=None)\n",
    "            Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it.\n",
    "        max_horizon: int, optional (default=None)\n",
    "            Train this many models, where each model will predict a specific horizon.            \n",
    "        refit : bool or int (default=True)\n",
    "            Retrain model for each cross validation window.\n",
    "            If False, the models are trained at the beginning and then used to predict each window.\n",
    "            If positive int, the models are retrained every `refit` windows.\n",
    "        before_predict_callback : callable, optional (default=None)\n",
    "            Function to call on the features before computing the predictions.\n",
    "                This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure.\n",
    "                The series identifier is on the index.\n",
    "        after_predict_callback : callable, optional (default=None)\n",
    "            Function to call on the predictions before updating the targets.\n",
    "                This function will take a pandas Series with the predictions and should return another one with the same structure.\n",
    "                The series identifier is on the index.\n",
    "        prediction_intervals : PredictionIntervals, optional (default=None)\n",
    "            Configuration to calibrate prediction intervals (Conformal Prediction).\n",
    "        level : list of ints or floats, optional (default=None)\n",
    "            Confidence levels between 0 and 100 for prediction intervals.\n",
    "        input_size : int, optional (default=None)\n",
    "            Maximum training samples per serie in each window. If None, will use an expanding window.\n",
    "        fitted : bool (default=False)\n",
    "            Store the in-sample predictions.\n",
    "        as_numpy : bool (default = False)\n",
    "            Cast features to numpy array.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        result : pandas or polars DataFrame\n",
    "            Predictions for each window with the series id, timestamp, last train date, target value and predictions from each model.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        self.cv_models_ = []\n",
    "        self.ts._validate_freq(df, time_col)\n",
    "        splits = backtest_splits(\n",
    "            df,\n",
    "            n_windows=n_windows,\n",
    "            h=h,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            freq=self.freq,\n",
    "            step_size=step_size,\n",
    "            input_size=input_size,\n",
    "        )\n",
    "        self.cv_fitted_values_ = []\n",
    "        for i_window, (cutoffs, train, valid) in enumerate(splits):\n",
    "            should_fit = i_window == 0 or (refit > 0 and i_window % refit == 0)\n",
    "            if should_fit:\n",
    "                self.fit(\n",
    "                    train,\n",
    "                    id_col=id_col,\n",
    "                    time_col=time_col,\n",
    "                    target_col=target_col,\n",
    "                    static_features=static_features,\n",
    "                    dropna=dropna,\n",
    "                    keep_last_n=keep_last_n,\n",
    "                    max_horizon=max_horizon,\n",
    "                    prediction_intervals=prediction_intervals,\n",
    "                    fitted=fitted,\n",
    "                    as_numpy=as_numpy,\n",
    "                )\n",
    "                self.cv_models_.append(self.models_)\n",
    "                if fitted:\n",
    "                    self.cv_fitted_values_.append(assign_columns(self.fcst_fitted_values_, 'fold', i_window))\n",
    "            if fitted and not should_fit:\n",
    "                if self.ts.target_transforms is not None:\n",
    "                    for tfm in self.ts.target_transforms:\n",
    "                        if hasattr(tfm, 'store_fitted'):\n",
    "                            tfm.store_fitted = True\n",
    "                prep = self.preprocess(\n",
    "                    train,\n",
    "                    id_col=id_col,\n",
    "                    time_col=time_col,\n",
    "                    target_col=target_col,\n",
    "                    static_features=static_features,\n",
    "                    dropna=dropna,\n",
    "                    keep_last_n=keep_last_n,\n",
    "                    max_horizon=max_horizon,\n",
    "                    return_X_y=False,\n",
    "                )\n",
    "                assert not isinstance(prep, tuple)\n",
    "                base = prep[[id_col, time_col]]\n",
    "                train_X, train_y = self._extract_X_y(prep, target_col)\n",
    "                if as_numpy:\n",
    "                    train_X = to_numpy(train_X)\n",
    "                del prep\n",
    "                fitted_values = self._compute_fitted_values(\n",
    "                    base=base,\n",
    "                    X=train_X,\n",
    "                    y=train_y,\n",
    "                    id_col=id_col,\n",
    "                    time_col=time_col,\n",
    "                    target_col=target_col,\n",
    "                    max_horizon=max_horizon,\n",
    "                )\n",
    "                fitted_values = assign_columns(fitted_values, 'fold', i_window)\n",
    "                self.cv_fitted_values_.append(fitted_values)\n",
    "            static = [c for c in self.ts.static_features_.columns if c != id_col]\n",
    "            dynamic = [\n",
    "                c for c in valid.columns if c not in static + [id_col, time_col, target_col]\n",
    "            ]\n",
    "            if dynamic:\n",
    "                X_df: Optional[DataFrame] = valid.drop(columns=static + [target_col])\n",
    "            else:\n",
    "                X_df = None\n",
    "            y_pred = self.predict(\n",
    "                h=h,\n",
    "                before_predict_callback=before_predict_callback,\n",
    "                after_predict_callback=after_predict_callback,\n",
    "                new_df=train if not should_fit else None,\n",
    "                level=level,\n",
    "                X_df=X_df,\n",
    "            )\n",
    "            y_pred = join(y_pred, cutoffs, on=id_col, how='left')\n",
    "            result = join(\n",
    "                valid[[id_col, time_col, target_col]],\n",
    "                y_pred,\n",
    "                on=[id_col, time_col],\n",
    "            )\n",
    "            sort_idxs = maybe_compute_sort_indices(result, id_col, time_col)\n",
    "            if sort_idxs is not None:\n",
    "                result = take_rows(result, sort_idxs)\n",
    "            if result.shape[0] < valid.shape[0]:\n",
    "                raise ValueError(\n",
    "                    \"Cross validation result produced less results than expected. \"\n",
    "                    \"Please verify that the frequency set on the MLForecast constructor matches your series' \"\n",
    "                    \"and that there aren't any missing periods.\"\n",
    "                )\n",
    "            results.append(result)\n",
    "        del self.models_\n",
    "        out = vertical_concat(results, match_categories=False)\n",
    "        out = drop_index_if_pandas(out)\n",
    "        first_out_cols = [id_col, time_col, 'cutoff', target_col]\n",
    "        remaining_cols = [c for c in out.columns if c not in first_out_cols]\n",
    "        return out[first_out_cols + remaining_cols]\n",
    "\n",
    "    def cross_validation_fitted_values(self):\n",
    "        if not getattr(self, 'cv_fitted_values_', []):\n",
    "            raise ValueError('Please run cross_validation with fitted=True first.')\n",
    "        out = vertical_concat(self.cv_fitted_values_, match_categories=False)            \n",
    "        first_out_cols = [self.ts.id_col, self.ts.time_col, 'fold', self.ts.target_col]        \n",
    "        remaining_cols = [c for c in out.columns if c not in first_out_cols]\n",
    "        out = drop_index_if_pandas(out)\n",
    "        return out[first_out_cols + remaining_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5ec811-8876-4daa-84a2-2ebe0559a02b",
   "metadata": {},
   "source": [
    "### Data\n",
    "This shows an example with just 4 series of the M4 dataset. If you want to run it yourself on all of them, you can refer to [this notebook](https://www.kaggle.com/code/lemuz90/m4-competition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3032775-f610-4091-a750-73219d904c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from datasetsforecast.m4 import M4, M4Info\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from utilsforecast.plotting import plot_series\n",
    "from window_ops.ewm import ewm_mean\n",
    "from window_ops.expanding import expanding_mean\n",
    "from window_ops.rolling import rolling_mean\n",
    "\n",
    "from mlforecast.lgb_cv import LightGBMCV\n",
    "from mlforecast.target_transforms import Differences, LocalStandardScaler\n",
    "from mlforecast.utils import generate_daily_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c30ee9-38f3-4ef2-ab50-6ffc1e887f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>86796</th>\n",
       "      <td>H196</td>\n",
       "      <td>1</td>\n",
       "      <td>11.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86797</th>\n",
       "      <td>H196</td>\n",
       "      <td>2</td>\n",
       "      <td>11.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86798</th>\n",
       "      <td>H196</td>\n",
       "      <td>3</td>\n",
       "      <td>11.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86799</th>\n",
       "      <td>H196</td>\n",
       "      <td>4</td>\n",
       "      <td>10.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86800</th>\n",
       "      <td>H196</td>\n",
       "      <td>5</td>\n",
       "      <td>10.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325235</th>\n",
       "      <td>H413</td>\n",
       "      <td>1004</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325236</th>\n",
       "      <td>H413</td>\n",
       "      <td>1005</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325237</th>\n",
       "      <td>H413</td>\n",
       "      <td>1006</td>\n",
       "      <td>47.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325238</th>\n",
       "      <td>H413</td>\n",
       "      <td>1007</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325239</th>\n",
       "      <td>H413</td>\n",
       "      <td>1008</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4032 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       unique_id    ds     y\n",
       "86796       H196     1  11.8\n",
       "86797       H196     2  11.4\n",
       "86798       H196     3  11.1\n",
       "86799       H196     4  10.8\n",
       "86800       H196     5  10.6\n",
       "...          ...   ...   ...\n",
       "325235      H413  1004  99.0\n",
       "325236      H413  1005  88.0\n",
       "325237      H413  1006  47.0\n",
       "325238      H413  1007  41.0\n",
       "325239      H413  1008  34.0\n",
       "\n",
       "[4032 rows x 3 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group = 'Hourly'\n",
    "await M4.async_download('data', group=group)\n",
    "df, *_ = M4.load(directory='data', group=group)\n",
    "df['ds'] = df['ds'].astype('int')\n",
    "ids = df['unique_id'].unique()\n",
    "random.seed(0)\n",
    "sample_ids = random.choices(ids, k=4)\n",
    "sample_df = df[df['unique_id'].isin(sample_ids)]\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaa4e47-4eee-43b6-8dba-1807bb00fda5",
   "metadata": {},
   "source": [
    "We now split this data into train and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91485631-d855-47f9-bdb4-94130b23c67a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3840, 3), (192, 3))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info = M4Info[group]\n",
    "horizon = info.horizon\n",
    "valid = sample_df.groupby('unique_id').tail(horizon)\n",
    "train = sample_df.drop(valid.index)\n",
    "train.shape, valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3566ee6f-597d-45e2-a0b0-cd725c818e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/mlforecast/blob/main/mlforecast/forecast.py#L137){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MLForecast\n",
       "\n",
       ">      MLForecast (models:Union[sklearn.base.BaseEstimator,List[sklearn.base.Bas\n",
       ">                  eEstimator],Dict[str,sklearn.base.BaseEstimator]],\n",
       ">                  freq:Union[int,str,pandas._libs.tslibs.offsets.BaseOffset],\n",
       ">                  lags:Optional[Iterable[int]]=None, lag_transforms:Optional[Di\n",
       ">                  ct[int,List[Union[Callable,Tuple[Callable,Any]]]]]=None,\n",
       ">                  date_features:Optional[Iterable[Union[str,Callable]]]=None,\n",
       ">                  num_threads:int=1, target_transforms:Optional[List[Union[mlfo\n",
       ">                  recast.target_transforms.BaseTargetTransform,mlforecast.targe\n",
       ">                  t_transforms.BaseGroupedArrayTargetTransform]]]=None,\n",
       ">                  transforms_namer:Optional[Callable]=None)\n",
       "\n",
       "Forecasting pipeline\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| models | Union |  | Models that will be trained and used to compute the forecasts. |\n",
       "| freq | Union |  | Pandas offset, pandas offset alias, e.g. 'D', 'W-THU' or integer denoting the frequency of the series. |\n",
       "| lags | Optional | None | Lags of the target to use as features. |\n",
       "| lag_transforms | Optional | None | Mapping of target lags to their transformations. |\n",
       "| date_features | Optional | None | Features computed from the dates. Can be pandas date attributes or functions that will take the dates as input. |\n",
       "| num_threads | int | 1 | Number of threads to use when computing the features. |\n",
       "| target_transforms | Optional | None | Transformations that will be applied to the target before computing the features and restored after the forecasting step. |\n",
       "| transforms_namer | Optional | None | Function that takes a transformation (either function or class), a lag and extra arguments and produces a name. |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/mlforecast/blob/main/mlforecast/forecast.py#L137){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MLForecast\n",
       "\n",
       ">      MLForecast (models:Union[sklearn.base.BaseEstimator,List[sklearn.base.Bas\n",
       ">                  eEstimator],Dict[str,sklearn.base.BaseEstimator]],\n",
       ">                  freq:Union[int,str,pandas._libs.tslibs.offsets.BaseOffset],\n",
       ">                  lags:Optional[Iterable[int]]=None, lag_transforms:Optional[Di\n",
       ">                  ct[int,List[Union[Callable,Tuple[Callable,Any]]]]]=None,\n",
       ">                  date_features:Optional[Iterable[Union[str,Callable]]]=None,\n",
       ">                  num_threads:int=1, target_transforms:Optional[List[Union[mlfo\n",
       ">                  recast.target_transforms.BaseTargetTransform,mlforecast.targe\n",
       ">                  t_transforms.BaseGroupedArrayTargetTransform]]]=None,\n",
       ">                  transforms_namer:Optional[Callable]=None)\n",
       "\n",
       "Forecasting pipeline\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| models | Union |  | Models that will be trained and used to compute the forecasts. |\n",
       "| freq | Union |  | Pandas offset, pandas offset alias, e.g. 'D', 'W-THU' or integer denoting the frequency of the series. |\n",
       "| lags | Optional | None | Lags of the target to use as features. |\n",
       "| lag_transforms | Optional | None | Mapping of target lags to their transformations. |\n",
       "| date_features | Optional | None | Features computed from the dates. Can be pandas date attributes or functions that will take the dates as input. |\n",
       "| num_threads | int | 1 | Number of threads to use when computing the features. |\n",
       "| target_transforms | Optional | None | Transformations that will be applied to the target before computing the features and restored after the forecasting step. |\n",
       "| transforms_namer | Optional | None | Function that takes a transformation (either function or class), a lag and extra arguments and produces a name. |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MLForecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc50c95b-359d-4e49-8f1f-40dab560c722",
   "metadata": {},
   "source": [
    "The MLForecast object encapsulates the feature engineering + training the models + forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ebf59f-0fb7-45ff-b355-127952413245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLForecast(models=[LGBMRegressor], freq=1, lag_features=['lag24', 'lag48', 'lag72', 'lag96', 'lag120', 'lag144', 'lag168', 'ewm_mean_lag48_alpha0.3'], date_features=[], num_threads=1)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcst = MLForecast(\n",
    "    models=lgb.LGBMRegressor(random_state=0, verbosity=-1),\n",
    "    freq=1,\n",
    "    lags=[24 * (i+1) for i in range(7)],\n",
    "    lag_transforms={\n",
    "        48: [(ewm_mean, 0.3)],\n",
    "    },\n",
    "    num_threads=1,\n",
    "    target_transforms=[Differences([24])],\n",
    ")\n",
    "fcst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10229c7-a69e-48fe-9ce8-f4754c58bec1",
   "metadata": {},
   "source": [
    "Once we have this setup we can compute the features and fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d42544-91a7-4c08-a190-925343e3c0b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/mlforecast/blob/main/mlforecast/forecast.py#L443){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MLForecast.fit\n",
       "\n",
       ">      MLForecast.fit\n",
       ">                      (df:Union[pandas.core.frame.DataFrame,polars.dataframe.fr\n",
       ">                      ame.DataFrame], id_col:str='unique_id',\n",
       ">                      time_col:str='ds', target_col:str='y',\n",
       ">                      static_features:Optional[List[str]]=None,\n",
       ">                      dropna:bool=True, keep_last_n:Optional[int]=None,\n",
       ">                      max_horizon:Optional[int]=None, prediction_intervals:Opti\n",
       ">                      onal[mlforecast.utils.PredictionIntervals]=None,\n",
       ">                      fitted:bool=False, as_numpy:bool=False)\n",
       "\n",
       "Apply the feature engineering and train the models.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | Union |  | Series data in long format. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| time_col | str | ds | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str | y | Column that contains the target. |\n",
       "| static_features | Optional | None | Names of the features that are static and will be repeated when forecasting.<br>    If `None`, will consider all columns (except id_col and time_col) as static. |\n",
       "| dropna | bool | True | Drop rows with missing values produced by the transformations. |\n",
       "| keep_last_n | Optional | None | Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it. |\n",
       "| max_horizon | Optional | None | Train this many models, where each model will predict a specific horizon. |\n",
       "| prediction_intervals | Optional | None | Configuration to calibrate prediction intervals (Conformal Prediction). |\n",
       "| fitted | bool | False | Save in-sample predictions. |\n",
       "| as_numpy | bool | False | Cast features to numpy array. |\n",
       "| **Returns** | **MLForecast** |  | **Forecast object with series values and trained models.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/mlforecast/blob/main/mlforecast/forecast.py#L443){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MLForecast.fit\n",
       "\n",
       ">      MLForecast.fit\n",
       ">                      (df:Union[pandas.core.frame.DataFrame,polars.dataframe.fr\n",
       ">                      ame.DataFrame], id_col:str='unique_id',\n",
       ">                      time_col:str='ds', target_col:str='y',\n",
       ">                      static_features:Optional[List[str]]=None,\n",
       ">                      dropna:bool=True, keep_last_n:Optional[int]=None,\n",
       ">                      max_horizon:Optional[int]=None, prediction_intervals:Opti\n",
       ">                      onal[mlforecast.utils.PredictionIntervals]=None,\n",
       ">                      fitted:bool=False, as_numpy:bool=False)\n",
       "\n",
       "Apply the feature engineering and train the models.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | Union |  | Series data in long format. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| time_col | str | ds | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str | y | Column that contains the target. |\n",
       "| static_features | Optional | None | Names of the features that are static and will be repeated when forecasting.<br>    If `None`, will consider all columns (except id_col and time_col) as static. |\n",
       "| dropna | bool | True | Drop rows with missing values produced by the transformations. |\n",
       "| keep_last_n | Optional | None | Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it. |\n",
       "| max_horizon | Optional | None | Train this many models, where each model will predict a specific horizon. |\n",
       "| prediction_intervals | Optional | None | Configuration to calibrate prediction intervals (Conformal Prediction). |\n",
       "| fitted | bool | False | Save in-sample predictions. |\n",
       "| as_numpy | bool | False | Cast features to numpy array. |\n",
       "| **Returns** | **MLForecast** |  | **Forecast object with series values and trained models.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MLForecast.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb588b4-f01c-422b-98cf-21ef03cf8ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst = MLForecast(\n",
    "    models=lgb.LGBMRegressor(random_state=0, verbosity=-1),\n",
    "    freq=1,\n",
    "    lags=[24 * (i+1) for i in range(7)],\n",
    "    lag_transforms={\n",
    "        48: [(ewm_mean, 0.3)],\n",
    "    },\n",
    "    num_threads=1,\n",
    "    target_transforms=[Differences([24])],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714006c0-c883-480f-b3bd-bc70ea0555ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst.fit(train, fitted=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99552a5c-e923-4ee3-83e4-7d4dfdae92a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/mlforecast/blob/main/mlforecast/forecast.py#L544){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MLForecast.forecast_fitted_values\n",
       "\n",
       ">      MLForecast.forecast_fitted_values ()\n",
       "\n",
       "Access in-sample predictions."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/mlforecast/blob/main/mlforecast/forecast.py#L544){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MLForecast.forecast_fitted_values\n",
       "\n",
       ">      MLForecast.forecast_fitted_values ()\n",
       "\n",
       "Access in-sample predictions."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MLForecast.forecast_fitted_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d5014e-0afb-494d-80cf-62e5184f997e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>LGBMRegressor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H196</td>\n",
       "      <td>193</td>\n",
       "      <td>12.7</td>\n",
       "      <td>12.671271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H196</td>\n",
       "      <td>194</td>\n",
       "      <td>12.3</td>\n",
       "      <td>12.271271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H196</td>\n",
       "      <td>195</td>\n",
       "      <td>11.9</td>\n",
       "      <td>11.871271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H196</td>\n",
       "      <td>196</td>\n",
       "      <td>11.7</td>\n",
       "      <td>11.671271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H196</td>\n",
       "      <td>197</td>\n",
       "      <td>11.4</td>\n",
       "      <td>11.471271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3067</th>\n",
       "      <td>H413</td>\n",
       "      <td>956</td>\n",
       "      <td>59.0</td>\n",
       "      <td>68.280574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3068</th>\n",
       "      <td>H413</td>\n",
       "      <td>957</td>\n",
       "      <td>58.0</td>\n",
       "      <td>70.427570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3069</th>\n",
       "      <td>H413</td>\n",
       "      <td>958</td>\n",
       "      <td>53.0</td>\n",
       "      <td>44.767965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3070</th>\n",
       "      <td>H413</td>\n",
       "      <td>959</td>\n",
       "      <td>38.0</td>\n",
       "      <td>48.691257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3071</th>\n",
       "      <td>H413</td>\n",
       "      <td>960</td>\n",
       "      <td>46.0</td>\n",
       "      <td>46.652238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3072 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     unique_id   ds     y  LGBMRegressor\n",
       "0         H196  193  12.7      12.671271\n",
       "1         H196  194  12.3      12.271271\n",
       "2         H196  195  11.9      11.871271\n",
       "3         H196  196  11.7      11.671271\n",
       "4         H196  197  11.4      11.471271\n",
       "...        ...  ...   ...            ...\n",
       "3067      H413  956  59.0      68.280574\n",
       "3068      H413  957  58.0      70.427570\n",
       "3069      H413  958  53.0      44.767965\n",
       "3070      H413  959  38.0      48.691257\n",
       "3071      H413  960  46.0      46.652238\n",
       "\n",
       "[3072 rows x 4 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcst.forecast_fitted_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412615bf-3a39-414a-83f8-8469587f4df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# check that the fitted target from the fitted_values matches the original target after applying the transformation\n",
    "fcst2 = MLForecast(\n",
    "    models=lgb.LGBMRegressor(random_state=0, verbosity=-1),\n",
    "    freq=1,\n",
    "    lags=[24 * (i+1) for i in range(7)],\n",
    "    lag_transforms={\n",
    "        48: [(ewm_mean, 0.3)],\n",
    "    },\n",
    "    num_threads=1,\n",
    "    target_transforms=[Differences([24]), LocalStandardScaler()],\n",
    ")\n",
    "fcst2.fit(train, fitted=True)\n",
    "fitted_vals = fcst2.forecast_fitted_values()\n",
    "train_restored = train.merge(\n",
    "    fitted_vals.drop(columns='LGBMRegressor'),\n",
    "    on=['unique_id', 'ds'],\n",
    "    suffixes=('_expected', '_actual')\n",
    ")\n",
    "np.testing.assert_allclose(\n",
    "    train_restored['y_expected'].values,\n",
    "    train_restored['y_actual'].values,\n",
    ")\n",
    "\n",
    "# check fitted + max_horizon\n",
    "max_horizon = 7\n",
    "fcst2.fit(train, fitted=True, max_horizon=max_horizon)\n",
    "max_horizon_fitted_values = fcst2.forecast_fitted_values()\n",
    "# h is 1 to max_horizon\n",
    "np.testing.assert_equal(\n",
    "    np.sort(max_horizon_fitted_values['h'].unique()),\n",
    "    np.arange(1, max_horizon + 1),\n",
    ")\n",
    "# predictions for the first horizon are equal to the recursive\n",
    "pd.testing.assert_frame_equal(\n",
    "    fitted_vals.reset_index(drop=True),\n",
    "    max_horizon_fitted_values[max_horizon_fitted_values['h'] == 1].drop(columns='h'),\n",
    ")\n",
    "# restored values match\n",
    "xx = max_horizon_fitted_values[lambda x: x['unique_id'].eq('H413')].pivot_table(\n",
    "    index=['unique_id', 'ds'], columns='h', values='y'\n",
    ").loc['H413']\n",
    "first_ds = xx.index.min()\n",
    "last_ds = xx.index.max()\n",
    "for h in range(1, max_horizon):\n",
    "    np.testing.assert_allclose(\n",
    "        xx.loc[first_ds + h :, 1].values,\n",
    "        xx.loc[: last_ds - h, h + 1].values,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5794fec5-fcc4-48c4-8f35-8b5add89a698",
   "metadata": {},
   "source": [
    "Once we've run this we're ready to compute our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121e73f8-b88b-4c82-967e-9629587d7616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/mlforecast/blob/main/mlforecast/forecast.py#L550){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MLForecast.predict\n",
       "\n",
       ">      MLForecast.predict (h:int,\n",
       ">                          before_predict_callback:Optional[Callable]=None,\n",
       ">                          after_predict_callback:Optional[Callable]=None, new_d\n",
       ">                          f:Union[pandas.core.frame.DataFrame,polars.dataframe.\n",
       ">                          frame.DataFrame,NoneType]=None,\n",
       ">                          level:Optional[List[Union[int,float]]]=None, X_df:Uni\n",
       ">                          on[pandas.core.frame.DataFrame,polars.dataframe.frame\n",
       ">                          .DataFrame,NoneType]=None,\n",
       ">                          ids:Optional[List[str]]=None)\n",
       "\n",
       "Compute the predictions for the next `h` steps.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| h | int |  | Number of periods to predict. |\n",
       "| before_predict_callback | Optional | None | Function to call on the features before computing the predictions.<br>    This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure.<br>    The series identifier is on the index. |\n",
       "| after_predict_callback | Optional | None | Function to call on the predictions before updating the targets.<br>    This function will take a pandas Series with the predictions and should return another one with the same structure.<br>    The series identifier is on the index.    |\n",
       "| new_df | Union | None | Series data of new observations for which forecasts are to be generated. <br>    This dataframe should have the same structure as the one used to fit the model, including any features and time series data. <br>    If `new_df` is not None, the method will generate forecasts for the new observations. |\n",
       "| level | Optional | None | Confidence levels between 0 and 100 for prediction intervals. |\n",
       "| X_df | Union | None | Dataframe with the future exogenous features. Should have the id column and the time column. |\n",
       "| ids | Optional | None | List with subset of ids seen during training for which the forecasts should be computed.         |\n",
       "| **Returns** | **Union** |  | **Predictions for each serie and timestep, with one column per model.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/mlforecast/blob/main/mlforecast/forecast.py#L550){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MLForecast.predict\n",
       "\n",
       ">      MLForecast.predict (h:int,\n",
       ">                          before_predict_callback:Optional[Callable]=None,\n",
       ">                          after_predict_callback:Optional[Callable]=None, new_d\n",
       ">                          f:Union[pandas.core.frame.DataFrame,polars.dataframe.\n",
       ">                          frame.DataFrame,NoneType]=None,\n",
       ">                          level:Optional[List[Union[int,float]]]=None, X_df:Uni\n",
       ">                          on[pandas.core.frame.DataFrame,polars.dataframe.frame\n",
       ">                          .DataFrame,NoneType]=None,\n",
       ">                          ids:Optional[List[str]]=None)\n",
       "\n",
       "Compute the predictions for the next `h` steps.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| h | int |  | Number of periods to predict. |\n",
       "| before_predict_callback | Optional | None | Function to call on the features before computing the predictions.<br>    This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure.<br>    The series identifier is on the index. |\n",
       "| after_predict_callback | Optional | None | Function to call on the predictions before updating the targets.<br>    This function will take a pandas Series with the predictions and should return another one with the same structure.<br>    The series identifier is on the index.    |\n",
       "| new_df | Union | None | Series data of new observations for which forecasts are to be generated. <br>    This dataframe should have the same structure as the one used to fit the model, including any features and time series data. <br>    If `new_df` is not None, the method will generate forecasts for the new observations. |\n",
       "| level | Optional | None | Confidence levels between 0 and 100 for prediction intervals. |\n",
       "| X_df | Union | None | Dataframe with the future exogenous features. Should have the id column and the time column. |\n",
       "| ids | Optional | None | List with subset of ids seen during training for which the forecasts should be computed.         |\n",
       "| **Returns** | **Union** |  | **Predictions for each serie and timestep, with one column per model.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MLForecast.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c28426-e150-4fe8-b1f9-bcad401cbbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = fcst.predict(horizon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6cd880-ec12-46dc-9627-90d8521f9c38",
   "metadata": {},
   "source": [
    "We can see at a couple of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b00bc9d-412e-48c6-b4a7-d27803b21b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = valid.merge(predictions, on=['unique_id', 'ds'])\n",
    "fig = plot_series(train, results, max_insample_length=0)\n",
    "fig.savefig('figs/forecast__predict.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a31d2b4-b9e5-427b-9cfa-30a1733969f1",
   "metadata": {},
   "source": [
    "![](figs/forecast__predict.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20724d12-d6b4-4c21-a38d-fd4c71461c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test new_df argument\n",
    "pd.testing.assert_frame_equal(\n",
    "    fcst.predict(horizon, new_df=train),\n",
    "    predictions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfb0f7f-ea89-44a6-b557-ac12a278f111",
   "metadata": {},
   "source": [
    "#### Prediction intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b367dee8-fab4-406a-8a9e-c4ec86003a7d",
   "metadata": {},
   "source": [
    "With `MLForecast`, you can generate prediction intervals using Conformal Prediction. To configure Conformal Prediction, you need to pass an instance of the `PredictionIntervals` class to the `prediction_intervals` argument of the `fit` method. The class takes three parameters: `n_windows`, `h` and `method`.\n",
    "\n",
    "* `n_windows` represents the number of cross-validation windows used to calibrate the intervals\n",
    "* `h` is the forecast horizon\n",
    "* `method` can be `conformal_distribution` or `conformal_error`; `conformal_distribution` (default) creates forecasts paths based on the cross-validation errors and calculate quantiles using those paths, on the other hand `conformal_error` calculates the error quantiles to produce prediction intervals. The strategy will adjust the intervals for each horizon step, resulting in different widths for each step. Please note that a minimum of 2 cross-validation windows must be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cd3346-f1e5-4ccb-b39c-a257d45d51fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst.fit(\n",
    "    train,\n",
    "    prediction_intervals=PredictionIntervals(n_windows=3, h=48)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2273a783-1596-4d5e-b084-9252eb6c23af",
   "metadata": {},
   "source": [
    "After that, you just have to include your desired confidence levels to the `predict` method using the `level` argument. Levels must lie between 0 and 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02793e5f-cb32-4c61-8417-a89a531e1ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_w_intervals = fcst.predict(48, level=[50, 80, 95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba4b53a-c49a-4846-aafb-a604a3b158e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>LGBMRegressor</th>\n",
       "      <th>LGBMRegressor-lo-95</th>\n",
       "      <th>LGBMRegressor-lo-80</th>\n",
       "      <th>LGBMRegressor-lo-50</th>\n",
       "      <th>LGBMRegressor-hi-50</th>\n",
       "      <th>LGBMRegressor-hi-80</th>\n",
       "      <th>LGBMRegressor-hi-95</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H196</td>\n",
       "      <td>961</td>\n",
       "      <td>16.071271</td>\n",
       "      <td>15.958042</td>\n",
       "      <td>15.971271</td>\n",
       "      <td>16.005091</td>\n",
       "      <td>16.137452</td>\n",
       "      <td>16.171271</td>\n",
       "      <td>16.184501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H196</td>\n",
       "      <td>962</td>\n",
       "      <td>15.671271</td>\n",
       "      <td>15.553632</td>\n",
       "      <td>15.553632</td>\n",
       "      <td>15.578632</td>\n",
       "      <td>15.763911</td>\n",
       "      <td>15.788911</td>\n",
       "      <td>15.788911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H196</td>\n",
       "      <td>963</td>\n",
       "      <td>15.271271</td>\n",
       "      <td>15.153632</td>\n",
       "      <td>15.153632</td>\n",
       "      <td>15.162452</td>\n",
       "      <td>15.380091</td>\n",
       "      <td>15.388911</td>\n",
       "      <td>15.388911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H196</td>\n",
       "      <td>964</td>\n",
       "      <td>14.971271</td>\n",
       "      <td>14.858042</td>\n",
       "      <td>14.871271</td>\n",
       "      <td>14.905091</td>\n",
       "      <td>15.037452</td>\n",
       "      <td>15.071271</td>\n",
       "      <td>15.084501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H196</td>\n",
       "      <td>965</td>\n",
       "      <td>14.671271</td>\n",
       "      <td>14.553632</td>\n",
       "      <td>14.553632</td>\n",
       "      <td>14.562452</td>\n",
       "      <td>14.780091</td>\n",
       "      <td>14.788911</td>\n",
       "      <td>14.788911</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  unique_id   ds  LGBMRegressor  LGBMRegressor-lo-95  LGBMRegressor-lo-80  \\\n",
       "0      H196  961      16.071271            15.958042            15.971271   \n",
       "1      H196  962      15.671271            15.553632            15.553632   \n",
       "2      H196  963      15.271271            15.153632            15.153632   \n",
       "3      H196  964      14.971271            14.858042            14.871271   \n",
       "4      H196  965      14.671271            14.553632            14.553632   \n",
       "\n",
       "   LGBMRegressor-lo-50  LGBMRegressor-hi-50  LGBMRegressor-hi-80  \\\n",
       "0            16.005091            16.137452            16.171271   \n",
       "1            15.578632            15.763911            15.788911   \n",
       "2            15.162452            15.380091            15.388911   \n",
       "3            14.905091            15.037452            15.071271   \n",
       "4            14.562452            14.780091            14.788911   \n",
       "\n",
       "   LGBMRegressor-hi-95  \n",
       "0            16.184501  \n",
       "1            15.788911  \n",
       "2            15.388911  \n",
       "3            15.084501  \n",
       "4            14.788911  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_w_intervals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1381372c-3571-4db9-a76e-355d7e0ef118",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| head\n",
    "# test we can forecast horizon lower than h \n",
    "# with prediction intervals\n",
    "for method in ['conformal_distribution', 'conformal_errors']:\n",
    "    fcst.fit(\n",
    "        train, \n",
    "        prediction_intervals=PredictionIntervals(n_windows=3, h=48)\n",
    "    )\n",
    "\n",
    "    preds_h_lower_h = fcst.predict(1, level=[50, 80, 95])\n",
    "    preds_h_lower_h = fcst.predict(30, level=[50, 80, 95])\n",
    "\n",
    "    # test monotonicity of intervals\n",
    "    test_eq(\n",
    "        preds_h_lower_h.filter(regex='lo|hi').apply(\n",
    "            lambda x: x.is_monotonic_increasing,\n",
    "            axis=1\n",
    "        ).sum(),\n",
    "        len(preds_h_lower_h)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef791c7d-aa1f-423b-a362-22919cac778a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_fail(lambda: fcst.predict(49, level=[68]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04359aaa-ac32-4fe6-ae30-e83df75bb4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test we can recover point forecasts\n",
    "test_eq(\n",
    "    predictions,\n",
    "    predictions_w_intervals[predictions.columns]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0772c6f9-469e-4ce2-9e41-0e027edb4fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test we can recover mean forecasts with level 0\n",
    "test_close(\n",
    "    predictions['LGBMRegressor'].values,\n",
    "    fcst.predict(48, level=[0])['LGBMRegressor-lo-0'].values,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b94e8e-395f-4435-a03c-4749c1bea41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test monotonicity of intervals\n",
    "test_eq(\n",
    "    predictions_w_intervals.filter(regex='lo|hi').apply(\n",
    "        lambda x: x.is_monotonic_increasing,\n",
    "        axis=1\n",
    "    ).sum(),\n",
    "    len(predictions_w_intervals)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117a4080-fa17-4b13-a0b6-1d1afe70dfb8",
   "metadata": {},
   "source": [
    "Let's explore the generated intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6b4d95-f4c7-4774-9af3-646b17bb458e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = valid.merge(predictions_w_intervals, on=['unique_id', 'ds'])\n",
    "fig = plot_series(train, results, max_insample_length=0, level=[50, 80, 95])\n",
    "fig.savefig('figs/forecast__predict_intervals.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058a8cf6-5b32-464b-b6f9-60905eb82781",
   "metadata": {},
   "source": [
    "![](figs/forecast__predict_intervals.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6686093-f139-4241-8f7b-3a8bd0f2b47c",
   "metadata": {},
   "source": [
    "If you want to reduce the computational time and produce intervals with the same width for the whole forecast horizon, simple pass `h=1` to the `PredictionIntervals` class. The caveat of this strategy is that in some cases, variance of the absolute residuals maybe be small (even zero), so the intervals may be too narrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa76f77e-7341-4fc7-af11-80c3531d304b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst.fit(\n",
    "    train,  \n",
    "    prediction_intervals=PredictionIntervals(n_windows=3, h=1)\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ec6367-0fa3-4e40-b771-0b7182813cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_w_intervals_ws_1 = fcst.predict(48, level=[80, 90, 95])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d754fa-a20c-4cd2-bf6d-6274cd9d126d",
   "metadata": {},
   "source": [
    "Let's explore the generated intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a8f039-eee0-401a-afbe-43098d49839c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = valid.merge(predictions_w_intervals_ws_1, on=['unique_id', 'ds'])\n",
    "fig = plot_series(train, results, max_insample_length=0, level=[90])\n",
    "fig.savefig('figs/forecast__predict_intervals_window_size_1.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511f7426-dfa8-4988-80d3-6092e65211cb",
   "metadata": {},
   "source": [
    "![](figs/forecast__predict_intervals_window_size_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73473351-3386-4959-9a41-4ce4ed64354f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test indexed data, datetime ds\n",
    "fcst_test = MLForecast(\n",
    "    models=lgb.LGBMRegressor(random_state=0, verbosity=-1),\n",
    "    freq='D',\n",
    "    lags=[1],\n",
    "    num_threads=1,\n",
    ")\n",
    "df_test = generate_daily_series(1)\n",
    "fcst_test.fit(\n",
    "    df_test,\n",
    "    # prediction_intervals=PredictionIntervals()\n",
    ")\n",
    "pred_test = fcst_test.predict(12)\n",
    "pred_int_test = fcst_test.predict(12, level=[80, 90])\n",
    "# test same structure\n",
    "test_eq(\n",
    "    pred_test,\n",
    "    pred_int_test[pred_test.columns]\n",
    ")\n",
    "# test monotonicity of intervals\n",
    "test_eq(\n",
    "    pred_int_test.filter(regex='lo|hi').apply(\n",
    "        lambda x: x.is_monotonic_increasing,\n",
    "        axis=1\n",
    "    ).sum(),\n",
    "    len(pred_int_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505aadfd-076d-42e6-8103-aeef4eef61b7",
   "metadata": {},
   "source": [
    "#### Forecast using a pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1bef71-d893-46ee-82dd-6819193f2d0e",
   "metadata": {},
   "source": [
    "MLForecast allows you to use a pretrained model to generate forecasts for a new dataset. Simply provide a pandas dataframe containing the new observations as the value for the `new_df` argument when calling the `predict` method. The dataframe should have the same structure as the one used to fit the model, including any features and time series data. The function will then use the pretrained model to generate forecasts for the new observations. This allows you to easily apply a pretrained model to a new dataset and generate forecasts without the need to retrain the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efdc679-5201-42b6-ba70-80a7db322c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "ercot_df = pd.read_csv('https://datasets-nixtla.s3.amazonaws.com/ERCOT-clean.csv')\n",
    "# we have to convert the ds column to integers\n",
    "# since MLForecast was trained with that structure\n",
    "ercot_df['ds'] = np.arange(1, len(ercot_df) + 1)\n",
    "# use the `new_df` argument to pass the ercot dataset \n",
    "ercot_fcsts = fcst.predict(horizon, new_df=ercot_df)\n",
    "fig = plot_series(ercot_df, ercot_fcsts, max_insample_length=48 * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3201b7-73e5-468a-bbb5-51c4dd48a871",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "fig.get_axes()[0].set_title('ERCOT forecasts trained on M4-Hourly dataset')\n",
    "fig.savefig('figs/forecast__ercot.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60c5bab-453b-4ae8-a282-87233c4333e3",
   "metadata": {},
   "source": [
    "![](figs/forecast__ercot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b150efe0-bc84-483e-905f-c936e0aa0818",
   "metadata": {},
   "source": [
    "If you want to take a look at the data that will be used to train the models you can call `Forecast.preprocess`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21260f3-bd8f-4143-bb60-deb9ca0d46e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/mlforecast/blob/main/mlforecast/forecast.py#L212){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MLForecast.preprocess\n",
       "\n",
       ">      MLForecast.preprocess\n",
       ">                             (df:Union[pandas.core.frame.DataFrame,polars.dataf\n",
       ">                             rame.frame.DataFrame], id_col:str='unique_id',\n",
       ">                             time_col:str='ds', target_col:str='y',\n",
       ">                             static_features:Optional[List[str]]=None,\n",
       ">                             dropna:bool=True, keep_last_n:Optional[int]=None,\n",
       ">                             max_horizon:Optional[int]=None,\n",
       ">                             return_X_y:bool=False, as_numpy:bool=False)\n",
       "\n",
       "Add the features to `data`.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | Union |  | Series data in long format. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| time_col | str | ds | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str | y | Column that contains the target. |\n",
       "| static_features | Optional | None | Names of the features that are static and will be repeated when forecasting. |\n",
       "| dropna | bool | True | Drop rows with missing values produced by the transformations. |\n",
       "| keep_last_n | Optional | None | Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it. |\n",
       "| max_horizon | Optional | None | Train this many models, where each model will predict a specific horizon. |\n",
       "| return_X_y | bool | False | Return a tuple with the features and the target. If False will return a single dataframe. |\n",
       "| as_numpy | bool | False | Cast features to numpy array. Only works for `return_X_y=True`. |\n",
       "| **Returns** | **Union** |  | **`df` plus added features and target(s).** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/mlforecast/blob/main/mlforecast/forecast.py#L212){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MLForecast.preprocess\n",
       "\n",
       ">      MLForecast.preprocess\n",
       ">                             (df:Union[pandas.core.frame.DataFrame,polars.dataf\n",
       ">                             rame.frame.DataFrame], id_col:str='unique_id',\n",
       ">                             time_col:str='ds', target_col:str='y',\n",
       ">                             static_features:Optional[List[str]]=None,\n",
       ">                             dropna:bool=True, keep_last_n:Optional[int]=None,\n",
       ">                             max_horizon:Optional[int]=None,\n",
       ">                             return_X_y:bool=False, as_numpy:bool=False)\n",
       "\n",
       "Add the features to `data`.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | Union |  | Series data in long format. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| time_col | str | ds | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str | y | Column that contains the target. |\n",
       "| static_features | Optional | None | Names of the features that are static and will be repeated when forecasting. |\n",
       "| dropna | bool | True | Drop rows with missing values produced by the transformations. |\n",
       "| keep_last_n | Optional | None | Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it. |\n",
       "| max_horizon | Optional | None | Train this many models, where each model will predict a specific horizon. |\n",
       "| return_X_y | bool | False | Return a tuple with the features and the target. If False will return a single dataframe. |\n",
       "| as_numpy | bool | False | Cast features to numpy array. Only works for `return_X_y=True`. |\n",
       "| **Returns** | **Union** |  | **`df` plus added features and target(s).** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MLForecast.preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53214db1-05dd-4440-88b2-94763769e520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>lag24</th>\n",
       "      <th>lag48</th>\n",
       "      <th>lag72</th>\n",
       "      <th>lag96</th>\n",
       "      <th>lag120</th>\n",
       "      <th>lag144</th>\n",
       "      <th>lag168</th>\n",
       "      <th>ewm_mean_lag48_alpha0.3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>86988</th>\n",
       "      <td>H196</td>\n",
       "      <td>193</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.002810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86989</th>\n",
       "      <td>H196</td>\n",
       "      <td>194</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.031967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86990</th>\n",
       "      <td>H196</td>\n",
       "      <td>195</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.052377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86991</th>\n",
       "      <td>H196</td>\n",
       "      <td>196</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.036664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86992</th>\n",
       "      <td>H196</td>\n",
       "      <td>197</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.025665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325187</th>\n",
       "      <td>H413</td>\n",
       "      <td>956</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-53.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>-21.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7.963225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325188</th>\n",
       "      <td>H413</td>\n",
       "      <td>957</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>-46.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>-19.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>8.574257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325189</th>\n",
       "      <td>H413</td>\n",
       "      <td>958</td>\n",
       "      <td>16.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>-36.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>-13.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.501980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325190</th>\n",
       "      <td>H413</td>\n",
       "      <td>959</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-31.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>3.151386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325191</th>\n",
       "      <td>H413</td>\n",
       "      <td>960</td>\n",
       "      <td>15.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-17.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>-18.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.405970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3072 rows  11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       unique_id   ds     y  lag24  lag48  lag72  lag96  lag120  lag144  \\\n",
       "86988       H196  193   0.1    0.0    0.0    0.0    0.3     0.1     0.1   \n",
       "86989       H196  194   0.1   -0.1    0.1    0.0    0.3     0.1     0.1   \n",
       "86990       H196  195   0.1   -0.1    0.1    0.0    0.3     0.1     0.2   \n",
       "86991       H196  196   0.1    0.0    0.0    0.0    0.3     0.2     0.1   \n",
       "86992       H196  197   0.0    0.0    0.0    0.1    0.2     0.2     0.1   \n",
       "...          ...  ...   ...    ...    ...    ...    ...     ...     ...   \n",
       "325187      H413  956   0.0   10.0    1.0    6.0  -53.0    44.0   -21.0   \n",
       "325188      H413  957   9.0   10.0   10.0   -7.0  -46.0    27.0   -19.0   \n",
       "325189      H413  958  16.0    8.0    5.0   -9.0  -36.0    32.0   -13.0   \n",
       "325190      H413  959  -3.0   17.0   -7.0    2.0  -31.0    22.0     5.0   \n",
       "325191      H413  960  15.0   11.0   -6.0   -5.0  -17.0    22.0   -18.0   \n",
       "\n",
       "        lag168  ewm_mean_lag48_alpha0.3  \n",
       "86988      0.3                 0.002810  \n",
       "86989      0.3                 0.031967  \n",
       "86990      0.1                 0.052377  \n",
       "86991      0.2                 0.036664  \n",
       "86992      0.2                 0.025665  \n",
       "...        ...                      ...  \n",
       "325187    21.0                 7.963225  \n",
       "325188    24.0                 8.574257  \n",
       "325189     8.0                 7.501980  \n",
       "325190    -2.0                 3.151386  \n",
       "325191    10.0                 0.405970  \n",
       "\n",
       "[3072 rows x 11 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_df = fcst.preprocess(train)\n",
    "prep_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9959305d-8273-4994-8bab-4e1b5fd95966",
   "metadata": {},
   "source": [
    "If we do this we then have to call `Forecast.fit_models`, since this only stores the series information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91c259f-fba6-4832-9701-5a7ba92e5cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# transforms namer\n",
    "def tfms_namer(tfm, lag, *args):\n",
    "    return f'hello_from_{tfm.__name__}'\n",
    "\n",
    "fcst2 = MLForecast(\n",
    "    models=LinearRegression(),\n",
    "    freq=1,\n",
    "    lag_transforms={1: [expanding_mean]}, \n",
    "    transforms_namer=tfms_namer,\n",
    ")\n",
    "prep = fcst2.preprocess(train)\n",
    "assert 'hello_from_expanding_mean' in prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6863d9e7-92ac-4302-8271-9e70cd5531aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/mlforecast/blob/main/mlforecast/forecast.py#L268){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MLForecast.fit_models\n",
       "\n",
       ">      MLForecast.fit_models (X:Union[pandas.core.frame.DataFrame,polars.datafra\n",
       ">                             me.frame.DataFrame,numpy.ndarray],\n",
       ">                             y:numpy.ndarray)\n",
       "\n",
       "Manually train models. Use this if you called `MLForecast.preprocess` beforehand.\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| X | Union | Features. |\n",
       "| y | ndarray | Target. |\n",
       "| **Returns** | **MLForecast** | **Forecast object with trained models.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/mlforecast/blob/main/mlforecast/forecast.py#L268){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MLForecast.fit_models\n",
       "\n",
       ">      MLForecast.fit_models (X:Union[pandas.core.frame.DataFrame,polars.datafra\n",
       ">                             me.frame.DataFrame,numpy.ndarray],\n",
       ">                             y:numpy.ndarray)\n",
       "\n",
       "Manually train models. Use this if you called `MLForecast.preprocess` beforehand.\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| X | Union | Features. |\n",
       "| y | ndarray | Target. |\n",
       "| **Returns** | **MLForecast** | **Forecast object with trained models.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MLForecast.fit_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69d243e-a271-4518-8a0d-432716fc8d2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLForecast(models=[LGBMRegressor], freq=1, lag_features=['lag24', 'lag48', 'lag72', 'lag96', 'lag120', 'lag144', 'lag168', 'ewm_mean_lag48_alpha0.3'], date_features=[], num_threads=1)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = prep_df.drop(columns=['unique_id', 'ds', 'y']), prep_df['y']\n",
    "fcst.fit_models(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5eec03-60fa-4f35-b649-97ed5be35f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = fcst.predict(horizon)\n",
    "pd.testing.assert_frame_equal(predictions, predictions2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94559ce-7814-4719-a9c7-ec1a2c2b14d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test intervals multioutput\n",
    "max_horizon = 24\n",
    "individual_fcst_intervals = fcst.fit(\n",
    "    train,\n",
    "    max_horizon=max_horizon,\n",
    "    prediction_intervals=PredictionIntervals(h=max_horizon)\n",
    ")\n",
    "individual_preds = fcst.predict(max_horizon)\n",
    "individual_preds_intervals = fcst.predict(max_horizon, level=[90, 80])\n",
    "# test monotonicity of intervals\n",
    "test_eq(\n",
    "    individual_preds_intervals.filter(regex='lo|hi').apply(\n",
    "        lambda x: x.is_monotonic_increasing,\n",
    "        axis=1\n",
    "    ).sum(),\n",
    "    len(individual_preds_intervals)\n",
    ")\n",
    "# test we can recover point forecasts with intervals\n",
    "test_eq(\n",
    "    individual_preds,\n",
    "    individual_preds_intervals[individual_preds.columns]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb935f6-e31b-4631-9456-346740b94a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# check for bad max_horizon & models_ states before predict\n",
    "fcst = MLForecast(\n",
    "    models=[LinearRegression()],\n",
    "    freq=1,\n",
    "    lags=[12],\n",
    ")\n",
    "fcst.fit(df, max_horizon=2)\n",
    "fcst.preprocess(df, max_horizon=None)\n",
    "test_fail(lambda: fcst.predict(1), contains='Found one model per horizon')\n",
    "\n",
    "fcst.fit(df, max_horizon=None)\n",
    "fcst.preprocess(df, max_horizon=2)\n",
    "test_fail(lambda: fcst.predict(1), contains='Found a single model for all horizons')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e65bc38-7a1b-4f52-ace2-677fa9c3561d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/mlforecast/blob/main/mlforecast/forecast.py#L689){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MLForecast.cross_validation\n",
       "\n",
       ">      MLForecast.cross_validation\n",
       ">                                   (df:Union[pandas.core.frame.DataFrame,polars\n",
       ">                                   .dataframe.frame.DataFrame], n_windows:int,\n",
       ">                                   h:int, id_col:str='unique_id',\n",
       ">                                   time_col:str='ds', target_col:str='y',\n",
       ">                                   step_size:Optional[int]=None,\n",
       ">                                   static_features:Optional[List[str]]=None,\n",
       ">                                   dropna:bool=True,\n",
       ">                                   keep_last_n:Optional[int]=None,\n",
       ">                                   refit:Union[bool,int]=True,\n",
       ">                                   max_horizon:Optional[int]=None, before_predi\n",
       ">                                   ct_callback:Optional[Callable]=None, after_p\n",
       ">                                   redict_callback:Optional[Callable]=None, pre\n",
       ">                                   diction_intervals:Optional[mlforecast.utils.\n",
       ">                                   PredictionIntervals]=None,\n",
       ">                                   level:Optional[List[Union[int,float]]]=None,\n",
       ">                                   input_size:Optional[int]=None,\n",
       ">                                   fitted:bool=False, as_numpy:bool=False)\n",
       "\n",
       "Perform time series cross validation.\n",
       "Creates `n_windows` splits where each window has `h` test periods, \n",
       "trains the models, computes the predictions and merges the actuals.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | Union |  | Series data in long format. |\n",
       "| n_windows | int |  | Number of windows to evaluate. |\n",
       "| h | int |  | Forecast horizon. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| time_col | str | ds | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str | y | Column that contains the target. |\n",
       "| step_size | Optional | None | Step size between each cross validation window. If None it will be equal to `h`. |\n",
       "| static_features | Optional | None | Names of the features that are static and will be repeated when forecasting. |\n",
       "| dropna | bool | True | Drop rows with missing values produced by the transformations. |\n",
       "| keep_last_n | Optional | None | Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it. |\n",
       "| refit | Union | True | Retrain model for each cross validation window.<br>If False, the models are trained at the beginning and then used to predict each window.<br>If positive int, the models are retrained every `refit` windows. |\n",
       "| max_horizon | Optional | None |  |\n",
       "| before_predict_callback | Optional | None | Function to call on the features before computing the predictions.<br>    This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure.<br>    The series identifier is on the index. |\n",
       "| after_predict_callback | Optional | None | Function to call on the predictions before updating the targets.<br>    This function will take a pandas Series with the predictions and should return another one with the same structure.<br>    The series identifier is on the index. |\n",
       "| prediction_intervals | Optional | None | Configuration to calibrate prediction intervals (Conformal Prediction). |\n",
       "| level | Optional | None | Confidence levels between 0 and 100 for prediction intervals. |\n",
       "| input_size | Optional | None | Maximum training samples per serie in each window. If None, will use an expanding window. |\n",
       "| fitted | bool | False | Store the in-sample predictions. |\n",
       "| as_numpy | bool | False | Cast features to numpy array. |\n",
       "| **Returns** | **Union** |  | **Predictions for each window with the series id, timestamp, last train date, target value and predictions from each model.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/mlforecast/blob/main/mlforecast/forecast.py#L689){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MLForecast.cross_validation\n",
       "\n",
       ">      MLForecast.cross_validation\n",
       ">                                   (df:Union[pandas.core.frame.DataFrame,polars\n",
       ">                                   .dataframe.frame.DataFrame], n_windows:int,\n",
       ">                                   h:int, id_col:str='unique_id',\n",
       ">                                   time_col:str='ds', target_col:str='y',\n",
       ">                                   step_size:Optional[int]=None,\n",
       ">                                   static_features:Optional[List[str]]=None,\n",
       ">                                   dropna:bool=True,\n",
       ">                                   keep_last_n:Optional[int]=None,\n",
       ">                                   refit:Union[bool,int]=True,\n",
       ">                                   max_horizon:Optional[int]=None, before_predi\n",
       ">                                   ct_callback:Optional[Callable]=None, after_p\n",
       ">                                   redict_callback:Optional[Callable]=None, pre\n",
       ">                                   diction_intervals:Optional[mlforecast.utils.\n",
       ">                                   PredictionIntervals]=None,\n",
       ">                                   level:Optional[List[Union[int,float]]]=None,\n",
       ">                                   input_size:Optional[int]=None,\n",
       ">                                   fitted:bool=False, as_numpy:bool=False)\n",
       "\n",
       "Perform time series cross validation.\n",
       "Creates `n_windows` splits where each window has `h` test periods, \n",
       "trains the models, computes the predictions and merges the actuals.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | Union |  | Series data in long format. |\n",
       "| n_windows | int |  | Number of windows to evaluate. |\n",
       "| h | int |  | Forecast horizon. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| time_col | str | ds | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str | y | Column that contains the target. |\n",
       "| step_size | Optional | None | Step size between each cross validation window. If None it will be equal to `h`. |\n",
       "| static_features | Optional | None | Names of the features that are static and will be repeated when forecasting. |\n",
       "| dropna | bool | True | Drop rows with missing values produced by the transformations. |\n",
       "| keep_last_n | Optional | None | Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it. |\n",
       "| refit | Union | True | Retrain model for each cross validation window.<br>If False, the models are trained at the beginning and then used to predict each window.<br>If positive int, the models are retrained every `refit` windows. |\n",
       "| max_horizon | Optional | None |  |\n",
       "| before_predict_callback | Optional | None | Function to call on the features before computing the predictions.<br>    This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure.<br>    The series identifier is on the index. |\n",
       "| after_predict_callback | Optional | None | Function to call on the predictions before updating the targets.<br>    This function will take a pandas Series with the predictions and should return another one with the same structure.<br>    The series identifier is on the index. |\n",
       "| prediction_intervals | Optional | None | Configuration to calibrate prediction intervals (Conformal Prediction). |\n",
       "| level | Optional | None | Confidence levels between 0 and 100 for prediction intervals. |\n",
       "| input_size | Optional | None | Maximum training samples per serie in each window. If None, will use an expanding window. |\n",
       "| fitted | bool | False | Store the in-sample predictions. |\n",
       "| as_numpy | bool | False | Cast features to numpy array. |\n",
       "| **Returns** | **Union** |  | **Predictions for each window with the series id, timestamp, last train date, target value and predictions from each model.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MLForecast.cross_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f183fe13-2f33-4f5e-8801-690ebe9e2ada",
   "metadata": {},
   "source": [
    "If we would like to know how good our forecast will be for a specific model and set of features then we can perform cross validation. What cross validation does is take our data and split it in two parts, where the first part is used for training and the second one for validation. Since the data is time dependant we usually take the last *x* observations from our data as the validation set.\n",
    "\n",
    "This process is implemented in `MLForecast.cross_validation`, which takes our data and performs the process described above for `n_windows` times where each window has `h` validation samples in it. For example, if we have 100 samples and we want to perform 2 backtests each of size 14, the splits will be as follows:\n",
    "\n",
    "1. Train: 1 to 72. Validation: 73 to 86.\n",
    "2. Train: 1 to 86. Validation: 87 to 100.\n",
    "\n",
    "You can control the size between each cross validation window using the `step_size` argument. For example, if we have 100 samples and we want to perform 2 backtests each of size 14 and move one step ahead in each fold (`step_size=1`), the splits will be as follows:\n",
    "\n",
    "1. Train: 1 to 85. Validation: 86 to 99.\n",
    "2. Train: 1 to 86. Validation: 87 to 100.\n",
    "\n",
    "You can also perform cross validation without refitting your models for each window by setting `refit=False`. This allows you to evaluate the performance of your models using multiple window sizes without having to retrain them each time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9667452-b4b4-4d8c-a952-b7e70a7e92f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>cutoff</th>\n",
       "      <th>y</th>\n",
       "      <th>LGBMRegressor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H196</td>\n",
       "      <td>865</td>\n",
       "      <td>864</td>\n",
       "      <td>15.5</td>\n",
       "      <td>15.373393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H196</td>\n",
       "      <td>866</td>\n",
       "      <td>864</td>\n",
       "      <td>15.1</td>\n",
       "      <td>14.973393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H196</td>\n",
       "      <td>867</td>\n",
       "      <td>864</td>\n",
       "      <td>14.8</td>\n",
       "      <td>14.673393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H196</td>\n",
       "      <td>868</td>\n",
       "      <td>864</td>\n",
       "      <td>14.4</td>\n",
       "      <td>14.373393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H196</td>\n",
       "      <td>869</td>\n",
       "      <td>864</td>\n",
       "      <td>14.2</td>\n",
       "      <td>14.073393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>H413</td>\n",
       "      <td>956</td>\n",
       "      <td>912</td>\n",
       "      <td>59.0</td>\n",
       "      <td>64.284167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>H413</td>\n",
       "      <td>957</td>\n",
       "      <td>912</td>\n",
       "      <td>58.0</td>\n",
       "      <td>64.830429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>H413</td>\n",
       "      <td>958</td>\n",
       "      <td>912</td>\n",
       "      <td>53.0</td>\n",
       "      <td>40.726851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>H413</td>\n",
       "      <td>959</td>\n",
       "      <td>912</td>\n",
       "      <td>38.0</td>\n",
       "      <td>42.739657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>H413</td>\n",
       "      <td>960</td>\n",
       "      <td>912</td>\n",
       "      <td>46.0</td>\n",
       "      <td>52.802769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>384 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    unique_id   ds  cutoff     y  LGBMRegressor\n",
       "0        H196  865     864  15.5      15.373393\n",
       "1        H196  866     864  15.1      14.973393\n",
       "2        H196  867     864  14.8      14.673393\n",
       "3        H196  868     864  14.4      14.373393\n",
       "4        H196  869     864  14.2      14.073393\n",
       "..        ...  ...     ...   ...            ...\n",
       "379      H413  956     912  59.0      64.284167\n",
       "380      H413  957     912  58.0      64.830429\n",
       "381      H413  958     912  53.0      40.726851\n",
       "382      H413  959     912  38.0      42.739657\n",
       "383      H413  960     912  46.0      52.802769\n",
       "\n",
       "[384 rows x 5 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcst = MLForecast(\n",
    "    models=lgb.LGBMRegressor(random_state=0, verbosity=-1),\n",
    "    freq=1,\n",
    "    lags=[24 * (i+1) for i in range(7)],\n",
    "    lag_transforms={\n",
    "        1: [(rolling_mean, 24)],\n",
    "        24: [(rolling_mean, 24)],\n",
    "        48: [(ewm_mean, 0.3)],\n",
    "    },\n",
    "    num_threads=1,\n",
    "    target_transforms=[Differences([24])],\n",
    ")\n",
    "cv_results = fcst.cross_validation(\n",
    "    train,\n",
    "    n_windows=2,\n",
    "    h=horizon,\n",
    "    step_size=horizon,\n",
    "    fitted=True,\n",
    ")\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a71926a-fbfe-4d2f-99c3-804fe6e2908f",
   "metadata": {},
   "source": [
    "Since we set `fitted=True` we can access the predictions for the training sets as well with the `cross_validation_fitted_values` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c295f09-1416-4e55-8701-7a5ef8e2200d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>fold</th>\n",
       "      <th>y</th>\n",
       "      <th>LGBMRegressor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H196</td>\n",
       "      <td>193</td>\n",
       "      <td>0</td>\n",
       "      <td>12.7</td>\n",
       "      <td>12.673393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H196</td>\n",
       "      <td>194</td>\n",
       "      <td>0</td>\n",
       "      <td>12.3</td>\n",
       "      <td>12.273393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H196</td>\n",
       "      <td>195</td>\n",
       "      <td>0</td>\n",
       "      <td>11.9</td>\n",
       "      <td>11.873393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H196</td>\n",
       "      <td>196</td>\n",
       "      <td>0</td>\n",
       "      <td>11.7</td>\n",
       "      <td>11.673393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H196</td>\n",
       "      <td>197</td>\n",
       "      <td>0</td>\n",
       "      <td>11.4</td>\n",
       "      <td>11.473393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5563</th>\n",
       "      <td>H413</td>\n",
       "      <td>908</td>\n",
       "      <td>1</td>\n",
       "      <td>49.0</td>\n",
       "      <td>50.620196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5564</th>\n",
       "      <td>H413</td>\n",
       "      <td>909</td>\n",
       "      <td>1</td>\n",
       "      <td>39.0</td>\n",
       "      <td>35.972331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5565</th>\n",
       "      <td>H413</td>\n",
       "      <td>910</td>\n",
       "      <td>1</td>\n",
       "      <td>29.0</td>\n",
       "      <td>29.359678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5566</th>\n",
       "      <td>H413</td>\n",
       "      <td>911</td>\n",
       "      <td>1</td>\n",
       "      <td>24.0</td>\n",
       "      <td>25.784563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>H413</td>\n",
       "      <td>912</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>23.168413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5568 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     unique_id   ds  fold     y  LGBMRegressor\n",
       "0         H196  193     0  12.7      12.673393\n",
       "1         H196  194     0  12.3      12.273393\n",
       "2         H196  195     0  11.9      11.873393\n",
       "3         H196  196     0  11.7      11.673393\n",
       "4         H196  197     0  11.4      11.473393\n",
       "...        ...  ...   ...   ...            ...\n",
       "5563      H413  908     1  49.0      50.620196\n",
       "5564      H413  909     1  39.0      35.972331\n",
       "5565      H413  910     1  29.0      29.359678\n",
       "5566      H413  911     1  24.0      25.784563\n",
       "5567      H413  912     1  20.0      23.168413\n",
       "\n",
       "[5568 rows x 5 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcst.cross_validation_fitted_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d4565f-2a99-415d-85c2-33603861763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test fitted\n",
    "fcst2 = MLForecast(\n",
    "    models=lgb.LGBMRegressor(n_estimators=5, random_state=0, verbosity=-1),\n",
    "    freq=1,\n",
    "    lags=[24],\n",
    "    num_threads=1,\n",
    ")\n",
    "_ = fcst2.cross_validation(\n",
    "    train,\n",
    "    n_windows=2,\n",
    "    h=2,\n",
    "    fitted=True,\n",
    "    refit=False,\n",
    ")\n",
    "fitted_cv_results = fcst2.cross_validation_fitted_values()\n",
    "train_with_cv_fitted_values = train.merge(fitted_cv_results, on=['unique_id', 'ds'], suffixes=('_expected', ''))\n",
    "np.testing.assert_allclose(\n",
    "    train_with_cv_fitted_values['y_expected'].values,\n",
    "    train_with_cv_fitted_values['y'].values,\n",
    ")\n",
    "\n",
    "# test with max_horizon\n",
    "_ = fcst2.cross_validation(\n",
    "    train,\n",
    "    n_windows=2,\n",
    "    h=2,\n",
    "    fitted=True,\n",
    "    refit=False,    \n",
    "    max_horizon=2,\n",
    ")\n",
    "max_horizon_fitted_cv_results = fcst2.cross_validation_fitted_values()\n",
    "pd.testing.assert_frame_equal(\n",
    "    fitted_cv_results[lambda df: df['fold'].eq(0)],\n",
    "    (\n",
    "        max_horizon_fitted_cv_results\n",
    "        [lambda df: df['fold'].eq(0) & df['h'].eq(1)]\n",
    "        .drop(columns='h')\n",
    "        .reset_index(drop=True)\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044e8e28-e660-4d68-954a-a772d1e76985",
   "metadata": {},
   "source": [
    "We can also compute prediction intervals by passing a configuration to `prediction_intervals` as well as values for the width through `levels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f940f245-4a5c-4da6-a6b9-297c99610d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>cutoff</th>\n",
       "      <th>y</th>\n",
       "      <th>LGBMRegressor</th>\n",
       "      <th>LGBMRegressor-lo-90</th>\n",
       "      <th>LGBMRegressor-lo-80</th>\n",
       "      <th>LGBMRegressor-hi-80</th>\n",
       "      <th>LGBMRegressor-hi-90</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H196</td>\n",
       "      <td>865</td>\n",
       "      <td>864</td>\n",
       "      <td>15.5</td>\n",
       "      <td>15.373393</td>\n",
       "      <td>15.311379</td>\n",
       "      <td>15.316528</td>\n",
       "      <td>15.430258</td>\n",
       "      <td>15.435407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H196</td>\n",
       "      <td>866</td>\n",
       "      <td>864</td>\n",
       "      <td>15.1</td>\n",
       "      <td>14.973393</td>\n",
       "      <td>14.940556</td>\n",
       "      <td>14.940556</td>\n",
       "      <td>15.006230</td>\n",
       "      <td>15.006230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H196</td>\n",
       "      <td>867</td>\n",
       "      <td>864</td>\n",
       "      <td>14.8</td>\n",
       "      <td>14.673393</td>\n",
       "      <td>14.606230</td>\n",
       "      <td>14.606230</td>\n",
       "      <td>14.740556</td>\n",
       "      <td>14.740556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H196</td>\n",
       "      <td>868</td>\n",
       "      <td>864</td>\n",
       "      <td>14.4</td>\n",
       "      <td>14.373393</td>\n",
       "      <td>14.306230</td>\n",
       "      <td>14.306230</td>\n",
       "      <td>14.440556</td>\n",
       "      <td>14.440556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H196</td>\n",
       "      <td>869</td>\n",
       "      <td>864</td>\n",
       "      <td>14.2</td>\n",
       "      <td>14.073393</td>\n",
       "      <td>14.006230</td>\n",
       "      <td>14.006230</td>\n",
       "      <td>14.140556</td>\n",
       "      <td>14.140556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>H413</td>\n",
       "      <td>956</td>\n",
       "      <td>912</td>\n",
       "      <td>59.0</td>\n",
       "      <td>64.284167</td>\n",
       "      <td>29.890099</td>\n",
       "      <td>34.371545</td>\n",
       "      <td>94.196788</td>\n",
       "      <td>98.678234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>H413</td>\n",
       "      <td>957</td>\n",
       "      <td>912</td>\n",
       "      <td>58.0</td>\n",
       "      <td>64.830429</td>\n",
       "      <td>56.874572</td>\n",
       "      <td>57.827689</td>\n",
       "      <td>71.833169</td>\n",
       "      <td>72.786285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>H413</td>\n",
       "      <td>958</td>\n",
       "      <td>912</td>\n",
       "      <td>53.0</td>\n",
       "      <td>40.726851</td>\n",
       "      <td>35.296195</td>\n",
       "      <td>35.846206</td>\n",
       "      <td>45.607495</td>\n",
       "      <td>46.157506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>H413</td>\n",
       "      <td>959</td>\n",
       "      <td>912</td>\n",
       "      <td>38.0</td>\n",
       "      <td>42.739657</td>\n",
       "      <td>35.292153</td>\n",
       "      <td>35.807640</td>\n",
       "      <td>49.671674</td>\n",
       "      <td>50.187161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>H413</td>\n",
       "      <td>960</td>\n",
       "      <td>912</td>\n",
       "      <td>46.0</td>\n",
       "      <td>52.802769</td>\n",
       "      <td>42.465597</td>\n",
       "      <td>43.895670</td>\n",
       "      <td>61.709869</td>\n",
       "      <td>63.139941</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>384 rows  9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    unique_id   ds  cutoff     y  LGBMRegressor  LGBMRegressor-lo-90  \\\n",
       "0        H196  865     864  15.5      15.373393            15.311379   \n",
       "1        H196  866     864  15.1      14.973393            14.940556   \n",
       "2        H196  867     864  14.8      14.673393            14.606230   \n",
       "3        H196  868     864  14.4      14.373393            14.306230   \n",
       "4        H196  869     864  14.2      14.073393            14.006230   \n",
       "..        ...  ...     ...   ...            ...                  ...   \n",
       "379      H413  956     912  59.0      64.284167            29.890099   \n",
       "380      H413  957     912  58.0      64.830429            56.874572   \n",
       "381      H413  958     912  53.0      40.726851            35.296195   \n",
       "382      H413  959     912  38.0      42.739657            35.292153   \n",
       "383      H413  960     912  46.0      52.802769            42.465597   \n",
       "\n",
       "     LGBMRegressor-lo-80  LGBMRegressor-hi-80  LGBMRegressor-hi-90  \n",
       "0              15.316528            15.430258            15.435407  \n",
       "1              14.940556            15.006230            15.006230  \n",
       "2              14.606230            14.740556            14.740556  \n",
       "3              14.306230            14.440556            14.440556  \n",
       "4              14.006230            14.140556            14.140556  \n",
       "..                   ...                  ...                  ...  \n",
       "379            34.371545            94.196788            98.678234  \n",
       "380            57.827689            71.833169            72.786285  \n",
       "381            35.846206            45.607495            46.157506  \n",
       "382            35.807640            49.671674            50.187161  \n",
       "383            43.895670            61.709869            63.139941  \n",
       "\n",
       "[384 rows x 9 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results_intervals = fcst.cross_validation(\n",
    "    train,\n",
    "    n_windows=2,\n",
    "    h=horizon,\n",
    "    step_size=horizon,\n",
    "    prediction_intervals=PredictionIntervals(h=horizon),\n",
    "    level=[80, 90]\n",
    ")\n",
    "cv_results_intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d636c0c-dcbc-49da-8dd2-917b2d582878",
   "metadata": {},
   "source": [
    "The `refit` argument allows us to control if we want to retrain the models in every window. It can either be:\n",
    "\n",
    "* A boolean: True will retrain on every window and False only on the first one.\n",
    "* A positive integer: The models will be trained on the first window and then every `refit` windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5279d3-b83b-4f9f-8f2d-6e44939c0d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst = MLForecast(\n",
    "    models=LinearRegression(),\n",
    "    freq=1,\n",
    "    lags=[1, 24],\n",
    ")\n",
    "for refit, expected_models in zip([True, False, 2], [4, 1, 2]):\n",
    "    fcst.cross_validation(\n",
    "        train,\n",
    "        n_windows=4,\n",
    "        h=horizon,\n",
    "        refit=refit,\n",
    "    )\n",
    "    test_eq(len(fcst.cv_models_), expected_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24dc0ae-4847-4258-a2ab-ae56cf403a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "fcst = MLForecast(\n",
    "    models=lgb.LGBMRegressor(random_state=0, verbosity=-1),\n",
    "    freq=1,\n",
    "    lags=[24 * (i+1) for i in range(7)],\n",
    "    lag_transforms={\n",
    "        1: [(rolling_mean, 24)],\n",
    "        24: [(rolling_mean, 24)],\n",
    "        48: [(ewm_mean, 0.3)],\n",
    "    },\n",
    "    num_threads=1,\n",
    "    target_transforms=[Differences([24])],\n",
    ")\n",
    "cv_results_no_refit = fcst.cross_validation(\n",
    "    train,\n",
    "    n_windows=2,\n",
    "    h=horizon,\n",
    "    step_size=horizon,\n",
    "    refit=False\n",
    ")\n",
    "# test we recover the same \"metadata\"\n",
    "test_eq(\n",
    "    cv_results_no_refit.drop(columns='LGBMRegressor'),\n",
    "    cv_results.drop(columns='LGBMRegressor')\n",
    ")\n",
    "# test the first window has the same forecasts\n",
    "first_cutoff = cv_results['cutoff'].iloc[0]\n",
    "test_eq(\n",
    "    cv_results_no_refit.query('cutoff == @first_cutoff'),\n",
    "    cv_results.query('cutoff == @first_cutoff')\n",
    ")\n",
    "# test next windows have different forecasts\n",
    "test_ne(\n",
    "    cv_results_no_refit.query('cutoff != @first_cutoff'),\n",
    "    cv_results.query('cutoff != @first_cutoff')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa13128-57be-4f72-94da-3287eae36713",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# cv with input_size\n",
    "input_size = 300\n",
    "cv_results_input_size = fcst.cross_validation(\n",
    "    train,\n",
    "    n_windows=2,\n",
    "    h=horizon,\n",
    "    step_size=horizon,\n",
    "    input_size=input_size,\n",
    ")\n",
    "series_lengths = np.diff(fcst.ts.ga.indptr)\n",
    "unique_lengths = np.unique(series_lengths)\n",
    "assert unique_lengths.size == 1\n",
    "assert unique_lengths[0] == input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8ef2aa-daad-4b08-82e7-bf75938a0fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# one model per horizon\n",
    "cv_results2 = fcst.cross_validation(\n",
    "    train,\n",
    "    n_windows=2,\n",
    "    h=horizon,\n",
    "    step_size=horizon,\n",
    "    max_horizon=horizon,\n",
    ")\n",
    "# the first entry per id and window is the same\n",
    "pd.testing.assert_frame_equal(\n",
    "    cv_results.groupby(['unique_id', 'cutoff']).head(1),\n",
    "    cv_results2.groupby(['unique_id', 'cutoff']).head(1)\n",
    ")\n",
    "# the rest is different\n",
    "test_fail(lambda: pd.testing.assert_frame_equal(cv_results, cv_results2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5520ec36-e57c-49e0-a49c-7bf5fb8ff1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# one model per horizon with prediction intervals\n",
    "cv_results2_intervals = fcst.cross_validation(\n",
    "    train,\n",
    "    n_windows=2,\n",
    "    h=horizon,\n",
    "    step_size=horizon,\n",
    "    max_horizon=horizon,\n",
    "    prediction_intervals=PredictionIntervals(n_windows=2, h=horizon),\n",
    "    level=[80, 90]\n",
    ")\n",
    "# the first entry per id and window is the same\n",
    "pd.testing.assert_frame_equal(\n",
    "    cv_results_intervals.groupby(['unique_id', 'cutoff']).head(1),\n",
    "    cv_results2_intervals.groupby(['unique_id', 'cutoff']).head(1)\n",
    ")\n",
    "# the rest is different\n",
    "test_fail(lambda: pd.testing.assert_frame_equal(cv_results_intervals, cv_results2_intervals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6234482e-1e43-4168-88d2-0c06ba06b7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# wrong frequency raises error\n",
    "df_wrong_freq = pd.DataFrame({'ds': pd.to_datetime(['2020-01-02', '2020-02-02', '2020-03-02', '2020-04-02'])})\n",
    "df_wrong_freq['unique_id'] = 'id1'\n",
    "df_wrong_freq['y'] = 1\n",
    "fcst_wrong_freq = MLForecast(\n",
    "    models=[LinearRegression()],\n",
    "    freq='MS',\n",
    "    lags=[1],\n",
    ")\n",
    "test_fail(\n",
    "    lambda: fcst_wrong_freq.cross_validation(df_wrong_freq, n_windows=1, h=1),\n",
    "    contains='Cross validation result produced less results than expected',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f5a04f-5bdf-4710-b1cf-56172ea725bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_series(cv_results, cv_results.drop(columns='cutoff'), max_insample_length=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3151e7-879c-46b5-bca3-46f596be5a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "fig.savefig('figs/forecast__cross_validation.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e195029f-0de8-4425-8c91-8a3b4f2333c6",
   "metadata": {},
   "source": [
    "![](figs/forecast__cross_validation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6577ee8-adbd-49f1-b67d-7fa59c5f9f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_series(cv_results_intervals, cv_results_intervals.drop(columns='cutoff'), level=[90], max_insample_length=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c73f6d-39c3-4a37-bbd3-41fe6da3ed8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "fig.savefig('figs/forecast__cross_validation_intervals.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d7b641-19a9-4cff-97f0-45dc06161cc2",
   "metadata": {},
   "source": [
    "![](figs/forecast__cross_validation_intervals.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a0fe02-ddfe-41b5-a903-5194cfae6bde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/mlforecast/blob/main/mlforecast/forecast.py#L198){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MLForecast.from_cv\n",
       "\n",
       ">      MLForecast.from_cv (cv:mlforecast.lgb_cv.LightGBMCV)"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/mlforecast/blob/main/mlforecast/forecast.py#L198){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MLForecast.from_cv\n",
       "\n",
       ">      MLForecast.from_cv (cv:mlforecast.lgb_cv.LightGBMCV)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MLForecast.from_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0fe555-a79c-4347-bad6-00d4e6a4cf7c",
   "metadata": {},
   "source": [
    "Once you've found a set of features and parameters that work for your problem you can build a forecast object from it using `MLForecast.from_cv`, which takes the trained `LightGBMCV` object and builds an `MLForecast` object that will use the same features and parameters. Then you can call fit and predict as you normally would."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf6f8a1-a69c-44e6-b419-6f29fe7db2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Start training from score 0.084340\n",
      "[10] mape: 0.118569\n",
      "[20] mape: 0.111506\n",
      "[30] mape: 0.107314\n",
      "[40] mape: 0.106089\n",
      "[50] mape: 0.106630\n",
      "Early stopping at round 50\n",
      "Using best iteration: 40\n"
     ]
    }
   ],
   "source": [
    "cv = LightGBMCV(\n",
    "    freq=1,\n",
    "    lags=[24 * (i+1) for i in range(7)],\n",
    "    lag_transforms={\n",
    "        48: [(ewm_mean, 0.3)],\n",
    "    },\n",
    "    num_threads=1,\n",
    "    target_transforms=[Differences([24])]\n",
    ")\n",
    "hist = cv.fit(\n",
    "    train,\n",
    "    n_windows=2,\n",
    "    h=horizon,\n",
    "    params={'verbosity': -1},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2533aaa2-b668-402c-b86b-50125cca4158",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst = MLForecast.from_cv(cv)\n",
    "assert cv.best_iteration_ == fcst.models['LGBMRegressor'].n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2082a5-5562-4816-9d53-cba4599e0196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>LGBMRegressor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H196</td>\n",
       "      <td>961</td>\n",
       "      <td>16.111079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H196</td>\n",
       "      <td>962</td>\n",
       "      <td>15.711079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H196</td>\n",
       "      <td>963</td>\n",
       "      <td>15.311079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H196</td>\n",
       "      <td>964</td>\n",
       "      <td>15.011079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H196</td>\n",
       "      <td>965</td>\n",
       "      <td>14.711079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>H413</td>\n",
       "      <td>1004</td>\n",
       "      <td>92.722032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>H413</td>\n",
       "      <td>1005</td>\n",
       "      <td>69.153603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>H413</td>\n",
       "      <td>1006</td>\n",
       "      <td>68.811675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>H413</td>\n",
       "      <td>1007</td>\n",
       "      <td>53.693346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>H413</td>\n",
       "      <td>1008</td>\n",
       "      <td>46.055481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    unique_id    ds  LGBMRegressor\n",
       "0        H196   961      16.111079\n",
       "1        H196   962      15.711079\n",
       "2        H196   963      15.311079\n",
       "3        H196   964      15.011079\n",
       "4        H196   965      14.711079\n",
       "..        ...   ...            ...\n",
       "187      H413  1004      92.722032\n",
       "188      H413  1005      69.153603\n",
       "189      H413  1006      68.811675\n",
       "190      H413  1007      53.693346\n",
       "191      H413  1008      46.055481\n",
       "\n",
       "[192 rows x 3 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcst.fit(train)\n",
    "fcst.predict(horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9188fd-8264-41d1-a4d7-89fa51d915d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "series = generate_daily_series(100, equal_ends=True, n_static_features=2, static_as_categorical=False)\n",
    "non_std_series = series.copy()\n",
    "non_std_series['ds'] = non_std_series.groupby('unique_id').cumcount()\n",
    "non_std_series = non_std_series.rename(columns={'unique_id': 'some_id', 'ds': 'time', 'y': 'value'})\n",
    "models = [\n",
    "    lgb.LGBMRegressor(n_jobs=1, random_state=0, verbosity=-1),\n",
    "    xgb.XGBRegressor(n_jobs=1, random_state=0),\n",
    "]\n",
    "flow_params = dict(\n",
    "    models=models,\n",
    "    lags=[7],\n",
    "    lag_transforms={\n",
    "        1: [expanding_mean],\n",
    "        7: [(rolling_mean, 14)]\n",
    "    },\n",
    "    num_threads=2,\n",
    ")\n",
    "fcst = MLForecast(freq=1, **flow_params)\n",
    "non_std_preds = fcst.fit(non_std_series, id_col='some_id', time_col='time', target_col='value').predict(7)\n",
    "non_std_preds = non_std_preds.rename(columns={'some_id': 'unique_id'})\n",
    "fcst = MLForecast(freq='D', **flow_params)\n",
    "preds = fcst.fit(series).predict(7)\n",
    "pd.testing.assert_frame_equal(preds.drop(columns='ds'), non_std_preds.drop(columns='time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383ca7ea-b275-4bc6-8ee7-3a41a46f3213",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "def test_cross_validation(data=non_std_series, add_exogenous=False):\n",
    "    n_windows = 2\n",
    "    h = 14\n",
    "    fcst = MLForecast(lgb.LGBMRegressor(verbosity=-1), freq=1, lags=[7, 14])\n",
    "    if add_exogenous:\n",
    "        data = data.assign(ex1 = lambda x: np.arange(0, len(x)))\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "        backtest_results = fcst.cross_validation(\n",
    "            df=data,\n",
    "            n_windows=n_windows,\n",
    "            h=h,\n",
    "            id_col='some_id',\n",
    "            time_col='time',\n",
    "            target_col='value',\n",
    "            static_features=['some_id', 'static_0', 'static_1'],\n",
    "        )\n",
    "    renamer = {'some_id': 'unique_id', 'time': 'ds', 'value': 'y'}\n",
    "    backtest_results = backtest_results.rename(columns=renamer)\n",
    "    renamed = data.rename(columns=renamer)\n",
    "    manual_results = []\n",
    "    for cutoff, train, valid in backtest_splits(renamed, n_windows, h, 'unique_id', 'ds', 1):\n",
    "        fcst.fit(train, static_features=['unique_id', 'static_0', 'static_1'])\n",
    "        if add_exogenous:\n",
    "            X_df = valid.drop(columns=['y', 'static_0', 'static_1']).reset_index()\n",
    "        else:\n",
    "            X_df = None\n",
    "        pred = fcst.predict(h, X_df=X_df)\n",
    "        res = valid[['unique_id', 'ds', 'y']].copy()\n",
    "        res = res.merge(cutoff, on='unique_id')\n",
    "        res = res[['unique_id', 'ds', 'cutoff', 'y']].copy()\n",
    "        manual_results.append(res.merge(pred, on=['unique_id', 'ds'], how='left'))\n",
    "    manual_results = pd.concat(manual_results)\n",
    "    pd.testing.assert_frame_equal(backtest_results, manual_results.reset_index(drop=True))\n",
    "test_cross_validation()\n",
    "test_cross_validation(add_exogenous=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7567d4-4233-4895-9876-913b9e8f6ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test short series in cv\n",
    "series = generate_daily_series(\n",
    "    n_series=100, min_length=20, max_length=51, equal_ends=True,\n",
    ")\n",
    "horizon = 10\n",
    "n_windows = 4\n",
    "fcst = MLForecast(models=[LinearRegression()], freq='D', lags=[1])\n",
    "cv_res = fcst.cross_validation(series, h=horizon, n_windows=n_windows)\n",
    "series_per_cutoff = cv_res.groupby('cutoff')['unique_id'].nunique()\n",
    "series_sizes = series['unique_id'].value_counts().sort_index()\n",
    "for i in range(4):\n",
    "    test_eq(series_per_cutoff[i], series_sizes.gt((n_windows - i) * horizon).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bf3c73-0251-4c16-bd5c-e7a34cbe5ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| polars\n",
    "from itertools import product\n",
    "\n",
    "import polars as pl\n",
    "from utilsforecast.processing import match_if_categorical\n",
    "\n",
    "from mlforecast.utils import generate_prices_for_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b118d7d-1bab-4899-87b4-240d52393720",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| polars\n",
    "horizon = 2\n",
    "series_pl = generate_daily_series(\n",
    "    10, n_static_features=2, static_as_categorical=False, equal_ends=True, engine='polars'\n",
    ")\n",
    "series_pd = generate_daily_series(\n",
    "    10, n_static_features=2, static_as_categorical=False, equal_ends=True, engine='pandas'\n",
    ")\n",
    "series_pd = series_pd.rename(columns={'static_0': 'product_id'})\n",
    "prices_pd = generate_prices_for_series(series_pd, horizon)\n",
    "prices_pd['unique_id'] = prices_pd['unique_id'].astype(series_pd['unique_id'].dtype)\n",
    "series_pd = series_pd.merge(prices_pd, on=['unique_id', 'ds'])\n",
    "\n",
    "prices_pl = pl.from_pandas(prices_pd)\n",
    "uids_series, uids_prices = match_if_categorical(series_pl['unique_id'], prices_pl['unique_id'])\n",
    "series_pl = series_pl.with_columns(uids_series)\n",
    "prices_pl = prices_pl.with_columns(uids_prices)\n",
    "series_pl = series_pl.rename({'static_0': 'product_id'}).join(prices_pl, on=['unique_id', 'ds'])\n",
    "permutation = np.random.choice(series_pl.shape[0], series_pl.shape[0], replace=False)\n",
    "series_pl = series_pl[permutation]\n",
    "series_pd = series_pd.iloc[permutation]\n",
    "\n",
    "cfg = dict(\n",
    "    models=[LinearRegression(), lgb.LGBMRegressor(verbosity=-1)],\n",
    "    freq='1d',\n",
    "    lags=[1, 2],\n",
    "    lag_transforms={\n",
    "        1: [expanding_mean],\n",
    "        2: [expanding_mean],\n",
    "    },\n",
    "    date_features=['day', 'month', 'week', 'year'],\n",
    "    target_transforms=[Differences([1, 2]), LocalStandardScaler()],\n",
    ")\n",
    "fit_kwargs = dict(\n",
    "    fitted=True,\n",
    "    prediction_intervals=PredictionIntervals(h=horizon),\n",
    "    static_features=['product_id', 'static_1'],\n",
    ")\n",
    "predict_kwargs = dict(\n",
    "    h=2,\n",
    "    level=[80, 95],\n",
    ")\n",
    "horizons = [None, horizon]\n",
    "as_np = [True, False]\n",
    "for max_horizon, as_numpy in product(horizons, as_np):\n",
    "    fcst_pl = MLForecast(**cfg)\n",
    "    fcst_pl.fit(series_pl, max_horizon=max_horizon, as_numpy=as_numpy, **fit_kwargs)\n",
    "    fitted_pl = fcst_pl.forecast_fitted_values()\n",
    "    preds_pl = fcst_pl.predict(X_df=prices_pl, **predict_kwargs)\n",
    "    preds_pl_subset = fcst_pl.predict(X_df=prices_pl, ids=fcst_pl.ts.uids[:2], **predict_kwargs)\n",
    "    cv_pl = fcst_pl.cross_validation(series_pl, n_windows=2, h=horizon, fitted=True, as_numpy=as_numpy)\n",
    "    cv_fitted_pl = fcst_pl.cross_validation_fitted_values()\n",
    "    \n",
    "    fcst_pd = MLForecast(**cfg)\n",
    "    fcst_pd.fit(series_pd, max_horizon=max_horizon, as_numpy=as_numpy, **fit_kwargs)\n",
    "    fitted_pd = fcst_pd.forecast_fitted_values()\n",
    "    preds_pd = fcst_pd.predict(X_df=prices_pd, **predict_kwargs)\n",
    "    preds_pd_subset = fcst_pd.predict(X_df=prices_pd, ids=fcst_pd.ts.uids[:2], **predict_kwargs)\n",
    "    cv_pd = fcst_pd.cross_validation(series_pd, n_windows=2, h=horizon, fitted=True, as_numpy=as_numpy)\n",
    "    cv_fitted_pd = fcst_pd.cross_validation_fitted_values()\n",
    "\n",
    "    if max_horizon is not None:\n",
    "        fitted_pl = fitted_pl.with_columns(pl.col('h').cast(pl.Int64))\n",
    "        for h in range(max_horizon):\n",
    "            fitted_h = fitted_pl.filter(pl.col('h').eq(h + 1))\n",
    "            series_offset = (\n",
    "                series_pl\n",
    "                .sort('unique_id', 'ds')\n",
    "                .with_columns(pl.col('y').shift(-h).over('unique_id'))\n",
    "            )\n",
    "            series_filtered = (\n",
    "                fitted_h\n",
    "                [['unique_id', 'ds']]\n",
    "                .join(series_offset, on=['unique_id', 'ds'])\n",
    "                .sort(['unique_id', 'ds'])\n",
    "            )\n",
    "            np.testing.assert_allclose(\n",
    "                series_filtered['y'],\n",
    "                fitted_h['y']\n",
    "            )\n",
    "    else:\n",
    "        series_filtered = (\n",
    "            fitted_pl\n",
    "            [['unique_id', 'ds']]\n",
    "            .join(series_pl, on=['unique_id', 'ds'])\n",
    "            .sort(['unique_id', 'ds'])\n",
    "        )\n",
    "        np.testing.assert_allclose(\n",
    "            series_filtered['y'],\n",
    "            fitted_pl['y']\n",
    "        )\n",
    "\n",
    "    pd.testing.assert_frame_equal(fitted_pl.to_pandas(), fitted_pd)\n",
    "    pd.testing.assert_frame_equal(preds_pl.to_pandas(), preds_pd)\n",
    "    pd.testing.assert_frame_equal(preds_pl_subset.to_pandas(), preds_pd_subset)    \n",
    "    pd.testing.assert_frame_equal(cv_pl.to_pandas(), cv_pd)\n",
    "    pd.testing.assert_frame_equal(\n",
    "        cv_fitted_pl.with_columns(pl.col('fold').cast(pl.Int64)).to_pandas(),\n",
    "        cv_fitted_pd,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa1b8af-6fcf-4948-a7e6-21e4c8cda56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test transforms are inverted correctly when series were dropped\n",
    "series = generate_daily_series(10, min_length=5, max_length=20)\n",
    "fcst = MLForecast(\n",
    "    models=LinearRegression(),\n",
    "    freq='D',\n",
    "    lags=[10],\n",
    "    target_transforms=[Differences([1]), LocalStandardScaler()],\n",
    ")\n",
    "fcst.fit(series, fitted=True)\n",
    "assert fcst.ts._dropped_series.size > 0\n",
    "fitted_vals = fcst.forecast_fitted_values()\n",
    "full = fitted_vals.merge(series, on=['unique_id', 'ds'], suffixes=('_fitted', '_orig'))\n",
    "np.testing.assert_allclose(\n",
    "    full['y_fitted'].values,\n",
    "    full['y_orig'].values,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
