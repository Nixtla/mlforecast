{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b44ee1-af97-490a-ad62-03b2f804e006",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9693434b-0290-4ffd-88f9-a4267c11b548",
   "metadata": {},
   "source": [
    "# Quick start (distributed)\n",
    "\n",
    "> Minimal example of distributed training with MLForecast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07e9e05-e8b2-433e-9ff2-16e652cf6acb",
   "metadata": {},
   "source": [
    "The `DistributedMLForecast` class is a high level abstraction that encapsulates all the steps in the pipeline (preprocessing, fitting the model and computing predictions) and applies them in a distributed way.\n",
    "\n",
    "The different things that you need to use `DistributedMLForecast` (as opposed to `MLForecast`) are:\n",
    "\n",
    "1. You need to set up a cluster. We currently support dask, ray and spark.\n",
    "2. Your data needs to be a distributed collection (dask, ray or spark dataframe).\n",
    "3. You need to use a model that implements distributed training in your framework of choice, e.g. SynapseML for LightGBM in spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd7853f-df6e-4bf2-b2d9-cb3b4c6a0d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import git\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "from mlforecast.distributed import DistributedMLForecast\n",
    "from mlforecast.lag_transforms import ExpandingMean, ExponentiallyWeightedMean, RollingMean\n",
    "from mlforecast.target_transforms import Differences\n",
    "from mlforecast.utils import generate_daily_series, generate_prices_for_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f930a27-b15f-438a-aab2-ccafc77e09f4",
   "metadata": {},
   "source": [
    "## Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af70aed-bfb9-4c36-a4c4-cf546f8655a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fe3454-b2e1-40d0-a623-6ae1fe486536",
   "metadata": {},
   "source": [
    "### Client setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9868f8-38a5-4a9f-829c-47a10a522fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(n_workers=2, threads_per_worker=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd61b21-9a22-42dc-ba19-d6de68dc8eba",
   "metadata": {},
   "source": [
    "Here we define a client that connects to a `dask.distributed.LocalCluster`, however it could be any other kind of cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b968e6d-cbc8-499b-8a41-ef12fe4fdd69",
   "metadata": {},
   "source": [
    "### Data setup\n",
    "\n",
    "For dask, the data must be a `dask.dataframe.DataFrame`. You need to make sure that each time serie is only in one partition and it is recommended that you have as many partitions as you have workers. If you have more partitions than workers make sure to set `num_threads=1` to avoid having nested parallelism.\n",
    "\n",
    "The required input format is the same as for `MLForecast`, except that it's a `dask.dataframe.DataFrame` instead of a `pandas.Dataframe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8acdff6-49c0-4f28-8a2c-1a75e2c1c115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>static_0</th>\n",
       "      <th>static_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=10</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>object</td>\n",
       "      <td>datetime64[ns]</td>\n",
       "      <td>float64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_10</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_90</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_99</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<div>Dask Name: assign, 5 expressions</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "               unique_id              ds        y static_0 static_1\n",
       "npartitions=10                                                     \n",
       "id_00             object  datetime64[ns]  float64    int64    int64\n",
       "id_10                ...             ...      ...      ...      ...\n",
       "...                  ...             ...      ...      ...      ...\n",
       "id_90                ...             ...      ...      ...      ...\n",
       "id_99                ...             ...      ...      ...      ...\n",
       "Dask Name: assign, 5 expressions\n",
       "Expr=Assign(frame=MapPartitions(lambda))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series = generate_daily_series(100, n_static_features=2, equal_ends=True, static_as_categorical=False, min_length=500, max_length=1_000)\n",
    "npartitions = 10\n",
    "partitioned_series = dd.from_pandas(series.set_index('unique_id'), npartitions=npartitions)  # make sure we split by the id_col\n",
    "partitioned_series = partitioned_series.map_partitions(lambda df: df.reset_index())\n",
    "partitioned_series['unique_id'] = partitioned_series['unique_id'].astype(str)  # can't handle categoricals atm\n",
    "partitioned_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28aa4f88-1a4d-4175-a33c-80d34f61f8cc",
   "metadata": {},
   "source": [
    "### Models\n",
    "In order to perform distributed forecasting, we need to use a model that is able to train in a distributed way using `dask`. The current implementations are in `DaskLGBMForecast` and `DaskXGBForecast` which are just wrappers around the native implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5997af98-7a6e-468a-8f22-d7271b85ec67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlforecast.distributed.models.dask.lgb import DaskLGBMForecast\n",
    "from mlforecast.distributed.models.dask.xgb import DaskXGBForecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e482699-e3c3-4fef-b081-112c23bff40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [DaskXGBForecast(random_state=0), DaskLGBMForecast(random_state=0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab284f5f-e6d7-4d73-b0b6-efdf0aea581a",
   "metadata": {},
   "source": [
    "### Training\n",
    "Once we have our models we instantiate a `DistributedMLForecast` object defining our features. We can then call `fit` on this object passing our dask dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b5821d-6c04-4fd6-b5d6-9bbfefe9d442",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst = DistributedMLForecast(\n",
    "    models=models,\n",
    "    freq='D',\n",
    "    target_transforms=[Differences([7])],\n",
    "    lags=[7],\n",
    "    lag_transforms={\n",
    "        1: [ExpandingMean(), ExponentiallyWeightedMean(alpha=0.9)],\n",
    "        7: [RollingMean(window_size=14)],\n",
    "    },\n",
    "    date_features=['dayofweek', 'month'],\n",
    "    num_threads=1,\n",
    "    engine=client,\n",
    ")\n",
    "fcst.fit(partitioned_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a49e861-74ab-42b0-a220-a823e5c7070a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import fugue.api as fa\n",
    "from fastcore.test import test_eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4241e4da-fc38-4744-bb88-e72c77e63ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# function to test the partition_results data\n",
    "# has the right size\n",
    "def test_partition_results_size(fcst_object, expected_n_partitions):\n",
    "    test_eq(\n",
    "        fa.get_num_partitions(fcst_object._partition_results),\n",
    "        expected_n_partitions,\n",
    "    )\n",
    "    test_eq(\n",
    "        fa.count(fcst_object._partition_results),\n",
    "        expected_n_partitions,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b4219d-4c48-43a1-87c4-58dfb520022c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_partition_results_size(fcst, npartitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5a8531-014f-4d0e-a33e-fe8504d164a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test num_partitions works properly\n",
    "if sys.version_info >= (3, 9):\n",
    "    num_partitions_test = 4\n",
    "    test_dd = dd.from_pandas(series, npartitions=num_partitions_test) # In this case we dont have to specify the column\n",
    "    test_dd['unique_id'] = test_dd['unique_id'].astype(str)\n",
    "    fcst_np = DistributedMLForecast(\n",
    "        models=models,\n",
    "        freq='D',\n",
    "        target_transforms=[Differences([7])],    \n",
    "        lags=[7],\n",
    "        lag_transforms={\n",
    "            1: [ExpandingMean()],\n",
    "            7: [RollingMean(window_size=14)]\n",
    "        },\n",
    "        date_features=['dayofweek', 'month'],\n",
    "        num_threads=1,\n",
    "        engine=client,\n",
    "        num_partitions=num_partitions_test\n",
    "    )\n",
    "    fcst_np.fit(test_dd)\n",
    "    test_partition_results_size(fcst_np, num_partitions_test)\n",
    "    preds_np = fcst_np.predict(7).compute().sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "    preds = fcst.predict(7).compute().sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "    pd.testing.assert_frame_equal(\n",
    "        preds[['unique_id', 'ds']], \n",
    "        preds_np[['unique_id', 'ds']], \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f2d411-66f8-425a-bcfe-8c5cc28ca324",
   "metadata": {},
   "source": [
    "Once we have our fitted models we can compute the predictions for the next 7 timesteps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21511e5a-750b-4a27-8f68-e9ceff5c7536",
   "metadata": {},
   "source": [
    "### Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58769a05-3961-49c9-9b50-e1b0713d8c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>DaskXGBForecast</th>\n",
       "      <th>DaskLGBMForecast</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2002-09-27 00:00:00</td>\n",
       "      <td>21.024446</td>\n",
       "      <td>21.710263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2002-09-28 00:00:00</td>\n",
       "      <td>84.190221</td>\n",
       "      <td>84.160383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2002-09-29 00:00:00</td>\n",
       "      <td>164.370398</td>\n",
       "      <td>163.325095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2002-09-30 00:00:00</td>\n",
       "      <td>246.09351</td>\n",
       "      <td>246.099914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2002-10-01 00:00:00</td>\n",
       "      <td>311.239076</td>\n",
       "      <td>314.455627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  unique_id                   ds  DaskXGBForecast  DaskLGBMForecast\n",
       "0     id_00  2002-09-27 00:00:00        21.024446         21.710263\n",
       "1     id_00  2002-09-28 00:00:00        84.190221         84.160383\n",
       "2     id_00  2002-09-29 00:00:00       164.370398        163.325095\n",
       "3     id_00  2002-09-30 00:00:00        246.09351        246.099914\n",
       "4     id_00  2002-10-01 00:00:00       311.239076        314.455627"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = fcst.predict(7).compute()\n",
    "preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e6f1f2-d22b-48d7-a5e5-77d400cd4625",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "preds2 = fcst.predict(7).compute()\n",
    "preds3 = fcst.predict(7, new_df=partitioned_series).compute()\n",
    "pd.testing.assert_frame_equal(preds, preds2)\n",
    "pd.testing.assert_frame_equal(preds, preds3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d26427d-314e-4589-8567-097ddf5adce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test X_df\n",
    "prices = generate_prices_for_series(series)\n",
    "series_wexog = series.merge(prices, on=['unique_id', 'ds'])\n",
    "npartitions = 10\n",
    "partitioned_series_exog = dd.from_pandas(series_wexog.set_index('unique_id'), npartitions=npartitions)\n",
    "partitioned_series_exog = partitioned_series_exog.map_partitions(lambda df: df.reset_index())\n",
    "partitioned_series_exog['unique_id'] = partitioned_series_exog['unique_id'].astype(str)\n",
    "fcst_exog = DistributedMLForecast(\n",
    "    models=models,\n",
    "    freq='D',\n",
    "    target_transforms=[Differences([7])],    \n",
    "    lags=[7],\n",
    "    lag_transforms={\n",
    "        1: [ExpandingMean()],\n",
    "        7: [RollingMean(window_size=14)]\n",
    "    },\n",
    "    date_features=['dayofweek', 'month'],\n",
    "    num_threads=1,\n",
    "    engine=client,\n",
    ")\n",
    "fcst_exog.fit(partitioned_series_exog, static_features=['static_0', 'static_1'])\n",
    "preds_exog = fcst_exog.predict(h=7, X_df=prices).compute()\n",
    "full_preds = preds.merge(preds_exog, on=['unique_id', 'ds'], suffixes=('', '_exog'))\n",
    "for model in ('DaskXGBForecast', 'DaskLGBMForecast'):\n",
    "    pct_diff = abs(1 - full_preds[f'{model}_exog'].div(full_preds[f'{model}']).mean())\n",
    "    assert 0 < pct_diff < 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502aeadd-2fd5-4d16-8dfb-56a77080c072",
   "metadata": {},
   "source": [
    "### Saving and loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10073e2c-c36e-49d2-8996-b90fde11123a",
   "metadata": {},
   "source": [
    "Once you've trained your model you can use the `DistributedMLForecast.save` method to save the artifacts for inference. Keep in mind that if you're on a remote cluster you should set a remote storage like S3 as the destination.\n",
    "\n",
    "mlforecast uses [fsspec](https://filesystem-spec.readthedocs.io/en/latest/) to handle the different filesystems, so if you're using s3 for example you also need to install [s3fs](https://s3fs.readthedocs.io/en/latest/). If you're using pip you can just include the aws extra, e.g. `pip install 'mlforecast[aws,dask]'`, which will install the required dependencies to perform distributed training with dask and saving to S3. If you're using conda you'll have to manually install them (`conda install dask fsspec fugue s3fs`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486bc64f-9567-40be-a6df-70efc7f4dc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define unique name for CI\n",
    "def build_unique_name(engine):\n",
    "    pyver = f'{sys.version_info.major}_{sys.version_info.minor}'\n",
    "    repo = git.Repo(search_parent_directories=True)\n",
    "    sha = repo.head.object.hexsha\n",
    "    return f'{sys.platform}-{pyver}-{engine}-{sha}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebb761c-8183-4aab-86b9-84ff3277e9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = build_unique_name('dask')\n",
    "save_path = f's3://nixtla-tmp/mlf/{save_dir}'\n",
    "fcst.save(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7147a8d-5860-4eb8-88b2-78fdc96aa156",
   "metadata": {},
   "source": [
    "Once you've saved your forecast object you can then load it back by specifying the path where it was saved along with an engine, which will be used to perform the distributed computations (in this case the dask client)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ba10a0-9562-4373-a755-40fff7402e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst2 = DistributedMLForecast.load(save_path, engine=client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c6ec8a-8a50-4bd6-b16e-af57bd0063d8",
   "metadata": {},
   "source": [
    "We can verify that this object produces the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f982ed67-d7be-485b-9d25-82f34ba2fe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = fa.as_pandas(fcst.predict(10)).sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "preds2 = fa.as_pandas(fcst2.predict(10)).sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "pd.testing.assert_frame_equal(preds, preds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df49be1-f85c-4817-b873-5dddb413c50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "class Lag1Model(BaseEstimator):\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    @property\n",
    "    def model_(self):\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return X['lag1']\n",
    "\n",
    "upd_fcst = DistributedMLForecast(\n",
    "    models=[Lag1Model()],\n",
    "    freq='D',\n",
    "    lags=[1],\n",
    "    num_threads=1,\n",
    "    engine=client,\n",
    ")\n",
    "upd_fcst.fit(partitioned_series)\n",
    "\n",
    "new_df = (series.groupby('unique_id', observed=True)['ds'].max() + pd.offsets.Day()).reset_index()\n",
    "new_df['y'] = -1.0\n",
    "upd_fcst.update(new_df)\n",
    "expected = new_df.rename(columns={'y': 'Lag1Model'})\n",
    "expected = expected.astype({'unique_id': str})\n",
    "expected['ds'] += pd.offsets.Day()\n",
    "upd_preds = upd_fcst.predict(1).compute()\n",
    "pd.testing.assert_frame_equal(\n",
    "    upd_preds.reset_index(drop=True),\n",
    "    expected.reset_index(drop=True),\n",
    "    check_dtype=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4190830c-b2e5-4343-97da-023e9f532ef6",
   "metadata": {},
   "source": [
    "### Converting to local\n",
    "\n",
    "Another option to store your distributed forecast object is to first turn it into a local one and then save it. Keep in mind that in order to do that all the remote data that is stored from the series will have to be pulled into a single machine (the scheduler in dask, driver in spark, etc.), so you have to be sure that it'll fit in memory, it should consume about 2x the size of your target column (you can reduce this further by using the `keep_last_n` argument in the `fit` method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f2cefe-d032-413e-89d3-5d01271c5d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_fcst = fcst.to_local()\n",
    "local_preds = local_fcst.predict(10)\n",
    "# we don't check the dtype because sometimes these are arrow dtypes\n",
    "# or different precisions of float\n",
    "pd.testing.assert_frame_equal(preds, local_preds, check_dtype=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29841c02-b0bc-44cc-a8f3-da31b442584b",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296af9be-4149-4b47-9386-fc92bff65d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_res = fcst.cross_validation(\n",
    "    partitioned_series,\n",
    "    n_windows=3,\n",
    "    h=14,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e80450-d582-42bb-8bf4-ff925d5e74e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>DaskXGBForecast</th>\n",
       "      <th>DaskLGBMForecast</th>\n",
       "      <th>cutoff</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>id_01</td>\n",
       "      <td>2002-08-19 00:00:00</td>\n",
       "      <td>224.458336</td>\n",
       "      <td>222.742605</td>\n",
       "      <td>2002-08-15 00:00:00</td>\n",
       "      <td>210.723139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>id_03</td>\n",
       "      <td>2002-08-17 00:00:00</td>\n",
       "      <td>2.235601</td>\n",
       "      <td>2.210624</td>\n",
       "      <td>2002-08-15 00:00:00</td>\n",
       "      <td>2.416967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>id_03</td>\n",
       "      <td>2002-08-18 00:00:00</td>\n",
       "      <td>3.276747</td>\n",
       "      <td>3.239702</td>\n",
       "      <td>2002-08-15 00:00:00</td>\n",
       "      <td>3.060194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>id_08</td>\n",
       "      <td>2002-08-23 00:00:00</td>\n",
       "      <td>131.261689</td>\n",
       "      <td>131.180289</td>\n",
       "      <td>2002-08-15 00:00:00</td>\n",
       "      <td>138.668463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>id_09</td>\n",
       "      <td>2002-08-21 00:00:00</td>\n",
       "      <td>27.716417</td>\n",
       "      <td>28.263963</td>\n",
       "      <td>2002-08-15 00:00:00</td>\n",
       "      <td>22.88374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    unique_id                   ds  DaskXGBForecast  DaskLGBMForecast  \\\n",
       "17      id_01  2002-08-19 00:00:00       224.458336        222.742605   \n",
       "43      id_03  2002-08-17 00:00:00         2.235601          2.210624   \n",
       "44      id_03  2002-08-18 00:00:00         3.276747          3.239702   \n",
       "119     id_08  2002-08-23 00:00:00       131.261689        131.180289   \n",
       "131     id_09  2002-08-21 00:00:00        27.716417         28.263963   \n",
       "\n",
       "                  cutoff           y  \n",
       "17   2002-08-15 00:00:00  210.723139  \n",
       "43   2002-08-15 00:00:00    2.416967  \n",
       "44   2002-08-15 00:00:00    3.060194  \n",
       "119  2002-08-15 00:00:00  138.668463  \n",
       "131  2002-08-15 00:00:00    22.88374  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_res.compute().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3d27b0-ba00-4141-a50c-ab39385e8295",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from mlforecast.distributed.forecast import WindowInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62457500-a299-4eb7-b13f-78412f4d302e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# input_size\n",
    "input_size = 100\n",
    "reduced_train = fcst._preprocess(\n",
    "    partitioned_series,\n",
    "    id_col='unique_id',\n",
    "    time_col='ds',\n",
    "    target_col='y',\n",
    "    dropna=False,\n",
    "    window_info=WindowInfo(\n",
    "        n_windows=1,\n",
    "        window_size=10,\n",
    "        step_size=None,\n",
    "        i_window=0,\n",
    "        input_size=input_size,\n",
    "    ),\n",
    ")\n",
    "assert reduced_train.groupby('unique_id').size().compute().max() == input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429cdca0-6368-42d9-bc57-2390e6430b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "cv_res_no_refit = fcst.cross_validation(\n",
    "    partitioned_series,\n",
    "    n_windows=3,\n",
    "    h=14,\n",
    "    refit=False\n",
    ")\n",
    "cv_results_df = cv_res.compute().sort_values(['unique_id', 'ds'])\n",
    "cv_results_no_refit_df = cv_res_no_refit.compute().sort_values(['unique_id', 'ds'])\n",
    "# test we recover the same \"metadata\"\n",
    "models = ['DaskXGBForecast', 'DaskLGBMForecast']\n",
    "test_eq(\n",
    "    cv_results_no_refit_df.drop(columns=models),\n",
    "    cv_results_df.drop(columns=models)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34651ae-5be4-4e86-b44f-fbd15c4d72ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "non_std_series = partitioned_series.copy()\n",
    "non_std_series = non_std_series.rename(columns={'ds': 'time', 'y': 'value', 'unique_id': 'some_id'})\n",
    "flow_params = dict(\n",
    "    models=[DaskXGBForecast(random_state=0)],\n",
    "    target_transforms=[Differences([7])],    \n",
    "    lags=[7],\n",
    "    lag_transforms={\n",
    "        1: [ExpandingMean()],\n",
    "        7: [RollingMean(window_size=14)]\n",
    "    },\n",
    "    num_threads=1,\n",
    ")\n",
    "fcst = DistributedMLForecast(freq='D', **flow_params)\n",
    "fcst.fit(partitioned_series)\n",
    "preds = fcst.predict(7).compute()\n",
    "fcst2 = DistributedMLForecast(freq='D', **flow_params)\n",
    "fcst2.preprocess(non_std_series, id_col='some_id', time_col='time', target_col='value')\n",
    "fcst2.models_ = fcst.models_  # distributed training can end up with different fits\n",
    "non_std_preds = fcst2.predict(7).compute()\n",
    "pd.testing.assert_frame_equal(\n",
    "    preds.drop(columns='ds'),\n",
    "    non_std_preds.drop(columns='time').rename(columns={'some_id': 'unique_id'})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba9b3ae-3859-4989-aa56-85390f63cf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327df016-bc2a-47c1-afeb-4394ca7695cb",
   "metadata": {},
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfae838-6305-4a48-8a82-2f3c3eb653f5",
   "metadata": {},
   "source": [
    "### Session setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97595792-8173-4d0e-ac56-174c3d698d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9119c014-b51b-4ba0-ab28-585665c03ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.jars.packages\", \"com.microsoft.azure:synapseml_2.12:0.10.2\")\n",
    "    .config(\"spark.jars.repositories\", \"https://mmlspark.azureedge.net/maven\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0371e5b-5700-4466-86e3-07e35499200b",
   "metadata": {},
   "source": [
    "### Data setup\n",
    "For spark, the data must be a `pyspark DataFrame`. You need to make sure that each time serie is only in one partition (which you can do using `repartitionByRange`, for example) and it is recommended that you have as many partitions as you have workers. If you have more partitions than workers make sure to set `num_threads=1` to avoid having nested parallelism.\n",
    "\n",
    "The required input format is the same as for `MLForecast`, i.e. it should have at least an id column, a time column and a target column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc337af2-0952-4f7c-98ad-baa87bb16d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "numPartitions = 4\n",
    "series = generate_daily_series(100, n_static_features=2, equal_ends=True, static_as_categorical=False)\n",
    "spark_series = spark.createDataFrame(series).repartitionByRange(numPartitions, 'unique_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959aec60-ed2e-4538-8d6c-09b0a66bc35d",
   "metadata": {},
   "source": [
    "### Models\n",
    "In order to perform distributed forecasting, we need to use a model that is able to train in a distributed way using `spark`. The current implementations are in `SparkLGBMForecast` and `SparkXGBForecast` which are just wrappers around the native implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cea90e-ea7b-4244-b26f-a16d3e85fe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlforecast.distributed.models.spark.lgb import SparkLGBMForecast\n",
    "from mlforecast.distributed.models.spark.xgb import SparkXGBForecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03231a34-30a0-44a3-805a-c91ce6eba393",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [SparkLGBMForecast(seed=0), SparkXGBForecast(random_state=0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690336b5-ad1a-4ff2-a42b-fd2bf15870fe",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a458c1-855d-4955-b77b-5dbdb5ecacf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst = DistributedMLForecast(\n",
    "    models,\n",
    "    freq='D',\n",
    "    target_transforms=[Differences([7])],    \n",
    "    lags=[1],\n",
    "    lag_transforms={\n",
    "        1: [ExpandingMean(), ExponentiallyWeightedMean(alpha=0.9)],\n",
    "    },\n",
    "    date_features=['dayofweek'],\n",
    ")\n",
    "fcst.fit(\n",
    "    spark_series,\n",
    "    static_features=['static_0', 'static_1'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33443e25-3a35-4abc-85f1-2aa774d64689",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_partition_results_size(fcst, numPartitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa7ebdd-6d6a-4e0d-babf-d7fe11cd09ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test num_partitions works properly\n",
    "test_spark_df = spark.createDataFrame(series)\n",
    "num_partitions_test = 10\n",
    "fcst_np = DistributedMLForecast(\n",
    "    models=models,\n",
    "    freq='D',    \n",
    "    lags=[7],\n",
    "    lag_transforms={\n",
    "        1: [ExpandingMean()],\n",
    "        7: [RollingMean(window_size=14)]\n",
    "    },\n",
    "    date_features=['dayofweek', 'month'],\n",
    "    num_threads=1,\n",
    "    num_partitions=num_partitions_test,\n",
    ")\n",
    "fcst_np.fit(test_spark_df)\n",
    "test_partition_results_size(fcst_np, num_partitions_test)\n",
    "preds_np = fcst_np.predict(7).toPandas().sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "preds = fcst.predict(7).toPandas().sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "pd.testing.assert_frame_equal(\n",
    "    preds[['unique_id', 'ds']], \n",
    "    preds_np[['unique_id', 'ds']], \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a1b654-5641-4b4d-b503-6606f94396cd",
   "metadata": {},
   "source": [
    "### Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06d2230-60f5-47f1-820b-af2ca7311b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = fcst.predict(14).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da6bbd2-9806-4b12-91a2-1b5571ae1550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>SparkLGBMForecast</th>\n",
       "      <th>SparkXGBForecast</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-15</td>\n",
       "      <td>430.964632</td>\n",
       "      <td>431.202969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-16</td>\n",
       "      <td>505.411960</td>\n",
       "      <td>504.030227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-17</td>\n",
       "      <td>9.889056</td>\n",
       "      <td>9.706636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-18</td>\n",
       "      <td>99.359694</td>\n",
       "      <td>96.258271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-19</td>\n",
       "      <td>196.307731</td>\n",
       "      <td>197.443618</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  unique_id         ds  SparkLGBMForecast  SparkXGBForecast\n",
       "0     id_00 2001-05-15         430.964632        431.202969\n",
       "1     id_00 2001-05-16         505.411960        504.030227\n",
       "2     id_00 2001-05-17           9.889056          9.706636\n",
       "3     id_00 2001-05-18          99.359694         96.258271\n",
       "4     id_00 2001-05-19         196.307731        197.443618"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6577a473-0b27-4a3b-84c5-4f6053ba6f67",
   "metadata": {},
   "source": [
    "### Saving and loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836cde59-2758-4f49-936d-789929f80a00",
   "metadata": {},
   "source": [
    "Once you've trained your model you can use the `DistributedMLForecast.save` method to save the artifacts for inference. Keep in mind that if you're on a remote cluster you should set a remote storage like S3 as the destination.\n",
    "\n",
    "mlforecast uses [fsspec](https://filesystem-spec.readthedocs.io/en/latest/) to handle the different filesystems, so if you're using s3 for example you also need to install [s3fs](https://s3fs.readthedocs.io/en/latest/). If you're using pip you can just include the aws extra, e.g. `pip install 'mlforecast[aws,spark]'`, which will install the required dependencies to perform distributed training with spark and saving to S3. If you're using conda you'll have to manually install them (`conda install fsspec fugue pyspark s3fs`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1bff80-c052-4a24-a225-33b774d7d75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "save_dir = build_unique_name('spark')\n",
    "save_path = f's3://nixtla-tmp/mlf/{save_dir}'\n",
    "fcst.save(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd4a6de-1c34-4112-bb18-2818b808b8eb",
   "metadata": {},
   "source": [
    "Once you've saved your forecast object you can then load it back by specifying the path where it was saved along with an engine, which will be used to perform the distributed computations (in this case the spark session)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6191d277-ce70-464b-a44f-0332a96ec7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fcst2 = DistributedMLForecast.load(save_path, engine=spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18808439-8531-4cd9-95bc-c28fd2c7987a",
   "metadata": {},
   "source": [
    "We can verify that this object produces the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04005646-d244-48c8-b11c-124a41d9740c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "preds = fa.as_pandas(fcst.predict(10)).sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "preds2 = fa.as_pandas(fcst2.predict(10)).sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "pd.testing.assert_frame_equal(preds, preds2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dd6f64-834f-48af-8cfa-4da2d7adfffa",
   "metadata": {},
   "source": [
    "### Converting to local\n",
    "\n",
    "Another option to store your distributed forecast object is to first turn it into a local one and then save it. Keep in mind that in order to do that all the remote data that is stored from the series will have to be pulled into a single machine (the scheduler in dask, driver in spark, etc.), so you have to be sure that it'll fit in memory, it should consume about 2x the size of your target column (you can reduce this further by using the `keep_last_n` argument in the `fit` method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7af4994-44dd-48e6-b48d-e8faa1a2a846",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_fcst = fcst.to_local()\n",
    "local_preds = local_fcst.predict(10)\n",
    "# we don't check the dtype because sometimes these are arrow dtypes\n",
    "# or different precisions of float\n",
    "pd.testing.assert_frame_equal(preds, local_preds, check_dtype=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad24338-2634-4b27-8f73-dc162338dd38",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb396b93-b160-4325-9d5c-bf391ef87db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_res = fcst.cross_validation(\n",
    "    spark_series,\n",
    "    n_windows=3,\n",
    "    h=14,\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c735385-7104-4ced-a253-d4a16b8bbb4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>SparkLGBMForecast</th>\n",
       "      <th>SparkXGBForecast</th>\n",
       "      <th>cutoff</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_15</td>\n",
       "      <td>2001-04-04</td>\n",
       "      <td>88.438691</td>\n",
       "      <td>86.105463</td>\n",
       "      <td>2001-04-02</td>\n",
       "      <td>92.468763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_25</td>\n",
       "      <td>2001-04-12</td>\n",
       "      <td>355.712493</td>\n",
       "      <td>354.525400</td>\n",
       "      <td>2001-04-02</td>\n",
       "      <td>320.701359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_03</td>\n",
       "      <td>2001-04-08</td>\n",
       "      <td>257.243845</td>\n",
       "      <td>253.834157</td>\n",
       "      <td>2001-04-02</td>\n",
       "      <td>274.420045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_14</td>\n",
       "      <td>2001-04-07</td>\n",
       "      <td>24.925278</td>\n",
       "      <td>23.833504</td>\n",
       "      <td>2001-04-02</td>\n",
       "      <td>26.906679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_01</td>\n",
       "      <td>2001-04-16</td>\n",
       "      <td>89.180665</td>\n",
       "      <td>90.743194</td>\n",
       "      <td>2001-04-02</td>\n",
       "      <td>93.807725</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  unique_id         ds  SparkLGBMForecast  SparkXGBForecast     cutoff  \\\n",
       "0     id_15 2001-04-04          88.438691         86.105463 2001-04-02   \n",
       "1     id_25 2001-04-12         355.712493        354.525400 2001-04-02   \n",
       "2     id_03 2001-04-08         257.243845        253.834157 2001-04-02   \n",
       "3     id_14 2001-04-07          24.925278         23.833504 2001-04-02   \n",
       "4     id_01 2001-04-16          89.180665         90.743194 2001-04-02   \n",
       "\n",
       "            y  \n",
       "0   92.468763  \n",
       "1  320.701359  \n",
       "2  274.420045  \n",
       "3   26.906679  \n",
       "4   93.807725  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a2f8e2-f8d8-4efb-8114-47eaf8703170",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c218f6f9-22d5-4ec1-9f04-3b6b6368ea69",
   "metadata": {},
   "source": [
    "## Ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e192699-ecda-4eb2-8274-fc1246e4d6e8",
   "metadata": {},
   "source": [
    "### Session setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387d064e-1025-4407-b701-ccad70aa90ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.cluster_utils import Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e753bdd4-73e0-4f96-84f8-1f148d879dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray_cluster = Cluster(\n",
    "    initialize_head=True,\n",
    "    head_node_args={\"num_cpus\": 2}\n",
    ")\n",
    "ray.init(address=ray_cluster.address, ignore_reinit_error=True)\n",
    "# add mock node to simulate a cluster\n",
    "mock_node = ray_cluster.add_node(num_cpus=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946b81d4-598d-4eec-820e-72a81177cedb",
   "metadata": {},
   "source": [
    "### Data setup\n",
    "For ray, the data must be a `ray DataFrame`. It is recommended that you have as many partitions as you have workers. If you have more partitions than workers make sure to set `num_threads=1` to avoid having nested parallelism.\n",
    "\n",
    "The required input format is the same as for `MLForecast`, i.e. it should have at least an id column, a time column and a target column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f033849-67d3-469c-9608-d589153890df",
   "metadata": {},
   "outputs": [],
   "source": [
    "series = generate_daily_series(100, n_static_features=2, equal_ends=True, static_as_categorical=False)\n",
    "# we need noncategory unique_id\n",
    "series['unique_id'] = series['unique_id'].astype(str)\n",
    "ray_series = ray.data.from_pandas(series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2726794-b6cd-462a-98b4-e8ec4c84ea75",
   "metadata": {},
   "source": [
    "### Models\n",
    "The ray integration allows to include `lightgbm` (`RayLGBMRegressor`), and `xgboost` (`RayXGBRegressor`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c023f51-1d0c-4596-a8d7-6f167c4ad257",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlforecast.distributed.models.ray.lgb import RayLGBMForecast\n",
    "from mlforecast.distributed.models.ray.xgb import RayXGBForecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2708a2b-a648-4ce1-8702-ca8a392c8f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [RayLGBMForecast(random_state=0), RayXGBForecast(random_state=0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d90b92-36d4-4691-98c9-e9b1698d2bd4",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de0dab0-cd60-4519-a207-c087f193f30f",
   "metadata": {},
   "source": [
    "To control the number of partitions to use using Ray, we have to include `num_partitions` to `DistributedMLForecast`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935fd794-b380-40e2-821c-dacdc513bce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_partitions = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355afcd8-44ba-44c6-a92d-0f45c6fc9f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst = DistributedMLForecast(\n",
    "    models,\n",
    "    freq='D',\n",
    "    target_transforms=[Differences([7])],\n",
    "    lags=[1],\n",
    "    lag_transforms={\n",
    "        1: [ExpandingMean(), ExponentiallyWeightedMean(alpha=0.9)],\n",
    "    },\n",
    "    date_features=['dayofweek'],\n",
    "    num_partitions=num_partitions, # Use num_partitions to reduce overhead\n",
    ")\n",
    "fcst.fit(\n",
    "    ray_series,\n",
    "    static_features=['static_0', 'static_1'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b9642b-17ca-4571-9f8a-f42c13a69987",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_partition_results_size(fcst, num_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c8ad06-2f0c-4041-ac25-9aadac94c025",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test num_partitions works properly\n",
    "# In this case we test that the default behavior \n",
    "# for ray datasets works as expected\n",
    "fcst_np = DistributedMLForecast(\n",
    "    models=models,\n",
    "    freq='D',\n",
    "    lags=[7],\n",
    "    lag_transforms={\n",
    "        1: [ExpandingMean()],\n",
    "        7: [RollingMean(window_size=14)]\n",
    "    },\n",
    "    date_features=['dayofweek', 'month'],\n",
    "    num_threads=1,\n",
    ")\n",
    "fcst_np.fit(ray_series)\n",
    "# we dont use test_partition_results_size\n",
    "# since the number of objects is different \n",
    "# from the number of partitions\n",
    "test_eq(fa.count(fcst_np._partition_results), 100) # number of series\n",
    "preds_np = fcst_np.predict(7).to_pandas().sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "preds = fcst.predict(7).to_pandas().sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "pd.testing.assert_frame_equal(\n",
    "    preds[['unique_id', 'ds']], \n",
    "    preds_np[['unique_id', 'ds']], \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c4af13-c04c-4e9b-ab26-92f6d7b9745d",
   "metadata": {},
   "source": [
    "### Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e16b63-aeec-41d8-acff-ca982d3b952c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = fcst.predict(14).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962ee74e-1346-4991-bcf2-5b2a887eb5b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>RayLGBMForecast</th>\n",
       "      <th>RayXGBForecast</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_01</td>\n",
       "      <td>2001-05-15</td>\n",
       "      <td>118.505341</td>\n",
       "      <td>118.32222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_01</td>\n",
       "      <td>2001-05-16</td>\n",
       "      <td>152.321457</td>\n",
       "      <td>152.265915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_01</td>\n",
       "      <td>2001-05-17</td>\n",
       "      <td>181.979599</td>\n",
       "      <td>181.945618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_01</td>\n",
       "      <td>2001-05-18</td>\n",
       "      <td>9.530758</td>\n",
       "      <td>9.543224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_01</td>\n",
       "      <td>2001-05-19</td>\n",
       "      <td>40.503441</td>\n",
       "      <td>40.661186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  unique_id         ds  RayLGBMForecast  RayXGBForecast\n",
       "0     id_01 2001-05-15       118.505341       118.32222\n",
       "1     id_01 2001-05-16       152.321457      152.265915\n",
       "2     id_01 2001-05-17       181.979599      181.945618\n",
       "3     id_01 2001-05-18         9.530758        9.543224\n",
       "4     id_01 2001-05-19        40.503441       40.661186"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15a314c-a920-44a4-9570-a4bdaa0e37b5",
   "metadata": {},
   "source": [
    "### Saving and loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a713dc-dd3d-45dc-9725-52f3f4d2742a",
   "metadata": {},
   "source": [
    "Once you've trained your model you can use the `DistributedMLForecast.save` method to save the artifacts for inference. Keep in mind that if you're on a remote cluster you should set a remote storage like S3 as the destination.\n",
    "\n",
    "mlforecast uses [fsspec](https://filesystem-spec.readthedocs.io/en/latest/) to handle the different filesystems, so if you're using s3 for example you also need to install [s3fs](https://s3fs.readthedocs.io/en/latest/). If you're using pip you can just include the aws extra, e.g. `pip install 'mlforecast[aws,ray]'`, which will install the required dependencies to perform distributed training with ray and saving to S3. If you're using conda you'll have to manually install them (`conda install fsspec fugue ray s3fs`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211d77aa-8481-40b8-a062-10d45bc1941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = build_unique_name('ray')\n",
    "save_path = f's3://nixtla-tmp/mlf/{save_dir}'\n",
    "fcst.save(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcce44d0-3035-4919-830a-d45a3d47d9b1",
   "metadata": {},
   "source": [
    "Once you've saved your forecast object you can then load it back by specifying the path where it was saved along with an engine, which will be used to perform the distributed computations (in this case the 'ray' string)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747e75aa-42e4-4c83-93f3-a4fa9a5097ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst2 = DistributedMLForecast.load(save_path, engine='ray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e83efc5-0e7c-4345-b86a-624737c28d15",
   "metadata": {},
   "source": [
    "We can verify that this object produces the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a17fcb-4f0f-4551-b9f9-e31ec9bb0296",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = fa.as_pandas(fcst.predict(10)).sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "preds2 = fa.as_pandas(fcst2.predict(10)).sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "pd.testing.assert_frame_equal(preds, preds2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ea42c6-8a0a-4a6d-8416-e78e0f7eccd8",
   "metadata": {},
   "source": [
    "### Converting to local\n",
    "\n",
    "Another option to store your distributed forecast object is to first turn it into a local one and then save it. Keep in mind that in order to do that all the remote data that is stored from the series will have to be pulled into a single machine (the scheduler in dask, driver in spark, etc.), so you have to be sure that it'll fit in memory, it should consume about 2x the size of your target column (you can reduce this further by using the `keep_last_n` argument in the `fit` method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f844ebe4-423f-4790-b5b1-d9550e4f1835",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_fcst = fcst.to_local()\n",
    "local_preds = local_fcst.predict(10)\n",
    "# we don't check the dtype because sometimes these are arrow dtypes\n",
    "# or different precisions of float\n",
    "pd.testing.assert_frame_equal(preds, local_preds, check_dtype=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb901de2-c98e-47fb-81e2-714010114a91",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a0ec2a-cc6a-4195-a457-8f6243334d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_res = fcst.cross_validation(\n",
    "    ray_series,\n",
    "    n_windows=3,\n",
    "    h=14,\n",
    ").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ecc467-80c7-4c0e-98d6-af77c9fc7fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>RayLGBMForecast</th>\n",
       "      <th>RayXGBForecast</th>\n",
       "      <th>cutoff</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_10</td>\n",
       "      <td>2001-05-01</td>\n",
       "      <td>24.767561</td>\n",
       "      <td>24.528799</td>\n",
       "      <td>2001-04-30</td>\n",
       "      <td>31.878545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_10</td>\n",
       "      <td>2001-05-07</td>\n",
       "      <td>1.916985</td>\n",
       "      <td>2.323445</td>\n",
       "      <td>2001-04-30</td>\n",
       "      <td>7.365955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_13</td>\n",
       "      <td>2001-05-01</td>\n",
       "      <td>210.900330</td>\n",
       "      <td>212.959320</td>\n",
       "      <td>2001-04-30</td>\n",
       "      <td>190.485236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_14</td>\n",
       "      <td>2001-05-01</td>\n",
       "      <td>196.620819</td>\n",
       "      <td>196.253036</td>\n",
       "      <td>2001-04-30</td>\n",
       "      <td>213.631212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_14</td>\n",
       "      <td>2001-05-03</td>\n",
       "      <td>323.323334</td>\n",
       "      <td>322.372894</td>\n",
       "      <td>2001-04-30</td>\n",
       "      <td>338.234837</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  unique_id         ds  RayLGBMForecast  RayXGBForecast     cutoff           y\n",
       "0     id_10 2001-05-01        24.767561       24.528799 2001-04-30   31.878545\n",
       "1     id_10 2001-05-07         1.916985        2.323445 2001-04-30    7.365955\n",
       "2     id_13 2001-05-01       210.900330      212.959320 2001-04-30  190.485236\n",
       "3     id_14 2001-05-01       196.620819      196.253036 2001-04-30  213.631212\n",
       "4     id_14 2001-05-03       323.323334      322.372894 2001-04-30  338.234837"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fba9328-7183-4744-9089-8ef02d3e9c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
