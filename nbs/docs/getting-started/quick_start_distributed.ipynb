{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b44ee1-af97-490a-ad62-03b2f804e006",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9693434b-0290-4ffd-88f9-a4267c11b548",
   "metadata": {},
   "source": [
    "# Quick start (distributed)\n",
    "\n",
    "> Minimal example of distributed training with MLForecast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07e9e05-e8b2-433e-9ff2-16e652cf6acb",
   "metadata": {},
   "source": [
    "The `DistributedMLForecast` class is a high level abstraction that encapsulates all the steps in the pipeline (preprocessing, fitting the model and computing predictions) and applies them in a distributed way.\n",
    "\n",
    "The different things that you need to use `DistributedMLForecast` (as opposed to `MLForecast`) are:\n",
    "\n",
    "1. You need to set up a cluster. We currently support dask, ray and spark.\n",
    "2. Your data needs to be a distributed collection (dask, ray or spark dataframe).\n",
    "3. You need to use a model that implements distributed training in your framework of choice, e.g. SynapseML for LightGBM in spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd7853f-df6e-4bf2-b2d9-cb3b4c6a0d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import git\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "from mlforecast.distributed import DistributedMLForecast\n",
    "from mlforecast.lag_transforms import ExpandingMean, RollingMean\n",
    "from mlforecast.target_transforms import Differences\n",
    "from mlforecast.utils import generate_daily_series, generate_prices_for_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f930a27-b15f-438a-aab2-ccafc77e09f4",
   "metadata": {},
   "source": [
    "## Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af70aed-bfb9-4c36-a4c4-cf546f8655a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fe3454-b2e1-40d0-a623-6ae1fe486536",
   "metadata": {},
   "source": [
    "### Client setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9868f8-38a5-4a9f-829c-47a10a522fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(n_workers=2, threads_per_worker=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd61b21-9a22-42dc-ba19-d6de68dc8eba",
   "metadata": {},
   "source": [
    "Here we define a client that connects to a `dask.distributed.LocalCluster`, however it could be any other kind of cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b968e6d-cbc8-499b-8a41-ef12fe4fdd69",
   "metadata": {},
   "source": [
    "### Data setup\n",
    "\n",
    "For dask, the data must be a `dask.dataframe.DataFrame`. You need to make sure that each time serie is only in one partition and it is recommended that you have as many partitions as you have workers. If you have more partitions than workers make sure to set `num_threads=1` to avoid having nested parallelism.\n",
    "\n",
    "The required input format is the same as for `MLForecast`, except that it's a `dask.dataframe.DataFrame` instead of a `pandas.Dataframe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8acdff6-49c0-4f28-8a2c-1a75e2c1c115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>static_0</th>\n",
       "      <th>static_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=10</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>object</td>\n",
       "      <td>datetime64[ns]</td>\n",
       "      <td>float64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_10</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_90</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_99</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<div>Dask Name: assign, 5 expressions</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "               unique_id              ds        y static_0 static_1\n",
       "npartitions=10                                                     \n",
       "id_00             object  datetime64[ns]  float64    int64    int64\n",
       "id_10                ...             ...      ...      ...      ...\n",
       "...                  ...             ...      ...      ...      ...\n",
       "id_90                ...             ...      ...      ...      ...\n",
       "id_99                ...             ...      ...      ...      ...\n",
       "Dask Name: assign, 5 expressions\n",
       "Expr=Assign(frame=MapPartitions(lambda))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series = generate_daily_series(100, n_static_features=2, equal_ends=True, static_as_categorical=False, min_length=500, max_length=1_000)\n",
    "npartitions = 10\n",
    "partitioned_series = dd.from_pandas(series.set_index('unique_id'), npartitions=npartitions)  # make sure we split by the id_col\n",
    "partitioned_series = partitioned_series.map_partitions(lambda df: df.reset_index())\n",
    "partitioned_series['unique_id'] = partitioned_series['unique_id'].astype(str)  # can't handle categoricals atm\n",
    "partitioned_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28aa4f88-1a4d-4175-a33c-80d34f61f8cc",
   "metadata": {},
   "source": [
    "### Models\n",
    "In order to perform distributed forecasting, we need to use a model that is able to train in a distributed way using `dask`. The current implementations are in `DaskLGBMForecast` and `DaskXGBForecast` which are just wrappers around the native implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5997af98-7a6e-468a-8f22-d7271b85ec67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlforecast.distributed.models.dask.lgb import DaskLGBMForecast\n",
    "from mlforecast.distributed.models.dask.xgb import DaskXGBForecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e482699-e3c3-4fef-b081-112c23bff40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [DaskXGBForecast(random_state=0), DaskLGBMForecast(random_state=0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab284f5f-e6d7-4d73-b0b6-efdf0aea581a",
   "metadata": {},
   "source": [
    "### Training\n",
    "Once we have our models we instantiate a `DistributedMLForecast` object defining our features. We can then call `fit` on this object passing our dask dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b5821d-6c04-4fd6-b5d6-9bbfefe9d442",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst = DistributedMLForecast(\n",
    "    models=models,\n",
    "    freq='D',\n",
    "    target_transforms=[Differences([7])],\n",
    "    lags=[7],\n",
    "    lag_transforms={\n",
    "        1: [ExpandingMean()],\n",
    "        7: [RollingMean(window_size=14)],\n",
    "    },\n",
    "    date_features=['dayofweek', 'month'],\n",
    "    num_threads=1,\n",
    "    engine=client,\n",
    ")\n",
    "fcst.fit(partitioned_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a49e861-74ab-42b0-a220-a823e5c7070a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import fugue.api as fa\n",
    "from fastcore.test import test_eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4241e4da-fc38-4744-bb88-e72c77e63ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# function to test the partition_results data\n",
    "# has the right size\n",
    "def test_partition_results_size(fcst_object, expected_n_partitions):\n",
    "    test_eq(\n",
    "        fa.get_num_partitions(fcst_object._partition_results),\n",
    "        expected_n_partitions,\n",
    "    )\n",
    "    test_eq(\n",
    "        fa.count(fcst_object._partition_results),\n",
    "        expected_n_partitions,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b4219d-4c48-43a1-87c4-58dfb520022c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_partition_results_size(fcst, npartitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5a8531-014f-4d0e-a33e-fe8504d164a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test num_partitions works properly\n",
    "num_partitions_test = 4\n",
    "test_dd = dd.from_pandas(series, npartitions=num_partitions_test) # In this case we dont have to specify the column\n",
    "test_dd['unique_id'] = test_dd['unique_id'].astype(str)\n",
    "fcst_np = DistributedMLForecast(\n",
    "    models=models,\n",
    "    freq='D',\n",
    "    target_transforms=[Differences([7])],    \n",
    "    lags=[7],\n",
    "    lag_transforms={\n",
    "        1: [ExpandingMean()],\n",
    "        7: [RollingMean(window_size=14)]\n",
    "    },\n",
    "    date_features=['dayofweek', 'month'],\n",
    "    num_threads=1,\n",
    "    engine=client,\n",
    "    num_partitions=num_partitions_test\n",
    ")\n",
    "fcst_np.fit(test_dd)\n",
    "test_partition_results_size(fcst_np, num_partitions_test)\n",
    "preds_np = fcst_np.predict(7).compute().sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "preds = fcst.predict(7).compute().sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "pd.testing.assert_frame_equal(\n",
    "    preds[['unique_id', 'ds']], \n",
    "    preds_np[['unique_id', 'ds']], \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f2d411-66f8-425a-bcfe-8c5cc28ca324",
   "metadata": {},
   "source": [
    "Once we have our fitted models we can compute the predictions for the next 7 timesteps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21511e5a-750b-4a27-8f68-e9ceff5c7536",
   "metadata": {},
   "source": [
    "### Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58769a05-3961-49c9-9b50-e1b0713d8c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>DaskXGBForecast</th>\n",
       "      <th>DaskLGBMForecast</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2002-09-27 00:00:00</td>\n",
       "      <td>22.049939</td>\n",
       "      <td>21.938783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2002-09-28 00:00:00</td>\n",
       "      <td>81.800035</td>\n",
       "      <td>83.052487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2002-09-29 00:00:00</td>\n",
       "      <td>161.911589</td>\n",
       "      <td>163.521957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2002-09-30 00:00:00</td>\n",
       "      <td>246.184005</td>\n",
       "      <td>245.715239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2002-10-01 00:00:00</td>\n",
       "      <td>315.07874</td>\n",
       "      <td>315.36219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  unique_id                   ds  DaskXGBForecast  DaskLGBMForecast\n",
       "0     id_00  2002-09-27 00:00:00        22.049939         21.938783\n",
       "1     id_00  2002-09-28 00:00:00        81.800035         83.052487\n",
       "2     id_00  2002-09-29 00:00:00       161.911589        163.521957\n",
       "3     id_00  2002-09-30 00:00:00       246.184005        245.715239\n",
       "4     id_00  2002-10-01 00:00:00        315.07874         315.36219"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = fcst.predict(7).compute()\n",
    "preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e6f1f2-d22b-48d7-a5e5-77d400cd4625",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "preds2 = fcst.predict(7).compute()\n",
    "preds3 = fcst.predict(7, new_df=partitioned_series).compute()\n",
    "pd.testing.assert_frame_equal(preds, preds2)\n",
    "pd.testing.assert_frame_equal(preds, preds3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d26427d-314e-4589-8567-097ddf5adce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test X_df\n",
    "prices = generate_prices_for_series(series)\n",
    "series_wexog = series.merge(prices, on=['unique_id', 'ds'])\n",
    "npartitions = 10\n",
    "partitioned_series_exog = dd.from_pandas(series_wexog.set_index('unique_id'), npartitions=npartitions)\n",
    "partitioned_series_exog = partitioned_series_exog.map_partitions(lambda df: df.reset_index())\n",
    "partitioned_series_exog['unique_id'] = partitioned_series_exog['unique_id'].astype(str)\n",
    "fcst_exog = DistributedMLForecast(\n",
    "    models=models,\n",
    "    freq='D',\n",
    "    target_transforms=[Differences([7])],    \n",
    "    lags=[7],\n",
    "    lag_transforms={\n",
    "        1: [ExpandingMean()],\n",
    "        7: [RollingMean(window_size=14)]\n",
    "    },\n",
    "    date_features=['dayofweek', 'month'],\n",
    "    num_threads=1,\n",
    "    engine=client,\n",
    ")\n",
    "fcst_exog.fit(partitioned_series_exog, static_features=['static_0', 'static_1'])\n",
    "preds_exog = fcst_exog.predict(h=7, X_df=prices).compute()\n",
    "full_preds = preds.merge(preds_exog, on=['unique_id', 'ds'], suffixes=('', '_exog'))\n",
    "for model in ('DaskXGBForecast', 'DaskLGBMForecast'):\n",
    "    pct_diff = abs(1 - full_preds[f'{model}_exog'].div(full_preds[f'{model}']).mean())\n",
    "    assert 0 < pct_diff < 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502aeadd-2fd5-4d16-8dfb-56a77080c072",
   "metadata": {},
   "source": [
    "### Saving and loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10073e2c-c36e-49d2-8996-b90fde11123a",
   "metadata": {},
   "source": [
    "Once you've trained your model you can use the `DistributedMLForecast.save` method to save the artifacts for inference. Keep in mind that if you're on a remote cluster you should set a remote storage like S3 as the destination.\n",
    "\n",
    "mlforecast uses [fsspec](https://filesystem-spec.readthedocs.io/en/latest/) to handle the different filesystems, so if you're using s3 for example you also need to install [s3fs](https://s3fs.readthedocs.io/en/latest/). If you're using pip you can just include the aws extra, e.g. `pip install 'mlforecast[aws,dask]'`, which will install the required dependencies to perform distributed training with dask and saving to S3. If you're using conda you'll have to manually install them (`conda install dask fsspec fugue s3fs`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486bc64f-9567-40be-a6df-70efc7f4dc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define unique name for CI\n",
    "def build_unique_name(engine):\n",
    "    pyver = f'{sys.version_info.major}_{sys.version_info.minor}'\n",
    "    repo = git.Repo(search_parent_directories=True)\n",
    "    sha = repo.head.object.hexsha\n",
    "    return f'{sys.platform}-{pyver}-{engine}-{sha}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebb761c-8183-4aab-86b9-84ff3277e9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = build_unique_name('dask')\n",
    "save_path = f's3://nixtla-tmp/mlf/{save_dir}'\n",
    "fcst.save(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7147a8d-5860-4eb8-88b2-78fdc96aa156",
   "metadata": {},
   "source": [
    "Once you've saved your forecast object you can then load it back by specifying the path where it was saved along with an engine, which will be used to perform the distributed computations (in this case the dask client)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ba10a0-9562-4373-a755-40fff7402e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst2 = DistributedMLForecast.load(save_path, engine=client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c6ec8a-8a50-4bd6-b16e-af57bd0063d8",
   "metadata": {},
   "source": [
    "We can verify that this object produces the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f982ed67-d7be-485b-9d25-82f34ba2fe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = fa.as_pandas(fcst.predict(10)).sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "preds2 = fa.as_pandas(fcst2.predict(10)).sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "pd.testing.assert_frame_equal(preds, preds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df49be1-f85c-4817-b873-5dddb413c50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "class Lag1Model(BaseEstimator):\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    @property\n",
    "    def model_(self):\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return X['lag1']\n",
    "\n",
    "upd_fcst = DistributedMLForecast(\n",
    "    models=[Lag1Model()],\n",
    "    freq='D',\n",
    "    lags=[1],\n",
    "    num_threads=1,\n",
    "    engine=client,\n",
    ")\n",
    "upd_fcst.fit(partitioned_series)\n",
    "\n",
    "new_df = (series.groupby('unique_id', observed=True)['ds'].max() + pd.offsets.Day()).reset_index()\n",
    "new_df['y'] = -1.0\n",
    "upd_fcst.update(new_df)\n",
    "expected = new_df.rename(columns={'y': 'Lag1Model'})\n",
    "expected = expected.astype({'unique_id': str})\n",
    "expected['ds'] += pd.offsets.Day()\n",
    "upd_preds = upd_fcst.predict(1).compute()\n",
    "pd.testing.assert_frame_equal(\n",
    "    upd_preds.reset_index(drop=True),\n",
    "    expected.reset_index(drop=True),\n",
    "    check_dtype=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4190830c-b2e5-4343-97da-023e9f532ef6",
   "metadata": {},
   "source": [
    "### Converting to local\n",
    "\n",
    "Another option to store your distributed forecast object is to first turn it into a local one and then save it. Keep in mind that in order to do that all the remote data that is stored from the series will have to be pulled into a single machine (the scheduler in dask, driver in spark, etc.), so you have to be sure that it'll fit in memory, it should consume about 2x the size of your target column (you can reduce this further by using the `keep_last_n` argument in the `fit` method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f2cefe-d032-413e-89d3-5d01271c5d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_fcst = fcst.to_local()\n",
    "local_preds = local_fcst.predict(10)\n",
    "# we don't check the dtype because sometimes these are arrow dtypes\n",
    "# or different precisions of float\n",
    "pd.testing.assert_frame_equal(preds, local_preds, check_dtype=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29841c02-b0bc-44cc-a8f3-da31b442584b",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296af9be-4149-4b47-9386-fc92bff65d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_res = fcst.cross_validation(\n",
    "    partitioned_series,\n",
    "    n_windows=3,\n",
    "    h=14,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e80450-d582-42bb-8bf4-ff925d5e74e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>DaskXGBForecast</th>\n",
       "      <th>DaskLGBMForecast</th>\n",
       "      <th>cutoff</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>id_06</td>\n",
       "      <td>2002-08-24 00:00:00</td>\n",
       "      <td>138.895726</td>\n",
       "      <td>139.139591</td>\n",
       "      <td>2002-08-15 00:00:00</td>\n",
       "      <td>146.403053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>id_07</td>\n",
       "      <td>2002-08-16 00:00:00</td>\n",
       "      <td>15.346953</td>\n",
       "      <td>8.762784</td>\n",
       "      <td>2002-08-15 00:00:00</td>\n",
       "      <td>5.608704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>id_09</td>\n",
       "      <td>2002-08-16 00:00:00</td>\n",
       "      <td>132.040204</td>\n",
       "      <td>132.314999</td>\n",
       "      <td>2002-08-15 00:00:00</td>\n",
       "      <td>157.391349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>id_09</td>\n",
       "      <td>2002-08-27 00:00:00</td>\n",
       "      <td>402.353301</td>\n",
       "      <td>402.504638</td>\n",
       "      <td>2002-08-15 00:00:00</td>\n",
       "      <td>400.246358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>id_15</td>\n",
       "      <td>2002-08-21 00:00:00</td>\n",
       "      <td>121.353495</td>\n",
       "      <td>121.678176</td>\n",
       "      <td>2002-08-15 00:00:00</td>\n",
       "      <td>134.06434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    unique_id                   ds  DaskXGBForecast  DaskLGBMForecast  \\\n",
       "92      id_06  2002-08-24 00:00:00       138.895726        139.139591   \n",
       "98      id_07  2002-08-16 00:00:00        15.346953          8.762784   \n",
       "126     id_09  2002-08-16 00:00:00       132.040204        132.314999   \n",
       "137     id_09  2002-08-27 00:00:00       402.353301        402.504638   \n",
       "75      id_15  2002-08-21 00:00:00       121.353495        121.678176   \n",
       "\n",
       "                  cutoff           y  \n",
       "92   2002-08-15 00:00:00  146.403053  \n",
       "98   2002-08-15 00:00:00    5.608704  \n",
       "126  2002-08-15 00:00:00  157.391349  \n",
       "137  2002-08-15 00:00:00  400.246358  \n",
       "75   2002-08-15 00:00:00   134.06434  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_res.compute().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3d27b0-ba00-4141-a50c-ab39385e8295",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from mlforecast.distributed.forecast import WindowInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62457500-a299-4eb7-b13f-78412f4d302e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# input_size\n",
    "input_size = 100\n",
    "reduced_train = fcst._preprocess(\n",
    "    partitioned_series,\n",
    "    id_col='unique_id',\n",
    "    time_col='ds',\n",
    "    target_col='y',\n",
    "    dropna=False,\n",
    "    window_info=WindowInfo(\n",
    "        n_windows=1,\n",
    "        window_size=10,\n",
    "        step_size=None,\n",
    "        i_window=0,\n",
    "        input_size=input_size,\n",
    "    ),\n",
    ")\n",
    "assert reduced_train.groupby('unique_id').size().compute().max() == input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429cdca0-6368-42d9-bc57-2390e6430b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "cv_res_no_refit = fcst.cross_validation(\n",
    "    partitioned_series,\n",
    "    n_windows=3,\n",
    "    h=14,\n",
    "    refit=False\n",
    ")\n",
    "cv_results_df = cv_res.compute().sort_values(['unique_id', 'ds'])\n",
    "cv_results_no_refit_df = cv_res_no_refit.compute().sort_values(['unique_id', 'ds'])\n",
    "# test we recover the same \"metadata\"\n",
    "models = ['DaskXGBForecast', 'DaskLGBMForecast']\n",
    "test_eq(\n",
    "    cv_results_no_refit_df.drop(columns=models),\n",
    "    cv_results_df.drop(columns=models)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34651ae-5be4-4e86-b44f-fbd15c4d72ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "non_std_series = partitioned_series.copy()\n",
    "non_std_series = non_std_series.rename(columns={'ds': 'time', 'y': 'value', 'unique_id': 'some_id'})\n",
    "flow_params = dict(\n",
    "    models=[DaskXGBForecast(random_state=0)],\n",
    "    target_transforms=[Differences([7])],    \n",
    "    lags=[7],\n",
    "    lag_transforms={\n",
    "        1: [ExpandingMean()],\n",
    "        7: [RollingMean(window_size=14)]\n",
    "    },\n",
    "    num_threads=1,\n",
    ")\n",
    "fcst = DistributedMLForecast(freq='D', **flow_params)\n",
    "fcst.fit(partitioned_series)\n",
    "preds = fcst.predict(7).compute()\n",
    "fcst2 = DistributedMLForecast(freq='D', **flow_params)\n",
    "fcst2.preprocess(non_std_series, id_col='some_id', time_col='time', target_col='value')\n",
    "fcst2.models_ = fcst.models_  # distributed training can end up with different fits\n",
    "non_std_preds = fcst2.predict(7).compute()\n",
    "pd.testing.assert_frame_equal(\n",
    "    preds.drop(columns='ds'),\n",
    "    non_std_preds.drop(columns='time').rename(columns={'some_id': 'unique_id'})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba9b3ae-3859-4989-aa56-85390f63cf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327df016-bc2a-47c1-afeb-4394ca7695cb",
   "metadata": {},
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfae838-6305-4a48-8a82-2f3c3eb653f5",
   "metadata": {},
   "source": [
    "### Session setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97595792-8173-4d0e-ac56-174c3d698d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9119c014-b51b-4ba0-ab28-585665c03ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.jars.packages\", \"com.microsoft.azure:synapseml_2.12:0.10.2\")\n",
    "    .config(\"spark.jars.repositories\", \"https://mmlspark.azureedge.net/maven\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0371e5b-5700-4466-86e3-07e35499200b",
   "metadata": {},
   "source": [
    "### Data setup\n",
    "For spark, the data must be a `pyspark DataFrame`. You need to make sure that each time serie is only in one partition (which you can do using `repartitionByRange`, for example) and it is recommended that you have as many partitions as you have workers. If you have more partitions than workers make sure to set `num_threads=1` to avoid having nested parallelism.\n",
    "\n",
    "The required input format is the same as for `MLForecast`, i.e. it should have at least an id column, a time column and a target column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc337af2-0952-4f7c-98ad-baa87bb16d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "numPartitions = 4\n",
    "series = generate_daily_series(100, n_static_features=2, equal_ends=True, static_as_categorical=False)\n",
    "spark_series = spark.createDataFrame(series).repartitionByRange(numPartitions, 'unique_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959aec60-ed2e-4538-8d6c-09b0a66bc35d",
   "metadata": {},
   "source": [
    "### Models\n",
    "In order to perform distributed forecasting, we need to use a model that is able to train in a distributed way using `spark`. The current implementations are in `SparkLGBMForecast` and `SparkXGBForecast` which are just wrappers around the native implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cea90e-ea7b-4244-b26f-a16d3e85fe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlforecast.distributed.models.spark.lgb import SparkLGBMForecast\n",
    "from mlforecast.distributed.models.spark.xgb import SparkXGBForecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03231a34-30a0-44a3-805a-c91ce6eba393",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [SparkLGBMForecast(seed=0), SparkXGBForecast(random_state=0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690336b5-ad1a-4ff2-a42b-fd2bf15870fe",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a458c1-855d-4955-b77b-5dbdb5ecacf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst = DistributedMLForecast(\n",
    "    models,\n",
    "    freq='D',\n",
    "    target_transforms=[Differences([7])],    \n",
    "    lags=[1],\n",
    "    lag_transforms={\n",
    "        1: [ExpandingMean()],\n",
    "    },\n",
    "    date_features=['dayofweek'],\n",
    ")\n",
    "fcst.fit(\n",
    "    spark_series,\n",
    "    static_features=['static_0', 'static_1'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33443e25-3a35-4abc-85f1-2aa774d64689",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_partition_results_size(fcst, numPartitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa7ebdd-6d6a-4e0d-babf-d7fe11cd09ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test num_partitions works properly\n",
    "test_spark_df = spark.createDataFrame(series)\n",
    "num_partitions_test = 10\n",
    "fcst_np = DistributedMLForecast(\n",
    "    models=models,\n",
    "    freq='D',    \n",
    "    lags=[7],\n",
    "    lag_transforms={\n",
    "        1: [ExpandingMean()],\n",
    "        7: [RollingMean(window_size=14)]\n",
    "    },\n",
    "    date_features=['dayofweek', 'month'],\n",
    "    num_threads=1,\n",
    "    num_partitions=num_partitions_test,\n",
    ")\n",
    "fcst_np.fit(test_spark_df)\n",
    "test_partition_results_size(fcst_np, num_partitions_test)\n",
    "preds_np = fcst_np.predict(7).toPandas().sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "preds = fcst.predict(7).toPandas().sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "pd.testing.assert_frame_equal(\n",
    "    preds[['unique_id', 'ds']], \n",
    "    preds_np[['unique_id', 'ds']], \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a1b654-5641-4b4d-b503-6606f94396cd",
   "metadata": {},
   "source": [
    "### Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06d2230-60f5-47f1-820b-af2ca7311b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = fcst.predict(14).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da6bbd2-9806-4b12-91a2-1b5571ae1550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>SparkLGBMForecast</th>\n",
       "      <th>SparkXGBForecast</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-15</td>\n",
       "      <td>431.677682</td>\n",
       "      <td>424.488985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-16</td>\n",
       "      <td>503.673189</td>\n",
       "      <td>502.923172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-17</td>\n",
       "      <td>8.150285</td>\n",
       "      <td>8.019412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-18</td>\n",
       "      <td>97.620923</td>\n",
       "      <td>97.031792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-19</td>\n",
       "      <td>194.568960</td>\n",
       "      <td>193.862475</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  unique_id         ds  SparkLGBMForecast  SparkXGBForecast\n",
       "0     id_00 2001-05-15         431.677682        424.488985\n",
       "1     id_00 2001-05-16         503.673189        502.923172\n",
       "2     id_00 2001-05-17           8.150285          8.019412\n",
       "3     id_00 2001-05-18          97.620923         97.031792\n",
       "4     id_00 2001-05-19         194.568960        193.862475"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6577a473-0b27-4a3b-84c5-4f6053ba6f67",
   "metadata": {},
   "source": [
    "### Saving and loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836cde59-2758-4f49-936d-789929f80a00",
   "metadata": {},
   "source": [
    "Once you've trained your model you can use the `DistributedMLForecast.save` method to save the artifacts for inference. Keep in mind that if you're on a remote cluster you should set a remote storage like S3 as the destination.\n",
    "\n",
    "mlforecast uses [fsspec](https://filesystem-spec.readthedocs.io/en/latest/) to handle the different filesystems, so if you're using s3 for example you also need to install [s3fs](https://s3fs.readthedocs.io/en/latest/). If you're using pip you can just include the aws extra, e.g. `pip install 'mlforecast[aws,spark]'`, which will install the required dependencies to perform distributed training with spark and saving to S3. If you're using conda you'll have to manually install them (`conda install fsspec fugue pyspark s3fs`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1bff80-c052-4a24-a225-33b774d7d75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = build_unique_name('spark')\n",
    "save_path = f's3://nixtla-tmp/mlf/{save_dir}'\n",
    "fcst.save(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd4a6de-1c34-4112-bb18-2818b808b8eb",
   "metadata": {},
   "source": [
    "Once you've saved your forecast object you can then load it back by specifying the path where it was saved along with an engine, which will be used to perform the distributed computations (in this case the spark session)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6191d277-ce70-464b-a44f-0332a96ec7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst2 = DistributedMLForecast.load(save_path, engine=spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18808439-8531-4cd9-95bc-c28fd2c7987a",
   "metadata": {},
   "source": [
    "We can verify that this object produces the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04005646-d244-48c8-b11c-124a41d9740c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = fa.as_pandas(fcst.predict(10)).sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "preds2 = fa.as_pandas(fcst2.predict(10)).sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "pd.testing.assert_frame_equal(preds, preds2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dd6f64-834f-48af-8cfa-4da2d7adfffa",
   "metadata": {},
   "source": [
    "### Converting to local\n",
    "\n",
    "Another option to store your distributed forecast object is to first turn it into a local one and then save it. Keep in mind that in order to do that all the remote data that is stored from the series will have to be pulled into a single machine (the scheduler in dask, driver in spark, etc.), so you have to be sure that it'll fit in memory, it should consume about 2x the size of your target column (you can reduce this further by using the `keep_last_n` argument in the `fit` method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7af4994-44dd-48e6-b48d-e8faa1a2a846",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_fcst = fcst.to_local()\n",
    "local_preds = local_fcst.predict(10)\n",
    "# we don't check the dtype because sometimes these are arrow dtypes\n",
    "# or different precisions of float\n",
    "pd.testing.assert_frame_equal(preds, local_preds, check_dtype=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad24338-2634-4b27-8f73-dc162338dd38",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb396b93-b160-4325-9d5c-bf391ef87db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_res = fcst.cross_validation(\n",
    "    spark_series,\n",
    "    n_windows=3,\n",
    "    h=14,\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c735385-7104-4ced-a253-d4a16b8bbb4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>SparkLGBMForecast</th>\n",
       "      <th>SparkXGBForecast</th>\n",
       "      <th>cutoff</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_12</td>\n",
       "      <td>2001-04-03</td>\n",
       "      <td>342.978379</td>\n",
       "      <td>341.930127</td>\n",
       "      <td>2001-04-02</td>\n",
       "      <td>328.907629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_23</td>\n",
       "      <td>2001-04-03</td>\n",
       "      <td>429.591043</td>\n",
       "      <td>428.320398</td>\n",
       "      <td>2001-04-02</td>\n",
       "      <td>424.716749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_26</td>\n",
       "      <td>2001-04-10</td>\n",
       "      <td>7.554284</td>\n",
       "      <td>7.707686</td>\n",
       "      <td>2001-04-02</td>\n",
       "      <td>19.814264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_18</td>\n",
       "      <td>2001-04-11</td>\n",
       "      <td>98.885044</td>\n",
       "      <td>98.848126</td>\n",
       "      <td>2001-04-02</td>\n",
       "      <td>98.877898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-04-13</td>\n",
       "      <td>122.727000</td>\n",
       "      <td>117.713487</td>\n",
       "      <td>2001-04-02</td>\n",
       "      <td>98.526008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  unique_id         ds  SparkLGBMForecast  SparkXGBForecast     cutoff  \\\n",
       "0     id_12 2001-04-03         342.978379        341.930127 2001-04-02   \n",
       "1     id_23 2001-04-03         429.591043        428.320398 2001-04-02   \n",
       "2     id_26 2001-04-10           7.554284          7.707686 2001-04-02   \n",
       "3     id_18 2001-04-11          98.885044         98.848126 2001-04-02   \n",
       "4     id_00 2001-04-13         122.727000        117.713487 2001-04-02   \n",
       "\n",
       "            y  \n",
       "0  328.907629  \n",
       "1  424.716749  \n",
       "2   19.814264  \n",
       "3   98.877898  \n",
       "4   98.526008  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a2f8e2-f8d8-4efb-8114-47eaf8703170",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c218f6f9-22d5-4ec1-9f04-3b6b6368ea69",
   "metadata": {},
   "source": [
    "## Ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e192699-ecda-4eb2-8274-fc1246e4d6e8",
   "metadata": {},
   "source": [
    "### Session setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387d064e-1025-4407-b701-ccad70aa90ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.cluster_utils import Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e753bdd4-73e0-4f96-84f8-1f148d879dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray_cluster = Cluster(\n",
    "    initialize_head=True,\n",
    "    head_node_args={\"num_cpus\": 2}\n",
    ")\n",
    "ray.init(address=ray_cluster.address, ignore_reinit_error=True)\n",
    "# add mock node to simulate a cluster\n",
    "mock_node = ray_cluster.add_node(num_cpus=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946b81d4-598d-4eec-820e-72a81177cedb",
   "metadata": {},
   "source": [
    "### Data setup\n",
    "For ray, the data must be a `ray DataFrame`. It is recommended that you have as many partitions as you have workers. If you have more partitions than workers make sure to set `num_threads=1` to avoid having nested parallelism.\n",
    "\n",
    "The required input format is the same as for `MLForecast`, i.e. it should have at least an id column, a time column and a target column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f033849-67d3-469c-9608-d589153890df",
   "metadata": {},
   "outputs": [],
   "source": [
    "series = generate_daily_series(100, n_static_features=2, equal_ends=True, static_as_categorical=False)\n",
    "# we need noncategory unique_id\n",
    "series['unique_id'] = series['unique_id'].astype(str)\n",
    "ray_series = ray.data.from_pandas(series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2726794-b6cd-462a-98b4-e8ec4c84ea75",
   "metadata": {},
   "source": [
    "### Models\n",
    "The ray integration allows to include `lightgbm` (`RayLGBMRegressor`), and `xgboost` (`RayXGBRegressor`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c023f51-1d0c-4596-a8d7-6f167c4ad257",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlforecast.distributed.models.ray.lgb import RayLGBMForecast\n",
    "from mlforecast.distributed.models.ray.xgb import RayXGBForecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2708a2b-a648-4ce1-8702-ca8a392c8f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [RayLGBMForecast(random_state=0), RayXGBForecast(random_state=0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d90b92-36d4-4691-98c9-e9b1698d2bd4",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de0dab0-cd60-4519-a207-c087f193f30f",
   "metadata": {},
   "source": [
    "To control the number of partitions to use using Ray, we have to include `num_partitions` to `DistributedMLForecast`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935fd794-b380-40e2-821c-dacdc513bce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_partitions = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355afcd8-44ba-44c6-a92d-0f45c6fc9f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst = DistributedMLForecast(\n",
    "    models,\n",
    "    freq='D',\n",
    "    target_transforms=[Differences([7])],\n",
    "    lags=[1],\n",
    "    lag_transforms={\n",
    "        1: [ExpandingMean()],\n",
    "    },\n",
    "    date_features=['dayofweek'],\n",
    "    num_partitions=num_partitions, # Use num_partitions to reduce overhead\n",
    ")\n",
    "fcst.fit(\n",
    "    ray_series,\n",
    "    static_features=['static_0', 'static_1'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b9642b-17ca-4571-9f8a-f42c13a69987",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_partition_results_size(fcst, num_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c8ad06-2f0c-4041-ac25-9aadac94c025",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test num_partitions works properly\n",
    "# In this case we test that the default behavior \n",
    "# for ray datasets works as expected\n",
    "fcst_np = DistributedMLForecast(\n",
    "    models=models,\n",
    "    freq='D',\n",
    "    lags=[7],\n",
    "    lag_transforms={\n",
    "        1: [ExpandingMean()],\n",
    "        7: [RollingMean(window_size=14)]\n",
    "    },\n",
    "    date_features=['dayofweek', 'month'],\n",
    "    num_threads=1,\n",
    ")\n",
    "fcst_np.fit(ray_series)\n",
    "# we dont use test_partition_results_size\n",
    "# since the number of objects is different \n",
    "# from the number of partitions\n",
    "test_eq(fa.count(fcst_np._partition_results), 100) # number of series\n",
    "preds_np = fcst_np.predict(7).to_pandas().sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "preds = fcst.predict(7).to_pandas().sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "pd.testing.assert_frame_equal(\n",
    "    preds[['unique_id', 'ds']], \n",
    "    preds_np[['unique_id', 'ds']], \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c4af13-c04c-4e9b-ab26-92f6d7b9745d",
   "metadata": {},
   "source": [
    "### Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e16b63-aeec-41d8-acff-ca982d3b952c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = fcst.predict(14).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962ee74e-1346-4991-bcf2-5b2a887eb5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15a314c-a920-44a4-9570-a4bdaa0e37b5",
   "metadata": {},
   "source": [
    "### Saving and loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a713dc-dd3d-45dc-9725-52f3f4d2742a",
   "metadata": {},
   "source": [
    "Once you've trained your model you can use the `DistributedMLForecast.save` method to save the artifacts for inference. Keep in mind that if you're on a remote cluster you should set a remote storage like S3 as the destination.\n",
    "\n",
    "mlforecast uses [fsspec](https://filesystem-spec.readthedocs.io/en/latest/) to handle the different filesystems, so if you're using s3 for example you also need to install [s3fs](https://s3fs.readthedocs.io/en/latest/). If you're using pip you can just include the aws extra, e.g. `pip install 'mlforecast[aws,ray]'`, which will install the required dependencies to perform distributed training with ray and saving to S3. If you're using conda you'll have to manually install them (`conda install fsspec fugue ray s3fs`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211d77aa-8481-40b8-a062-10d45bc1941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = build_unique_name('ray')\n",
    "save_path = f's3://nixtla-tmp/mlf/{save_dir}'\n",
    "fcst.save(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcce44d0-3035-4919-830a-d45a3d47d9b1",
   "metadata": {},
   "source": [
    "Once you've saved your forecast object you can then load it back by specifying the path where it was saved along with an engine, which will be used to perform the distributed computations (in this case the 'ray' string)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747e75aa-42e4-4c83-93f3-a4fa9a5097ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst2 = DistributedMLForecast.load(save_path, engine='ray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e83efc5-0e7c-4345-b86a-624737c28d15",
   "metadata": {},
   "source": [
    "We can verify that this object produces the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a17fcb-4f0f-4551-b9f9-e31ec9bb0296",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = fa.as_pandas(fcst.predict(10)).sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "preds2 = fa.as_pandas(fcst2.predict(10)).sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "pd.testing.assert_frame_equal(preds, preds2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ea42c6-8a0a-4a6d-8416-e78e0f7eccd8",
   "metadata": {},
   "source": [
    "### Converting to local\n",
    "\n",
    "Another option to store your distributed forecast object is to first turn it into a local one and then save it. Keep in mind that in order to do that all the remote data that is stored from the series will have to be pulled into a single machine (the scheduler in dask, driver in spark, etc.), so you have to be sure that it'll fit in memory, it should consume about 2x the size of your target column (you can reduce this further by using the `keep_last_n` argument in the `fit` method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f844ebe4-423f-4790-b5b1-d9550e4f1835",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_fcst = fcst.to_local()\n",
    "local_preds = local_fcst.predict(10)\n",
    "# we don't check the dtype because sometimes these are arrow dtypes\n",
    "# or different precisions of float\n",
    "pd.testing.assert_frame_equal(preds, local_preds, check_dtype=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb901de2-c98e-47fb-81e2-714010114a91",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a0ec2a-cc6a-4195-a457-8f6243334d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_res = fcst.cross_validation(\n",
    "    ray_series,\n",
    "    n_windows=3,\n",
    "    h=14,\n",
    ").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ecc467-80c7-4c0e-98d6-af77c9fc7fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fba9328-7183-4744-9089-8ef02d3e9c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
