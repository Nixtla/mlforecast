{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Frequency based param spaces place holder as dicts, needs expansion!\"\"\"\n",
    "\n",
    "from mlforecast.target_transforms import Differences, LocalStandardScaler\n",
    "\n",
    "\n",
    "hourly = {\n",
    "    'min_lags': 1,\n",
    "    'max_lags': 24,\n",
    "    'target_transforms': [[LocalStandardScaler()], [Differences([1])], [Differences([1]), LocalStandardScaler()]]\n",
    "    }\n",
    "\n",
    "weekly = {\n",
    "    'min_lags': 1,\n",
    "    'max_lags': 52,\n",
    "    'target_transforms': [[LocalStandardScaler()], [Differences([1])], [Differences([1]), LocalStandardScaler()]]\n",
    "    }\n",
    "\n",
    "monthly = {\n",
    "    'min_lags': 1,\n",
    "    'max_lags': 12,\n",
    "    'target_transforms': [[LocalStandardScaler()], [Differences([1])], [Differences([1]), LocalStandardScaler()]]\n",
    "    }\n",
    "\n",
    "daily = {\n",
    "    'min_lags': 1,\n",
    "    'max_lags': 7,\n",
    "    'target_transforms': [[LocalStandardScaler()], [Differences([1])], [Differences([1]), LocalStandardScaler()]]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Model Specific param space functions\"\"\"\n",
    "def lightgbm(trial):\n",
    "    return {\n",
    "        \"n_estimators\": trial.suggest_int(name=\"n_estimators\", low=20, high=1000),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 1, 100),\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 512),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', .1, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', .1, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 0, 15),\n",
    "        \"objective\": trial.suggest_categorical(\"objective\", ['regression', 'regression_l1']),\n",
    "    }\n",
    "    \n",
    "def xgboost(trial):\n",
    "    return {\n",
    "        \"n_estimators\": trial.suggest_int(name=\"n_estimators\", low=50, high=1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, .2, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', .1, 1.0),\n",
    "        'bagging_freq': trial.suggest_float('bagging_freq', .1, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', .1, 1.0),\n",
    "        'min_data_in_leaf': trial.suggest_float('min_data_in_leaf', 1, 100),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 1.0, log=True),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 1.0, log=True),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 2, 10),\n",
    "    }\n",
    "    \n",
    "def catboost(trial):\n",
    "    return {\n",
    "        \"n_estimators\": trial.suggest_int(name=\"n_estimators\", low=50, high=1000),\n",
    "        'depth': trial.suggest_int('depth', 1, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, .2, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', .1, 1.0),\n",
    "        'colsample_bylevel': trial.suggest_float('colsample_bylevel', .1, 1.0),\n",
    "        'min_data_in_leaf': trial.suggest_float('min_data_in_leaf', 1, 100),\n",
    "    }\n",
    "    \n",
    "def linear_regression(trial):\n",
    "    return {\n",
    "        \"fit_intercept\": trial.suggest_categorical(\"fit_intercept\", [True, False])\n",
    "    }\n",
    "    \n",
    "def ridge(trial):\n",
    "    return {\n",
    "        \"fit_intercept\": trial.suggest_categorical(\"fit_intercept\", [True, False]),\n",
    "        'alpha': trial.suggest_float('alpha', .001, 10.0)\n",
    "    }\n",
    "    \n",
    "def lasso(trial):\n",
    "    return {\n",
    "        \"fit_intercept\": trial.suggest_categorical(\"fit_intercept\", [True, False]),\n",
    "        'alpha': trial.suggest_float('alpha', .001, 10.0)\n",
    "    }\n",
    "    \n",
    "def elasticnet(trial):\n",
    "    return {\n",
    "        \"fit_intercept\": trial.suggest_categorical(\"fit_intercept\", [True, False]),\n",
    "        'alpha': trial.suggest_float('alpha', .001, 10.0),\n",
    "        'l1_ratio': trial.suggest_float('l1_ratio', 0.0, 1.0)\n",
    "    }\n",
    "\n",
    "def random_forest(trial):\n",
    "    return {\n",
    "        \"n_estimators\": trial.suggest_int(name=\"n_estimators\", low=50, high=1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 10),\n",
    "        'min_samples_split': trial.suggest_int('min_child_samples', 1, 100),\n",
    "        'max_features': trial.suggest_float('max_features', .5, 1.0),\n",
    "        \"criterion\": trial.suggest_categorical(\"criterion\", ['squared_error', 'poisson']),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import optuna\n",
    "import numpy as np\n",
    "from typing import List, Optional\n",
    "from utilsforecast.compat import DataFrame\n",
    "from utilsforecast.losses import rmse\n",
    "from mlforecast.forecast import MLForecast\n",
    "from mlforecast.target_transforms import LocalStandardScaler\n",
    "from mlforecast.core import (\n",
    "    Freq,\n",
    "    Models,\n",
    "    _name_models,\n",
    ")\n",
    "\n",
    "\n",
    "class Optimizer:\n",
    "    _mlforecast_params = ('lag_transforms', 'target_transforms', 'date_features', 'static_features', 'id_col', 'time_col', 'target_col', 'lags')\n",
    "    _optuna_list_params = ('lag_transforms', 'target_transforms', 'date_features')\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        freq: Freq,\n",
    "        models: Models,\n",
    "        model_configs=None,\n",
    "        min_lags=1,\n",
    "        max_lags=4,\n",
    "        target_transforms=[LocalStandardScaler()],\n",
    "        date_features=None,\n",
    "        lag_transforms=None,\n",
    "        id_col: str = \"unique_id\",\n",
    "        time_col: str = \"ds\",\n",
    "        target_col: str = \"y\",\n",
    "        static_features: Optional[List[str]] = None,\n",
    "    ):\n",
    "        \"\"\"Optimizer class which takes in a model config of Optuna trial spaces and MLForecast specific parameters\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        models : Models\n",
    "            list of models \n",
    "        freq : Freq \n",
    "            Frequency of the time column\n",
    "        id_col : str (default='unique_id')\n",
    "            Column that identifies each serie.\n",
    "        time_col : str (default='ds')\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str (default='y')\n",
    "            Column that contains the target.\n",
    "        static_features : list of str, optional (default=None)\n",
    "            Names of the features that are static and will be repeated when forecasting.\n",
    "                If `None`, will consider all columns (except id_col and time_col) as static.\n",
    "        \"\"\"\n",
    "        self.freq = freq\n",
    "        self.models = models\n",
    "        self.model_names = _name_models([m.__name__ for m in models])\n",
    "        self.model_configs = model_configs\n",
    "        self.target_transforms = Optimizer._sanitize_params(target_transforms)\n",
    "        self.lag_transforms = Optimizer._sanitize_params(lag_transforms)\n",
    "        self.date_features = Optimizer._sanitize_params(date_features)\n",
    "        self.param_configs = Optimizer._build_optuna_param(\n",
    "            min_lags,\n",
    "            max_lags,\n",
    "            self.target_transforms,\n",
    "            self.lag_transforms,\n",
    "            self.date_features\n",
    "        )\n",
    "        self.id_col = id_col\n",
    "        self.time_col = time_col\n",
    "        self.target_col = target_col\n",
    "        self.static_features = static_features\n",
    "        return\n",
    "\n",
    "    def _sanitize_params(parameter):\n",
    "        if parameter is None:\n",
    "            return [parameter]\n",
    "        else:\n",
    "            return parameter\n",
    "\n",
    "    @staticmethod\n",
    "    def _sanitize_output(optuna_param, mlforecast_param):\n",
    "        return mlforecast_param[optuna_param]\n",
    "\n",
    "    @staticmethod\n",
    "    def _build_output(model_obj, params, mlforecast_params):\n",
    "        param_copy = params.copy()\n",
    "        model_dict = {key: value for key, value in param_copy.items() if key not in mlforecast_params}\n",
    "        mlforecast_dict = {key: value for key, value in param_copy.items() if key in mlforecast_params}\n",
    "        output_dict = {\n",
    "            'models': [model_obj(**model_dict)],\n",
    "        }\n",
    "        output_dict.update(**mlforecast_dict)\n",
    "        return output_dict\n",
    "\n",
    "    @staticmethod\n",
    "    def _build_optuna_param(\n",
    "        min_lags,\n",
    "        max_lags,\n",
    "        target_transforms,\n",
    "        lag_transforms,\n",
    "        date_features\n",
    "    ):\n",
    "        def configs(trial):\n",
    "            return {\n",
    "                'lags': trial.suggest_int(name=\"lags\", low=min_lags, high=max_lags + 1), #seasonal period plus 1 so it creates a list of lags to hand over to mlforecast\n",
    "                'target_transforms': trial.suggest_int(\"target_transforms\", 0, len(target_transforms)-1),\n",
    "                'date_features': trial.suggest_int(\"date_features\", 0, len(date_features)-1),\n",
    "                'lag_transforms': trial.suggest_int(\"lag_transforms\", 0, len(lag_transforms)-1),\n",
    "            }\n",
    "        return configs\n",
    "\n",
    "    def scorer(self, model_params, param_params, loss):\n",
    "        \"\"\"Take the given params from the tuner and fit a MLForecast CV to get the score\n",
    "\n",
    "        Args:\n",
    "            model_params (_type_): _description_\n",
    "            param_params (_type_): _description_\n",
    "            metric (_type_): _description_\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        try: #try except commented out for testing\n",
    "            #taken from CV example: https://nixtlaverse.nixtla.io/mlforecast/docs/how-to-guides/cross_validation.html\n",
    "            model_obj = self.models[self.model_iter]\n",
    "            model_name = self.model_names[self.model_iter]\n",
    "            if 'lags' in param_params.keys():\n",
    "                param_params['lags'] = list(range(1, param_params['lags']+1)) #going from int to list for mlforecast\n",
    "            param_params['target_transforms'] = self.target_transforms[param_params['target_transforms']]\n",
    "            param_params['lag_transforms'] = self.lag_transforms[param_params['lag_transforms']]\n",
    "            param_params['date_features'] = self.date_features[param_params['date_features']]\n",
    "            fcst = MLForecast(\n",
    "                models=[model_obj(**model_params)],\n",
    "                freq=self.freq,\n",
    "                **param_params,\n",
    "            )\n",
    "            cv_results = fcst.cross_validation(\n",
    "                self.df,\n",
    "                dropna=self.dropna,\n",
    "                n_windows=self.n_windows,\n",
    "                h=self.h,\n",
    "                step_size=self.step_size,\n",
    "            )\n",
    "            cv_results['id_cutoff'] = cv_results['unique_id'] + '_' + cv_results['cutoff'].astype(str)\n",
    "            score = loss(cv_results, models=[model_name], id_col='id_cutoff')[model_name].mean()\n",
    "        except Exception as e:\n",
    "                    score = np.inf\n",
    "                    print(f'ERROR WHILE TUNING: {e}')\n",
    "        return score\n",
    "\n",
    "    def objective(self, trial):\n",
    "        model_params = self.model_configs[self.model_iter](trial)\n",
    "        param_params = self.param_configs(trial) #bad name\n",
    "        score = self.scorer(model_params, param_params, self.loss)\n",
    "        return score\n",
    "\n",
    "    def optimize(\n",
    "        self,\n",
    "        df: DataFrame,\n",
    "        n_windows: int,\n",
    "        h: int,\n",
    "        n_trials=100,\n",
    "        timeout=None,\n",
    "        loss=rmse,\n",
    "        step_size: Optional[int] = None,\n",
    "        dropna: bool = True,\n",
    "        seed: Optional[int] = None,\n",
    "    ):\n",
    "        \"\"\"Optimize the provided models and params.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas or polars DataFrame\n",
    "            Series data in long format.\n",
    "        n_windows : int\n",
    "            Number of windows to evaluate.\n",
    "        h : int\n",
    "            Forecast horizon.\n",
    "        dropna : bool (default=True)\n",
    "            Drop rows with missing values produced by the transformations.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            list of optuna studies, one element per model provided in model list.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.loss = loss\n",
    "        self.n_windows = n_windows\n",
    "        self.h = h\n",
    "        self.step_size = step_size\n",
    "        self.dropna = dropna\n",
    "        optuna_studies = []\n",
    "        param_list = []\n",
    "        self.raw_params = []\n",
    "        for i, model in enumerate(self.models):\n",
    "            self.model_iter = i\n",
    "            study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=seed))\n",
    "            study.optimize(\n",
    "                self.objective,\n",
    "                n_trials=n_trials,\n",
    "                timeout=timeout)\n",
    "            best_params = study.best_params\n",
    "            best_params.update({'lags': list(range(1, best_params['lags']+1))})\n",
    "            for param in zip(Optimizer._optuna_list_params, [\n",
    "                self.lag_transforms,\n",
    "                self.target_transforms,\n",
    "                self.date_features]):\n",
    "                best_params.update({param[0]: Optimizer._sanitize_output(best_params[param[0]],param[1])})\n",
    "            self.raw_params.append(best_params)\n",
    "            output_dict = Optimizer._build_output(model, best_params, Optimizer._mlforecast_params)\n",
    "            param_list.append(output_dict)\n",
    "            optuna_studies.append(study)\n",
    "        return param_list, optuna_studies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
