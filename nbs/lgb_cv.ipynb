{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fb976f-eafc-4236-8a27-02e142948574",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp lgb_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cceedc-91c9-4d75-bd51-32ce33829ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7948c2e2-c5ee-40fe-90d3-dab34d7d6bbf",
   "metadata": {},
   "source": [
    "# LightGBMCV\n",
    "\n",
    "> Time series cross validation with LightGBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f556b78-d988-4bbe-8d9a-c52b3fe5c4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import copy\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import partial\n",
    "from typing import Any, Callable, Dict, Iterable, List, Optional, Sequence, Tuple, Union\n",
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from mlforecast.core import (\n",
    "    DateFeature,\n",
    "    Differences,\n",
    "    Freq,\n",
    "    LagTransforms,\n",
    "    Lags,\n",
    "    TimeSeries,\n",
    ")\n",
    "from mlforecast.forecast import MLForecast\n",
    "from mlforecast.utils import backtest_splits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90df0fff-8ff9-4f71-9653-5af1ce56ae3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from nbdev import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3cc238-1d21-4e47-bda7-1ac1a8665395",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _mape(y_true, y_pred):\n",
    "    abs_pct_err = abs(y_true - y_pred) / y_true\n",
    "    return abs_pct_err.groupby(y_true.index.get_level_values(0), observed=True).mean().mean()\n",
    "\n",
    "def _rmse(y_true, y_pred):\n",
    "    sq_err = (y_true - y_pred) ** 2\n",
    "    return sq_err.groupby(y_true.index.get_level_values(0), observed=True).mean().pow(0.5).mean()\n",
    "\n",
    "_metric2fn = {'mape': _mape, 'rmse': _rmse}\n",
    "\n",
    "def _update(bst, n):\n",
    "    for _ in range(n):\n",
    "        bst.update()\n",
    "\n",
    "def _predict(ts, bst, valid, h, time_col, dynamic_dfs, predict_fn, **predict_fn_kwargs):\n",
    "    preds = ts.predict(bst, h, dynamic_dfs, predict_fn, **predict_fn_kwargs).set_index(time_col, append=True)\n",
    "    return valid.join(preds)\n",
    "\n",
    "def _update_and_predict(ts, bst, valid, n, h, time_col, dynamic_dfs, predict_fn, **predict_fn_kwargs):\n",
    "    _update(bst, n)\n",
    "    return _predict(ts, bst, valid, h, time_col, dynamic_dfs, predict_fn, **predict_fn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae86e51e-f068-407f-be41-5c5e9c9ac3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "CVResult = Tuple[int, float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6dd93f-7d13-4905-9fa6-727e9b9c8bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LightGBMCV:\n",
    "    def __init__(\n",
    "        self,\n",
    "        freq: Optional[Freq] = None,\n",
    "        lags: Optional[Lags] = None,\n",
    "        lag_transforms: Optional[LagTransforms] = None,\n",
    "        date_features: Optional[Iterable[DateFeature]] = None,\n",
    "        differences: Optional[Differences] = None,\n",
    "        num_threads: int = 1,\n",
    "    ):\n",
    "        \"\"\"Create LightGBM CV object.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        freq : str or int, optional (default=None)\n",
    "            Pandas offset alias, e.g. 'D', 'W-THU' or integer denoting the frequency of the series.\n",
    "        lags : list of int, optional (default=None)\n",
    "            Lags of the target to use as features.\n",
    "        lag_transforms : dict of int to list of functions, optional (default=None)\n",
    "            Mapping of target lags to their transformations.\n",
    "        date_features : list of str or callable, optional (default=None)\n",
    "            Features computed from the dates. Can be pandas date attributes or functions that will take the dates as input.\n",
    "        differences : list of int, optional (default=None)\n",
    "            Differences to take of the target before computing the features. These are restored at the forecasting step.\n",
    "        num_threads : int (default=1)\n",
    "            Number of threads to use when computing the features.\n",
    "        \"\"\"            \n",
    "        self.num_threads = num_threads\n",
    "        cpu_count = os.cpu_count()\n",
    "        if cpu_count is None:\n",
    "            num_cpus = 1\n",
    "        else:\n",
    "            num_cpus = cpu_count\n",
    "        self.bst_threads = max(num_cpus // num_threads, 1)\n",
    "        self.ts = TimeSeries(freq, lags, lag_transforms, date_features, differences, self.bst_threads)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            f'{self.__class__.__name__}('\n",
    "            f'freq={self.ts.freq}, '\n",
    "            f'lag_features={list(self.ts.transforms.keys())}, '\n",
    "            f'date_features={self.ts.date_features}, '\n",
    "            f'num_threads={self.num_threads}, '\n",
    "            f'bst_threads={self.bst_threads})'\n",
    "        )\n",
    "    \n",
    "    def setup(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        n_windows: int,\n",
    "        window_size: int,\n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        params: Optional[Dict[str, Any]] = None,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "        weights: Optional[Sequence[float]] = None,\n",
    "        metric: Union[str, Callable] = 'mape',\n",
    "    ):\n",
    "        \"\"\"Initialize internal data structures to iteratively train the boosters. Use this before calling partial_fit.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data : pandas DataFrame\n",
    "            Series data in long format.\n",
    "        n_windows : int\n",
    "            Number of windows to evaluate.\n",
    "        window_size : int\n",
    "            Number of test periods in each window.\n",
    "        id_col : str\n",
    "            Column that identifies each serie. If 'index' then the index is used.\n",
    "        time_col : str\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str\n",
    "            Column that contains the target.\n",
    "        params : dict, optional(default=None)\n",
    "            Parameters to be passed to the LightGBM Boosters.       \n",
    "        static_features : list of str, optional (default=None)\n",
    "            Names of the features that are static and will be repeated when forecasting.\n",
    "        dropna : bool (default=True)\n",
    "            Drop rows with missing values produced by the transformations.\n",
    "        keep_last_n : int, optional (default=None)\n",
    "            Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it.\n",
    "        weights : sequence of float, optional (default=None)\n",
    "            Weights to multiply the metric of each window. If None, all windows have the same weight.\n",
    "        metric : str or callable, default='mape'\n",
    "            Metric used to assess the performance of the models and perform early stopping.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self : LightGBMCV\n",
    "            CV object with internal data structures for partial_fit.\n",
    "        \"\"\"\n",
    "        if weights is None:\n",
    "            self.weights = np.full(n_windows, 1 / n_windows)        \n",
    "        elif len(weights) != n_windows:\n",
    "            raise ValueError('Must specify as many weights as the number of windows')\n",
    "        else:\n",
    "            self.weights = np.asarray(weights)\n",
    "        if callable(metric):\n",
    "            self.metric_fn = metric\n",
    "            self.metric_name = 'custom_metric'\n",
    "        else:\n",
    "            if metric not in _metric2fn:\n",
    "                raise ValueError(f'{metric} is not one of the implemented metrics: ({\", \".join(_metric2fn.keys())})')\n",
    "            self.metric_fn = _metric2fn[metric]\n",
    "            self.metric_name = metric\n",
    "\n",
    "        if id_col != 'index':\n",
    "            data = data.set_index(id_col)\n",
    "        \n",
    "        if np.issubdtype(data[time_col].dtype.type, np.integer):\n",
    "            freq = 1\n",
    "        else:\n",
    "            freq = self.ts.freq\n",
    "        self.items = []\n",
    "        self.window_size = window_size\n",
    "        self.time_col = time_col\n",
    "        self.target_col = target_col\n",
    "        params = {} if params is None else params\n",
    "        for _, train, valid in backtest_splits(data, n_windows, window_size, freq, time_col):\n",
    "            ts = copy.deepcopy(self.ts)\n",
    "            prep = ts.fit_transform(train, 'index', time_col, target_col, static_features, dropna, keep_last_n)\n",
    "            ds = lgb.Dataset(prep.drop(columns=[time_col, target_col]), prep[target_col]).construct()\n",
    "            bst = lgb.Booster({**params, 'num_threads': self.bst_threads}, ds)\n",
    "            bst.predict = partial(bst.predict, num_threads=self.bst_threads)\n",
    "            valid = valid.set_index(time_col, append=True)\n",
    "            self.items.append((ts, bst, valid))\n",
    "        return self\n",
    "\n",
    "    def _single_threaded_partial_fit(\n",
    "        self,\n",
    "        metric_values,\n",
    "        num_iterations,\n",
    "        dynamic_dfs,\n",
    "        predict_fn,\n",
    "        **predict_fn_kwargs,\n",
    "    ):  \n",
    "        for j, (ts, bst, valid) in enumerate(self.items):                        \n",
    "            preds = _update_and_predict(\n",
    "                ts,\n",
    "                bst,\n",
    "                valid,\n",
    "                num_iterations,\n",
    "                self.window_size,\n",
    "                self.time_col,\n",
    "                dynamic_dfs,\n",
    "                predict_fn,\n",
    "                **predict_fn_kwargs\n",
    "            )\n",
    "            metric_values[j] = self.metric_fn(preds[self.target_col], preds['Booster'])\n",
    "\n",
    "    def _multithreaded_partial_fit(\n",
    "        self,\n",
    "        metric_values,\n",
    "        num_iterations,\n",
    "        dynamic_dfs,\n",
    "        predict_fn,\n",
    "        **predict_fn_kwargs,\n",
    "    ):                           \n",
    "        with ThreadPoolExecutor(self.num_threads) as executor:\n",
    "            futures = []\n",
    "            for ts, bst, valid in self.items:\n",
    "                _update(bst, num_iterations)\n",
    "                future = executor.submit(\n",
    "                    _predict,\n",
    "                    ts,\n",
    "                    bst,\n",
    "                    valid,\n",
    "                    self.window_size,\n",
    "                    self.time_col,\n",
    "                    dynamic_dfs,\n",
    "                    predict_fn,\n",
    "                    **predict_fn_kwargs\n",
    "                )\n",
    "                futures.append(future)\n",
    "            cv_preds = [f.result() for f in futures]\n",
    "        metric_values[:] = [self.metric_fn(preds[self.target_col], preds['Booster']) for preds in cv_preds]\n",
    "        \n",
    "    def partial_fit(\n",
    "        self,\n",
    "        num_iterations: int,\n",
    "        dynamic_dfs: Optional[List[pd.DataFrame]] = None,\n",
    "        predict_fn: Optional[Callable] = None,\n",
    "        **predict_fn_kwargs,\n",
    "    ) -> float:\n",
    "        \"\"\"Train the boosters for some iterations.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        num_iterations : int\n",
    "            Number of boosting iterations to run\n",
    "        dynamic_dfs : list of pandas DataFrame, optional (default=None)\n",
    "            Future values of the dynamic features, e.g. prices.\n",
    "        predict_fn : callable, optional (default=None)\n",
    "            Custom function to compute predictions.\n",
    "            This function will recieve: model, new_x, dynamic_dfs, features_order and kwargs,\n",
    "            and should return an array with the predictions, where:\n",
    "                model : regressor\n",
    "                    Fitted model.\n",
    "                new_x : pandas DataFrame\n",
    "                    Current values of the features.\n",
    "                dynamic_dfs : list of pandas DataFrame\n",
    "                    Future values of the dynamic features\n",
    "                features_order : list of str\n",
    "                    Column names in the order in which they were used to train the model.\n",
    "                **kwargs\n",
    "                    Other keyword arguments passed to `MLForecast.predict`.\n",
    "        **predict_fn_kwargs\n",
    "            Additional arguments passed to predict_fn\n",
    "                    \n",
    "        Returns\n",
    "        -------\n",
    "        metric_value : float\n",
    "            Weighted metric after training for num_iterations.\n",
    "        \"\"\"\n",
    "        metric_values = np.empty(len(self.items))\n",
    "        if self.num_threads == 1:\n",
    "            self._single_threaded_partial_fit(metric_values, num_iterations, dynamic_dfs, predict_fn, **predict_fn_kwargs)\n",
    "        else:\n",
    "            self._multithreaded_partial_fit(metric_values, num_iterations, dynamic_dfs, predict_fn, **predict_fn_kwargs)\n",
    "        return metric_values @ self.weights\n",
    "    \n",
    "    def _should_stop(self, hist, early_stopping_evals, early_stopping_pct) -> bool:\n",
    "        if len(hist) < early_stopping_evals + 1:\n",
    "            return False\n",
    "        improvement_pct = 1 - hist[-1][1] / hist[-(early_stopping_evals + 1)][1]\n",
    "        return improvement_pct < early_stopping_pct\n",
    "\n",
    "    def _best_iter(self, hist, early_stopping_evals) -> int:\n",
    "        best_iter, best_score = hist[-1]\n",
    "        for r, m in hist[-(early_stopping_evals + 1):-1]:\n",
    "            if m < best_score:\n",
    "                best_score = m\n",
    "                best_iter = r\n",
    "        return best_iter\n",
    "   \n",
    "    def fit(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        n_windows: int,\n",
    "        window_size: int,\n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        num_iterations: int = 100,\n",
    "        params: Optional[Dict[str, Any]] = None,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "        dynamic_dfs: Optional[List[pd.DataFrame]] = None,\n",
    "        eval_every: int = 10,\n",
    "        weights: Optional[Sequence[float]] = None,\n",
    "        metric: Union[str, Callable] = 'mape',\n",
    "        verbose_eval: bool = True,\n",
    "        early_stopping_evals: int = 2,\n",
    "        early_stopping_pct: float = 0.01,\n",
    "        compute_cv_preds: bool = False,\n",
    "        fit_on_all: bool = False,\n",
    "        predict_fn: Optional[Callable] = None,\n",
    "        **predict_fn_kwargs,\n",
    "    ) -> List[CVResult]:\n",
    "        \"\"\"Train boosters simultaneously and assess their performance on the complete forecasting window.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data : pandas DataFrame\n",
    "            Series data in long format.\n",
    "        n_windows : int\n",
    "            Number of windows to evaluate.\n",
    "        window_size : int\n",
    "            Number of test periods in each window.    \n",
    "        id_col : str\n",
    "            Column that identifies each serie. If 'index' then the index is used.\n",
    "        time_col : str\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str\n",
    "            Column that contains the target.\n",
    "        num_iterations : int (default=100)\n",
    "            Maximum number of boosting iterations to run.\n",
    "        params : dict, optional(default=None)\n",
    "            Parameters to be passed to the LightGBM Boosters.            \n",
    "        static_features : list of str, optional (default=None)\n",
    "            Names of the features that are static and will be repeated when forecasting.\n",
    "        dropna : bool (default=True)\n",
    "            Drop rows with missing values produced by the transformations.\n",
    "        keep_last_n : int, optional (default=None)\n",
    "            Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it.\n",
    "        dynamic_dfs : list of pandas DataFrame, optional (default=None)\n",
    "            Future values of the dynamic features, e.g. prices.\n",
    "        eval_every : int (default=10)\n",
    "            Number of boosting iterations to train before evaluating on the whole forecast window.\n",
    "        weights : sequence of float, optional (default=None)\n",
    "            Weights to multiply the metric of each window. If None, all windows have the same weight.\n",
    "        metric : str or callable, default='mape'\n",
    "            Metric used to assess the performance of the models and perform early stopping.\n",
    "        verbose_eval : bool\n",
    "            Print the metrics of each evaluation.\n",
    "        early_stopping_evals : int (default=2)\n",
    "            Maximum number of evaluations to run without improvement.\n",
    "        early_stopping_pct : float (default=0.01)\n",
    "            Minimum percentage improvement in metric value in `early_stopping_evals` evaluations.\n",
    "        compute_cv_preds : bool (default=True)\n",
    "            Compute predictions for each window after finding the best iteration.        \n",
    "        fit_on_all : bool (default=True)\n",
    "            Return model fitted on full dataset.\n",
    "        predict_fn : callable, optional (default=None)\n",
    "            Custom function to compute predictions.\n",
    "            This function will recieve: model, new_x, dynamic_dfs, features_order and kwargs,\n",
    "            and should return an array with the predictions, where:\n",
    "                model : regressor\n",
    "                    Fitted model.\n",
    "                new_x : pandas DataFrame\n",
    "                    Current values of the features.\n",
    "                dynamic_dfs : list of pandas DataFrame\n",
    "                    Future values of the dynamic features\n",
    "                features_order : list of str\n",
    "                    Column names in the order in which they were used to train the model.\n",
    "                **kwargs\n",
    "                    Other keyword arguments passed to `MLForecast.predict`.\n",
    "        **predict_fn_kwargs\n",
    "            Additional arguments passed to predict_fn                    \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        cv_result : list of tuple.\n",
    "            List of (boosting rounds, metric value) tuples.\n",
    "        \"\"\"\n",
    "        self.setup(\n",
    "            data=data,\n",
    "            n_windows=n_windows,\n",
    "            window_size=window_size,\n",
    "            params=params,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            static_features=static_features,\n",
    "            dropna=dropna,\n",
    "            keep_last_n=keep_last_n,\n",
    "            weights=weights,\n",
    "            metric=metric,\n",
    "        )\n",
    "        hist = []\n",
    "        for i in range(0, num_iterations, eval_every):\n",
    "            metric_value = self.partial_fit(eval_every, dynamic_dfs, predict_fn, **predict_fn_kwargs)\n",
    "            rounds = eval_every + i\n",
    "            hist.append((rounds, metric_value))\n",
    "            if verbose_eval:\n",
    "                print(f'[{rounds:,d}] {self.metric_name}: {metric_value:,f}')                \n",
    "            if self._should_stop(hist, early_stopping_evals, early_stopping_pct):\n",
    "                print(f\"Early stopping at round {rounds:,}\")\n",
    "                break\n",
    "        rounds = self._best_iter(hist, early_stopping_evals)\n",
    "        print(f'Using best iteration: {rounds:,}')\n",
    "        hist = hist[:rounds // eval_every]\n",
    "        for _, bst, _ in self.items:\n",
    "            bst.best_iteration = rounds\n",
    "\n",
    "        self.cv_models_ = [item[1] for item in self.items]\n",
    "        if compute_cv_preds:\n",
    "            with ThreadPoolExecutor(self.num_threads) as executor:\n",
    "                futures = []            \n",
    "                for ts, bst, valid in self.items:\n",
    "                    future = executor.submit(\n",
    "                        _predict,\n",
    "                        ts,\n",
    "                        bst,\n",
    "                        valid,\n",
    "                        window_size,\n",
    "                        time_col,\n",
    "                        dynamic_dfs,\n",
    "                        predict_fn,\n",
    "                        **predict_fn_kwargs\n",
    "                    )\n",
    "                    futures.append(future)            \n",
    "                self.cv_preds_ = pd.concat([f.result().assign(window=i) for i, f in enumerate(futures)])\n",
    "\n",
    "        if fit_on_all:\n",
    "            params = params if params is not None else {}            \n",
    "            self.fcst = MLForecast([lgb.LGBMRegressor(**{**params, 'n_estimators': rounds})])\n",
    "            self.fcst.ts = self.ts\n",
    "            self.fcst.fit(\n",
    "                data,\n",
    "                id_col,\n",
    "                time_col,\n",
    "                target_col,\n",
    "                static_features,\n",
    "                dropna,\n",
    "                keep_last_n,\n",
    "            )\n",
    "        else:\n",
    "            self.ts._fit(data, id_col, time_col, target_col, static_features, keep_last_n)\n",
    "        return hist\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        horizon: int,\n",
    "        dynamic_dfs: Optional[List[pd.DataFrame]] = None,\n",
    "        predict_fn: Optional[Callable] = None,\n",
    "        **predict_fn_kwargs,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Compute predictions using the model trained on all data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        horizon : int\n",
    "            Number of periods to predict.\n",
    "        dynamic_dfs : list of pandas DataFrame, optional (default=None)\n",
    "            Future values of the dynamic features, e.g. prices.\n",
    "        predict_fn : callable, optional (default=None)\n",
    "            Custom function to compute predictions.\n",
    "            This function will recieve: model, new_x, dynamic_dfs, features_order and kwargs,\n",
    "            and should return an array with the predictions, where:\n",
    "                model : regressor\n",
    "                    Fitted model.\n",
    "                new_x : pandas DataFrame\n",
    "                    Current values of the features.\n",
    "                dynamic_dfs : list of pandas DataFrame\n",
    "                    Future values of the dynamic features\n",
    "                features_order : list of str\n",
    "                    Column names in the order in which they were used to train the model.\n",
    "                **kwargs\n",
    "                    Other keyword arguments passed to `MLForecast.predict`.\n",
    "        **predict_fn_kwargs\n",
    "            Additional arguments passed to predict_fn\n",
    "                    \n",
    "        Returns\n",
    "        -------\n",
    "        result : pandas DataFrame\n",
    "            Predictions for each serie and timestep.\n",
    "        \"\"\"        \n",
    "        if not hasattr(self, 'fcst'):\n",
    "            raise ValueError('Must call fit with fit_on_all=True before. You can also call cv_predict instead.')\n",
    "        return self.fcst.predict(horizon, dynamic_dfs, predict_fn, **predict_fn_kwargs)\n",
    "    \n",
    "    def cv_predict(\n",
    "        self,\n",
    "        horizon: int,\n",
    "        dynamic_dfs: Optional[List[pd.DataFrame]] = None,\n",
    "        predict_fn: Optional[Callable] = None,\n",
    "        **predict_fn_kwargs,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Compute predictions with each of the trained boosters.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        horizon : int\n",
    "            Number of periods to predict.\n",
    "        dynamic_dfs : list of pandas DataFrame, optional (default=None)\n",
    "            Future values of the dynamic features, e.g. prices.\n",
    "        predict_fn : callable, optional (default=None)\n",
    "            Custom function to compute predictions.\n",
    "            This function will recieve: model, new_x, dynamic_dfs, features_order and kwargs,\n",
    "            and should return an array with the predictions, where:\n",
    "                model : regressor\n",
    "                    Fitted model.\n",
    "                new_x : pandas DataFrame\n",
    "                    Current values of the features.\n",
    "                dynamic_dfs : list of pandas DataFrame\n",
    "                    Future values of the dynamic features\n",
    "                features_order : list of str\n",
    "                    Column names in the order in which they were used to train the model.\n",
    "                **kwargs\n",
    "                    Other keyword arguments passed to `MLForecast.predict`.\n",
    "        **predict_fn_kwargs\n",
    "            Additional arguments passed to predict_fn\n",
    "                    \n",
    "        Returns\n",
    "        -------\n",
    "        result : pandas DataFrame\n",
    "            Predictions for each serie and timestep, with one column per window.\n",
    "        \"\"\"\n",
    "        return self.ts.predict(\n",
    "            self.cv_models_,\n",
    "            horizon,\n",
    "            dynamic_dfs,\n",
    "            predict_fn,\n",
    "            **predict_fn_kwargs\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3313362b-125e-4f76-9267-cc6cb5df0d05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### LightGBMCV\n",
       "\n",
       ">      LightGBMCV (freq:Union[int,str,NoneType]=None,\n",
       ">                  lags:Optional[Iterable[int]]=None, lag_transforms:Optional[Di\n",
       ">                  ct[int,List[Union[Callable,Tuple[Callable,Any]]]]]=None,\n",
       ">                  date_features:Optional[Iterable[Union[str,Callable]]]=None,\n",
       ">                  differences:Optional[Iterable[int]]=None, num_threads:int=1)\n",
       "\n",
       "Create LightGBM CV object.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| freq | typing.Union[int, str, NoneType] | None | Pandas offset alias, e.g. 'D', 'W-THU' or integer denoting the frequency of the series. |\n",
       "| lags | typing.Optional[typing.Iterable[int]] | None | Lags of the target to use as features. |\n",
       "| lag_transforms | typing.Optional[typing.Dict[int, typing.List[typing.Union[typing.Callable, typing.Tuple[typing.Callable, typing.Any]]]]] | None | Mapping of target lags to their transformations. |\n",
       "| date_features | typing.Optional[typing.Iterable[typing.Union[str, typing.Callable]]] | None | Features computed from the dates. Can be pandas date attributes or functions that will take the dates as input. |\n",
       "| differences | typing.Optional[typing.Iterable[int]] | None | Differences to take of the target before computing the features. These are restored at the forecasting step. |\n",
       "| num_threads | int | 1 | Number of threads to use when computing the features. |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### LightGBMCV\n",
       "\n",
       ">      LightGBMCV (freq:Union[int,str,NoneType]=None,\n",
       ">                  lags:Optional[Iterable[int]]=None, lag_transforms:Optional[Di\n",
       ">                  ct[int,List[Union[Callable,Tuple[Callable,Any]]]]]=None,\n",
       ">                  date_features:Optional[Iterable[Union[str,Callable]]]=None,\n",
       ">                  differences:Optional[Iterable[int]]=None, num_threads:int=1)\n",
       "\n",
       "Create LightGBM CV object.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| freq | typing.Union[int, str, NoneType] | None | Pandas offset alias, e.g. 'D', 'W-THU' or integer denoting the frequency of the series. |\n",
       "| lags | typing.Optional[typing.Iterable[int]] | None | Lags of the target to use as features. |\n",
       "| lag_transforms | typing.Optional[typing.Dict[int, typing.List[typing.Union[typing.Callable, typing.Tuple[typing.Callable, typing.Any]]]]] | None | Mapping of target lags to their transformations. |\n",
       "| date_features | typing.Optional[typing.Iterable[typing.Union[str, typing.Callable]]] | None | Features computed from the dates. Can be pandas date attributes or functions that will take the dates as input. |\n",
       "| differences | typing.Optional[typing.Iterable[int]] | None | Differences to take of the target before computing the features. These are restored at the forecasting step. |\n",
       "| num_threads | int | 1 | Number of threads to use when computing the features. |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(LightGBMCV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d051f249-4807-4883-af1a-8ba953e4c18d",
   "metadata": {},
   "source": [
    "## Example\n",
    "This shows an example with just 4 series of the M4 dataset. If you want to run it yourself on all of them, you can refer to [this notebook](https://www.kaggle.com/code/lemuz90/m4-competition-cv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6f82f1-a305-4be3-8f63-78dd38a6f6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from datasetsforecast.m4 import M4, M4Info\n",
    "from fastcore.test import test_eq, test_fail\n",
    "from nbdev import show_doc\n",
    "from window_ops.ewm import ewm_mean\n",
    "from window_ops.rolling import rolling_mean, seasonal_rolling_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aa019f-11f4-49b1-b9d0-6688dda44f72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>86796</th>\n",
       "      <td>H196</td>\n",
       "      <td>1</td>\n",
       "      <td>11.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86797</th>\n",
       "      <td>H196</td>\n",
       "      <td>2</td>\n",
       "      <td>11.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86798</th>\n",
       "      <td>H196</td>\n",
       "      <td>3</td>\n",
       "      <td>11.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86799</th>\n",
       "      <td>H196</td>\n",
       "      <td>4</td>\n",
       "      <td>10.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86800</th>\n",
       "      <td>H196</td>\n",
       "      <td>5</td>\n",
       "      <td>10.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325235</th>\n",
       "      <td>H413</td>\n",
       "      <td>1004</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325236</th>\n",
       "      <td>H413</td>\n",
       "      <td>1005</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325237</th>\n",
       "      <td>H413</td>\n",
       "      <td>1006</td>\n",
       "      <td>47.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325238</th>\n",
       "      <td>H413</td>\n",
       "      <td>1007</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325239</th>\n",
       "      <td>H413</td>\n",
       "      <td>1008</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4032 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       unique_id    ds     y\n",
       "86796       H196     1  11.8\n",
       "86797       H196     2  11.4\n",
       "86798       H196     3  11.1\n",
       "86799       H196     4  10.8\n",
       "86800       H196     5  10.6\n",
       "...          ...   ...   ...\n",
       "325235      H413  1004  99.0\n",
       "325236      H413  1005  88.0\n",
       "325237      H413  1006  47.0\n",
       "325238      H413  1007  41.0\n",
       "325239      H413  1008  34.0\n",
       "\n",
       "[4032 rows x 3 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group = 'Hourly'\n",
    "await M4.async_download('data', group=group)\n",
    "df, *_ = M4.load(directory='data', group=group)\n",
    "df['ds'] = df['ds'].astype('int')\n",
    "ids = df['unique_id'].unique()\n",
    "random.seed(0)\n",
    "sample_ids = random.choices(ids, k=4)\n",
    "sample_df = df[df['unique_id'].isin(sample_ids)]\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5672199-74ca-49e8-94b1-464d4effb57e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3840, 3), (192, 3))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info = M4Info[group]\n",
    "horizon = info.horizon\n",
    "valid = sample_df.groupby('unique_id').tail(horizon)\n",
    "train = sample_df.drop(valid.index)\n",
    "train.shape, valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8d419c-993e-4934-8b31-a95814252562",
   "metadata": {},
   "source": [
    "What LightGBMCV does is emulate [LightGBM's cv function](https://lightgbm.readthedocs.io/en/v3.3.2/pythonapi/lightgbm.cv.html#lightgbm.cv) where several Boosters are trained simultaneously on different partitions of the data, that is, one boosting iteration is performed on all of them at a time. This allows to have an estimate of the error by iteration, so if we combine this with early stopping we can find the best iteration to train a final model using all the data or even use these individual models' predictions to compute an ensemble.\n",
    "\n",
    "In order to have a good estimate of the forecasting performance of our model we compute predictions for the whole test period and compute a metric on that. Since this step can slow down training, there's an `eval_every` parameter that can be used to control this, that is, if `eval_every=10` (the default) every 10 boosting iterations we're going to compute forecasts for the complete window and report the error.\n",
    "\n",
    "We also have early stopping parameters:\n",
    "\n",
    "* `early_stopping_evals`: how many evaluations of the full window should we go without improving to stop training?\n",
    "* `early_stopping_pct`: what's the minimum percentage improvement we want in these `early_stopping_evals` in order to keep training?\n",
    "\n",
    "This makes the LightGBMCV class a good tool to quickly test different configurations of the model. Consider the following example, where we're going to try to find out which features can improve the performance of our model. We start just using lags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec4ed2f-d47a-4b06-a1db-1d72d3c2f6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_fit_config = dict(\n",
    "    id_col='unique_id',\n",
    "    time_col='ds',\n",
    "    target_col='y',\n",
    "    n_windows=2,\n",
    "    window_size=horizon,\n",
    "    params={'verbose': -1},\n",
    "    fit_on_all=True,\n",
    ")\n",
    "cv = LightGBMCV(\n",
    "    freq=1,\n",
    "    lags=[24 * (i+1) for i in range(7)],  # one week of lags\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b38e21-1df2-4914-92c7-fb0c5910f023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### LightGBMCV.fit\n",
       "\n",
       ">      LightGBMCV.fit (data:pandas.core.frame.DataFrame, n_windows:int,\n",
       ">                      window_size:int, id_col:str, time_col:str,\n",
       ">                      target_col:str, num_iterations:int=100,\n",
       ">                      params:Optional[Dict[str,Any]]=None,\n",
       ">                      static_features:Optional[List[str]]=None,\n",
       ">                      dropna:bool=True, keep_last_n:Optional[int]=None, dynamic\n",
       ">                      _dfs:Optional[List[pandas.core.frame.DataFrame]]=None,\n",
       ">                      eval_every:int=10,\n",
       ">                      weights:Optional[Sequence[float]]=None,\n",
       ">                      metric:Union[str,Callable]='mape',\n",
       ">                      verbose_eval:bool=True, early_stopping_evals:int=2,\n",
       ">                      early_stopping_pct:float=0.01,\n",
       ">                      compute_cv_preds:bool=False, fit_on_all:bool=False,\n",
       ">                      predict_fn:Optional[Callable]=None, **predict_fn_kwargs)\n",
       "\n",
       "Train boosters simultaneously and assess their performance on the complete forecasting window.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| data | DataFrame |  | Series data in long format. |\n",
       "| n_windows | int |  | Number of windows to evaluate. |\n",
       "| window_size | int |  | Number of test periods in each window.     |\n",
       "| id_col | str |  | Column that identifies each serie. If 'index' then the index is used. |\n",
       "| time_col | str |  | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str |  | Column that contains the target. |\n",
       "| num_iterations | int | 100 | Maximum number of boosting iterations to run. |\n",
       "| params | typing.Optional[typing.Dict[str, typing.Any]] | None | Parameters to be passed to the LightGBM Boosters.             |\n",
       "| static_features | typing.Optional[typing.List[str]] | None | Names of the features that are static and will be repeated when forecasting. |\n",
       "| dropna | bool | True | Drop rows with missing values produced by the transformations. |\n",
       "| keep_last_n | typing.Optional[int] | None | Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it. |\n",
       "| dynamic_dfs | typing.Optional[typing.List[pandas.core.frame.DataFrame]] | None | Future values of the dynamic features, e.g. prices. |\n",
       "| eval_every | int | 10 | Number of boosting iterations to train before evaluating on the whole forecast window. |\n",
       "| weights | typing.Optional[typing.Sequence[float]] | None | Weights to multiply the metric of each window. If None, all windows have the same weight. |\n",
       "| metric | typing.Union[str, typing.Callable] | mape | Metric used to assess the performance of the models and perform early stopping. |\n",
       "| verbose_eval | bool | True | Print the metrics of each evaluation. |\n",
       "| early_stopping_evals | int | 2 | Maximum number of evaluations to run without improvement. |\n",
       "| early_stopping_pct | float | 0.01 | Minimum percentage improvement in metric value in `early_stopping_evals` evaluations. |\n",
       "| compute_cv_preds | bool | False | Compute predictions for each window after finding the best iteration.         |\n",
       "| fit_on_all | bool | False | Return model fitted on full dataset. |\n",
       "| predict_fn | typing.Optional[typing.Callable] | None | Custom function to compute predictions.<br>This function will recieve: model, new_x, dynamic_dfs, features_order and kwargs,<br>and should return an array with the predictions, where:<br>    model : regressor<br>        Fitted model.<br>    new_x : pandas DataFrame<br>        Current values of the features.<br>    dynamic_dfs : list of pandas DataFrame<br>        Future values of the dynamic features<br>    features_order : list of str<br>        Column names in the order in which they were used to train the model.<br>    **kwargs<br>        Other keyword arguments passed to `MLForecast.predict`. |\n",
       "| predict_fn_kwargs |  |  |  |\n",
       "| **Returns** | **typing.List[typing.Tuple[int, float]]** |  | **List of (boosting rounds, metric value) tuples.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### LightGBMCV.fit\n",
       "\n",
       ">      LightGBMCV.fit (data:pandas.core.frame.DataFrame, n_windows:int,\n",
       ">                      window_size:int, id_col:str, time_col:str,\n",
       ">                      target_col:str, num_iterations:int=100,\n",
       ">                      params:Optional[Dict[str,Any]]=None,\n",
       ">                      static_features:Optional[List[str]]=None,\n",
       ">                      dropna:bool=True, keep_last_n:Optional[int]=None, dynamic\n",
       ">                      _dfs:Optional[List[pandas.core.frame.DataFrame]]=None,\n",
       ">                      eval_every:int=10,\n",
       ">                      weights:Optional[Sequence[float]]=None,\n",
       ">                      metric:Union[str,Callable]='mape',\n",
       ">                      verbose_eval:bool=True, early_stopping_evals:int=2,\n",
       ">                      early_stopping_pct:float=0.01,\n",
       ">                      compute_cv_preds:bool=False, fit_on_all:bool=False,\n",
       ">                      predict_fn:Optional[Callable]=None, **predict_fn_kwargs)\n",
       "\n",
       "Train boosters simultaneously and assess their performance on the complete forecasting window.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| data | DataFrame |  | Series data in long format. |\n",
       "| n_windows | int |  | Number of windows to evaluate. |\n",
       "| window_size | int |  | Number of test periods in each window.     |\n",
       "| id_col | str |  | Column that identifies each serie. If 'index' then the index is used. |\n",
       "| time_col | str |  | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str |  | Column that contains the target. |\n",
       "| num_iterations | int | 100 | Maximum number of boosting iterations to run. |\n",
       "| params | typing.Optional[typing.Dict[str, typing.Any]] | None | Parameters to be passed to the LightGBM Boosters.             |\n",
       "| static_features | typing.Optional[typing.List[str]] | None | Names of the features that are static and will be repeated when forecasting. |\n",
       "| dropna | bool | True | Drop rows with missing values produced by the transformations. |\n",
       "| keep_last_n | typing.Optional[int] | None | Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it. |\n",
       "| dynamic_dfs | typing.Optional[typing.List[pandas.core.frame.DataFrame]] | None | Future values of the dynamic features, e.g. prices. |\n",
       "| eval_every | int | 10 | Number of boosting iterations to train before evaluating on the whole forecast window. |\n",
       "| weights | typing.Optional[typing.Sequence[float]] | None | Weights to multiply the metric of each window. If None, all windows have the same weight. |\n",
       "| metric | typing.Union[str, typing.Callable] | mape | Metric used to assess the performance of the models and perform early stopping. |\n",
       "| verbose_eval | bool | True | Print the metrics of each evaluation. |\n",
       "| early_stopping_evals | int | 2 | Maximum number of evaluations to run without improvement. |\n",
       "| early_stopping_pct | float | 0.01 | Minimum percentage improvement in metric value in `early_stopping_evals` evaluations. |\n",
       "| compute_cv_preds | bool | False | Compute predictions for each window after finding the best iteration.         |\n",
       "| fit_on_all | bool | False | Return model fitted on full dataset. |\n",
       "| predict_fn | typing.Optional[typing.Callable] | None | Custom function to compute predictions.<br>This function will recieve: model, new_x, dynamic_dfs, features_order and kwargs,<br>and should return an array with the predictions, where:<br>    model : regressor<br>        Fitted model.<br>    new_x : pandas DataFrame<br>        Current values of the features.<br>    dynamic_dfs : list of pandas DataFrame<br>        Future values of the dynamic features<br>    features_order : list of str<br>        Column names in the order in which they were used to train the model.<br>    **kwargs<br>        Other keyword arguments passed to `MLForecast.predict`. |\n",
       "| predict_fn_kwargs |  |  |  |\n",
       "| **Returns** | **typing.List[typing.Tuple[int, float]]** |  | **List of (boosting rounds, metric value) tuples.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(LightGBMCV.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6757b862-79dd-46e9-a4aa-b6b52394a83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Start training from score 51.745632\n",
      "[10] mape: 0.590690\n",
      "[20] mape: 0.251093\n",
      "[30] mape: 0.143643\n",
      "[40] mape: 0.109723\n",
      "[50] mape: 0.102099\n",
      "[60] mape: 0.099448\n",
      "[70] mape: 0.098349\n",
      "[80] mape: 0.098006\n",
      "[90] mape: 0.098718\n",
      "Early stopping at round 90\n",
      "Using best iteration: 80\n"
     ]
    }
   ],
   "source": [
    "hist = cv.fit(train, **static_fit_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebe1f31-293c-4cde-ac51-4f33c39c3d62",
   "metadata": {},
   "source": [
    "By setting `fit_on_all=True` a final model gets trained with all of the data and the best iteration found, then we can call `predict` to get the predictions from this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0475122b-0d32-4a4b-bfa9-ddc18f5ddab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### LightGBMCV.predict\n",
       "\n",
       ">      LightGBMCV.predict (horizon:int,\n",
       ">                          dynamic_dfs:Optional[List[pandas.core.frame.DataFrame\n",
       ">                          ]]=None, predict_fn:Optional[Callable]=None,\n",
       ">                          **predict_fn_kwargs)\n",
       "\n",
       "Compute predictions using the model trained on all data.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| horizon | int |  | Number of periods to predict. |\n",
       "| dynamic_dfs | typing.Optional[typing.List[pandas.core.frame.DataFrame]] | None | Future values of the dynamic features, e.g. prices. |\n",
       "| predict_fn | typing.Optional[typing.Callable] | None | Custom function to compute predictions.<br>This function will recieve: model, new_x, dynamic_dfs, features_order and kwargs,<br>and should return an array with the predictions, where:<br>    model : regressor<br>        Fitted model.<br>    new_x : pandas DataFrame<br>        Current values of the features.<br>    dynamic_dfs : list of pandas DataFrame<br>        Future values of the dynamic features<br>    features_order : list of str<br>        Column names in the order in which they were used to train the model.<br>    **kwargs<br>        Other keyword arguments passed to `MLForecast.predict`. |\n",
       "| predict_fn_kwargs |  |  |  |\n",
       "| **Returns** | **DataFrame** |  | **Predictions for each serie and timestep.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### LightGBMCV.predict\n",
       "\n",
       ">      LightGBMCV.predict (horizon:int,\n",
       ">                          dynamic_dfs:Optional[List[pandas.core.frame.DataFrame\n",
       ">                          ]]=None, predict_fn:Optional[Callable]=None,\n",
       ">                          **predict_fn_kwargs)\n",
       "\n",
       "Compute predictions using the model trained on all data.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| horizon | int |  | Number of periods to predict. |\n",
       "| dynamic_dfs | typing.Optional[typing.List[pandas.core.frame.DataFrame]] | None | Future values of the dynamic features, e.g. prices. |\n",
       "| predict_fn | typing.Optional[typing.Callable] | None | Custom function to compute predictions.<br>This function will recieve: model, new_x, dynamic_dfs, features_order and kwargs,<br>and should return an array with the predictions, where:<br>    model : regressor<br>        Fitted model.<br>    new_x : pandas DataFrame<br>        Current values of the features.<br>    dynamic_dfs : list of pandas DataFrame<br>        Future values of the dynamic features<br>    features_order : list of str<br>        Column names in the order in which they were used to train the model.<br>    **kwargs<br>        Other keyword arguments passed to `MLForecast.predict`. |\n",
       "| predict_fn_kwargs |  |  |  |\n",
       "| **Returns** | **DataFrame** |  | **Predictions for each serie and timestep.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(LightGBMCV.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967568da-560f-4c1a-b332-98ad7c1d79c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>LGBMRegressor</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>H196</th>\n",
       "      <td>961</td>\n",
       "      <td>15.644404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H196</th>\n",
       "      <td>962</td>\n",
       "      <td>15.571694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H196</th>\n",
       "      <td>963</td>\n",
       "      <td>15.044647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H196</th>\n",
       "      <td>964</td>\n",
       "      <td>14.931199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H196</th>\n",
       "      <td>965</td>\n",
       "      <td>14.547140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H413</th>\n",
       "      <td>1004</td>\n",
       "      <td>69.663252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H413</th>\n",
       "      <td>1005</td>\n",
       "      <td>62.815338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H413</th>\n",
       "      <td>1006</td>\n",
       "      <td>62.073884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H413</th>\n",
       "      <td>1007</td>\n",
       "      <td>47.510853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H413</th>\n",
       "      <td>1008</td>\n",
       "      <td>44.728784</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ds  LGBMRegressor\n",
       "unique_id                     \n",
       "H196        961      15.644404\n",
       "H196        962      15.571694\n",
       "H196        963      15.044647\n",
       "H196        964      14.931199\n",
       "H196        965      14.547140\n",
       "...         ...            ...\n",
       "H413       1004      69.663252\n",
       "H413       1005      62.815338\n",
       "H413       1006      62.073884\n",
       "H413       1007      47.510853\n",
       "H413       1008      44.728784\n",
       "\n",
       "[192 rows x 2 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = cv.predict(horizon)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f72b12c-f3eb-4db4-a705-08f26957d85f",
   "metadata": {},
   "source": [
    "We can evaluate these predictions on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cfc4d3-ca0a-4db3-a653-564cc6883f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11214839487071401"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_on_valid(preds):\n",
    "    merged = preds.merge(valid, on=['unique_id', 'ds'])\n",
    "    merged['abs_err'] = abs(merged['LGBMRegressor'] - merged['y']) / merged['y']\n",
    "    return merged.groupby('unique_id')['abs_err'].mean().mean()\n",
    "\n",
    "evaluate_on_valid(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4f47bc-bc1a-4199-bd0e-19edb3e05740",
   "metadata": {},
   "source": [
    "The individual models we trained are also saved, so there's also `cv_predict` that uses these models instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f964ef36-3354-486f-bb0f-b011dce8dba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### LightGBMCV.cv_predict\n",
       "\n",
       ">      LightGBMCV.cv_predict (horizon:int,\n",
       ">                             dynamic_dfs:Optional[List[pandas.core.frame.DataFr\n",
       ">                             ame]]=None, predict_fn:Optional[Callable]=None,\n",
       ">                             **predict_fn_kwargs)\n",
       "\n",
       "Compute predictions with each of the trained boosters.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| horizon | int |  | Number of periods to predict. |\n",
       "| dynamic_dfs | typing.Optional[typing.List[pandas.core.frame.DataFrame]] | None | Future values of the dynamic features, e.g. prices. |\n",
       "| predict_fn | typing.Optional[typing.Callable] | None | Custom function to compute predictions.<br>This function will recieve: model, new_x, dynamic_dfs, features_order and kwargs,<br>and should return an array with the predictions, where:<br>    model : regressor<br>        Fitted model.<br>    new_x : pandas DataFrame<br>        Current values of the features.<br>    dynamic_dfs : list of pandas DataFrame<br>        Future values of the dynamic features<br>    features_order : list of str<br>        Column names in the order in which they were used to train the model.<br>    **kwargs<br>        Other keyword arguments passed to `MLForecast.predict`. |\n",
       "| predict_fn_kwargs |  |  |  |\n",
       "| **Returns** | **DataFrame** |  | **Predictions for each serie and timestep, with one column per window.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### LightGBMCV.cv_predict\n",
       "\n",
       ">      LightGBMCV.cv_predict (horizon:int,\n",
       ">                             dynamic_dfs:Optional[List[pandas.core.frame.DataFr\n",
       ">                             ame]]=None, predict_fn:Optional[Callable]=None,\n",
       ">                             **predict_fn_kwargs)\n",
       "\n",
       "Compute predictions with each of the trained boosters.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| horizon | int |  | Number of periods to predict. |\n",
       "| dynamic_dfs | typing.Optional[typing.List[pandas.core.frame.DataFrame]] | None | Future values of the dynamic features, e.g. prices. |\n",
       "| predict_fn | typing.Optional[typing.Callable] | None | Custom function to compute predictions.<br>This function will recieve: model, new_x, dynamic_dfs, features_order and kwargs,<br>and should return an array with the predictions, where:<br>    model : regressor<br>        Fitted model.<br>    new_x : pandas DataFrame<br>        Current values of the features.<br>    dynamic_dfs : list of pandas DataFrame<br>        Future values of the dynamic features<br>    features_order : list of str<br>        Column names in the order in which they were used to train the model.<br>    **kwargs<br>        Other keyword arguments passed to `MLForecast.predict`. |\n",
       "| predict_fn_kwargs |  |  |  |\n",
       "| **Returns** | **DataFrame** |  | **Predictions for each serie and timestep, with one column per window.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(LightGBMCV.cv_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568cf79a-bf39-4ed1-85a1-ce0d62769a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>Booster</th>\n",
       "      <th>Booster2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>H196</th>\n",
       "      <td>961</td>\n",
       "      <td>15.670252</td>\n",
       "      <td>15.848888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H196</th>\n",
       "      <td>962</td>\n",
       "      <td>15.522924</td>\n",
       "      <td>15.697399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H196</th>\n",
       "      <td>963</td>\n",
       "      <td>14.985832</td>\n",
       "      <td>15.166213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H196</th>\n",
       "      <td>964</td>\n",
       "      <td>14.985832</td>\n",
       "      <td>14.723238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H196</th>\n",
       "      <td>965</td>\n",
       "      <td>14.562152</td>\n",
       "      <td>14.451092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H413</th>\n",
       "      <td>1004</td>\n",
       "      <td>70.695242</td>\n",
       "      <td>65.917620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H413</th>\n",
       "      <td>1005</td>\n",
       "      <td>66.216580</td>\n",
       "      <td>62.615788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H413</th>\n",
       "      <td>1006</td>\n",
       "      <td>63.896573</td>\n",
       "      <td>67.848598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H413</th>\n",
       "      <td>1007</td>\n",
       "      <td>46.922797</td>\n",
       "      <td>50.981950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H413</th>\n",
       "      <td>1008</td>\n",
       "      <td>45.006541</td>\n",
       "      <td>42.752819</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ds    Booster   Booster2\n",
       "unique_id                            \n",
       "H196        961  15.670252  15.848888\n",
       "H196        962  15.522924  15.697399\n",
       "H196        963  14.985832  15.166213\n",
       "H196        964  14.985832  14.723238\n",
       "H196        965  14.562152  14.451092\n",
       "...         ...        ...        ...\n",
       "H413       1004  70.695242  65.917620\n",
       "H413       1005  66.216580  62.615788\n",
       "H413       1006  63.896573  67.848598\n",
       "H413       1007  46.922797  50.981950\n",
       "H413       1008  45.006541  42.752819\n",
       "\n",
       "[192 rows x 3 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_preds = cv.cv_predict(horizon)\n",
    "cv_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783fa597-edc7-4192-ae53-a0550f293715",
   "metadata": {},
   "source": [
    "We can average these predictions and evaluate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b64f32d-9875-4b07-a79b-e9d6a1a0f504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11036194712311806"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_preds['LGBMRegressor'] = cv_preds.drop(columns='ds').mean(1)\n",
    "evaluate_on_valid(cv_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beef0686-c58b-467f-9cb7-99bcf3db69b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert evaluate_on_valid(cv_preds) < evaluate_on_valid(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d94021b-a39b-40e3-a802-e4c8bc525358",
   "metadata": {},
   "source": [
    "In this example this achieves a better score!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c076f0-b96c-4372-81df-d7c3fb9626cd",
   "metadata": {},
   "source": [
    "Now, since these series are hourly, maybe we can try to remove the daily seasonality by taking the 168th (24 * 7) difference, that is, substract the value at the same hour from one week ago, thus our target will be $z_t = y_{t} - y_{t-168}$. The features will be computed from this target and when we predict they will be automatically re-applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f03f43a-9db5-4b39-92ee-637bdff73d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Start training from score 0.519010\n",
      "[10] mape: 0.089024\n",
      "[20] mape: 0.090683\n",
      "[30] mape: 0.092316\n",
      "Early stopping at round 30\n",
      "Using best iteration: 10\n"
     ]
    }
   ],
   "source": [
    "cv2 = LightGBMCV(\n",
    "    freq=1,\n",
    "    differences=[24 * 7],\n",
    "    lags=[24 * (i+1) for i in range(7)],\n",
    ")\n",
    "hist2 = cv2.fit(train, **static_fit_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a508692-f8a2-4916-8dab-4254bcab0a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert hist2[-1][1] < hist[-1][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123e14bc-f7dc-4ba9-b7a4-184498dac38c",
   "metadata": {},
   "source": [
    "Nice! We achieve a better score in less iterations. Let's see if this improvement translates to the validation set as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75533283-a2f3-4554-a092-72f08351f013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0912676397267144"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds2 = cv2.predict(horizon)\n",
    "evaluate_on_valid(preds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb0e1ff-d932-4b32-b03c-45a377faf051",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert evaluate_on_valid(preds2) < evaluate_on_valid(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced11e6a-8f0f-4e48-a3c3-fb13c338f41b",
   "metadata": {},
   "source": [
    "Great! Maybe we can try some lag transforms now. We'll try the seasonal rolling mean that averages the values \"every season\", that is, if we set `season_length=24` and `window_size=7` then we'll average the value at the same hour for every day of the week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c728dca-ebff-4dd6-a48f-bce30b16607f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Start training from score 0.273641\n",
      "[10] mape: 0.086724\n",
      "[20] mape: 0.088466\n",
      "[30] mape: 0.090536\n",
      "Early stopping at round 30\n",
      "Using best iteration: 10\n"
     ]
    }
   ],
   "source": [
    "cv3 = LightGBMCV(\n",
    "    freq=1,\n",
    "    differences=[24 * 7],\n",
    "    lags=[24 * (i+1) for i in range(7)],\n",
    "    lag_transforms={\n",
    "        48: [(seasonal_rolling_mean, 24, 7)],\n",
    "    },\n",
    ")\n",
    "hist3 = cv3.fit(train, **static_fit_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6508805-af32-4a6e-9506-6fb21a9f6dec",
   "metadata": {},
   "source": [
    "Seems like this is helping as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cee3dd-5f9c-4ecd-bcaf-c1ab433ac33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert hist3[-1][1] < hist2[-1][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec48e962-a863-48f3-bb2b-e67222ae453c",
   "metadata": {},
   "source": [
    "Does this reflect on the validation set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a924517-4f76-4f33-85cd-8623767897ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0907707496184505"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds3 = cv3.predict(horizon)\n",
    "evaluate_on_valid(preds3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15868ffe-751b-4c4f-a054-74a4750c9504",
   "metadata": {},
   "source": [
    "Nice! mlforecast also supports date features, but in this case our time column is made from integers so there aren't many possibilites here. As you can see this allows you to iterate faster and get better estimates of the forecasting performance you can expect from your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c79e80-a9ee-4217-88b1-a2cce67bc2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_eq(cv._best_iter([(0, 1), (1, 0.5)], 1), 1)\n",
    "test_eq(cv._best_iter([(0, 1), (1, 0.5), (2, 0.6)], 1), 1)\n",
    "test_eq(cv._best_iter([(0, 1), (1, 0.5), (2, 0.6), (3, 0.4)], 2), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677de7cf-80e9-43b9-8780-95bb48a30541",
   "metadata": {},
   "source": [
    "If you're doing hyperparameter tuning it's useful to be able to run a couple of iterations, assess the performance, and determine if this particular configuration isn't promising and should be discarded. For example, [optuna](https://optuna.org/) has [pruners](https://optuna.readthedocs.io/en/stable/reference/pruners.html) that you can call with your current score and it decides if the trial should be discarded. We'll now show how to do that.\n",
    "\n",
    "Since the CV requires a bit of setup, like the LightGBM datasets and the internal features, we have this `setup` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d64983-2947-4c22-8824-3d8041aed277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### LightGBMCV.setup\n",
       "\n",
       ">      LightGBMCV.setup (data:pandas.core.frame.DataFrame, n_windows:int,\n",
       ">                        window_size:int, id_col:str, time_col:str,\n",
       ">                        target_col:str, params:Optional[Dict[str,Any]]=None,\n",
       ">                        static_features:Optional[List[str]]=None,\n",
       ">                        dropna:bool=True, keep_last_n:Optional[int]=None,\n",
       ">                        weights:Optional[Sequence[float]]=None,\n",
       ">                        metric:Union[str,Callable]='mape')\n",
       "\n",
       "Initialize internal data structures to iteratively train the boosters. Use this before calling partial_fit.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| data | DataFrame |  | Series data in long format. |\n",
       "| n_windows | int |  | Number of windows to evaluate. |\n",
       "| window_size | int |  | Number of test periods in each window. |\n",
       "| id_col | str |  | Column that identifies each serie. If 'index' then the index is used. |\n",
       "| time_col | str |  | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str |  | Column that contains the target. |\n",
       "| params | typing.Optional[typing.Dict[str, typing.Any]] | None | Parameters to be passed to the LightGBM Boosters.        |\n",
       "| static_features | typing.Optional[typing.List[str]] | None | Names of the features that are static and will be repeated when forecasting. |\n",
       "| dropna | bool | True | Drop rows with missing values produced by the transformations. |\n",
       "| keep_last_n | typing.Optional[int] | None | Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it. |\n",
       "| weights | typing.Optional[typing.Sequence[float]] | None | Weights to multiply the metric of each window. If None, all windows have the same weight. |\n",
       "| metric | typing.Union[str, typing.Callable] | mape | Metric used to assess the performance of the models and perform early stopping. |\n",
       "| **Returns** | **LightGBMCV** |  | **CV object with internal data structures for partial_fit.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### LightGBMCV.setup\n",
       "\n",
       ">      LightGBMCV.setup (data:pandas.core.frame.DataFrame, n_windows:int,\n",
       ">                        window_size:int, id_col:str, time_col:str,\n",
       ">                        target_col:str, params:Optional[Dict[str,Any]]=None,\n",
       ">                        static_features:Optional[List[str]]=None,\n",
       ">                        dropna:bool=True, keep_last_n:Optional[int]=None,\n",
       ">                        weights:Optional[Sequence[float]]=None,\n",
       ">                        metric:Union[str,Callable]='mape')\n",
       "\n",
       "Initialize internal data structures to iteratively train the boosters. Use this before calling partial_fit.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| data | DataFrame |  | Series data in long format. |\n",
       "| n_windows | int |  | Number of windows to evaluate. |\n",
       "| window_size | int |  | Number of test periods in each window. |\n",
       "| id_col | str |  | Column that identifies each serie. If 'index' then the index is used. |\n",
       "| time_col | str |  | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str |  | Column that contains the target. |\n",
       "| params | typing.Optional[typing.Dict[str, typing.Any]] | None | Parameters to be passed to the LightGBM Boosters.        |\n",
       "| static_features | typing.Optional[typing.List[str]] | None | Names of the features that are static and will be repeated when forecasting. |\n",
       "| dropna | bool | True | Drop rows with missing values produced by the transformations. |\n",
       "| keep_last_n | typing.Optional[int] | None | Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it. |\n",
       "| weights | typing.Optional[typing.Sequence[float]] | None | Weights to multiply the metric of each window. If None, all windows have the same weight. |\n",
       "| metric | typing.Union[str, typing.Callable] | mape | Metric used to assess the performance of the models and perform early stopping. |\n",
       "| **Returns** | **LightGBMCV** |  | **CV object with internal data structures for partial_fit.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(LightGBMCV.setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52afa961-47d3-4208-bf68-ec4f1c01d7e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LightGBMCV(freq=1, lag_features=['lag-24', 'lag-48', 'lag-72', 'lag-96', 'lag-120', 'lag-144', 'lag-168'], date_features=[], num_threads=1, bst_threads=8)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv4 = LightGBMCV(\n",
    "    freq=1,\n",
    "    lags=[24 * (i+1) for i in range(7)],\n",
    ")\n",
    "cv4.setup(\n",
    "    train,\n",
    "    n_windows=2,\n",
    "    window_size=horizon,\n",
    "    id_col='unique_id',\n",
    "    time_col='ds',\n",
    "    target_col='y',\n",
    "    params={'verbose': -1},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d839ecc8-879d-4b80-8d43-0ee8818f3297",
   "metadata": {},
   "source": [
    "Once we have this we can call `partial_fit` to only train for some iterations and return the score of the forecast window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c3bbdb-bbf1-4f45-9d29-548edc205e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### LightGBMCV.partial_fit\n",
       "\n",
       ">      LightGBMCV.partial_fit (num_iterations:int,\n",
       ">                              dynamic_dfs:Optional[List[pandas.core.frame.DataF\n",
       ">                              rame]]=None, predict_fn:Optional[Callable]=None,\n",
       ">                              **predict_fn_kwargs)\n",
       "\n",
       "Train the boosters for some iterations.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| num_iterations | int |  | Number of boosting iterations to run |\n",
       "| dynamic_dfs | typing.Optional[typing.List[pandas.core.frame.DataFrame]] | None | Future values of the dynamic features, e.g. prices. |\n",
       "| predict_fn | typing.Optional[typing.Callable] | None | Custom function to compute predictions.<br>This function will recieve: model, new_x, dynamic_dfs, features_order and kwargs,<br>and should return an array with the predictions, where:<br>    model : regressor<br>        Fitted model.<br>    new_x : pandas DataFrame<br>        Current values of the features.<br>    dynamic_dfs : list of pandas DataFrame<br>        Future values of the dynamic features<br>    features_order : list of str<br>        Column names in the order in which they were used to train the model.<br>    **kwargs<br>        Other keyword arguments passed to `MLForecast.predict`. |\n",
       "| predict_fn_kwargs |  |  |  |\n",
       "| **Returns** | **float** |  | **Weighted metric after training for num_iterations.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### LightGBMCV.partial_fit\n",
       "\n",
       ">      LightGBMCV.partial_fit (num_iterations:int,\n",
       ">                              dynamic_dfs:Optional[List[pandas.core.frame.DataF\n",
       ">                              rame]]=None, predict_fn:Optional[Callable]=None,\n",
       ">                              **predict_fn_kwargs)\n",
       "\n",
       "Train the boosters for some iterations.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| num_iterations | int |  | Number of boosting iterations to run |\n",
       "| dynamic_dfs | typing.Optional[typing.List[pandas.core.frame.DataFrame]] | None | Future values of the dynamic features, e.g. prices. |\n",
       "| predict_fn | typing.Optional[typing.Callable] | None | Custom function to compute predictions.<br>This function will recieve: model, new_x, dynamic_dfs, features_order and kwargs,<br>and should return an array with the predictions, where:<br>    model : regressor<br>        Fitted model.<br>    new_x : pandas DataFrame<br>        Current values of the features.<br>    dynamic_dfs : list of pandas DataFrame<br>        Future values of the dynamic features<br>    features_order : list of str<br>        Column names in the order in which they were used to train the model.<br>    **kwargs<br>        Other keyword arguments passed to `MLForecast.predict`. |\n",
       "| predict_fn_kwargs |  |  |  |\n",
       "| **Returns** | **float** |  | **Weighted metric after training for num_iterations.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(LightGBMCV.partial_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b32778-371f-48c5-b4b2-ae98032b1c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Start training from score 51.745632\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5906900462828166"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = cv4.partial_fit(10)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1f8b24-51c1-4aa5-9b16-58dda4a856ed",
   "metadata": {},
   "source": [
    "This is equal to the first evaluation from our first example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9041285-9c3e-4699-af5a-b817ebdb5a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert hist[0][1] == score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1402118c-2918-413d-9952-1d8355e89478",
   "metadata": {},
   "source": [
    "We can now use this score to decide if this configuration is promising. If we want to we can train some more iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db42351c-f5c7-4e95-85ab-5c1dc78964fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "score2 = cv4.partial_fit(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdebefa-dc38-4358-adf7-d6642b5a4c47",
   "metadata": {},
   "source": [
    "This is now equal to our third metric from the first example, since this time we trained for 20 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894c1069-a5e3-487a-a244-b1cdf6b86981",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert hist[2][1] == score2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a44e00a-c83c-41a7-ada1-7c094a021922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Start training from score 51.745632\n",
      "Using best iteration: 10\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "cv5 = LightGBMCV(\n",
    "    freq=1,\n",
    "    lags=[24 * (i+1) for i in range(7)],\n",
    ")\n",
    "[(_, score3)] = cv5.fit(train, **{**static_fit_config, 'fit_on_all': False}, num_iterations=10, verbose_eval=False)\n",
    "assert score3 == score\n",
    "test_fail(lambda: cv5.predict(1), contains='Must call fit with fit_on_all=True')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
