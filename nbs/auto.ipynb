{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceffd55-8df5-46ca-bc53-b7fc9e2e6380",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a618ae2b-2754-40eb-a649-0667f6dc87a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a30d1be-fb7e-457d-9ac2-9923636ff9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import Callable, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from utilsforecast.compat import DataFrame\n",
    "from utilsforecast.losses import smape\n",
    "\n",
    "from mlforecast import MLForecast\n",
    "from mlforecast.core import Freq, Models, _get_model_name, _name_models\n",
    "from mlforecast.optimization import mlforecast_objective\n",
    "from mlforecast.target_transforms import Differences, LocalStandardScaler, GlobalSklearnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82276eb-22df-4f0b-88b0-61d41b5bc4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "_log1p_tfm = GlobalSklearnTransformer(\n",
    "    FunctionTransformer(func=np.log1p, inverse_func=np.expm1)\n",
    ")\n",
    "\n",
    "def _seasonality_based_config(season_length: int):\n",
    "    def config(trial):\n",
    "        candidate_targ_tfms = [\n",
    "            [LocalStandardScaler()],\n",
    "            [Differences([1]), LocalStandardScaler()],\n",
    "            [_log1p_tfm, LocalStandardScaler()],\n",
    "            [_log1p_tfm, Differences([1]), LocalStandardScaler()],\n",
    "        ]\n",
    "        # if season_length > 1:\n",
    "        #     candidate_targ_tfms.extend(\n",
    "        #         [\n",
    "        #             [_log1p_tfm, Differences([1, season_length]), LocalStandardScaler()],\n",
    "        #             [Differences([1, season_length]), LocalStandardScaler()],\n",
    "        #         ]\n",
    "        #     )\n",
    "        max_lags = trial.suggest_int('max_lags', 1, season_length)\n",
    "        targ_tfms_idx = trial.suggest_categorical(\n",
    "            'target_transforms_idx', range(len(candidate_targ_tfms))\n",
    "        )\n",
    "        return {\n",
    "            'lags': range(1, max_lags + 1),\n",
    "            'target_transforms': candidate_targ_tfms[targ_tfms_idx],\n",
    "        }\n",
    "\n",
    "    return config\n",
    "\n",
    "def _lightgbm_space(trial):\n",
    "    return {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 20, 1000, log=True),\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 1024, log=True),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.1, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.1, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 0, 1),\n",
    "        \"objective\": trial.suggest_categorical(\"objective\", ['l1', 'l2']),\n",
    "        \"verbosity\": -1,\n",
    "    }\n",
    "    \n",
    "def _xgboost_space(trial):\n",
    "    return {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 20, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.2, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.1, 1.0),\n",
    "        'bagging_freq': trial.suggest_float('bagging_freq', 0.1, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 1.0),\n",
    "        'min_data_in_leaf': trial.suggest_float('min_data_in_leaf', 1, 100),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 1.0, log=True),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 1.0, log=True),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 2, 10),\n",
    "    }\n",
    "    \n",
    "def _catboost_space(trial):\n",
    "    return {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 1000),\n",
    "        'depth': trial.suggest_int('depth', 1, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.2, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.1, 1.0),\n",
    "        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.1, 1.0),\n",
    "        'min_data_in_leaf': trial.suggest_float('min_data_in_leaf', 1, 100),\n",
    "    }\n",
    "    \n",
    "def _linear_regression_space(trial):\n",
    "    return {\n",
    "        \"fit_intercept\": trial.suggest_categorical(\"fit_intercept\", [True, False])\n",
    "    }\n",
    "    \n",
    "def _ridge_space(trial):\n",
    "    return {\n",
    "        \"fit_intercept\": trial.suggest_categorical(\"fit_intercept\", [True, False]),\n",
    "        'alpha': trial.suggest_float('alpha', 0.001, 10.0)\n",
    "    }\n",
    "    \n",
    "def _lasso_space(trial):\n",
    "    return {\n",
    "        \"fit_intercept\": trial.suggest_categorical(\"fit_intercept\", [True, False]),\n",
    "        'alpha': trial.suggest_float('alpha', 0.001, 10.0)\n",
    "    }\n",
    "    \n",
    "def _elasticnet_space(trial):\n",
    "    return {\n",
    "        \"fit_intercept\": trial.suggest_categorical(\"fit_intercept\", [True, False]),\n",
    "        'alpha': trial.suggest_float('alpha', 0.001, 10.0),\n",
    "        'l1_ratio': trial.suggest_float('l1_ratio', 0.0, 1.0)\n",
    "    }\n",
    "\n",
    "def _random_forest_space(trial):\n",
    "    return {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 10),\n",
    "        'min_samples_split': trial.suggest_int('min_child_samples', 1, 100),\n",
    "        'max_features': trial.suggest_float('max_features', 0.5, 1.0),\n",
    "        \"criterion\": trial.suggest_categorical(\"criterion\", ['squared_error', 'poisson']),\n",
    "    }\n",
    "\n",
    "_default_model_spaces = {\n",
    "    'LGBMRegressor': _lightgbm_space,\n",
    "    'XGBRegressor': _xgboost_space,\n",
    "    'CatBoostRegressor': _catboost_space,\n",
    "    'LinearRegression': _linear_regression_space,\n",
    "    'Ridge': _ridge_space,\n",
    "    'Lasso': _lasso_space,\n",
    "    'ElasticNet': _elasticnet_space,\n",
    "    'RandomForest': _random_forest_space,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6cb2d6-4507-4f21-9ea8-9c90ce63de01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AutoMLForecast:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        models: Models,        \n",
    "        freq: Freq,\n",
    "        season_length: int,\n",
    "        model_configs: Optional[List[Callable]] = None,\n",
    "        init_config: Optional[Callable] = None,\n",
    "        fit_config: Optional[Callable] = None,\n",
    "    ):\n",
    "        self.freq = freq\n",
    "        if not isinstance(models, dict) and not isinstance(models, list):\n",
    "            models = [models]\n",
    "        if isinstance(models, list):\n",
    "            model_names = _name_models([_get_model_name(m) for m in models])\n",
    "            models_with_names = dict(zip(model_names, models))\n",
    "        else:\n",
    "            models_with_names = models\n",
    "        self.models = models_with_names\n",
    "        if model_configs is not None:\n",
    "            self.model_configs = model_configs\n",
    "        else:\n",
    "            self.model_configs = len(self.models) * [None]\n",
    "        for i, (name, config) in enumerate(zip(self.models.keys(), self.model_configs)):\n",
    "            if config is not None:\n",
    "                continue\n",
    "            if name not in _default_model_spaces:\n",
    "                raise NotImplementedError(\n",
    "                    f\"{name} does not have a default config. Please provide one.\"\n",
    "                )\n",
    "            self.model_configs[i] = _default_model_spaces[name]\n",
    "        if init_config is not None:\n",
    "            self.init_config = init_config\n",
    "        else:\n",
    "            self.init_config = _seasonality_based_config(season_length)\n",
    "        if fit_config is not None:\n",
    "            self.fit_config = fit_config\n",
    "        else:\n",
    "            self.fit_config = lambda trial: {}\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        df: DataFrame,\n",
    "        n_windows: int,\n",
    "        h: int,\n",
    "        num_samples: int,        \n",
    "        loss: Optional[Callable] = None,\n",
    "        search_alg: Optional[optuna.samplers.BaseSampler] = None,\n",
    "        storage: Optional[optuna.storages.BaseStorage] = None,\n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        target_col: str = 'y',\n",
    "    ) -> 'AutoMLForecast':\n",
    "        if loss is None:\n",
    "            def loss(df):\n",
    "                return smape(df, models=['model'])['model'].mean()\n",
    "        if search_alg is None:\n",
    "            search_alg = optuna.samplers.TPESampler(seed=0)\n",
    "\n",
    "        self.results_ = []\n",
    "        self.models_ = {}\n",
    "        for (name, model), model_config in zip(self.models.items(), self.model_configs):\n",
    "            def config_fn(trial: optuna.Trial) -> float:\n",
    "                return {\n",
    "                    'model_params': model_config(trial),\n",
    "                    'mlf_init_params': self.init_config(trial),\n",
    "                    'mlf_fit_params': self.fit_config(trial)\n",
    "                }\n",
    "\n",
    "            objective = mlforecast_objective(\n",
    "                df=df,\n",
    "                config_fn=config_fn,\n",
    "                eval_fn=loss,\n",
    "                model=model,\n",
    "                freq=self.freq,\n",
    "                n_windows=n_windows,\n",
    "                h=h,\n",
    "                id_col=id_col,\n",
    "                time_col=time_col,\n",
    "                target_col=target_col,\n",
    "            )\n",
    "            study = optuna.create_study(\n",
    "                direction='minimize',\n",
    "                sampler=search_alg,\n",
    "                storage=storage,\n",
    "            )\n",
    "            study.optimize(objective, n_trials=num_samples, n_jobs=1)\n",
    "            self.results_.append(study)\n",
    "            best_config = study.best_trial.user_attrs['config']            \n",
    "            best_model = clone(model)\n",
    "            best_model.set_params(**best_config['model_params'])\n",
    "            self.models_[name] = MLForecast(\n",
    "                models={name: best_model},\n",
    "                freq=self.freq,\n",
    "                **best_config['mlf_init_params']\n",
    "            )\n",
    "            self.models_[name].fit(\n",
    "                df,\n",
    "                **best_config['mlf_fit_params']\n",
    "            )\n",
    "        return self\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        h: int,\n",
    "        X_df: Optional[DataFrame] = None,\n",
    "    ) -> DataFrame:\n",
    "        all_preds = None\n",
    "        for name, model in self.models_.items():\n",
    "            preds = model.predict(h=h, X_df=X_df)\n",
    "            if all_preds is None:\n",
    "                all_preds = preds\n",
    "            else:\n",
    "                all_preds[name] = preds[name]\n",
    "        return all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fa7b02-4b05-4027-87d3-1e0a9e3d5225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from datasetsforecast.m4 import M4, M4Evaluation, M4Info\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59de63a9-11e7-4fe0-ae8d-a3e91a7a5ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_split(group):\n",
    "    df, *_ = M4.load(directory='data', group=group)\n",
    "    df['ds'] = df['ds'].astype('int')\n",
    "    horizon = M4Info[group].horizon\n",
    "    valid = df.groupby('unique_id').tail(horizon)\n",
    "    train = df.drop(valid.index)\n",
    "    return train, valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b821d9f5-d374-4058-9961-5434e146e372",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = M4Info['Weekly'].horizon\n",
    "weekly_train, weekly_valid = train_valid_split('Weekly')\n",
    "weekly_train['unique_id'] = weekly_train['unique_id'].astype('category')\n",
    "weekly_valid['unique_id'] = weekly_valid['unique_id'].astype(weekly_train['unique_id'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ccb9b1-484a-4938-a762-1a843a716983",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_mlf = AutoMLForecast(\n",
    "    freq=1,\n",
    "    season_length=52,\n",
    "    models=[\n",
    "        lgb.LGBMRegressor(),\n",
    "        make_pipeline(\n",
    "            ColumnTransformer(\n",
    "                [('encoder', OneHotEncoder(), ['unique_id'])],\n",
    "                remainder='passthrough',\n",
    "            ),\n",
    "            Ridge(),\n",
    "        ),\n",
    "    ],\n",
    "    model_configs=[\n",
    "        None,\n",
    "        lambda trial: {f'ridge__{k}': v for k, v in _ridge_space(trial).items()},\n",
    "    ],\n",
    "    fit_config=lambda trial: {'static_features': ['unique_id']}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3186ac27-ac92-45cc-a5de-09a10a3e03ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.AutoMLForecast>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "auto_mlf.fit(\n",
    "    df=weekly_train,\n",
    "    n_windows=2,\n",
    "    h=h,\n",
    "    num_samples=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa44a167-d480-4ebd-a505-286594757302",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = auto_mlf.predict(h=h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935efa32-d037-42ea-ae92-15f21248466f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMRegressor\n",
      "           SMAPE      MASE       OWA\n",
      "Weekly  7.884069  2.179517  0.822674\n",
      "Ridge\n",
      "           SMAPE      MASE       OWA\n",
      "Weekly  9.167321  2.543791  0.958291\n"
     ]
    }
   ],
   "source": [
    "for model in auto_mlf.models.keys():\n",
    "    print(model)\n",
    "    print(M4Evaluation.evaluate('data', 'Weekly', preds[model].values.reshape(-1, 13)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda7ba11-5d5b-47fe-a256-cd275d3c5ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'model_params': {'n_estimators': 100,\n",
       "   'lambda_l1': 0.00199167444491787,\n",
       "   'lambda_l2': 1.9488396642196672e-07,\n",
       "   'num_leaves': 456,\n",
       "   'feature_fraction': 0.8692518074746102,\n",
       "   'bagging_fraction': 0.5080421148439958,\n",
       "   'bagging_freq': 1,\n",
       "   'objective': 'l1',\n",
       "   'verbosity': -1},\n",
       "  'mlf_init_params': {'lags': range(1, 48),\n",
       "   'target_transforms': [<mlforecast.target_transforms.GlobalSklearnTransformer>,\n",
       "    <mlforecast.target_transforms.Differences>,\n",
       "    <mlforecast.target_transforms.LocalStandardScaler>]},\n",
       "  'mlf_fit_params': {'static_features': ['unique_id']}},\n",
       " {'model_params': {'ridge__fit_intercept': True,\n",
       "   'ridge__alpha': 5.398556881750432},\n",
       "  'mlf_init_params': {'lags': range(1, 2),\n",
       "   'target_transforms': [<mlforecast.target_transforms.GlobalSklearnTransformer>,\n",
       "    <mlforecast.target_transforms.Differences>,\n",
       "    <mlforecast.target_transforms.LocalStandardScaler>]},\n",
       "  'mlf_fit_params': {'static_features': ['unique_id']}}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[res.best_trial.user_attrs['config'] for res in auto_mlf.results_]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
