{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceffd55-8df5-46ca-bc53-b7fc9e2e6380",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a618ae2b-2754-40eb-a649-0667f6dc87a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a30d1be-fb7e-457d-9ac2-9923636ff9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from collections import defaultdict\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, clone\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from utilsforecast.compat import DataFrame\n",
    "from utilsforecast.losses import mase, smape\n",
    "from utilsforecast.processing import counts_by_id\n",
    "from utilsforecast.validation import validate_freq\n",
    "\n",
    "from mlforecast import MLForecast\n",
    "from mlforecast.core import Freq, Models, _get_model_name, _name_models\n",
    "from mlforecast.lag_transforms import ExpandingMean, ExponentiallyWeightedMean, RollingMean\n",
    "from mlforecast.optimization import mlforecast_objective\n",
    "from mlforecast.target_transforms import Differences, LocalStandardScaler, GlobalSklearnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0cbbcb-b4df-4998-801b-75bebdc9f2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def lightgbm_space(trial):\n",
    "    return {\n",
    "        \"bagging_freq\": 1,\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"verbosity\": -1,        \n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 20, 1000, log=True),\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 4096, log=True),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
    "        \"objective\": trial.suggest_categorical(\"objective\", ['l1', 'l2']),\n",
    "    }\n",
    "\n",
    "def xgboost_space(trial):\n",
    "    return {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 20, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.2, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.1, 1.0),\n",
    "        'bagging_freq': trial.suggest_float('bagging_freq', 0.1, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 1.0),\n",
    "        'min_data_in_leaf': trial.suggest_float('min_data_in_leaf', 1, 100),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 1.0, log=True),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 1.0, log=True),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 2, 10),\n",
    "    }\n",
    "    \n",
    "def catboost_space(trial):\n",
    "    return {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 1000),\n",
    "        'depth': trial.suggest_int('depth', 1, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.2, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.1, 1.0),\n",
    "        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.1, 1.0),\n",
    "        'min_data_in_leaf': trial.suggest_float('min_data_in_leaf', 1, 100),\n",
    "    }\n",
    "    \n",
    "def linear_regression_space(trial):\n",
    "    return {\n",
    "        \"fit_intercept\": trial.suggest_categorical(\"fit_intercept\", [True, False])\n",
    "    }\n",
    "    \n",
    "def ridge_space(trial):\n",
    "    return {\n",
    "        \"fit_intercept\": trial.suggest_categorical(\"fit_intercept\", [True, False]),\n",
    "        'alpha': trial.suggest_float('alpha', 0.001, 10.0)\n",
    "    }\n",
    "    \n",
    "def lasso_space(trial):\n",
    "    return {\n",
    "        \"fit_intercept\": trial.suggest_categorical(\"fit_intercept\", [True, False]),\n",
    "        'alpha': trial.suggest_float('alpha', 0.001, 10.0)\n",
    "    }\n",
    "    \n",
    "def elasticnet_space(trial):\n",
    "    return {\n",
    "        \"fit_intercept\": trial.suggest_categorical(\"fit_intercept\", [True, False]),\n",
    "        'alpha': trial.suggest_float('alpha', 0.001, 10.0),\n",
    "        'l1_ratio': trial.suggest_float('l1_ratio', 0.0, 1.0)\n",
    "    }\n",
    "\n",
    "def random_forest_space(trial):\n",
    "    return {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 10),\n",
    "        'min_samples_split': trial.suggest_int('min_child_samples', 1, 100),\n",
    "        'max_features': trial.suggest_float('max_features', 0.5, 1.0),\n",
    "        \"criterion\": trial.suggest_categorical(\"criterion\", ['squared_error', 'poisson']),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01bfb2f-43ab-4848-add3-3e4c01cf3c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "_default_model_spaces = {\n",
    "    'LGBMRegressor': lightgbm_space,\n",
    "    'XGBRegressor': xgboost_space,\n",
    "    'CatBoostRegressor': catboost_space,\n",
    "    'LinearRegression': linear_regression_space,\n",
    "    'Ridge': ridge_space,\n",
    "    'Lasso': lasso_space,\n",
    "    'ElasticNet': elasticnet_space,\n",
    "    'RandomForest': random_forest_space,\n",
    "}\n",
    "\n",
    "_ModelWithConfig = Tuple[BaseEstimator, Optional[Dict[str, Callable]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6cb2d6-4507-4f21-9ea8-9c90ce63de01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AutoMLForecast:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        models_with_configs: Union[List[_ModelWithConfig], Dict[str, _ModelWithConfig]],\n",
    "        freq: Freq,\n",
    "        season_length: int,\n",
    "        init_config: Optional[Callable] = None,\n",
    "        fit_config: Optional[Callable] = None,\n",
    "        num_threads: int = 1,\n",
    "    ):\n",
    "        self.freq = freq\n",
    "        self.season_length = season_length\n",
    "        self.num_threads = num_threads\n",
    "        if isinstance(models_with_configs, list):\n",
    "            names = _name_models([_get_model_name(m) for m, _ in models_with_configs])\n",
    "            self.models_with_configs = {\n",
    "                name: (model, config)\n",
    "                for name, (model, config) in zip(names, models_with_configs)\n",
    "            }\n",
    "        elif isinstance(models_with_configs, dict):\n",
    "            self.models_with_configs = models_with_configs\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                '`models_with_configs` should be a list of tuples '\n",
    "                'or a dict from str to tuple.'\n",
    "            )\n",
    "        for name, (model, config) in self.models_with_configs.items():\n",
    "            if config is not None:\n",
    "                continue\n",
    "            if name not in _default_model_spaces:\n",
    "                raise NotImplementedError(\n",
    "                    f\"{name} does not have a default config. Please provide one.\"\n",
    "                )\n",
    "            self.models_with_configs[name] = (model, _default_model_spaces[name])\n",
    "        self.init_config = init_config\n",
    "        if fit_config is not None:\n",
    "            self.fit_config = fit_config\n",
    "        else:\n",
    "            self.fit_config = lambda trial: {}\n",
    "\n",
    "    def _seasonality_based_config(\n",
    "        self,\n",
    "        h: int,\n",
    "        min_samples: int,\n",
    "        min_value: float,\n",
    "    ) -> Callable:\n",
    "        # target transforms  \n",
    "        candidate_targ_tfms = [\n",
    "            None,\n",
    "            [LocalStandardScaler()],\n",
    "            [Differences([1]), LocalStandardScaler()],\n",
    "        ]\n",
    "        log1p_tfm = GlobalSklearnTransformer(\n",
    "            FunctionTransformer(func=np.log1p, inverse_func=np.expm1)\n",
    "        )\n",
    "        if min_value >= 0:\n",
    "            candidate_targ_tfms.extend(\n",
    "                [\n",
    "                    [log1p_tfm, LocalStandardScaler()],\n",
    "                    [log1p_tfm, Differences([1]), LocalStandardScaler()],\n",
    "                ]\n",
    "            )\n",
    "        # we leave two seasonal periods for the features and model\n",
    "        if self.season_length > 1 and min_samples > 3 * self.season_length + 1:\n",
    "            candidate_targ_tfms.append([Differences([1, self.season_length]), LocalStandardScaler()])\n",
    "            if min_value >= 0:\n",
    "                candidate_targ_tfms.append(\n",
    "                    [log1p_tfm, Differences([1, self.season_length]), LocalStandardScaler()],\n",
    "                )\n",
    "\n",
    "        # lags\n",
    "        candidate_lags = [None]\n",
    "        if self.season_length > 1:\n",
    "            candidate_lags.append([self.season_length])\n",
    "        seasonality2extra_candidate_lags = {\n",
    "            7: [\n",
    "                [7, 14],\n",
    "                [7, 28],\n",
    "            ],\n",
    "            12: [range(1, 13)],\n",
    "            24: [\n",
    "                range(1, 25),\n",
    "                range(24, 24 * 7 + 1, 24),\n",
    "            ],\n",
    "        }\n",
    "        candidate_lags.extend(\n",
    "            seasonality2extra_candidate_lags.get(self.season_length, [])\n",
    "        )\n",
    "        if h >= 2 * self.season_length:\n",
    "            candidate_lags.extend(\n",
    "                [\n",
    "                    range(self.season_length, h + 1, self.season_length),\n",
    "                    [h],\n",
    "                    [self.season_length, h],\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        # lag transforms\n",
    "        candidate_lag_tfms = [None, {1: [ExponentiallyWeightedMean(0.9)]}]\n",
    "        if self.season_length > 1:\n",
    "            candidate_lag_tfms.append(\n",
    "                {\n",
    "                    1: [ExponentiallyWeightedMean(0.9)],\n",
    "                    self.season_length: [\n",
    "                        RollingMean(window_size=self.season_length, min_samples=1),\n",
    "                    ]\n",
    "                }\n",
    "            )\n",
    "        if self.season_length != h:\n",
    "            candidate_lag_tfms.append(\n",
    "                {\n",
    "                    1: [ExponentiallyWeightedMean(0.9)],\n",
    "                    self.season_length: [\n",
    "                        RollingMean(window_size=self.season_length, min_samples=1),\n",
    "                    ],\n",
    "                    h: [\n",
    "                        RollingMean(window_size=self.season_length, min_samples=1),\n",
    "                    ]\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # date features\n",
    "        seasonality2date_features = {\n",
    "            1: ['year'],\n",
    "            4: ['quarter', 'year'],\n",
    "            7: ['weekday', 'month', 'year'],\n",
    "            12: ['month', 'year'],\n",
    "            24: ['hour', 'weekday', 'month', 'year'],\n",
    "            52: ['week', 'year'],\n",
    "            60: ['weekday', 'hour', 'second'],\n",
    "        }\n",
    "        date_features = seasonality2date_features.get(self.season_length, None)\n",
    "        if isinstance(self.freq, int):\n",
    "            date_features = None\n",
    "        if date_features is not None:\n",
    "            use_date_features = trial.suggest_int('use_date_features', 0, 1)\n",
    "            if not use_date_features:\n",
    "                date_features = None        \n",
    "\n",
    "        def config(trial):\n",
    "            # target transforms\n",
    "            targ_tfms_idx = trial.suggest_categorical(\n",
    "                'target_transforms_idx', range(len(candidate_targ_tfms))\n",
    "            )\n",
    "            target_transforms = candidate_targ_tfms[targ_tfms_idx]\n",
    "    \n",
    "            # lags\n",
    "            lags_idx = trial.suggest_categorical('lags_idx', range(len(candidate_lags)))\n",
    "            lags = candidate_lags[lags_idx]\n",
    "    \n",
    "            # lag transforms\n",
    "            if candidate_lag_tfms:\n",
    "                lag_tfms_idx = trial.suggest_categorical(\n",
    "                    'lag_transforms_idx', range(len(candidate_lag_tfms))\n",
    "                )\n",
    "                lag_transforms = candidate_lag_tfms[lag_tfms_idx]\n",
    "            else:\n",
    "                lag_transforms = None\n",
    "            \n",
    "            return {\n",
    "                'lags': lags,\n",
    "                'target_transforms': target_transforms,\n",
    "                'lag_transforms': lag_transforms,\n",
    "                'date_features': date_features,            \n",
    "            }\n",
    "\n",
    "        return config\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        df: DataFrame,\n",
    "        n_windows: int,\n",
    "        h: int,\n",
    "        num_samples: int,        \n",
    "        loss: Optional[Callable] = None,\n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        target_col: str = 'y',\n",
    "        study_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        optimize_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    ) -> 'AutoMLForecast':\n",
    "        validate_freq(df[time_col], self.freq)\n",
    "        if self.init_config is not None:\n",
    "            init_config = self.init_config\n",
    "        else:\n",
    "            min_size = counts_by_id(df, id_col)['counts'].min()\n",
    "            min_train_size = min_size - n_windows * h\n",
    "            init_config = self._seasonality_based_config(\n",
    "                h=h,\n",
    "                min_samples=min_train_size,\n",
    "                min_value=df[target_col].min(),\n",
    "            )\n",
    "\n",
    "        if loss is None:\n",
    "            def loss(df, train_df):\n",
    "                return smape(df, models=['model'])['model'].mean()\n",
    "        if study_kwargs is None:\n",
    "            study_kwargs = {}\n",
    "        if 'sampler' not in study_kwargs:\n",
    "            # for reproducibility\n",
    "            study_kwargs['sampler'] = optuna.samplers.TPESampler(seed=0)\n",
    "        if optimize_kwargs is None:\n",
    "            optimize_kwargs = {}\n",
    "        if 'n_jobs' not in optimize_kwargs:\n",
    "            optimize_kwargs['n_jobs'] = 1\n",
    "\n",
    "        self.results_ = []\n",
    "        self.models_ = {}\n",
    "        for name, (model, model_config) in self.models_with_configs.items():\n",
    "            def config_fn(trial: optuna.Trial) -> float:\n",
    "                return {\n",
    "                    'model_params': model_config(trial),\n",
    "                    'mlf_init_params': {\n",
    "                        **init_config(trial),\n",
    "                        'num_threads': self.num_threads,\n",
    "                    },\n",
    "                    'mlf_fit_params': self.fit_config(trial)\n",
    "                }\n",
    "\n",
    "            objective = mlforecast_objective(\n",
    "                df=df,\n",
    "                config_fn=config_fn,\n",
    "                eval_fn=loss,\n",
    "                model=model,\n",
    "                freq=self.freq,\n",
    "                n_windows=n_windows,\n",
    "                h=h,\n",
    "                id_col=id_col,\n",
    "                time_col=time_col,\n",
    "                target_col=target_col,\n",
    "            )\n",
    "            study = optuna.create_study(direction='minimize', **study_kwargs)\n",
    "            study.optimize(objective, n_trials=num_samples, **optimize_kwargs)\n",
    "            self.results_.append(study)\n",
    "            best_config = study.best_trial.user_attrs['config']            \n",
    "            best_model = clone(model)\n",
    "            best_model.set_params(**best_config['model_params'])\n",
    "            self.models_[name] = MLForecast(\n",
    "                models={name: best_model},\n",
    "                freq=self.freq,\n",
    "                **best_config['mlf_init_params']\n",
    "            )\n",
    "            self.models_[name].fit(\n",
    "                df,\n",
    "                **best_config['mlf_fit_params']\n",
    "            )\n",
    "        return self\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        h: int,\n",
    "        X_df: Optional[DataFrame] = None,\n",
    "    ) -> DataFrame:\n",
    "        all_preds = None\n",
    "        for name, model in self.models_.items():\n",
    "            preds = model.predict(h=h, X_df=X_df)\n",
    "            if all_preds is None:\n",
    "                all_preds = preds\n",
    "            else:\n",
    "                all_preds[name] = preds[name]\n",
    "        return all_preds\n",
    "\n",
    "    def save(self, path: str) -> None:\n",
    "        for name, model in self.models_.items():\n",
    "            model.save(f'{path}/{name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fa7b02-4b05-4027-87d3-1e0a9e3d5225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import lightgbm as lgb\n",
    "from datasetsforecast.m4 import M4, M4Evaluation, M4Info\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59de63a9-11e7-4fe0-ae8d-a3e91a7a5ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_split(group):\n",
    "    df, *_ = M4.load(directory='data', group=group)\n",
    "    df['ds'] = df['ds'].astype('int')\n",
    "    horizon = M4Info[group].horizon\n",
    "    valid = df.groupby('unique_id').tail(horizon).copy()\n",
    "    train = df.drop(valid.index).reset_index(drop=True)\n",
    "    return train, valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f131da88-6243-4b26-92d0-c0a1f559234b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ccb9b1-484a-4938-a762-1a843a716983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_group(group):\n",
    "    train, valid = train_valid_split(group)\n",
    "    train['unique_id'] = train['unique_id'].astype('category')\n",
    "    valid['unique_id'] = valid['unique_id'].astype(train['unique_id'].dtype)\n",
    "    info = M4Info[group]\n",
    "    h = info.horizon\n",
    "    season_length = info.seasonality\n",
    "    ridge_pipeline = make_pipeline(\n",
    "        ColumnTransformer(\n",
    "            [('encoder', OneHotEncoder(), ['unique_id'])],\n",
    "            remainder='passthrough',\n",
    "        ),\n",
    "        Ridge(),\n",
    "    )\n",
    "    auto_mlf = AutoMLForecast(\n",
    "        freq=1,\n",
    "        season_length=season_length,\n",
    "        models_with_configs=[(lgb.LGBMRegressor(), None)],\n",
    "        fit_config=lambda trial: {'static_features': ['unique_id']} if trial.suggest_int('static_features', 0, 1) else {},\n",
    "    )\n",
    "    with warnings.catch_warnings(record=False):\n",
    "        warnings.simplefilter('ignore', category=UserWarning)\n",
    "        auto_mlf.fit(\n",
    "            df=train,\n",
    "            n_windows=2,\n",
    "            h=h,\n",
    "            num_samples=30,\n",
    "            optimize_kwargs={'timeout': 60 * 60},\n",
    "        )\n",
    "    with open(f'{group}_opt.pkl', 'wb') as f:\n",
    "        pickle.dump(auto_mlf.results_, f)\n",
    "    preds = auto_mlf.predict(h=h)\n",
    "    for model in auto_mlf.models_with_configs.keys():\n",
    "        print(model)\n",
    "        print(M4Evaluation.evaluate('data', group, preds[model].values.reshape(-1, h)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02af992-d4a0-492f-b929-cd9309687daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
    "\n",
    "for group in ('Yearly', 'Quarterly', 'Weekly', 'Hourly', 'Monthly'):\n",
    "    print(f'Running {group}')\n",
    "    start = time.perf_counter()\n",
    "    run_group(group)\n",
    "    print(f'{group} took {(time.perf_counter() - start) / 60:.1f} minutes')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
