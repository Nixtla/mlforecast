{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceffd55-8df5-46ca-bc53-b7fc9e2e6380",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a618ae2b-2754-40eb-a649-0667f6dc87a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp auto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7b2082-4138-4d54-8e10-9daa3747768c",
   "metadata": {},
   "source": [
    "# Auto\n",
    "Automatic model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a30d1be-fb7e-457d-9ac2-9923636ff9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Dict, List, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import optuna\n",
    "import utilsforecast.processing as ufp\n",
    "from sklearn.base import BaseEstimator, clone\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from utilsforecast.compat import DataFrame\n",
    "from utilsforecast.losses import smape\n",
    "from utilsforecast.validation import validate_freq\n",
    "\n",
    "from mlforecast import MLForecast\n",
    "from mlforecast.core import Freq, _get_model_name, _name_models\n",
    "from mlforecast.lag_transforms import ExponentiallyWeightedMean, RollingMean\n",
    "from mlforecast.optimization import _TrialToConfig, mlforecast_objective\n",
    "from mlforecast.target_transforms import Differences, LocalStandardScaler, GlobalSklearnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0cbbcb-b4df-4998-801b-75bebdc9f2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def lightgbm_space(trial: optuna.Trial):\n",
    "    return {\n",
    "        'bagging_freq': 1,\n",
    "        'learning_rate': 0.05,\n",
    "        'verbosity': -1,        \n",
    "        'n_estimators': trial.suggest_int('n_estimators', 20, 1000, log=True),\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 4096, log=True),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
    "        'objective': trial.suggest_categorical('objective', ['l1', 'l2']),\n",
    "    }\n",
    "\n",
    "def xgboost_space(trial: optuna.Trial):\n",
    "    return {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 20, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.2, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.1, 1.0),\n",
    "        'bagging_freq': trial.suggest_float('bagging_freq', 0.1, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 1.0),\n",
    "        'min_data_in_leaf': trial.suggest_float('min_data_in_leaf', 1, 100),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 2, 10),\n",
    "    }\n",
    "    \n",
    "def catboost_space(trial: optuna.Trial):\n",
    "    return {\n",
    "        'silent': True,\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 1000),\n",
    "        'depth': trial.suggest_int('depth', 1, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.2, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.1, 1.0),\n",
    "        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.1, 1.0),\n",
    "        'min_data_in_leaf': trial.suggest_float('min_data_in_leaf', 1, 100),\n",
    "    }\n",
    "    \n",
    "def linear_regression_space(trial: optuna.Trial):\n",
    "    return {\n",
    "        'fit_intercept': trial.suggest_categorical('fit_intercept', [True, False])\n",
    "    }\n",
    "    \n",
    "def ridge_space(trial: optuna.Trial):\n",
    "    return {\n",
    "        'fit_intercept': trial.suggest_categorical('fit_intercept', [True, False]),\n",
    "        'alpha': trial.suggest_float('alpha', 0.001, 10.0)\n",
    "    }\n",
    "    \n",
    "def lasso_space(trial: optuna.Trial):\n",
    "    return {\n",
    "        'fit_intercept': trial.suggest_categorical('fit_intercept', [True, False]),\n",
    "        'alpha': trial.suggest_float('alpha', 0.001, 10.0)\n",
    "    }\n",
    "    \n",
    "def elastic_net_space(trial: optuna.Trial):\n",
    "    return {\n",
    "        'fit_intercept': trial.suggest_categorical('fit_intercept', [True, False]),\n",
    "        'alpha': trial.suggest_float('alpha', 0.001, 10.0),\n",
    "        'l1_ratio': trial.suggest_float('l1_ratio', 0.0, 1.0)\n",
    "    }\n",
    "\n",
    "def random_forest_space(trial: optuna.Trial):\n",
    "    return {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 10),\n",
    "        'min_samples_split': trial.suggest_int('min_child_samples', 1, 100),\n",
    "        'max_features': trial.suggest_float('max_features', 0.5, 1.0),\n",
    "        'criterion': trial.suggest_categorical('criterion', ['squared_error', 'absolute_error']),\n",
    "    }\n",
    "\n",
    "class AutoModel:\n",
    "    \"\"\"Structure to hold a model and its search space\n",
    " \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : BaseEstimator\n",
    "        scikit-learn compatible regressor\n",
    "    config : callable \n",
    "        function that takes an optuna trial and produces a configuration\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: BaseEstimator,\n",
    "        config: _TrialToConfig,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'AutoModel(model={_get_model_name(self.model)})'\n",
    "\n",
    "class AutoLightGBM(AutoModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: Optional[_TrialToConfig] = None,\n",
    "    ):\n",
    "        from mlforecast.compat import LGBMRegressor\n",
    "        super().__init__(\n",
    "            LGBMRegressor(),\n",
    "            config if config is not None else lightgbm_space,\n",
    "        )\n",
    "\n",
    "class AutoXGBoost(AutoModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: Optional[_TrialToConfig] = None,\n",
    "    ):\n",
    "        from mlforecast.compat import XGBRegressor\n",
    "        super().__init__(\n",
    "            XGBRegressor(),\n",
    "            config if config is not None else xgboost_space,\n",
    "        )\n",
    "\n",
    "class AutoCatboost(AutoModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: Optional[_TrialToConfig] = None,\n",
    "    ):\n",
    "        from mlforecast.compat import CatBoostRegressor\n",
    "        super().__init__(\n",
    "            CatBoostRegressor(),\n",
    "            config if config is not None else catboost_space,\n",
    "        )\n",
    "\n",
    "class AutoLinearRegression(AutoModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: Optional[_TrialToConfig] = None,\n",
    "    ):\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        super().__init__(\n",
    "            LinearRegression(),\n",
    "            config if config is not None else linear_regression_space,\n",
    "        )\n",
    "\n",
    "class AutoRidge(AutoModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: Optional[_TrialToConfig] = None,\n",
    "    ):\n",
    "        from sklearn.linear_model import Ridge\n",
    "        super().__init__(\n",
    "            Ridge(),\n",
    "            config if config is not None else ridge_space,\n",
    "        )\n",
    "\n",
    "class AutoLasso(AutoModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: Optional[_TrialToConfig] = None,\n",
    "    ):\n",
    "        from sklearn.linear_model import Lasso\n",
    "        super().__init__(\n",
    "            Lasso(),\n",
    "            config if config is not None else lasso_space,\n",
    "        )\n",
    "\n",
    "class AutoElasticNet(AutoModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: Optional[_TrialToConfig] = None,\n",
    "    ):\n",
    "        from sklearn.linear_model import ElasticNet\n",
    "        super().__init__(\n",
    "            ElasticNet(),\n",
    "            config if config is not None else elastic_net_space,\n",
    "        )\n",
    "\n",
    "class AutoRandomForest(AutoModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: Optional[_TrialToConfig] = None,\n",
    "    ):\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        super().__init__(\n",
    "            RandomForestRegressor(),\n",
    "            config if config is not None else random_forest_space,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2819ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9f3693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/mlforecast/blob/main/mlforecast/auto.py#L113){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AutoModel\n",
       "\n",
       ">      AutoModel (model:sklearn.base.BaseEstimator,\n",
       ">                 config:Callable[[optuna.trial._trial.Trial],Dict[str,Any]])\n",
       "\n",
       "Structure to hold a model and its search space\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| model | BaseEstimator | scikit-learn compatible regressor |\n",
       "| config | Callable | function that takes an optuna trial and produces a configuration |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/mlforecast/blob/main/mlforecast/auto.py#L113){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AutoModel\n",
       "\n",
       ">      AutoModel (model:sklearn.base.BaseEstimator,\n",
       ">                 config:Callable[[optuna.trial._trial.Trial],Dict[str,Any]])\n",
       "\n",
       "Structure to hold a model and its search space\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| model | BaseEstimator | scikit-learn compatible regressor |\n",
       "| config | Callable | function that takes an optuna trial and produces a configuration |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AutoModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6cb2d6-4507-4f21-9ea8-9c90ce63de01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AutoMLForecast:\n",
    "    \"\"\"Hyperparameter optimization helper\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    models : list or dict\n",
    "        Auto models to be optimized.\n",
    "    freq : str or int\n",
    "        pandas' or polars' offset alias or integer denoting the frequency of the series.\n",
    "    season_length : int\n",
    "        Length of the seasonal period. This is used for producing the feature space.\n",
    "    init_config : callable, optional (default=None)\n",
    "        Function that takes an optuna trial and produces a configuration passed to the MLForecast constructor.\n",
    "    fit_config : callable, optional (default=None)\n",
    "        Function that takes an optuna trial and produces a configuration passed to the MLForecast fit method.\n",
    "    num_threads : int (default=1)\n",
    "        Number of threads to use when computing the features.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        models: Union[List[AutoModel], Dict[str, AutoModel]],\n",
    "        freq: Freq,\n",
    "        season_length: int,\n",
    "        init_config: Optional[_TrialToConfig] = None,\n",
    "        fit_config: Optional[_TrialToConfig] = None,\n",
    "        num_threads: int = 1,\n",
    "    ):\n",
    "        self.freq = freq\n",
    "        self.season_length = season_length\n",
    "        self.num_threads = num_threads\n",
    "        if isinstance(models, list):\n",
    "            model_names = _name_models([_get_model_name(m) for m in models])\n",
    "            models_with_names = dict(zip(model_names, models))\n",
    "        else:\n",
    "            models_with_names = models\n",
    "        self.models = models_with_names\n",
    "        if init_config is not None and not callable(init_config):\n",
    "            raise ValueError('`init_config` must be a function.')            \n",
    "        self.init_config = init_config\n",
    "        if fit_config is not None:\n",
    "            if not callable(fit_config):\n",
    "                raise ValueError('`fit_config` must be a function.')\n",
    "            self.fit_config = fit_config\n",
    "        else:\n",
    "            self.fit_config = lambda trial: {}  # noqa: ARG005\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'AutoMLForecast(models={self.models})'\n",
    "\n",
    "    def _seasonality_based_config(\n",
    "        self,\n",
    "        h: int,\n",
    "        min_samples: int,\n",
    "        min_value: float,\n",
    "    ) -> _TrialToConfig:\n",
    "        # target transforms  \n",
    "        candidate_targ_tfms: List[Any] = [\n",
    "            None,\n",
    "            [LocalStandardScaler()],\n",
    "            [Differences([1]), LocalStandardScaler()],\n",
    "        ]\n",
    "        log1p_tfm = GlobalSklearnTransformer(\n",
    "            FunctionTransformer(func=np.log1p, inverse_func=np.expm1)\n",
    "        )\n",
    "        if min_value >= 0:\n",
    "            candidate_targ_tfms.extend(\n",
    "                [\n",
    "                    [log1p_tfm, LocalStandardScaler()],\n",
    "                    [log1p_tfm, Differences([1]), LocalStandardScaler()],\n",
    "                ]\n",
    "            )\n",
    "        # we leave two seasonal periods for the features and model\n",
    "        if self.season_length > 1 and min_samples > 3 * self.season_length + 1:\n",
    "            candidate_targ_tfms.append([Differences([1, self.season_length]), LocalStandardScaler()])\n",
    "            if min_value >= 0:\n",
    "                candidate_targ_tfms.append(\n",
    "                    [log1p_tfm, Differences([1, self.season_length]), LocalStandardScaler()],\n",
    "                )\n",
    "\n",
    "        # lags\n",
    "        candidate_lags = [None, [self.season_length]]\n",
    "        seasonality2extra_candidate_lags = {\n",
    "            7: [\n",
    "                [7, 14],\n",
    "                [7, 28],\n",
    "            ],\n",
    "            12: [range(1, 13)],\n",
    "            24: [\n",
    "                range(1, 25),\n",
    "                range(24, 24 * 7 + 1, 24),\n",
    "            ],\n",
    "            52: [\n",
    "                range(4, 53, 4),\n",
    "            ]\n",
    "        }\n",
    "        if self.season_length in seasonality2extra_candidate_lags:\n",
    "            candidate_lags.extend(\n",
    "                seasonality2extra_candidate_lags[self.season_length]  # type: ignore\n",
    "            )\n",
    "        if h >= 2 * self.season_length:\n",
    "            candidate_lags.extend(\n",
    "                [\n",
    "                    range(self.season_length, h + 1, self.season_length),  # type: ignore\n",
    "                    [h],\n",
    "                    [self.season_length, h],\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        # lag transforms\n",
    "        candidate_lag_tfms = [None, {1: [ExponentiallyWeightedMean(0.9)]}]\n",
    "        if self.season_length > 1:\n",
    "            candidate_lag_tfms.append(\n",
    "                {\n",
    "                    1: [ExponentiallyWeightedMean(0.9)],\n",
    "                    self.season_length: [\n",
    "                        RollingMean(window_size=self.season_length, min_samples=1),\n",
    "                    ]\n",
    "                }\n",
    "            )\n",
    "        if self.season_length != h:\n",
    "            candidate_lag_tfms.append(\n",
    "                {\n",
    "                    1: [ExponentiallyWeightedMean(0.9)],\n",
    "                    self.season_length: [\n",
    "                        RollingMean(window_size=self.season_length, min_samples=1),\n",
    "                    ],\n",
    "                    h: [\n",
    "                        RollingMean(window_size=self.season_length, min_samples=1),\n",
    "                    ]\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # date features\n",
    "        seasonality2date_features = {\n",
    "            1: ['year'],\n",
    "            4: ['quarter', 'year'],\n",
    "            7: ['weekday', 'month', 'year'],\n",
    "            12: ['month', 'year'],\n",
    "            24: ['hour', 'weekday', 'month', 'year'],\n",
    "            52: ['week', 'year'],\n",
    "            60: ['weekday', 'hour', 'second'],\n",
    "        }\n",
    "        candidate_date_features = seasonality2date_features.get(self.season_length, [])\n",
    "        if isinstance(self.freq, int):\n",
    "            candidate_date_features = []\n",
    "\n",
    "        def config(trial):\n",
    "            # target transforms\n",
    "            targ_tfms_idx = trial.suggest_categorical(\n",
    "                'target_transforms_idx', range(len(candidate_targ_tfms))\n",
    "            )\n",
    "            target_transforms = candidate_targ_tfms[targ_tfms_idx]\n",
    "    \n",
    "            # lags\n",
    "            lags_idx = trial.suggest_categorical('lags_idx', range(len(candidate_lags)))\n",
    "            lags = candidate_lags[lags_idx]\n",
    "    \n",
    "            # lag transforms\n",
    "            if candidate_lag_tfms:\n",
    "                lag_tfms_idx = trial.suggest_categorical(\n",
    "                    'lag_transforms_idx', range(len(candidate_lag_tfms))\n",
    "                )\n",
    "                lag_transforms = candidate_lag_tfms[lag_tfms_idx]\n",
    "            else:\n",
    "                lag_transforms = None\n",
    "\n",
    "            # date features\n",
    "            if candidate_date_features:\n",
    "                use_date_features = trial.suggest_int('use_date_features', 0, 1)\n",
    "                if use_date_features:\n",
    "                    date_features = candidate_date_features\n",
    "                else:\n",
    "                    date_features = None        \n",
    "            else:\n",
    "                date_features = None\n",
    "            \n",
    "            return {\n",
    "                'lags': lags,\n",
    "                'target_transforms': target_transforms,\n",
    "                'lag_transforms': lag_transforms,\n",
    "                'date_features': date_features,            \n",
    "            }\n",
    "\n",
    "        return config\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        df: DataFrame,\n",
    "        n_windows: int,\n",
    "        h: int,\n",
    "        num_samples: int,\n",
    "        refit: Union[bool, int] = False,\n",
    "        loss: Optional[Callable[[DataFrame, DataFrame], float]] = None,\n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        target_col: str = 'y',\n",
    "        study_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        optimize_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        fitted: bool = False,\n",
    "    ) -> 'AutoMLForecast':\n",
    "        \"\"\"Carry out the optimization process.\n",
    "        Each model is optimized independently and the best one is trained on all data\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas or polars DataFrame\n",
    "            Series data in long format.\n",
    "        n_windows : int\n",
    "            Number of windows to evaluate.\n",
    "        h : int\n",
    "            Forecast horizon.\n",
    "        num_samples : int\n",
    "            Number of trials to run\n",
    "        refit : bool or int (default=False)\n",
    "            Retrain model for each cross validation window.\n",
    "            If False, the models are trained at the beginning and then used to predict each window.\n",
    "            If positive int, the models are retrained every `refit` windows.\n",
    "        loss : callable, optional (default=None)\n",
    "            Function that takes the validation and train dataframes and produces a float.\n",
    "            If `None` will use the average SMAPE across series.\n",
    "        id_col : str (default='unique_id')\n",
    "            Column that identifies each serie.\n",
    "        time_col : str (default='ds')\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str (default='y')\n",
    "            Column that contains the target.        \n",
    "        study_kwargs : dict, optional (default=None)\n",
    "            Keyword arguments to be passed to the optuna.Study constructor.\n",
    "        optimize_kwargs : dict, optional (default=None)\n",
    "            Keyword arguments to be passed to the optuna.Study.optimize method.\n",
    "        fitted : bool (default=False)\n",
    "            Whether to compute the fitted values when retraining the best model.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        AutoMLForecast\n",
    "            object with best models and optimization results\n",
    "        \"\"\"\n",
    "        validate_freq(df[time_col], self.freq)\n",
    "        if self.init_config is not None:\n",
    "            init_config = self.init_config\n",
    "        else:\n",
    "            min_size = ufp.counts_by_id(df, id_col)['counts'].min()\n",
    "            min_train_size = min_size - n_windows * h\n",
    "            init_config = self._seasonality_based_config(\n",
    "                h=h,\n",
    "                min_samples=min_train_size,\n",
    "                min_value=df[target_col].min(),\n",
    "            )\n",
    "\n",
    "        if loss is None:\n",
    "            def loss(df, train_df):  # noqa: ARG001\n",
    "                return smape(df, models=['model'])['model'].mean()\n",
    "        if study_kwargs is None:\n",
    "            study_kwargs = {}\n",
    "        if 'sampler' not in study_kwargs:\n",
    "            # for reproducibility\n",
    "            study_kwargs['sampler'] = optuna.samplers.TPESampler(seed=0)\n",
    "        if optimize_kwargs is None:\n",
    "            optimize_kwargs = {}\n",
    "\n",
    "        self.results_ = {}\n",
    "        self.models_ = {}\n",
    "        for name, auto_model in self.models.items():\n",
    "            def config_fn(trial: optuna.Trial) -> Dict[str, Any]:\n",
    "                return {\n",
    "                    'model_params': auto_model.config(trial),\n",
    "                    'mlf_init_params': {\n",
    "                        **init_config(trial),\n",
    "                        'num_threads': self.num_threads,\n",
    "                    },\n",
    "                    'mlf_fit_params': self.fit_config(trial)\n",
    "                }\n",
    "\n",
    "            objective = mlforecast_objective(\n",
    "                df=df,\n",
    "                config_fn=config_fn,\n",
    "                loss=loss,\n",
    "                model=auto_model.model,\n",
    "                freq=self.freq,\n",
    "                n_windows=n_windows,\n",
    "                h=h,\n",
    "                refit=refit,\n",
    "                id_col=id_col,\n",
    "                time_col=time_col,\n",
    "                target_col=target_col,\n",
    "            )\n",
    "            study = optuna.create_study(direction='minimize', **study_kwargs)\n",
    "            study.optimize(objective, n_trials=num_samples, **optimize_kwargs)\n",
    "            self.results_[name] = study\n",
    "            best_config = study.best_trial.user_attrs['config']\n",
    "            best_config['mlf_fit_params'].pop('fitted', None)\n",
    "            best_model = clone(auto_model.model)\n",
    "            best_model.set_params(**best_config['model_params'])\n",
    "            self.models_[name] = MLForecast(\n",
    "                models={name: best_model},\n",
    "                freq=self.freq,\n",
    "                **best_config['mlf_init_params']\n",
    "            )\n",
    "            self.models_[name].fit(\n",
    "                df,\n",
    "                fitted=fitted,\n",
    "                **best_config['mlf_fit_params'],\n",
    "            )\n",
    "        return self\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        h: int,\n",
    "        X_df: Optional[DataFrame] = None,\n",
    "    ) -> DataFrame:\n",
    "        \"\"\"\"Compute forecasts\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        h : int\n",
    "            Number of periods to predict.\n",
    "        X_df : pandas or polars DataFrame, optional (default=None)\n",
    "            Dataframe with the future exogenous features. Should have the id column and the time column.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pandas or polars DataFrame\n",
    "            Predictions for each serie and timestep, with one column per model.\n",
    "        \"\"\"\n",
    "        all_preds = None\n",
    "        for name, model in self.models_.items():\n",
    "            preds = model.predict(h=h, X_df=X_df)\n",
    "            if all_preds is None:\n",
    "                all_preds = preds\n",
    "            else:\n",
    "                all_preds = ufp.assign_columns(all_preds, name, preds[name])\n",
    "        return all_preds\n",
    "\n",
    "    def save(self, path: Union[str, Path]) -> None:\n",
    "        \"\"\"Save AutoMLForecast objects\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        path : str or pathlib.Path\n",
    "            Directory where artifacts will be stored.\"\"\"\n",
    "        for name, model in self.models_.items():\n",
    "            model.save(f'{path}/{name}')\n",
    "\n",
    "    def forecast_fitted_values(\n",
    "        self,\n",
    "        level: Optional[List[Union[int, float]]] = None,\n",
    "    ) -> DataFrame:\n",
    "        \"\"\"Access in-sample predictions.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        level : list of ints or floats, optional (default=None)\n",
    "            Confidence levels between 0 and 100 for prediction intervals.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pandas or polars DataFrame\n",
    "            Dataframe with predictions for the training set\n",
    "        \"\"\"\n",
    "        fitted_vals = None\n",
    "        for name, model in self.models_.items():\n",
    "            model_fitted = model.forecast_fitted_values(level=level)\n",
    "            if fitted_vals is None:\n",
    "                fitted_vals = model_fitted\n",
    "            else:\n",
    "                fitted_vals = ufp.join(\n",
    "                    fitted_vals,\n",
    "                    ufp.drop_columns(model_fitted, model.ts.target_col),\n",
    "                    on=[model.ts.id_col, model.ts.time_col],\n",
    "                    how='inner',\n",
    "                )\n",
    "        return fitted_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e311237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/mlforecast/blob/main/mlforecast/auto.py#L240){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AutoMLForecast\n",
       "\n",
       ">      AutoMLForecast\n",
       ">                      (models:Union[List[__main__.AutoModel],Dict[str,__main__.\n",
       ">                      AutoModel]], freq:Union[int,str], season_length:int, init\n",
       ">                      _config:Optional[Callable[[optuna.trial._trial.Trial],Dic\n",
       ">                      t[str,Any]]]=None, fit_config:Optional[Callable[[optuna.t\n",
       ">                      rial._trial.Trial],Dict[str,Any]]]=None,\n",
       ">                      num_threads:int=1)\n",
       "\n",
       "Hyperparameter optimization helper\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| models | Union |  | Auto models to be optimized. |\n",
       "| freq | Union |  | pandas' or polars' offset alias or integer denoting the frequency of the series. |\n",
       "| season_length | int |  | Length of the seasonal period. This is used for producing the feature space. |\n",
       "| init_config | Optional | None | Function that takes an optuna trial and produces a configuration passed to the MLForecast constructor. |\n",
       "| fit_config | Optional | None | Function that takes an optuna trial and produces a configuration passed to the MLForecast fit method. |\n",
       "| num_threads | int | 1 | Number of threads to use when computing the features. |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/mlforecast/blob/main/mlforecast/auto.py#L240){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AutoMLForecast\n",
       "\n",
       ">      AutoMLForecast\n",
       ">                      (models:Union[List[__main__.AutoModel],Dict[str,__main__.\n",
       ">                      AutoModel]], freq:Union[int,str], season_length:int, init\n",
       ">                      _config:Optional[Callable[[optuna.trial._trial.Trial],Dic\n",
       ">                      t[str,Any]]]=None, fit_config:Optional[Callable[[optuna.t\n",
       ">                      rial._trial.Trial],Dict[str,Any]]]=None,\n",
       ">                      num_threads:int=1)\n",
       "\n",
       "Hyperparameter optimization helper\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| models | Union |  | Auto models to be optimized. |\n",
       "| freq | Union |  | pandas' or polars' offset alias or integer denoting the frequency of the series. |\n",
       "| season_length | int |  | Length of the seasonal period. This is used for producing the feature space. |\n",
       "| init_config | Optional | None | Function that takes an optuna trial and produces a configuration passed to the MLForecast constructor. |\n",
       "| fit_config | Optional | None | Function that takes an optuna trial and produces a configuration passed to the MLForecast fit method. |\n",
       "| num_threads | int | 1 | Number of threads to use when computing the features. |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AutoMLForecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ff37fb-7697-4cc7-90ed-6091a9d48817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/mlforecast/blob/main/mlforecast/auto.py#L432){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AutoMLForecast.fit\n",
       "\n",
       ">      AutoMLForecast.fit\n",
       ">                          (df:Union[pandas.core.frame.DataFrame,polars.datafram\n",
       ">                          e.frame.DataFrame], n_windows:int, h:int,\n",
       ">                          num_samples:int, refit:Union[bool,int]=False, loss:Op\n",
       ">                          tional[Callable[[Union[pandas.core.frame.DataFrame,po\n",
       ">                          lars.dataframe.frame.DataFrame],Union[pandas.core.fra\n",
       ">                          me.DataFrame,polars.dataframe.frame.DataFrame]],float\n",
       ">                          ]]=None, id_col:str='unique_id', time_col:str='ds',\n",
       ">                          target_col:str='y',\n",
       ">                          study_kwargs:Optional[Dict[str,Any]]=None,\n",
       ">                          optimize_kwargs:Optional[Dict[str,Any]]=None,\n",
       ">                          fitted:bool=False)\n",
       "\n",
       "Carry out the optimization process.\n",
       "Each model is optimized independently and the best one is trained on all data\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | Union |  | Series data in long format. |\n",
       "| n_windows | int |  | Number of windows to evaluate. |\n",
       "| h | int |  | Forecast horizon. |\n",
       "| num_samples | int |  | Number of trials to run |\n",
       "| refit | Union | False | Retrain model for each cross validation window.<br>If False, the models are trained at the beginning and then used to predict each window.<br>If positive int, the models are retrained every `refit` windows. |\n",
       "| loss | Optional | None | Function that takes the validation and train dataframes and produces a float.<br>If `None` will use the average SMAPE across series. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| time_col | str | ds | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str | y | Column that contains the target.         |\n",
       "| study_kwargs | Optional | None | Keyword arguments to be passed to the optuna.Study constructor. |\n",
       "| optimize_kwargs | Optional | None | Keyword arguments to be passed to the optuna.Study.optimize method. |\n",
       "| fitted | bool | False | Whether to compute the fitted values when retraining the best model. |\n",
       "| **Returns** | **AutoMLForecast** |  | **object with best models and optimization results** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/mlforecast/blob/main/mlforecast/auto.py#L432){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AutoMLForecast.fit\n",
       "\n",
       ">      AutoMLForecast.fit\n",
       ">                          (df:Union[pandas.core.frame.DataFrame,polars.datafram\n",
       ">                          e.frame.DataFrame], n_windows:int, h:int,\n",
       ">                          num_samples:int, refit:Union[bool,int]=False, loss:Op\n",
       ">                          tional[Callable[[Union[pandas.core.frame.DataFrame,po\n",
       ">                          lars.dataframe.frame.DataFrame],Union[pandas.core.fra\n",
       ">                          me.DataFrame,polars.dataframe.frame.DataFrame]],float\n",
       ">                          ]]=None, id_col:str='unique_id', time_col:str='ds',\n",
       ">                          target_col:str='y',\n",
       ">                          study_kwargs:Optional[Dict[str,Any]]=None,\n",
       ">                          optimize_kwargs:Optional[Dict[str,Any]]=None,\n",
       ">                          fitted:bool=False)\n",
       "\n",
       "Carry out the optimization process.\n",
       "Each model is optimized independently and the best one is trained on all data\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | Union |  | Series data in long format. |\n",
       "| n_windows | int |  | Number of windows to evaluate. |\n",
       "| h | int |  | Forecast horizon. |\n",
       "| num_samples | int |  | Number of trials to run |\n",
       "| refit | Union | False | Retrain model for each cross validation window.<br>If False, the models are trained at the beginning and then used to predict each window.<br>If positive int, the models are retrained every `refit` windows. |\n",
       "| loss | Optional | None | Function that takes the validation and train dataframes and produces a float.<br>If `None` will use the average SMAPE across series. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| time_col | str | ds | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str | y | Column that contains the target.         |\n",
       "| study_kwargs | Optional | None | Keyword arguments to be passed to the optuna.Study constructor. |\n",
       "| optimize_kwargs | Optional | None | Keyword arguments to be passed to the optuna.Study.optimize method. |\n",
       "| fitted | bool | False | Whether to compute the fitted values when retraining the best model. |\n",
       "| **Returns** | **AutoMLForecast** |  | **object with best models and optimization results** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AutoMLForecast.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74f8ce3-021d-43bc-bef0-1ae057e6bde7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/mlforecast/blob/main/mlforecast/auto.py#L556){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AutoMLForecast.predict\n",
       "\n",
       ">      AutoMLForecast.predict (h:int, X_df:Union[pandas.core.frame.DataFrame,pol\n",
       ">                              ars.dataframe.frame.DataFrame,NoneType]=None)\n",
       "\n",
       "\"Compute forecasts\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| h | int |  | Number of periods to predict. |\n",
       "| X_df | Union | None | Dataframe with the future exogenous features. Should have the id column and the time column. |\n",
       "| **Returns** | **Union** |  | **Predictions for each serie and timestep, with one column per model.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/mlforecast/blob/main/mlforecast/auto.py#L556){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AutoMLForecast.predict\n",
       "\n",
       ">      AutoMLForecast.predict (h:int, X_df:Union[pandas.core.frame.DataFrame,pol\n",
       ">                              ars.dataframe.frame.DataFrame,NoneType]=None)\n",
       "\n",
       "\"Compute forecasts\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| h | int |  | Number of periods to predict. |\n",
       "| X_df | Union | None | Dataframe with the future exogenous features. Should have the id column and the time column. |\n",
       "| **Returns** | **Union** |  | **Predictions for each serie and timestep, with one column per model.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AutoMLForecast.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2283bcc-38f6-4631-853c-e3c6e520e02b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/mlforecast/blob/main/mlforecast/auto.py#L584){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AutoMLForecast.save\n",
       "\n",
       ">      AutoMLForecast.save (path:Union[str,pathlib.Path])\n",
       "\n",
       "Save AutoMLForecast objects\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| path | Union | Directory where artifacts will be stored. |\n",
       "| **Returns** | **None** |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/mlforecast/blob/main/mlforecast/auto.py#L584){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AutoMLForecast.save\n",
       "\n",
       ">      AutoMLForecast.save (path:Union[str,pathlib.Path])\n",
       "\n",
       "Save AutoMLForecast objects\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| path | Union | Directory where artifacts will be stored. |\n",
       "| **Returns** | **None** |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AutoMLForecast.save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd9ac56-7e1f-43aa-86e4-bc4765430254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/mlforecast/blob/main/mlforecast/auto.py#L594){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AutoMLForecast.forecast_fitted_values\n",
       "\n",
       ">      AutoMLForecast.forecast_fitted_values\n",
       ">                                             (level:Optional[List[Union[int,flo\n",
       ">                                             at]]]=None)\n",
       "\n",
       "Access in-sample predictions.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| level | Optional | None | Confidence levels between 0 and 100 for prediction intervals. |\n",
       "| **Returns** | **Union** |  | **Dataframe with predictions for the training set** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/mlforecast/blob/main/mlforecast/auto.py#L594){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AutoMLForecast.forecast_fitted_values\n",
       "\n",
       ">      AutoMLForecast.forecast_fitted_values\n",
       ">                                             (level:Optional[List[Union[int,flo\n",
       ">                                             at]]]=None)\n",
       "\n",
       "Access in-sample predictions.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| level | Optional | None | Confidence levels between 0 and 100 for prediction intervals. |\n",
       "| **Returns** | **Union** |  | **Dataframe with predictions for the training set** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AutoMLForecast.forecast_fitted_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fa7b02-4b05-4027-87d3-1e0a9e3d5225",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasetsforecast.m4 import M4, M4Evaluation, M4Info\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59de63a9-11e7-4fe0-ae8d-a3e91a7a5ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_split(group):\n",
    "    df, *_ = M4.load(directory='data', group=group)\n",
    "    df['ds'] = df['ds'].astype('int')\n",
    "    horizon = M4Info[group].horizon\n",
    "    valid = df.groupby('unique_id').tail(horizon).copy()\n",
    "    train = df.drop(valid.index).reset_index(drop=True)\n",
    "    return train, valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7888cb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_pipeline = make_pipeline(\n",
    "    ColumnTransformer(\n",
    "        [('encoder', OneHotEncoder(), ['unique_id'])],\n",
    "        remainder='passthrough',\n",
    "    ),\n",
    "    Ridge()\n",
    ")\n",
    "auto_ridge = AutoModel(ridge_pipeline, lambda trial: {f'ridge__{k}': v for k, v in ridge_space(trial).items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b2aa47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>lgb</th>\n",
       "      <th>ridge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>W1</td>\n",
       "      <td>2180</td>\n",
       "      <td>35529.435224</td>\n",
       "      <td>36110.921202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>W1</td>\n",
       "      <td>2181</td>\n",
       "      <td>35521.764894</td>\n",
       "      <td>36195.175757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>W1</td>\n",
       "      <td>2182</td>\n",
       "      <td>35537.417268</td>\n",
       "      <td>36107.528852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>W1</td>\n",
       "      <td>2183</td>\n",
       "      <td>35538.058206</td>\n",
       "      <td>36027.139248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>W1</td>\n",
       "      <td>2184</td>\n",
       "      <td>35614.611211</td>\n",
       "      <td>36092.858489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4662</th>\n",
       "      <td>W99</td>\n",
       "      <td>2292</td>\n",
       "      <td>15071.536978</td>\n",
       "      <td>15319.146221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4663</th>\n",
       "      <td>W99</td>\n",
       "      <td>2293</td>\n",
       "      <td>15058.145278</td>\n",
       "      <td>15299.549555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4664</th>\n",
       "      <td>W99</td>\n",
       "      <td>2294</td>\n",
       "      <td>15042.493434</td>\n",
       "      <td>15271.744712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4665</th>\n",
       "      <td>W99</td>\n",
       "      <td>2295</td>\n",
       "      <td>15042.144846</td>\n",
       "      <td>15250.070504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4666</th>\n",
       "      <td>W99</td>\n",
       "      <td>2296</td>\n",
       "      <td>15038.729044</td>\n",
       "      <td>15232.127800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4667 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     unique_id    ds           lgb         ridge\n",
       "0           W1  2180  35529.435224  36110.921202\n",
       "1           W1  2181  35521.764894  36195.175757\n",
       "2           W1  2182  35537.417268  36107.528852\n",
       "3           W1  2183  35538.058206  36027.139248\n",
       "4           W1  2184  35614.611211  36092.858489\n",
       "...        ...   ...           ...           ...\n",
       "4662       W99  2292  15071.536978  15319.146221\n",
       "4663       W99  2293  15058.145278  15299.549555\n",
       "4664       W99  2294  15042.493434  15271.744712\n",
       "4665       W99  2295  15042.144846  15250.070504\n",
       "4666       W99  2296  15038.729044  15232.127800\n",
       "\n",
       "[4667 rows x 4 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
    "group = 'Weekly'\n",
    "train, valid = train_valid_split(group)\n",
    "train['unique_id'] = train['unique_id'].astype('category')\n",
    "valid['unique_id'] = valid['unique_id'].astype(train['unique_id'].dtype)\n",
    "info = M4Info[group]\n",
    "h = info.horizon\n",
    "season_length = info.seasonality\n",
    "auto_mlf = AutoMLForecast(\n",
    "    freq=1,\n",
    "    season_length=season_length,\n",
    "    models={\n",
    "        'lgb': AutoLightGBM(),\n",
    "        'ridge': auto_ridge,\n",
    "    },\n",
    "    fit_config=lambda trial: {'static_features': ['unique_id']},\n",
    "    num_threads=2,\n",
    ")\n",
    "auto_mlf.fit(\n",
    "    df=train,\n",
    "    n_windows=2,\n",
    "    h=h,\n",
    "    num_samples=2,\n",
    "    optimize_kwargs={'timeout': 60},\n",
    "    fitted=True,\n",
    ")\n",
    "auto_mlf.predict(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df947f3e-af97-4bd4-83e1-b864f04f9c7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>lgb</th>\n",
       "      <th>lgb-lo-95</th>\n",
       "      <th>lgb-hi-95</th>\n",
       "      <th>ridge</th>\n",
       "      <th>ridge-lo-95</th>\n",
       "      <th>ridge-hi-95</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>W1</td>\n",
       "      <td>15</td>\n",
       "      <td>1071.06</td>\n",
       "      <td>1060.584344</td>\n",
       "      <td>599.618355</td>\n",
       "      <td>1521.550334</td>\n",
       "      <td>1076.990151</td>\n",
       "      <td>556.535492</td>\n",
       "      <td>1597.444810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>W1</td>\n",
       "      <td>16</td>\n",
       "      <td>1073.73</td>\n",
       "      <td>1072.669242</td>\n",
       "      <td>611.703252</td>\n",
       "      <td>1533.635232</td>\n",
       "      <td>1083.633276</td>\n",
       "      <td>563.178617</td>\n",
       "      <td>1604.087936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>W1</td>\n",
       "      <td>17</td>\n",
       "      <td>1066.97</td>\n",
       "      <td>1072.452128</td>\n",
       "      <td>611.486139</td>\n",
       "      <td>1533.418118</td>\n",
       "      <td>1084.724311</td>\n",
       "      <td>564.269652</td>\n",
       "      <td>1605.178970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>W1</td>\n",
       "      <td>18</td>\n",
       "      <td>1066.17</td>\n",
       "      <td>1065.837828</td>\n",
       "      <td>604.871838</td>\n",
       "      <td>1526.803818</td>\n",
       "      <td>1080.127197</td>\n",
       "      <td>559.672538</td>\n",
       "      <td>1600.581856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>W1</td>\n",
       "      <td>19</td>\n",
       "      <td>1064.43</td>\n",
       "      <td>1065.214681</td>\n",
       "      <td>604.248691</td>\n",
       "      <td>1526.180671</td>\n",
       "      <td>1080.636826</td>\n",
       "      <td>560.182167</td>\n",
       "      <td>1601.091485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361881</th>\n",
       "      <td>W99</td>\n",
       "      <td>2279</td>\n",
       "      <td>15738.54</td>\n",
       "      <td>15887.661228</td>\n",
       "      <td>15721.237195</td>\n",
       "      <td>16054.085261</td>\n",
       "      <td>15927.918181</td>\n",
       "      <td>15723.222760</td>\n",
       "      <td>16132.613603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361882</th>\n",
       "      <td>W99</td>\n",
       "      <td>2280</td>\n",
       "      <td>15388.13</td>\n",
       "      <td>15755.943789</td>\n",
       "      <td>15589.519756</td>\n",
       "      <td>15922.367823</td>\n",
       "      <td>15841.599064</td>\n",
       "      <td>15636.903642</td>\n",
       "      <td>16046.294485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361883</th>\n",
       "      <td>W99</td>\n",
       "      <td>2281</td>\n",
       "      <td>15187.62</td>\n",
       "      <td>15432.224701</td>\n",
       "      <td>15265.800668</td>\n",
       "      <td>15598.648735</td>\n",
       "      <td>15584.462232</td>\n",
       "      <td>15379.766811</td>\n",
       "      <td>15789.157654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361884</th>\n",
       "      <td>W99</td>\n",
       "      <td>2282</td>\n",
       "      <td>15172.27</td>\n",
       "      <td>15177.040831</td>\n",
       "      <td>15010.616797</td>\n",
       "      <td>15343.464864</td>\n",
       "      <td>15396.243223</td>\n",
       "      <td>15191.547801</td>\n",
       "      <td>15600.938644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361885</th>\n",
       "      <td>W99</td>\n",
       "      <td>2283</td>\n",
       "      <td>15101.03</td>\n",
       "      <td>15162.090803</td>\n",
       "      <td>14995.666770</td>\n",
       "      <td>15328.514836</td>\n",
       "      <td>15335.982465</td>\n",
       "      <td>15131.287044</td>\n",
       "      <td>15540.677887</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>361886 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       unique_id    ds         y           lgb     lgb-lo-95     lgb-hi-95  \\\n",
       "0             W1    15   1071.06   1060.584344    599.618355   1521.550334   \n",
       "1             W1    16   1073.73   1072.669242    611.703252   1533.635232   \n",
       "2             W1    17   1066.97   1072.452128    611.486139   1533.418118   \n",
       "3             W1    18   1066.17   1065.837828    604.871838   1526.803818   \n",
       "4             W1    19   1064.43   1065.214681    604.248691   1526.180671   \n",
       "...          ...   ...       ...           ...           ...           ...   \n",
       "361881       W99  2279  15738.54  15887.661228  15721.237195  16054.085261   \n",
       "361882       W99  2280  15388.13  15755.943789  15589.519756  15922.367823   \n",
       "361883       W99  2281  15187.62  15432.224701  15265.800668  15598.648735   \n",
       "361884       W99  2282  15172.27  15177.040831  15010.616797  15343.464864   \n",
       "361885       W99  2283  15101.03  15162.090803  14995.666770  15328.514836   \n",
       "\n",
       "               ridge   ridge-lo-95   ridge-hi-95  \n",
       "0        1076.990151    556.535492   1597.444810  \n",
       "1        1083.633276    563.178617   1604.087936  \n",
       "2        1084.724311    564.269652   1605.178970  \n",
       "3        1080.127197    559.672538   1600.581856  \n",
       "4        1080.636826    560.182167   1601.091485  \n",
       "...              ...           ...           ...  \n",
       "361881  15927.918181  15723.222760  16132.613603  \n",
       "361882  15841.599064  15636.903642  16046.294485  \n",
       "361883  15584.462232  15379.766811  15789.157654  \n",
       "361884  15396.243223  15191.547801  15600.938644  \n",
       "361885  15335.982465  15131.287044  15540.677887  \n",
       "\n",
       "[361886 rows x 9 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_mlf.forecast_fitted_values(level=[95])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
