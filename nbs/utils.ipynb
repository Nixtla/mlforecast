{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import random\n",
    "from itertools import chain\n",
    "from math import ceil, log10\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numba import njit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def generate_daily_series(\n",
    "    n_series: int, \n",
    "    min_length: int = 50,\n",
    "    max_length: int = 500,\n",
    "    n_static_features: int = 0,\n",
    "    equal_ends: bool = False,\n",
    "    seed: int = 0,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Generates `n_series` of different lengths in the interval [`min_length`, `max_length`].\n",
    "    \n",
    "    If `n_static_features > 0`, then each serie gets static features with random values.\n",
    "    If `equal_ends == True` then all series end at the same date.\"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    random.seed(seed)\n",
    "    series_lengths = rng.randint(min_length, max_length + 1, n_series)\n",
    "    total_length = series_lengths.sum()\n",
    "    n_digits = ceil(log10(n_series))\n",
    "    \n",
    "    dates = pd.date_range('2000-01-01', periods=max_length, freq='D').values\n",
    "    uids = [\n",
    "        [f'id_{i:0{n_digits}}'] * serie_length\n",
    "        for i, serie_length in enumerate(series_lengths)\n",
    "    ]\n",
    "    if equal_ends:\n",
    "        ds = [dates[-serie_length:] for serie_length in series_lengths]\n",
    "    else:\n",
    "        ds = [dates[:serie_length] for serie_length in series_lengths]\n",
    "    y = np.arange(total_length) % 7 + rng.rand(total_length) * 0.5\n",
    "    series = pd.DataFrame(\n",
    "        {\n",
    "            'unique_id': list(chain.from_iterable(uids)),\n",
    "            'ds': list(chain.from_iterable(ds)),\n",
    "            'y': y,\n",
    "        }\n",
    "    )\n",
    "    for i in range(n_static_features):\n",
    "        static_values = [\n",
    "            [random.randint(0, 100)] * serie_length for serie_length in series_lengths\n",
    "        ]\n",
    "        series[f'static_{i}'] = list(chain.from_iterable(static_values))\n",
    "        series[f'static_{i}'] = series[f'static_{i}'].astype('category')\n",
    "        if i == 0:\n",
    "            series['y'] = series['y'] * (1 + series[f'static_{i}'].cat.codes)\n",
    "    series['unique_id'] = series['unique_id'].astype('category')\n",
    "    series['unique_id'] = series['unique_id'].cat.as_ordered()\n",
    "    series = series.set_index('unique_id')\n",
    "    return series\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate 20 series with lengths between 100 and 1,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_series = 20\n",
    "min_length = 100\n",
    "max_length = 1000\n",
    "\n",
    "series = generate_daily_series(n_series, min_length, max_length)\n",
    "series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_sizes = series.groupby('unique_id').size()\n",
    "assert series_sizes.size == n_series\n",
    "assert series_sizes.min() >= min_length\n",
    "assert series_sizes.max() <= max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also add static features to each serie (these can be things like product_id or store_id). Only the first static feature (`static_0`) is relevant to the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_static_features = 2\n",
    "\n",
    "series_with_statics = generate_daily_series(n_series, min_length, max_length, n_static_features)\n",
    "series_with_statics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_static_features):\n",
    "    assert all(series_with_statics.groupby('unique_id')[f'static_{i}'].nunique() == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `equal_ends=False` (the default) then every serie has a different end date. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert series_with_statics.groupby('unique_id')['ds'].max().nunique() > 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have all of them end at the same date by specifying `equal_ends=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_equal_ends = generate_daily_series(n_series, min_length, max_length, equal_ends=True)\n",
    "\n",
    "assert series_equal_ends.groupby('unique_id')['ds'].max().nunique() == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "@njit\n",
    "def _get_last_n_mask(x: np.ndarray, n: int) -> np.ndarray:\n",
    "    n_samples = x.size\n",
    "    mask = np.full(n_samples, True)\n",
    "    n_first = max(0, n_samples - n)\n",
    "    mask[:n_first] = False\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "np.testing.assert_equal(_get_last_n_mask(np.array([1, 2, 3]), 2), np.array([False, True, True]))\n",
    "np.testing.assert_equal(_get_last_n_mask(np.array([1, 2, 3]), 4), np.array([True, True, True]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def data_indptr_from_sorted_df(df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    grouped = df.groupby('unique_id')\n",
    "    sizes = grouped.size().values\n",
    "    indptr = np.append(0, sizes.cumsum())\n",
    "    data = df['y'].values\n",
    "    return data, indptr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "@njit\n",
    "def _get_mask(data: np.ndarray, indptr: np.ndarray, n: int) -> np.ndarray:\n",
    "    mask = np.empty_like(data)\n",
    "    for start, end in zip(indptr[:-1], indptr[1:]):\n",
    "        mask[start:end] = _get_last_n_mask(data[start:end], n)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def _get_dataframe_mask(df, n) -> pd.Series:\n",
    "    data, indptr = data_indptr_from_sorted_df(df)\n",
    "    mask = _get_mask(data, indptr, n)\n",
    "    return mask.astype(bool)\n",
    "\n",
    "\n",
    "def _split_frame(data, n_windows, window, valid_size):\n",
    "    full_valid_size = (n_windows - window) * valid_size\n",
    "    extra_valid_size = full_valid_size - valid_size\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        full_valid_mask = _get_dataframe_mask(data, full_valid_size)\n",
    "        train_mask = ~full_valid_mask\n",
    "        extra_valid_mask = _get_dataframe_mask(data, extra_valid_size)\n",
    "    else:\n",
    "        full_valid_mask = data.map_partitions(\n",
    "            _get_dataframe_mask, full_valid_size, meta=bool\n",
    "        )\n",
    "        train_mask = ~full_valid_mask\n",
    "        extra_valid_mask = data.map_partitions(\n",
    "            _get_dataframe_mask, extra_valid_size, meta=bool\n",
    "        )\n",
    "    valid_mask = full_valid_mask & ~extra_valid_mask\n",
    "    return data[train_mask], data[valid_mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def backtest_splits(data, n_windows, window_size):\n",
    "    \"\"\"Returns a generator of `n_windows` for train, valid splits of \n",
    "    `data` where each valid has `window_size` samples.\"\"\"\n",
    "    for window in range(n_windows):\n",
    "        train, valid = _split_frame(data, n_windows, window, window_size)\n",
    "        yield train, valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_windows = 3\n",
    "window_size = 14\n",
    "max_dates = series.groupby('unique_id')['ds'].max()\n",
    "day_offset = pd.tseries.frequencies.Day()\n",
    "series_ddf = dd.from_pandas(series, npartitions=2)\n",
    "\n",
    "for df in (series, series_ddf):\n",
    "    splits = backtest_splits(df, n_windows, window_size)\n",
    "    for window, (train, valid) in enumerate(splits):\n",
    "        expected_max_train_dates = max_dates - day_offset * (n_windows - window) * window_size\n",
    "        max_train_dates = train.groupby('unique_id')['ds'].max()\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            max_train_dates = max_train_dates.compute()\n",
    "        assert max_train_dates.equals(expected_max_train_dates)\n",
    "\n",
    "        expected_min_valid_dates = expected_max_train_dates + day_offset\n",
    "        min_valid_dates = valid.groupby('unique_id')['ds'].min()\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            min_valid_dates = min_valid_dates.compute()\n",
    "        assert min_valid_dates.equals(expected_min_valid_dates)\n",
    "\n",
    "        expected_max_valid_dates = expected_max_train_dates + day_offset * window_size\n",
    "        max_valid_dates = valid.groupby('unique_id')['ds'].max()\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            max_valid_dates = max_valid_dates.compute()\n",
    "        assert max_valid_dates.equals(expected_max_valid_dates)\n",
    "\n",
    "        if window == n_windows - 1:\n",
    "            assert max_valid_dates.equals(max_dates)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
