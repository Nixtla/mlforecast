{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import inspect\n",
    "import random\n",
    "import reprlib\n",
    "import warnings\n",
    "from functools import wraps\n",
    "from itertools import chain\n",
    "from math import ceil, log10\n",
    "from typing import Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from utilsforecast.data import generate_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.test import test_eq, test_fail, test_warns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def generate_daily_series(\n",
    "    n_series: int, \n",
    "    min_length: int = 50,\n",
    "    max_length: int = 500,\n",
    "    n_static_features: int = 0,\n",
    "    equal_ends: bool = False,\n",
    "    static_as_categorical: bool = True,\n",
    "    with_trend: bool = False,\n",
    "    seed: int = 0,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Generates `n_series` of different lengths in the interval [`min_length`, `max_length`].\n",
    "    \n",
    "    If `n_static_features > 0`, then each serie gets static features with random values.\n",
    "    If `equal_ends == True` then all series end at the same date.\"\"\"\n",
    "    series = generate_series(\n",
    "        n_series=n_series,\n",
    "        freq='D',\n",
    "        min_length=min_length,\n",
    "        max_length=max_length,\n",
    "        n_static_features=n_static_features,\n",
    "        equal_ends=equal_ends,\n",
    "        static_as_categorical=static_as_categorical,        \n",
    "        with_trend=with_trend,\n",
    "        seed=seed,\n",
    "    )\n",
    "    n_digits = ceil(log10(n_series))\n",
    "    \n",
    "    def int_id_to_str(uid):\n",
    "        return f'id_{uid:0{n_digits}}'\n",
    "\n",
    "    series['unique_id'] = series['unique_id'].map(int_id_to_str).astype('category')\n",
    "    return series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate 20 series with lengths between 100 and 1,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>0.395863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-01-02</td>\n",
       "      <td>1.264447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>2.284022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-01-04</td>\n",
       "      <td>3.462798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-01-05</td>\n",
       "      <td>4.035518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12446</th>\n",
       "      <td>id_19</td>\n",
       "      <td>2002-03-11</td>\n",
       "      <td>0.309275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12447</th>\n",
       "      <td>id_19</td>\n",
       "      <td>2002-03-12</td>\n",
       "      <td>1.189464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12448</th>\n",
       "      <td>id_19</td>\n",
       "      <td>2002-03-13</td>\n",
       "      <td>2.325032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12449</th>\n",
       "      <td>id_19</td>\n",
       "      <td>2002-03-14</td>\n",
       "      <td>3.333198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12450</th>\n",
       "      <td>id_19</td>\n",
       "      <td>2002-03-15</td>\n",
       "      <td>4.306117</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12451 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      unique_id         ds         y\n",
       "0         id_00 2000-01-01  0.395863\n",
       "1         id_00 2000-01-02  1.264447\n",
       "2         id_00 2000-01-03  2.284022\n",
       "3         id_00 2000-01-04  3.462798\n",
       "4         id_00 2000-01-05  4.035518\n",
       "...         ...        ...       ...\n",
       "12446     id_19 2002-03-11  0.309275\n",
       "12447     id_19 2002-03-12  1.189464\n",
       "12448     id_19 2002-03-13  2.325032\n",
       "12449     id_19 2002-03-14  3.333198\n",
       "12450     id_19 2002-03-15  4.306117\n",
       "\n",
       "[12451 rows x 3 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_series = 20\n",
    "min_length = 100\n",
    "max_length = 1000\n",
    "\n",
    "series = generate_daily_series(n_series, min_length, max_length)\n",
    "series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also add static features to each serie (these can be things like product_id or store_id). Only the first static feature (`static_0`) is relevant to the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>static_0</th>\n",
       "      <th>static_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>7.521388</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-01-02</td>\n",
       "      <td>24.024502</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>43.396423</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-01-04</td>\n",
       "      <td>65.793168</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-01-05</td>\n",
       "      <td>76.674843</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12446</th>\n",
       "      <td>id_19</td>\n",
       "      <td>2002-03-11</td>\n",
       "      <td>27.834771</td>\n",
       "      <td>89</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12447</th>\n",
       "      <td>id_19</td>\n",
       "      <td>2002-03-12</td>\n",
       "      <td>107.051746</td>\n",
       "      <td>89</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12448</th>\n",
       "      <td>id_19</td>\n",
       "      <td>2002-03-13</td>\n",
       "      <td>209.252845</td>\n",
       "      <td>89</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12449</th>\n",
       "      <td>id_19</td>\n",
       "      <td>2002-03-14</td>\n",
       "      <td>299.987801</td>\n",
       "      <td>89</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12450</th>\n",
       "      <td>id_19</td>\n",
       "      <td>2002-03-15</td>\n",
       "      <td>387.550536</td>\n",
       "      <td>89</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12451 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      unique_id         ds           y static_0 static_1\n",
       "0         id_00 2000-01-01    7.521388       18       10\n",
       "1         id_00 2000-01-02   24.024502       18       10\n",
       "2         id_00 2000-01-03   43.396423       18       10\n",
       "3         id_00 2000-01-04   65.793168       18       10\n",
       "4         id_00 2000-01-05   76.674843       18       10\n",
       "...         ...        ...         ...      ...      ...\n",
       "12446     id_19 2002-03-11   27.834771       89       42\n",
       "12447     id_19 2002-03-12  107.051746       89       42\n",
       "12448     id_19 2002-03-13  209.252845       89       42\n",
       "12449     id_19 2002-03-14  299.987801       89       42\n",
       "12450     id_19 2002-03-15  387.550536       89       42\n",
       "\n",
       "[12451 rows x 5 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_static_features = 2\n",
    "\n",
    "series_with_statics = generate_daily_series(n_series, min_length, max_length, n_static_features)\n",
    "series_with_statics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_static_features):\n",
    "    assert all(series_with_statics.groupby('unique_id')[f'static_{i}'].nunique() == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `equal_ends=False` (the default) then every serie has a different end date. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert series_with_statics.groupby('unique_id')['ds'].max().nunique() > 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have all of them end at the same date by specifying `equal_ends=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_equal_ends = generate_daily_series(n_series, min_length, max_length, equal_ends=True)\n",
    "\n",
    "assert series_equal_ends.groupby('unique_id')['ds'].max().nunique() == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def generate_prices_for_series(series: pd.DataFrame, horizon: int = 7, seed: int = 0) -> pd.DataFrame:\n",
    "    rng = np.random.RandomState(seed)\n",
    "    unique_last_dates = series.groupby('unique_id')['ds'].max().nunique()\n",
    "    if unique_last_dates > 1:\n",
    "        raise ValueError('series must have equal ends.')\n",
    "    day_offset = pd.tseries.frequencies.Day()\n",
    "    starts_ends = series.groupby('unique_id')['ds'].agg([min, max])\n",
    "    dfs = []\n",
    "    for idx, (start, end) in starts_ends.iterrows():\n",
    "        product_df = pd.DataFrame(\n",
    "            {\n",
    "                'unique_id': idx,\n",
    "                'price': rng.rand((end - start).days + 1 + horizon),\n",
    "            },\n",
    "            index=pd.date_range(start, end + horizon * day_offset, name='ds'),\n",
    "        )\n",
    "        dfs.append(product_df)\n",
    "    prices_catalog = pd.concat(dfs).reset_index()\n",
    "    return prices_catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>unique_id</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-10-05</td>\n",
       "      <td>id_00</td>\n",
       "      <td>0.548814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-10-06</td>\n",
       "      <td>id_00</td>\n",
       "      <td>0.715189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-10-07</td>\n",
       "      <td>id_00</td>\n",
       "      <td>0.602763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-10-08</td>\n",
       "      <td>id_00</td>\n",
       "      <td>0.544883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-10-09</td>\n",
       "      <td>id_00</td>\n",
       "      <td>0.423655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5009</th>\n",
       "      <td>2001-05-17</td>\n",
       "      <td>id_19</td>\n",
       "      <td>0.288027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5010</th>\n",
       "      <td>2001-05-18</td>\n",
       "      <td>id_19</td>\n",
       "      <td>0.846305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5011</th>\n",
       "      <td>2001-05-19</td>\n",
       "      <td>id_19</td>\n",
       "      <td>0.791284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5012</th>\n",
       "      <td>2001-05-20</td>\n",
       "      <td>id_19</td>\n",
       "      <td>0.578636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5013</th>\n",
       "      <td>2001-05-21</td>\n",
       "      <td>id_19</td>\n",
       "      <td>0.288589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5014 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ds unique_id     price\n",
       "0    2000-10-05     id_00  0.548814\n",
       "1    2000-10-06     id_00  0.715189\n",
       "2    2000-10-07     id_00  0.602763\n",
       "3    2000-10-08     id_00  0.544883\n",
       "4    2000-10-09     id_00  0.423655\n",
       "...         ...       ...       ...\n",
       "5009 2001-05-17     id_19  0.288027\n",
       "5010 2001-05-18     id_19  0.846305\n",
       "5011 2001-05-19     id_19  0.791284\n",
       "5012 2001-05-20     id_19  0.578636\n",
       "5013 2001-05-21     id_19  0.288589\n",
       "\n",
       "[5014 rows x 3 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series_for_prices = generate_daily_series(20, n_static_features=2, equal_ends=True)\n",
    "series_for_prices.rename(columns={'static_1': 'product_id'}, inplace=True)\n",
    "prices_catalog = generate_prices_for_series(series_for_prices, horizon=7)\n",
    "prices_catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(set(prices_catalog['unique_id']), set(series_for_prices['unique_id']))\n",
    "test_fail(lambda: generate_prices_for_series(series), contains='equal ends')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def single_split(\n",
    "    df: pd.DataFrame,\n",
    "    i_window: int,    \n",
    "    n_windows: int,\n",
    "    h: int,\n",
    "    id_col: str,\n",
    "    time_col: str,\n",
    "    freq: Union[pd.offsets.BaseOffset, int],\n",
    "    max_dates: pd.Series,  \n",
    "    step_size: Optional[int] = None,\n",
    "    input_size: Optional[int] = None,\n",
    "):\n",
    "    if step_size is None:\n",
    "        step_size = h\n",
    "    test_size = h + step_size * (n_windows - 1)\n",
    "    offset = test_size - i_window * step_size\n",
    "    train_ends = max_dates - offset * freq\n",
    "    valid_ends = train_ends + h * freq\n",
    "    train_mask = df[time_col].le(train_ends)\n",
    "    if input_size is not None:\n",
    "        train_mask &= df[time_col].gt(train_ends - input_size * freq)\n",
    "    train_sizes = train_mask.groupby(df[id_col], observed=True).sum()\n",
    "    if train_sizes.eq(0).any():\n",
    "        ids = reprlib.repr(train_sizes[train_sizes.eq(0)].index.tolist())\n",
    "        raise ValueError(f'The following series are too short for the window: {ids}')        \n",
    "    valid_mask = df[time_col].gt(train_ends) & df[time_col].le(valid_ends)\n",
    "    cutoffs = (\n",
    "        train_ends\n",
    "        .set_axis(df[id_col])\n",
    "        .groupby(id_col, observed=True)\n",
    "        .head(1)\n",
    "        .rename('cutoff')\n",
    "    )\n",
    "    return cutoffs, train_mask, valid_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def backtest_splits(\n",
    "    df: pd.DataFrame,\n",
    "    n_windows: int,\n",
    "    h: int,\n",
    "    id_col: str,\n",
    "    time_col: str,\n",
    "    freq: Union[pd.offsets.BaseOffset, int],\n",
    "    step_size: Optional[int] = None,\n",
    "    input_size: Optional[int] = None,\n",
    "):\n",
    "    max_dates = df.groupby(id_col, observed=True)[time_col].transform('max')    \n",
    "    for i in range(n_windows):\n",
    "        cutoffs, train_mask, valid_mask = single_split(\n",
    "            df,\n",
    "            i_window=i,\n",
    "            n_windows=n_windows,\n",
    "            h=h,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            freq=freq,\n",
    "            max_dates=max_dates,\n",
    "            step_size=step_size,\n",
    "            input_size=input_size,\n",
    "        )\n",
    "        train, valid = df[train_mask], df[valid_mask]\n",
    "        yield cutoffs, train, valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "short_series = generate_daily_series(100, max_length=50)\n",
    "backtest_results = list(\n",
    "    backtest_splits(\n",
    "        short_series,\n",
    "        n_windows=1,\n",
    "        h=49,\n",
    "        id_col='unique_id',\n",
    "        time_col='ds',\n",
    "        freq=pd.offsets.Day(),\n",
    "    )\n",
    ")[0]\n",
    "test_fail(\n",
    "    lambda: list(\n",
    "        backtest_splits(\n",
    "            short_series,\n",
    "            n_windows=1,\n",
    "            h=50,\n",
    "            id_col='unique_id',\n",
    "            time_col='ds',\n",
    "            freq=pd.offsets.Day(),\n",
    "        )\n",
    "    ),\n",
    "    contains='The following series are too short'\n",
    ")\n",
    "short_series_int = short_series.copy()\n",
    "short_series_int['ds'] = short_series.groupby('unique_id').transform('cumcount')\n",
    "backtest_int_results = list(\n",
    "    backtest_splits(\n",
    "        short_series_int,\n",
    "        n_windows=1,\n",
    "        h=40,\n",
    "        id_col='unique_id',\n",
    "        time_col='ds',\n",
    "        freq=1\n",
    "    )\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "max_dates = series.groupby('unique_id')['ds'].max()\n",
    "day_offset = pd.offsets.Day()\n",
    "\n",
    "def test_backtest_splits(df, n_windows, h, step_size, input_size):\n",
    "    common_kwargs = dict(\n",
    "        n_windows=n_windows,\n",
    "        h=h,\n",
    "        id_col='unique_id',\n",
    "        time_col='ds',\n",
    "        freq=pd.offsets.Day(), \n",
    "        step_size=step_size,\n",
    "        input_size=input_size,        \n",
    "    )\n",
    "    permuted_df = df.sample(frac=1.0)\n",
    "    splits = backtest_splits(df, **common_kwargs)\n",
    "    splits_on_permuted = list(backtest_splits(permuted_df, **common_kwargs))\n",
    "    if step_size is None:\n",
    "        step_size = h\n",
    "    test_size = h + step_size * (n_windows - 1)\n",
    "    for window, (cutoffs, train, valid) in enumerate(splits):\n",
    "        offset = test_size - window * step_size\n",
    "        expected_max_train_dates = max_dates - day_offset * offset\n",
    "        max_train_dates = train.groupby('unique_id')['ds'].max()\n",
    "        pd.testing.assert_series_equal(max_train_dates, expected_max_train_dates)\n",
    "        pd.testing.assert_series_equal(cutoffs, max_train_dates.rename('cutoff'))\n",
    "        \n",
    "        if input_size is not None:\n",
    "            expected_min_train_dates = expected_max_train_dates - day_offset * (input_size - 1)\n",
    "            min_train_dates = train.groupby('unique_id')['ds'].min()\n",
    "            pd.testing.assert_series_equal(min_train_dates, expected_min_train_dates)\n",
    "\n",
    "        expected_min_valid_dates = expected_max_train_dates + day_offset\n",
    "        min_valid_dates = valid.groupby('unique_id')['ds'].min()\n",
    "        pd.testing.assert_series_equal(min_valid_dates, expected_min_valid_dates)\n",
    "\n",
    "        expected_max_valid_dates = expected_max_train_dates + day_offset * h\n",
    "        max_valid_dates = valid.groupby('unique_id')['ds'].max()\n",
    "        pd.testing.assert_series_equal(max_valid_dates, expected_max_valid_dates)\n",
    "\n",
    "        if window == n_windows - 1:\n",
    "            pd.testing.assert_series_equal(max_valid_dates, max_dates)\n",
    "\n",
    "        _, permuted_train, permuted_valid = splits_on_permuted[window]            \n",
    "        pd.testing.assert_frame_equal(train, permuted_train.sort_values(['unique_id', 'ds']))\n",
    "    pd.testing.assert_frame_equal(valid, permuted_valid.sort_values(['unique_id', 'ds']))\n",
    "\n",
    "for step_size in (None, 1, 2):\n",
    "    for input_size in (None, 4):\n",
    "        test_backtest_splits(series, n_windows=3, h=14, step_size=step_size, input_size=input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def old_kw_to_pos(old_names, new_positions):\n",
    "    def decorator(f):\n",
    "        @wraps(f)\n",
    "        def inner(*args, **kwargs):\n",
    "            arg_names = inspect.getfullargspec(f).args\n",
    "            new_args = list(args)\n",
    "            for old_name, pos in zip(old_names, new_positions):\n",
    "                if old_name in kwargs:\n",
    "                    new_name = arg_names[pos]\n",
    "                    warnings.warn(\n",
    "                        f'`{old_name}` has been deprecated, please use `{new_name}` instead.',\n",
    "                        DeprecationWarning\n",
    "                    )\n",
    "                    if len(new_args) > pos:\n",
    "                        new_args = [*new_args[:pos], kwargs[old_name], *new_args[pos + 1:]]\n",
    "                    else:\n",
    "                        new_args = list(new_args)\n",
    "                        for i in range(len(new_args), pos):\n",
    "                            new_args.append(kwargs.pop(arg_names[i]))\n",
    "                        new_args.append(kwargs.pop(old_name))\n",
    "            return f(*new_args, **kwargs)\n",
    "        return inner\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "@old_kw_to_pos(['d', 'e'], [0, 2])\n",
    "def f(a, b, c, *, d=None, e=None):\n",
    "    return a + b + c\n",
    "\n",
    "test_eq(f(1, 2, 3), 6)\n",
    "test_eq(f(a=1, b=2, c=3), 6)\n",
    "f1 = lambda: f(1, b=2, e=3)\n",
    "f2 = lambda: f(d=1, b=2, e=3)\n",
    "with warnings.catch_warnings(record=True) as issued_warnings:\n",
    "    warnings.simplefilter('always', DeprecationWarning)\n",
    "    f1()\n",
    "    f1()\n",
    "assert all('`e` has been deprecated, please use `c` instead' in str(w.message) for w in issued_warnings)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    test_eq(f1(), 6)\n",
    "    test_eq(f2(), 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class PredictionIntervals:\n",
    "    \"\"\"Class for storing prediction intervals metadata information.\"\"\"\n",
    "    @old_kw_to_pos(['window_size'], [2])\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_windows: int = 2,\n",
    "        h: int = 1,\n",
    "        method: str = 'conformal_distribution',\n",
    "        window_size: Optional[int] = None,  # noqa: ARG002\n",
    "    ):\n",
    "        if n_windows < 2:\n",
    "            raise ValueError('You need at least two windows to compute conformal intervals')\n",
    "        allowed_methods = ['conformal_error', 'conformal_distribution']            \n",
    "        if method not in allowed_methods:\n",
    "            raise ValueError(f'method must be one of {allowed_methods}')\n",
    "        self.n_windows = n_windows\n",
    "        self.h = h\n",
    "        self.method = method\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"PredictionIntervals(n_windows={self.n_windows}, h={self.h}, method='{self.method}')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _ensure_shallow_copy(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    from packaging.version import Version\n",
    "    \n",
    "    if Version(pd.__version__) < Version(\"1.4\"):\n",
    "        # https://github.com/pandas-dev/pandas/pull/43406\n",
    "        df = df.copy()\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
