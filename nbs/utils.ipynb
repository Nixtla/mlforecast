{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import random\n",
    "from itertools import chain\n",
    "from math import ceil, log10\n",
    "from typing import Generator, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numba import njit\n",
    "\n",
    "from mlforecast.compat import Frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def generate_daily_series(\n",
    "    n_series: int, \n",
    "    min_length: int = 50,\n",
    "    max_length: int = 500,\n",
    "    n_static_features: int = 0,\n",
    "    equal_ends: bool = False,\n",
    "    seed: int = 0,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Generates `n_series` of different lengths in the interval [`min_length`, `max_length`].\n",
    "    \n",
    "    If `n_static_features > 0`, then each serie gets static features with random values.\n",
    "    If `equal_ends == True` then all series end at the same date.\"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    random.seed(seed)\n",
    "    series_lengths = rng.randint(min_length, max_length + 1, n_series)\n",
    "    total_length = series_lengths.sum()\n",
    "    n_digits = ceil(log10(n_series))\n",
    "    \n",
    "    dates = pd.date_range('2000-01-01', periods=max_length, freq='D').values\n",
    "    uids = [\n",
    "        [f'id_{i:0{n_digits}}'] * serie_length\n",
    "        for i, serie_length in enumerate(series_lengths)\n",
    "    ]\n",
    "    if equal_ends:\n",
    "        ds = [dates[-serie_length:] for serie_length in series_lengths]\n",
    "    else:\n",
    "        ds = [dates[:serie_length] for serie_length in series_lengths]\n",
    "    y = np.arange(total_length) % 7 + rng.rand(total_length) * 0.5\n",
    "    series = pd.DataFrame(\n",
    "        {\n",
    "            'unique_id': list(chain.from_iterable(uids)),\n",
    "            'ds': list(chain.from_iterable(ds)),\n",
    "            'y': y,\n",
    "        }\n",
    "    )\n",
    "    for i in range(n_static_features):\n",
    "        static_values = [\n",
    "            [random.randint(0, 100)] * serie_length for serie_length in series_lengths\n",
    "        ]\n",
    "        series[f'static_{i}'] = list(chain.from_iterable(static_values))\n",
    "        series[f'static_{i}'] = series[f'static_{i}'].astype('category')\n",
    "        if i == 0:\n",
    "            series['y'] = series['y'] * (1 + series[f'static_{i}'].cat.codes)\n",
    "    series['unique_id'] = series['unique_id'].astype('category')\n",
    "    series['unique_id'] = series['unique_id'].cat.as_ordered()\n",
    "    series = series.set_index('unique_id')\n",
    "    return series\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate 20 series with lengths between 100 and 1,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_series = 20\n",
    "min_length = 100\n",
    "max_length = 1000\n",
    "\n",
    "series = generate_daily_series(n_series, min_length, max_length)\n",
    "series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_sizes = series.groupby('unique_id').size()\n",
    "assert series_sizes.size == n_series\n",
    "assert series_sizes.min() >= min_length\n",
    "assert series_sizes.max() <= max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also add static features to each serie (these can be things like product_id or store_id). Only the first static feature (`static_0`) is relevant to the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_static_features = 2\n",
    "\n",
    "series_with_statics = generate_daily_series(n_series, min_length, max_length, n_static_features)\n",
    "series_with_statics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_static_features):\n",
    "    assert all(series_with_statics.groupby('unique_id')[f'static_{i}'].nunique() == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `equal_ends=False` (the default) then every serie has a different end date. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert series_with_statics.groupby('unique_id')['ds'].max().nunique() > 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have all of them end at the same date by specifying `equal_ends=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_equal_ends = generate_daily_series(n_series, min_length, max_length, equal_ends=True)\n",
    "\n",
    "assert series_equal_ends.groupby('unique_id')['ds'].max().nunique() == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "@njit\n",
    "def _get_last_n_mask(x: np.ndarray, n: int) -> np.ndarray:\n",
    "    n_samples = x.size\n",
    "    mask = np.full(n_samples, True)\n",
    "    n_first = max(0, n_samples - n)\n",
    "    mask[:n_first] = False\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "np.testing.assert_equal(_get_last_n_mask(np.array([1, 2, 3]), 2), np.array([False, True, True]))\n",
    "np.testing.assert_equal(_get_last_n_mask(np.array([1, 2, 3]), 4), np.array([True, True, True]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def data_indptr_from_sorted_df(df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    grouped = df.groupby('unique_id')\n",
    "    sizes = grouped.size().values\n",
    "    indptr = np.append(0, sizes.cumsum())\n",
    "    data = df['y'].values\n",
    "    return data, indptr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "@njit\n",
    "def _get_mask(data: np.ndarray, indptr: np.ndarray, n: int) -> np.ndarray:\n",
    "    mask = np.empty_like(data)\n",
    "    for start, end in zip(indptr[:-1], indptr[1:]):\n",
    "        mask[start:end] = _get_last_n_mask(data[start:end], n)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def _get_dataframe_mask(df, n) -> pd.Series:\n",
    "    data, indptr = data_indptr_from_sorted_df(df)\n",
    "    mask = _get_mask(data, indptr, n)\n",
    "    return mask.astype(bool)\n",
    "\n",
    "\n",
    "def _split_frame(data, n_windows, window, valid_size):\n",
    "    full_valid_size = (n_windows - window) * valid_size\n",
    "    extra_valid_size = full_valid_size - valid_size\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        full_valid_mask = _get_dataframe_mask(data, full_valid_size)\n",
    "        train_mask = ~full_valid_mask\n",
    "        extra_valid_mask = _get_dataframe_mask(data, extra_valid_size)\n",
    "    else:\n",
    "        full_valid_mask = data.map_partitions(\n",
    "            _get_dataframe_mask, full_valid_size, meta=bool\n",
    "        )\n",
    "        train_mask = ~full_valid_mask\n",
    "        extra_valid_mask = data.map_partitions(\n",
    "            _get_dataframe_mask, extra_valid_size, meta=bool\n",
    "        )\n",
    "    valid_mask = full_valid_mask & ~extra_valid_mask\n",
    "    return data[train_mask], data[valid_mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def ensure_sorted(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.set_index('ds', append=True)\n",
    "    if not df.index.is_monotonic_increasing:\n",
    "        df = df.sort_index()\n",
    "    return df.reset_index('ds')\n",
    "\n",
    "\n",
    "def backtest_splits(\n",
    "    data: Frame, n_windows: int, window_size: int\n",
    ") -> Generator[Frame, None, None]:\n",
    "    \"\"\"Returns a generator of `n_windows` for train, valid splits of \n",
    "    `data` where each valid has `window_size` samples.\"\"\"\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        data = ensure_sorted(data)\n",
    "    else:\n",
    "        data = data.map_partitions(ensure_sorted)\n",
    "    for window in range(n_windows):\n",
    "        train, valid = _split_frame(data, n_windows, window, window_size)\n",
    "        yield train, valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_windows = 3\n",
    "window_size = 14\n",
    "max_dates = series.groupby('unique_id')['ds'].max()\n",
    "day_offset = pd.tseries.frequencies.Day()\n",
    "series_ddf = dd.from_pandas(series, npartitions=2)\n",
    "\n",
    "for df in (series, series_ddf):\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        permuted_df = df.sample(frac=1.)\n",
    "    else:\n",
    "        permuted_df = df.map_partitions(lambda part: part.sample(frac=1.))    \n",
    "    splits = backtest_splits(df, n_windows, window_size)\n",
    "    splits_on_permuted = list(backtest_splits(permuted_df, n_windows, window_size))\n",
    "    for window, (train, valid) in enumerate(splits):\n",
    "        expected_max_train_dates = max_dates - day_offset * (n_windows - window) * window_size\n",
    "        max_train_dates = train.groupby('unique_id')['ds'].max()\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            max_train_dates = max_train_dates.compute()\n",
    "        assert max_train_dates.equals(expected_max_train_dates)\n",
    "\n",
    "        expected_min_valid_dates = expected_max_train_dates + day_offset\n",
    "        min_valid_dates = valid.groupby('unique_id')['ds'].min()\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            min_valid_dates = min_valid_dates.compute()\n",
    "        assert min_valid_dates.equals(expected_min_valid_dates)\n",
    "\n",
    "        expected_max_valid_dates = expected_max_train_dates + day_offset * window_size\n",
    "        max_valid_dates = valid.groupby('unique_id')['ds'].max()\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            max_valid_dates = max_valid_dates.compute()\n",
    "        assert max_valid_dates.equals(expected_max_valid_dates)\n",
    "\n",
    "        if window == n_windows - 1:\n",
    "            assert max_valid_dates.equals(max_dates)\n",
    "            \n",
    "        permuted_train, permuted_valid = splits_on_permuted[window]            \n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            train = train.compute()\n",
    "            valid = valid.compute()\n",
    "            permuted_train = permuted_train.compute()\n",
    "            permuted_valid = permuted_valid.compute()\n",
    "        pd.testing.assert_frame_equal(train, permuted_train)\n",
    "        pd.testing.assert_frame_equal(valid, permuted_valid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
