{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API\n",
    "\n",
    "> High level functions for easy interaction\n",
    "\n",
    "This module defines the building blocks for the CLI. These functions can be leveraged to define other custom workflows more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import importlib\n",
    "import inspect\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Generator, Tuple, Union\n",
    "\n",
    "try:\n",
    "    import dask.dataframe as dd\n",
    "    from dask.dataframe import DataFrame as dd_Frame\n",
    "    from dask.distributed import Client\n",
    "except ImportError:\n",
    "    class dd: pass  # type: ignore\n",
    "    dd_Frame = type(None)\n",
    "    class Client: pass  # type: ignore\n",
    "try:\n",
    "    from s3path import S3Path\n",
    "except ImportError:\n",
    "    class S3Path: pass  # type: ignore\n",
    "try:\n",
    "    from mlforecast.distributed.forecast import DistributedForecast\n",
    "except ImportError:\n",
    "    class DistributedForecast: pass  # type: ignore\n",
    "\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from fastcore.script import Param, call_parse\n",
    "from pandas.api.types import is_datetime64_dtype\n",
    "\n",
    "from mlforecast.data_model import (Backtest, Cluster, Config, Data, DataFormat,\n",
    "                         DistributedModelConfig, DistributedModelName,\n",
    "                         Features, ModelConfig, _available_tfms)\n",
    "from mlforecast.forecast import Forecast\n",
    "from mlforecast.utils import get_last_n_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "from fastcore.test import test_eq, test_fail\n",
    "from window_ops.rolling import *\n",
    "from window_ops.expanding import *\n",
    "from window_ops.ewm import *\n",
    "\n",
    "from mlforecast.utils import generate_daily_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "Frame = Union[pd.DataFrame, dd_Frame]\n",
    "\n",
    "_available_tfms_kwargs = {name: list(inspect.signature(tfm).parameters)[1:] \n",
    "                          for name, tfm in _available_tfms.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def validate_data_format(data: Frame) -> Frame:\n",
    "    \"\"\"Checks whether data is in the correct format and tries to fix it if possible.\"\"\"\n",
    "    if not isinstance(data, (pd.DataFrame, dd_Frame)):\n",
    "        raise ValueError('data must be either pandas or dask dataframe.')\n",
    "    if not data.index.name == 'unique_id':\n",
    "        if 'unique_id' in data:\n",
    "            data = data.set_index('unique_id')\n",
    "        else:\n",
    "            raise ValueError('unique_id not found in data.')\n",
    "    if 'ds' not in data:\n",
    "        raise ValueError('ds column not found in data.')\n",
    "    if not is_datetime64_dtype(data['ds']):\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            data['ds'] = pd.to_datetime(data['ds'])\n",
    "        else:\n",
    "            data['ds'] = dd.to_datetime(data['ds'])\n",
    "    if 'y' not in data:\n",
    "        raise ValueError('y column not found in data.')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_pandas = np.array([1])\n",
    "test_fail(lambda: validate_data_format(not_pandas), contains='data must be either pandas')\n",
    "\n",
    "no_uid = pd.DataFrame({'x': [1]})\n",
    "test_fail(lambda: validate_data_format(no_uid), contains='unique_id not found')\n",
    "\n",
    "uid_in_col = pd.DataFrame({'unique_id': [1], 'ds': pd.to_datetime(['2020-01-01']), 'y': [1.]})\n",
    "assert validate_data_format(uid_in_col).equals(uid_in_col.set_index('unique_id'))\n",
    "\n",
    "no_ds = pd.DataFrame({'unique_id': [1]})\n",
    "test_fail(lambda: validate_data_format(no_ds), contains='ds column not found')\n",
    "\n",
    "ds_not_datetime = pd.DataFrame({'unique_id': [1], 'ds': ['2020-01-01'], 'y': [1.]})\n",
    "assert is_datetime64_dtype(validate_data_format(ds_not_datetime)['ds'])\n",
    "\n",
    "if hasattr(dd, 'to_datetime'):\n",
    "    ds_not_datetime_dask = dd.from_pandas(ds_not_datetime, npartitions=1)\n",
    "    assert is_datetime64_dtype(validate_data_format(ds_not_datetime_dask).compute()['ds'])\n",
    "    \n",
    "no_y = pd.DataFrame({'unique_id': [1], 'ds': pd.to_datetime(['2020-01-01'])})\n",
    "test_fail(lambda: validate_data_format(no_y), contains='y column not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def _is_s3_path(path: str) -> bool:\n",
    "    return path.startswith('s3://')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "assert _is_s3_path('s3://bucket/file')\n",
    "assert not _is_s3_path('bucket/file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def _path_as_str(path: Union[Path, S3Path]) -> str:\n",
    "    if isinstance(path, S3Path):\n",
    "        return path.as_uri()\n",
    "    return str(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "test_eq(_path_as_str(Path('data')), 'data')\n",
    "test_eq(_path_as_str(S3Path('/bucket/file')), 's3://bucket/file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def read_data(config: Data, is_distributed: bool) -> Frame:\n",
    "    \"\"\"Read data from `config.prefix/config.input`.\n",
    "    \n",
    "    If we're in distributed mode dask is used for IO, else pandas.\"\"\"\n",
    "    prefix = config.prefix\n",
    "    path = S3Path.from_uri(prefix) if _is_s3_path(prefix) else Path(prefix)\n",
    "    input_path = path/config.input\n",
    "    io_module = dd if is_distributed else pd\n",
    "    reader = getattr(io_module, f'read_{config.format}')\n",
    "    read_path = _path_as_str(input_path)\n",
    "    if io_module is dd and config.format is DataFormat.csv:\n",
    "        read_path += '/*'\n",
    "    data = reader(read_path)\n",
    "    if (\n",
    "        io_module is dd \n",
    "        and config.format is DataFormat.parquet\n",
    "        and data.index.name == 'unique_id'\n",
    "        and pd.api.types.is_categorical_dtype(data.index)\n",
    "    ):\n",
    "        data.index = data.index.cat.as_known().as_ordered()\n",
    "    return validate_data_format(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = generate_daily_series(20, 100, 200)\n",
    "\n",
    "for data_format in ('csv', 'parquet'):\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        tmpdir = Path(tmpdir)\n",
    "        writer = getattr(series, f'to_{data_format}')\n",
    "        writer(tmpdir/f'single_file.{data_format}')\n",
    "        data_cfg = Data(prefix=str(tmpdir), input=f'single_file.{data_format}', output='output', format=data_format)\n",
    "        df = read_data(data_cfg, is_distributed=False)\n",
    "        assert df.drop('y', 1).equals(series.drop('y', 1))\n",
    "        np.testing.assert_allclose(df.y, series.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_ddf = dd.from_pandas(series, npartitions=2)\n",
    "\n",
    "for data_format in ('csv', 'parquet'):\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        tmpdir = Path(tmpdir)\n",
    "        writer = getattr(series_ddf, f'to_{data_format}')\n",
    "        writer(tmpdir/'partitions')\n",
    "        data_cfg = Data(prefix=str(tmpdir), input='partitions', output='output', format=data_format)\n",
    "        df = read_data(data_cfg, is_distributed=True)\n",
    "        assert df.drop('y', 1).compute().equals(series.drop('y', 1))\n",
    "        np.testing.assert_allclose(df.y.compute(), series.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def _get_pandas_mask(data: pd.DataFrame, n: int) -> pd.Series:\n",
    "    return data.groupby('unique_id')['y'].transform(get_last_n_mask, n)\n",
    "\n",
    "\n",
    "def _split_frame(data: Frame, n_windows: int, window: int, valid_size: int) -> Tuple[Frame, Frame]:\n",
    "    full_valid_size = (n_windows - window) * valid_size\n",
    "    extra_valid_size = full_valid_size - valid_size\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        full_valid_mask = _get_pandas_mask(data, full_valid_size)\n",
    "        train_mask = ~full_valid_mask\n",
    "        extra_valid_mask = _get_pandas_mask(data, extra_valid_size)\n",
    "    else:\n",
    "        full_valid_mask = data.map_partitions(_get_pandas_mask, full_valid_size, meta=bool)\n",
    "        train_mask = ~full_valid_mask\n",
    "        extra_valid_mask = data.map_partitions(_get_pandas_mask, extra_valid_size, meta=bool)\n",
    "    valid_mask = full_valid_mask & ~extra_valid_mask\n",
    "    return data[train_mask], data[valid_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def _backtest_splits(data: Frame, config: Backtest) -> Generator[Frame, None, None]:\n",
    "    \"\"\"Returns a generator for train, valid splits of `data` using `config`\"\"\"\n",
    "    for window in range(config.n_windows):\n",
    "        train, valid = _split_frame(data, config.n_windows, window, config.window_size)\n",
    "        yield train, valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "backtest_cfg = Backtest(n_windows=3, window_size=14)\n",
    "max_dates = series.groupby('unique_id')['ds'].max()\n",
    "day_offset = pd.tseries.frequencies.Day()\n",
    "\n",
    "for df in (series, series_ddf):\n",
    "    for window, (train, valid) in enumerate(_backtest_splits(df, backtest_cfg)):\n",
    "        expected_max_train_dates = max_dates - day_offset * (backtest_cfg.n_windows - window) * backtest_cfg.window_size\n",
    "        max_train_dates = train.groupby('unique_id')['ds'].max()\n",
    "        if isinstance(df, dd_Frame):\n",
    "            max_train_dates = max_train_dates.compute()\n",
    "        assert max_train_dates.equals(expected_max_train_dates)\n",
    "\n",
    "        expected_min_valid_dates = expected_max_train_dates + day_offset\n",
    "        min_valid_dates = valid.groupby('unique_id')['ds'].min()\n",
    "        if isinstance(df, dd_Frame):\n",
    "            min_valid_dates = min_valid_dates.compute()\n",
    "        assert min_valid_dates.equals(expected_min_valid_dates)\n",
    "\n",
    "        expected_max_valid_dates = expected_max_train_dates + day_offset * backtest_cfg.window_size\n",
    "        max_valid_dates = valid.groupby('unique_id')['ds'].max()\n",
    "        if isinstance(df, dd_Frame):\n",
    "            max_valid_dates = max_valid_dates.compute()\n",
    "        assert max_valid_dates.equals(expected_max_valid_dates)\n",
    "\n",
    "        if window == backtest_cfg.n_windows - 1:\n",
    "            assert max_valid_dates.equals(max_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def _instantiate_transforms(config: Features) -> Dict:\n",
    "    \"\"\"Turn the function names into the actual functions and make sure their positional arguments are in order.\"\"\"\n",
    "    if config.lag_transforms is None:\n",
    "        return {}\n",
    "    lag_tfms = defaultdict(list)\n",
    "    for lag, tfms in config.lag_transforms.items():\n",
    "        for tfm in tfms:\n",
    "            if isinstance(tfm, dict):\n",
    "                [(tfm_name, tfm_kwargs)] = tfm.items()\n",
    "            else:\n",
    "                tfm_name, tfm_kwargs = tfm, ()\n",
    "            if tfm_name not in _available_tfms:\n",
    "                raise NotImplementedError(tfm_name)\n",
    "            tfm_func = _available_tfms[tfm_name]\n",
    "            tfm_args: Tuple[Any, ...] = ()\n",
    "            for kwarg in _available_tfms_kwargs[tfm_name]:\n",
    "                if kwarg in tfm_kwargs:\n",
    "                    tfm_args += (tfm_kwargs[kwarg], )\n",
    "            lag_tfms[lag].append((tfm_func, *tfm_args))\n",
    "    return lag_tfms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "features_cfg = Features(freq='D',\n",
    "                        lags=[1, 2],\n",
    "                        lag_transforms={\n",
    "                            1: ['expanding_mean', {'rolling_mean': {'window_size': 7}}],\n",
    "                            2: [{'rolling_mean': {'min_samples': 2, 'window_size': 3}}]\n",
    "                        })\n",
    "\n",
    "test_eq(_instantiate_transforms(features_cfg),\n",
    "        {\n",
    "            1: [(expanding_mean,), (rolling_mean, 7)],\n",
    "            2: [(rolling_mean, 3, 2)]\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def _fcst_from_local(model_config: ModelConfig,\n",
    "                     flow_config: Dict) -> Forecast:\n",
    "    module_name, model_cls = model_config.name.rsplit('.', maxsplit=1)\n",
    "    module = importlib.import_module(module_name)\n",
    "    model = getattr(module, model_cls)(**(model_config.params or {}))\n",
    "    return Forecast(model, flow_config)\n",
    "\n",
    "\n",
    "def _fcst_from_distributed(model_config: DistributedModelConfig,\n",
    "                           flow_config: Dict) -> DistributedForecast:\n",
    "    if model_config.name is DistributedModelName.LightGBM:\n",
    "        from mlforecast.distributed.models.lgb import LGBMForecast\n",
    "        model_cls = LGBMForecast\n",
    "    else:\n",
    "        from mlforecast.distributed.models.xgb import XGBForecast\n",
    "        model_cls = XGBForecast\n",
    "    model = model_cls(**(model_config.params or {}))  \n",
    "    return DistributedForecast(model, flow_config)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def fcst_from_config(config: Config) -> Union[Forecast, DistributedForecast]:\n",
    "    \"\"\"Instantiate Forecast class from config.\"\"\"\n",
    "    flow_config = config.features.dict()\n",
    "    flow_config['lag_transforms'] = _instantiate_transforms(config.features)\n",
    "    \n",
    "    if config.local is not None:\n",
    "        return _fcst_from_local(config.local.model, flow_config)\n",
    "    # because of the config validation, either local or distributed will be not None\n",
    "    return _fcst_from_distributed(config.distributed.model, flow_config)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../sample_configs/local.yaml', 'rt') as f:\n",
    "    cfg = Config(**yaml.safe_load(f))\n",
    "\n",
    "fcst = fcst_from_config(cfg)\n",
    "test_eq(fcst.model.__class__.__name__, cfg.local.model.name.split('.')[-1])\n",
    "model_params = fcst.model.get_params()\n",
    "for param_name, param_value in cfg.local.model.params.items():\n",
    "    test_eq(model_params[param_name], param_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(n_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../sample_configs/distributed.yaml', 'rt') as f:\n",
    "    cfg = Config(**yaml.safe_load(f))\n",
    "\n",
    "fcst = fcst_from_config(cfg)\n",
    "test_eq(fcst.model.__class__.__name__, cfg.distributed.model.name)\n",
    "model_params = fcst.model.get_params()\n",
    "for param_name, param_value in cfg.distributed.model.params.items():\n",
    "    test_eq(model_params[param_name], param_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def perform_backtest(fcst: Union[Forecast, DistributedForecast],\n",
    "                     data: Frame,\n",
    "                     config: Config,\n",
    "                     output_path: Union[Path, S3Path]):\n",
    "    \"\"\"Performs backtesting of `fcst` using `data` and the strategy defined in `config`. \n",
    "    Writes the results to `output_path`.\"\"\"\n",
    "    if config.backtest is None:\n",
    "        return\n",
    "    splits = _backtest_splits(data, config.backtest)\n",
    "    data_is_dask = isinstance(data, dd_Frame)\n",
    "    for split, (train, valid) in enumerate(splits):\n",
    "        fcst.fit(train)\n",
    "        preds = fcst.predict(config.backtest.window_size)      \n",
    "        todo = valid.merge(preds, on=['unique_id', 'ds'], how='left')\n",
    "        todo['y_pred'] = todo['y_pred'].fillna(0)\n",
    "        split_path = _path_as_str(output_path/f'valid_{split}')\n",
    "        if not data_is_dask:\n",
    "            split_path += f'.{config.data.format}'\n",
    "        writer = getattr(todo, f'to_{config.data.format}')\n",
    "        writer(split_path)\n",
    "        todo['sq_err'] = (todo['y'] - todo['y_pred'])**2\n",
    "        mse = todo.groupby(\"unique_id\")[\"sq_err\"].mean().mean()\n",
    "        if data_is_dask:\n",
    "            mse = mse.compute()\n",
    "        print(f'Split {split+1} MSE: {mse:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../sample_configs/local.yaml', 'rt') as f:\n",
    "    cfg = Config(**yaml.safe_load(f))\n",
    "\n",
    "fcst = fcst_from_config(cfg)\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    perform_backtest(fcst, series, cfg, Path(tmpdir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../sample_configs/distributed.yaml', 'rt') as f:\n",
    "    cfg = Config(**yaml.safe_load(f))\n",
    "\n",
    "fcst = fcst_from_config(cfg)\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    perform_backtest(fcst, series_ddf, cfg, Path(tmpdir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def parse_config(config_file: str):\n",
    "    \"\"\"Create a `Config` object using the contents of `config_file`\"\"\"\n",
    "    with open(config_file, 'r') as f:\n",
    "        config = Config(**yaml.safe_load(f))\n",
    "    return config\n",
    "\n",
    "def setup_client(config: Cluster) -> Client:\n",
    "    \"\"\"Spins up a cluster and returns a client connected to it.\"\"\"\n",
    "    module_name, cluster_cls = config.class_name.rsplit('.', maxsplit=1)\n",
    "    module = importlib.import_module(module_name)\n",
    "    cluster = getattr(module, cluster_cls)(**config.class_kwargs)\n",
    "    client = Client(cluster)\n",
    "    n_workers = config.class_kwargs.get('n_workers', 0)\n",
    "    client.wait_for_workers(n_workers)\n",
    "    return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = setup_client(cfg.distributed.cluster)\n",
    "client.cluster.close()\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@call_parse\n",
    "def run_forecast(config_file: Param('Configuration file', str)):  # type: ignore # NOQA\n",
    "    \"\"\"Run the forecasting pipeline using the configuration defined in `config_file`.\"\"\"\n",
    "    config = parse_config(config_file)\n",
    "    is_distributed = config.distributed is not None\n",
    "    if is_distributed:\n",
    "        client = setup_client(config.distributed.cluster)\n",
    "    try:\n",
    "        data = read_data(config.data, is_distributed)\n",
    "        prefix = config.data.prefix\n",
    "        path = S3Path.from_uri(prefix) if _is_s3_path(prefix) else Path(prefix)\n",
    "        output_path = path/config.data.output\n",
    "        output_path.mkdir(exist_ok=True)\n",
    "\n",
    "        fcst = fcst_from_config(config)\n",
    "        if config.backtest is not None:\n",
    "            perform_backtest(fcst, data, config, output_path)\n",
    "        if config.forecast is not None:\n",
    "            fcst.fit(data)\n",
    "            preds = fcst.predict(config.forecast.horizon)\n",
    "            writer = getattr(preds, f'to_{config.data.format}')\n",
    "            write_path = _path_as_str(output_path/'forecast')\n",
    "            if isinstance(data, pd.DataFrame):\n",
    "                write_path += f'.{config.data.format}'\n",
    "            writer(write_path)\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "    finally:\n",
    "        if is_distributed:\n",
    "            client.cluster.close()\n",
    "            client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_format in ('csv', 'parquet'):\n",
    "    for df in (series, series_ddf):\n",
    "        data_is_dask = isinstance(df, dd_Frame)\n",
    "        config_name = 'distributed.yaml' if data_is_dask else 'local.yaml'\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            tmpdir = Path(tmpdir)\n",
    "            train_path = 'train'\n",
    "            config_path = tmpdir/config_name\n",
    "            if not data_is_dask:\n",
    "                train_path += f'.{data_format}'\n",
    "            writer = getattr(df, f'to_{data_format}')\n",
    "            writer(tmpdir/train_path)\n",
    "\n",
    "            with open(f'../sample_configs/{config_name}', 'rt') as f:\n",
    "                cfg = yaml.safe_load(f)\n",
    "            cfg['data']['prefix'] = str(tmpdir)\n",
    "            cfg['data']['input'] = train_path\n",
    "            cfg['data']['format'] = data_format\n",
    "            with open(config_path, 'wt') as f:\n",
    "                yaml.dump(cfg, f)\n",
    "            run_forecast(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "series.to_parquet('data/train')\n",
    "!mlforecast ../sample_configs/local.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls data/outputs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
