{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API\n",
    "\n",
    "> High level functions for easy interaction\n",
    "\n",
    "This module defines the building blocks for the CLI. These functions can be leveraged to define other custom workflows more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import importlib\n",
    "import inspect\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Tuple, Union\n",
    "\n",
    "try:\n",
    "    import dask.dataframe as dd\n",
    "    from dask.dataframe import DataFrame as dd_Frame\n",
    "    from dask.distributed import Client, LocalCluster\n",
    "except ImportError:\n",
    "    class dd: pass  # type: ignore\n",
    "    dd_Frame = type(None)\n",
    "    class Client: pass  # type: ignore\n",
    "    class LocalCluster: pass  # type: ignore\n",
    "try:\n",
    "    from s3path import S3Path\n",
    "except ImportError:\n",
    "    class S3Path: pass  # type: ignore\n",
    "try:\n",
    "    from mlforecast.distributed.forecast import DistributedForecast\n",
    "except ImportError:\n",
    "    class DistributedForecast: pass  # type: ignore\n",
    "\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from fastcore.script import Param, call_parse\n",
    "from pandas.api.types import is_datetime64_dtype\n",
    "\n",
    "from mlforecast.core import predictions_flow\n",
    "from mlforecast.data_model import (ClusterConfig, DataConfig, DataFormat, \n",
    "                         DistributedModelConfig, DistributedModelName,\n",
    "                         FeaturesConfig, FlowConfig, ModelConfig,\n",
    "                         _available_tfms)\n",
    "from mlforecast.forecast import Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "from fastcore.test import test_eq, test_fail\n",
    "from window_ops.rolling import *\n",
    "from window_ops.expanding import *\n",
    "from window_ops.ewm import *\n",
    "\n",
    "from mlforecast.utils import generate_daily_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "Frame = Union[pd.DataFrame, dd_Frame]\n",
    "\n",
    "_available_tfms_kwargs = {name: list(inspect.signature(tfm).parameters)[1:] \n",
    "                          for name, tfm in _available_tfms.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def validate_data_format(data: Frame) -> Frame:\n",
    "    \"\"\"Checks whether data is in the correct format and tries to fix it if possible.\"\"\"\n",
    "    if not isinstance(data, (pd.DataFrame, dd_Frame)):\n",
    "        raise ValueError('data must be either pandas or dask dataframe.')\n",
    "    if not data.index.name == 'unique_id':\n",
    "        if 'unique_id' in data:\n",
    "            data = data.set_index('unique_id')\n",
    "        else:\n",
    "            raise ValueError('unique_id not found in data.')\n",
    "    if 'ds' not in data:\n",
    "        raise ValueError('ds column not found in data.')\n",
    "    if not is_datetime64_dtype(data['ds']):\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            data['ds'] = pd.to_datetime(data['ds'])\n",
    "        else:\n",
    "            data['ds'] = dd.to_datetime(data['ds'])\n",
    "    if 'y' not in data:\n",
    "        raise ValueError('y column not found in data.')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_pandas = np.array([1])\n",
    "test_fail(lambda: validate_data_format(not_pandas), contains='data must be either pandas')\n",
    "\n",
    "no_uid = pd.DataFrame({'x': [1]})\n",
    "test_fail(lambda: validate_data_format(no_uid), contains='unique_id not found')\n",
    "\n",
    "uid_in_col = pd.DataFrame({'unique_id': [1], 'ds': pd.to_datetime(['2020-01-01']), 'y': [1.]})\n",
    "assert validate_data_format(uid_in_col).equals(uid_in_col.set_index('unique_id'))\n",
    "\n",
    "no_ds = pd.DataFrame({'unique_id': [1]})\n",
    "test_fail(lambda: validate_data_format(no_ds), contains='ds column not found')\n",
    "\n",
    "ds_not_datetime = pd.DataFrame({'unique_id': [1], 'ds': ['2020-01-01'], 'y': [1.]})\n",
    "assert is_datetime64_dtype(validate_data_format(ds_not_datetime)['ds'])\n",
    "\n",
    "if hasattr(dd, 'to_datetime'):\n",
    "    ds_not_datetime_dask = dd.from_pandas(ds_not_datetime, npartitions=1)\n",
    "    assert is_datetime64_dtype(validate_data_format(ds_not_datetime_dask).compute()['ds'])\n",
    "    \n",
    "no_y = pd.DataFrame({'unique_id': [1], 'ds': pd.to_datetime(['2020-01-01'])})\n",
    "test_fail(lambda: validate_data_format(no_y), contains='y column not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def _is_s3_path(path: str) -> bool:\n",
    "    return path.startswith('s3://')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "assert _is_s3_path('s3://bucket/file')\n",
    "assert not _is_s3_path('bucket/file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def _path_as_str(path: Union[Path, S3Path]) -> str:\n",
    "    if isinstance(path, S3Path):\n",
    "        return path.as_uri()\n",
    "    return str(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "test_eq(_path_as_str(Path('data')), 'data')\n",
    "test_eq(_path_as_str(S3Path('/bucket/file')), 's3://bucket/file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def read_data(config: DataConfig, is_distributed: bool) -> Frame:\n",
    "    \"\"\"Read data from `config.prefix/config.input`.\n",
    "    \n",
    "    If we're in distributed mode dask is used for IO, else pandas.\"\"\"\n",
    "    prefix = config.prefix\n",
    "    path = S3Path.from_uri(prefix) if _is_s3_path(prefix) else Path(prefix)\n",
    "    input_path = path/config.input\n",
    "    io_module = dd if is_distributed else pd\n",
    "    reader = getattr(io_module, f'read_{config.format}')\n",
    "    read_path = _path_as_str(input_path)\n",
    "    if io_module is dd and config.format is DataFormat.csv:\n",
    "        read_path += '/*'\n",
    "    data = reader(read_path)\n",
    "    if (\n",
    "        io_module is dd \n",
    "        and config.format is DataFormat.parquet\n",
    "        and data.index.name == 'unique_id'\n",
    "        and pd.api.types.is_categorical_dtype(data.index)\n",
    "    ):\n",
    "        data.index = data.index.cat.as_known().as_ordered()\n",
    "    return validate_data_format(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = generate_daily_series(20, 100, 200)\n",
    "series_ddf = dd.from_pandas(series, npartitions=2)\n",
    "\n",
    "for data_format in ('csv', 'parquet'):\n",
    "    for df in (series, series_ddf):\n",
    "        is_distributed = df is series_ddf\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            tmpdir = Path(tmpdir)\n",
    "            writer = getattr(df, f'to_{data_format}')\n",
    "            writer(tmpdir/'train')\n",
    "            data_cfg = DataConfig(prefix=str(tmpdir), input='train', \n",
    "                                  output='output', format=data_format)\n",
    "            read_df = read_data(data_cfg, is_distributed)\n",
    "            if is_distributed:\n",
    "                read_df, df = read_df.compute(), df.compute()\n",
    "            assert read_df.drop('y', 1).equals(df.drop('y', 1))\n",
    "            np.testing.assert_allclose(read_df.y, df.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def _instantiate_transforms(config: FeaturesConfig) -> Dict:\n",
    "    \"\"\"Turn the function names into the actual functions and make sure their positional arguments are in order.\"\"\"\n",
    "    if config.lag_transforms is None:\n",
    "        return {}\n",
    "    lag_tfms = defaultdict(list)\n",
    "    for lag, tfms in config.lag_transforms.items():\n",
    "        for tfm in tfms:\n",
    "            if isinstance(tfm, dict):\n",
    "                [(tfm_name, tfm_kwargs)] = tfm.items()\n",
    "            else:\n",
    "                tfm_name, tfm_kwargs = tfm, ()\n",
    "            if tfm_name not in _available_tfms:\n",
    "                raise NotImplementedError(tfm_name)\n",
    "            tfm_func = _available_tfms[tfm_name]\n",
    "            tfm_args: Tuple[Any, ...] = ()\n",
    "            for kwarg in _available_tfms_kwargs[tfm_name]:\n",
    "                if kwarg in tfm_kwargs:\n",
    "                    tfm_args += (tfm_kwargs[kwarg], )\n",
    "            lag_tfms[lag].append((tfm_func, *tfm_args))\n",
    "    return lag_tfms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "features_cfg = FeaturesConfig(freq='D',\n",
    "                              lags=[1, 2],\n",
    "                              lag_transforms={\n",
    "                                  1: ['expanding_mean', {'rolling_mean': {'window_size': 7}}],\n",
    "                                  2: [{'rolling_mean': {'min_samples': 2, 'window_size': 3}}]\n",
    "                              })\n",
    "\n",
    "test_eq(_instantiate_transforms(features_cfg),\n",
    "        {\n",
    "            1: [(expanding_mean,), (rolling_mean, 7)],\n",
    "            2: [(rolling_mean, 3, 2)]\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def _fcst_from_local(model_config: ModelConfig,\n",
    "                     flow_config: Dict) -> Forecast:\n",
    "    module_name, model_cls = model_config.name.rsplit('.', maxsplit=1)\n",
    "    module = importlib.import_module(module_name)\n",
    "    model = getattr(module, model_cls)(**(model_config.params or {}))\n",
    "    return Forecast(model, flow_config)\n",
    "\n",
    "\n",
    "def _fcst_from_distributed(model_config: DistributedModelConfig,\n",
    "                           flow_config: Dict) -> DistributedForecast:\n",
    "    if model_config.name is DistributedModelName.LightGBM:\n",
    "        from mlforecast.distributed.models.lgb import LGBMForecast\n",
    "        model_cls = LGBMForecast\n",
    "    else:\n",
    "        from mlforecast.distributed.models.xgb import XGBForecast\n",
    "        model_cls = XGBForecast  # type: ignore\n",
    "    model = model_cls(**(model_config.params or {}))  \n",
    "    return DistributedForecast(model, flow_config)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def fcst_from_config(config: FlowConfig) -> Union[Forecast, DistributedForecast]:\n",
    "    \"\"\"Instantiate Forecast class from config.\"\"\"\n",
    "    flow_config = config.features.dict()\n",
    "    flow_config['lag_transforms'] = _instantiate_transforms(config.features)\n",
    "    \n",
    "    if config.local is not None:\n",
    "        return _fcst_from_local(config.local.model, flow_config)\n",
    "    # because of the config validation, either local or distributed will be not None\n",
    "    return _fcst_from_distributed(config.distributed.model, flow_config)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../sample_configs/local.yaml', 'rt') as f:\n",
    "    cfg = FlowConfig(**yaml.safe_load(f))\n",
    "\n",
    "fcst = fcst_from_config(cfg)\n",
    "test_eq(fcst.model.__class__.__name__, cfg.local.model.name.split('.')[-1])\n",
    "model_params = fcst.model.get_params()\n",
    "for param_name, param_value in cfg.local.model.params.items():\n",
    "    test_eq(model_params[param_name], param_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Client(n_workers=2) as client:\n",
    "    with open('../sample_configs/distributed.yaml', 'rt') as f:\n",
    "        cfg = FlowConfig(**yaml.safe_load(f))\n",
    "\n",
    "    fcst = fcst_from_config(cfg)\n",
    "    test_eq(fcst.model.__class__.__name__, cfg.distributed.model.name)\n",
    "    model_params = fcst.model.get_params()\n",
    "    for param_name, param_value in cfg.distributed.model.params.items():\n",
    "        test_eq(model_params[param_name], param_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def perform_backtest(fcst: Union[Forecast, DistributedForecast],\n",
    "                     data: Frame,\n",
    "                     config: FlowConfig,\n",
    "                     output_path: Union[Path, S3Path]):\n",
    "    \"\"\"Performs backtesting of `fcst` using `data` and the strategy defined in `config`. \n",
    "    Writes the results to `output_path`.\"\"\"\n",
    "    if config.backtest is None:\n",
    "        return\n",
    "    data_is_dask = isinstance(data, dd_Frame)\n",
    "    results = fcst.backtest(data,\n",
    "                            config.backtest.n_windows,\n",
    "                            config.backtest.window_size, \n",
    "                            predictions_flow)\n",
    "    for i, result in enumerate(results):\n",
    "        result = result.fillna(0)\n",
    "        split_path = _path_as_str(output_path/f'valid_{i}')\n",
    "        if not data_is_dask:\n",
    "            split_path += f'.{config.data.format}'\n",
    "        writer = getattr(result, f'to_{config.data.format}')\n",
    "        writer(split_path)\n",
    "        result['sq_err'] = (result['y'] - result['y_pred'])**2\n",
    "        mse = result.groupby(\"unique_id\")[\"sq_err\"].mean().mean()\n",
    "        if data_is_dask:\n",
    "            mse = mse.compute()\n",
    "        print(f'Split {i+1} MSE: {mse:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../sample_configs/local.yaml', 'rt') as f:\n",
    "    cfg = FlowConfig(**yaml.safe_load(f))\n",
    "fcst = fcst_from_config(cfg)\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    perform_backtest(fcst, series, cfg, Path(tmpdir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distributed\n",
    "with Client(n_workers=2) as client:\n",
    "    with open(f'../sample_configs/distributed.yaml', 'rt') as f:\n",
    "        cfg = FlowConfig(**yaml.safe_load(f))\n",
    "    fcst = fcst_from_config(cfg)\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        perform_backtest(fcst, series_ddf, cfg, Path(tmpdir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def parse_config(config_file: str) -> FlowConfig:\n",
    "    \"\"\"Create a `FlowConfig` object using the contents of `config_file`\"\"\"\n",
    "    with open(config_file, 'r') as f:\n",
    "        config = FlowConfig(**yaml.safe_load(f))\n",
    "    return config\n",
    "\n",
    "def setup_client(config: ClusterConfig) -> Client:\n",
    "    \"\"\"Spins up a cluster with the specifications defined in `config` and returns a client connected to it.\"\"\"\n",
    "    module_name, cluster_cls = config.class_name.rsplit('.', maxsplit=1)\n",
    "    module = importlib.import_module(module_name)\n",
    "    cluster = getattr(module, cluster_cls)(**config.class_kwargs)\n",
    "    client = Client(cluster)\n",
    "    n_workers = config.class_kwargs.get('n_workers', 0)\n",
    "    client.wait_for_workers(n_workers)\n",
    "    return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distributed\n",
    "client = setup_client(cfg.distributed.cluster)\n",
    "assert isinstance(client.cluster, LocalCluster)\n",
    "assert len(client.scheduler_info()['workers']) == cfg.distributed.cluster.class_kwargs['n_workers']\n",
    "client.cluster.close()\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@call_parse\n",
    "def run_forecast(config_file: Param('Configuration file', str)):  # type: ignore # NOQA\n",
    "    \"\"\"Run the forecasting pipeline using the configuration defined in `config_file`.\"\"\"\n",
    "    config = parse_config(config_file)\n",
    "    is_distributed = config.distributed is not None\n",
    "    if config.distributed is not None:  # mypy\n",
    "        client = setup_client(config.distributed.cluster)\n",
    "    try:\n",
    "        data = read_data(config.data, is_distributed)\n",
    "        prefix = config.data.prefix\n",
    "        path = S3Path.from_uri(prefix) if _is_s3_path(prefix) else Path(prefix)\n",
    "        output_path = path/config.data.output\n",
    "        output_path.mkdir(exist_ok=True)\n",
    "\n",
    "        fcst = fcst_from_config(config)\n",
    "        if config.backtest is not None:\n",
    "            perform_backtest(fcst, data, config, output_path)\n",
    "        if config.forecast is not None:\n",
    "            fcst.fit(data)\n",
    "            preds = fcst.predict(config.forecast.horizon)\n",
    "            writer = getattr(preds, f'to_{config.data.format}')\n",
    "            write_path = _path_as_str(output_path/'forecast')\n",
    "            if isinstance(data, pd.DataFrame):\n",
    "                write_path += f'.{config.data.format}'\n",
    "            writer(write_path)\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "    finally:\n",
    "        if is_distributed:\n",
    "            client.cluster.close()\n",
    "            client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_format in ('csv', 'parquet'):\n",
    "    config_name = 'local.yaml'\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        tmpdir = Path(tmpdir)\n",
    "        train_path = f'train.{data_format}'\n",
    "        config_path = tmpdir/config_name\n",
    "        writer = getattr(series, f'to_{data_format}')\n",
    "        writer(tmpdir/train_path)\n",
    "\n",
    "        with open(f'../sample_configs/{config_name}', 'rt') as f:\n",
    "            cfg = yaml.safe_load(f)\n",
    "        cfg['data']['prefix'] = str(tmpdir)\n",
    "        cfg['data']['input'] = train_path\n",
    "        cfg['data']['format'] = data_format\n",
    "        with open(config_path, 'wt') as f:\n",
    "            yaml.dump(cfg, f)\n",
    "        run_forecast(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distributed\n",
    "for data_format in ('csv', 'parquet'):\n",
    "    config_name = 'distributed.yaml'\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        tmpdir = Path(tmpdir)\n",
    "        train_path = 'train'\n",
    "        config_path = tmpdir/config_name\n",
    "        writer = getattr(series_ddf, f'to_{data_format}')\n",
    "        writer(tmpdir/train_path)\n",
    "\n",
    "        with open(f'../sample_configs/{config_name}', 'rt') as f:\n",
    "            cfg = yaml.safe_load(f)\n",
    "        cfg['data']['prefix'] = str(tmpdir)\n",
    "        cfg['data']['input'] = train_path\n",
    "        cfg['data']['format'] = data_format\n",
    "        with open(config_path, 'wt') as f:\n",
    "            yaml.dump(cfg, f)\n",
    "        run_forecast(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path('data')\n",
    "data_path.mkdir()\n",
    "series.to_parquet(data_path/'train')\n",
    "!mlforecast ../sample_configs/local.yaml\n",
    "assert 'forecast.parquet' in [file.name for file in (data_path/'outputs').iterdir()]\n",
    "shutil.rmtree(data_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
