{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API\n",
    "\n",
    "> High level functions for easy interaction\n",
    "\n",
    "This module defines the building blocks for the CLI. These functions can be leveraged to define other custom workflows more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import importlib\n",
    "import inspect\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "    \n",
    "import pandas as pd\n",
    "import yaml\n",
    "from pandas.api.types import is_categorical_dtype, is_datetime64_dtype\n",
    "\n",
    "from mlforecast.compat import Client, DistributedForecast, Frame, S3Path, dd, dd_Frame\n",
    "from mlforecast.core import TimeSeries\n",
    "from mlforecast.data_model import (\n",
    "    ClusterConfig,\n",
    "    DataConfig,\n",
    "    DataFormat, \n",
    "    DistributedModelConfig,\n",
    "    DistributedModelName,\n",
    "    FeaturesConfig,\n",
    "    FlowConfig,\n",
    "    ModelConfig,\n",
    "    _available_tfms,\n",
    ")\n",
    "from mlforecast.forecast import Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import tempfile\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "from dask.distributed import LocalCluster\n",
    "from fastcore.test import test_eq, test_fail\n",
    "from window_ops.rolling import *\n",
    "from window_ops.expanding import *\n",
    "from window_ops.ewm import *\n",
    "\n",
    "from mlforecast.compat import Frame\n",
    "from mlforecast.utils import generate_daily_series, generate_prices_for_series\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "_available_tfms_kwargs = {\n",
    "    name: list(inspect.signature(tfm).parameters)[1:] \n",
    "    for name, tfm in _available_tfms.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def validate_data_format(data: Frame) -> Frame:\n",
    "    \"\"\"Checks whether data is in the correct format and tries to fix it if possible.\"\"\"\n",
    "    if not isinstance(data, (pd.DataFrame, dd_Frame)):\n",
    "        raise ValueError('data must be either pandas or dask dataframe.')\n",
    "    if not data.index.name == 'unique_id':\n",
    "        if 'unique_id' in data:\n",
    "            data = data.set_index('unique_id')\n",
    "        else:\n",
    "            raise ValueError('unique_id not found in data.')\n",
    "    if 'ds' not in data:\n",
    "        raise ValueError('ds column not found in data.')\n",
    "    if not is_datetime64_dtype(data['ds']):\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            data['ds'] = pd.to_datetime(data['ds'])\n",
    "        else:\n",
    "            data['ds'] = dd.to_datetime(data['ds'])\n",
    "    if 'y' not in data:\n",
    "        raise ValueError('y column not found in data.')\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_pandas = np.array([1])\n",
    "test_fail(lambda: validate_data_format(not_pandas), contains='data must be either pandas')\n",
    "\n",
    "no_uid = pd.DataFrame({'x': [1]})\n",
    "test_fail(lambda: validate_data_format(no_uid), contains='unique_id not found')\n",
    "\n",
    "uid_in_col = pd.DataFrame({'unique_id': [1], 'ds': pd.to_datetime(['2020-01-01']), 'y': [1.]})\n",
    "assert validate_data_format(uid_in_col).equals(uid_in_col.set_index('unique_id'))\n",
    "\n",
    "no_ds = pd.DataFrame({'unique_id': [1]})\n",
    "test_fail(lambda: validate_data_format(no_ds), contains='ds column not found')\n",
    "\n",
    "ds_not_datetime = pd.DataFrame({'unique_id': [1], 'ds': ['2020-01-01'], 'y': [1.]})\n",
    "assert is_datetime64_dtype(validate_data_format(ds_not_datetime)['ds'])\n",
    "\n",
    "if hasattr(dd, 'to_datetime'):\n",
    "    ds_not_datetime_dask = dd.from_pandas(ds_not_datetime, npartitions=1)\n",
    "    assert is_datetime64_dtype(validate_data_format(ds_not_datetime_dask).compute()['ds'])\n",
    "    \n",
    "no_y = pd.DataFrame({'unique_id': [1], 'ds': pd.to_datetime(['2020-01-01'])})\n",
    "test_fail(lambda: validate_data_format(no_y), contains='y column not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def _is_s3_path(path: str) -> bool:\n",
    "    return path.startswith('s3://')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "assert _is_s3_path('s3://bucket/file')\n",
    "assert not _is_s3_path('bucket/file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def _path_as_str(path: Union[Path, S3Path]) -> str:\n",
    "    if isinstance(path, S3Path):\n",
    "        return path.as_uri()\n",
    "    return str(path)\n",
    "\n",
    "\n",
    "def _prefix_as_path(prefix: str) -> Union[Path, S3Path]:\n",
    "    return S3Path.from_uri(prefix) if _is_s3_path(prefix) else Path(prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "test_eq(_path_as_str(Path('data')), 'data')\n",
    "test_eq(_path_as_str(S3Path('/bucket/file')), 's3://bucket/file')\n",
    "\n",
    "test_eq(_prefix_as_path('s3://bucket/'), S3Path('/bucket/'))\n",
    "test_eq(_prefix_as_path('/home/'), Path('/home/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def read_data(config: DataConfig, is_distributed: bool) -> Frame:\n",
    "    \"\"\"Read data from `config.prefix/config.input`.\n",
    "    \n",
    "    If we're in distributed mode dask is used for IO, else pandas.\"\"\"\n",
    "    path = _prefix_as_path(config.prefix)\n",
    "    input_path = path / config.input\n",
    "    io_module = dd if is_distributed else pd\n",
    "    reader = getattr(io_module, f'read_{config.format}')\n",
    "    read_path = _path_as_str(input_path)\n",
    "    if io_module is dd and config.format is DataFormat.csv:\n",
    "        read_path += '/*'\n",
    "    data = reader(read_path)\n",
    "    if (\n",
    "        io_module is dd \n",
    "        and config.format is DataFormat.parquet\n",
    "        and data.index.name == 'unique_id'\n",
    "        and is_categorical_dtype(data.index)\n",
    "    ):\n",
    "        data.index = data.index.cat.as_known().as_ordered()\n",
    "        for col in data.select_dtypes(include='category'):\n",
    "            data[col] = data[col].cat.as_known()\n",
    "        \n",
    "    return validate_data_format(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = generate_daily_series(20, 100, 200)\n",
    "series_ddf = dd.from_pandas(series, npartitions=2)\n",
    "\n",
    "for data_format in ('csv', 'parquet'):\n",
    "    for df in (series, series_ddf):\n",
    "        is_distributed = df is series_ddf\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            tmpdir = Path(tmpdir)\n",
    "            writer = getattr(df, f'to_{data_format}')\n",
    "            writer(tmpdir/'train')\n",
    "            data_cfg = DataConfig(prefix=str(tmpdir), input='train', \n",
    "                                  output='output', format=data_format)\n",
    "            read_df = read_data(data_cfg, is_distributed)\n",
    "            if is_distributed:\n",
    "                read_df, df = read_df.compute(), df.compute()\n",
    "            assert read_df.drop('y', 1).equals(df.drop('y', 1))\n",
    "            np.testing.assert_allclose(read_df.y, df.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def _read_dynamic(config: DataConfig) -> Optional[List[pd.DataFrame]]:\n",
    "    if config.dynamic is None:\n",
    "        return None\n",
    "    reader = getattr(pd, f'read_{config.format}')\n",
    "    input_path = _prefix_as_path(config.prefix)\n",
    "    dynamic_dfs = []\n",
    "    for fname in config.dynamic:\n",
    "        path = _path_as_str(input_path / fname)\n",
    "        kwargs = {}\n",
    "        if config.format is DataFormat.csv:\n",
    "            kwargs['parse_dates'] = ['ds']\n",
    "        df = reader(path, **kwargs)\n",
    "        dynamic_dfs.append(df)\n",
    "    return dynamic_dfs\n",
    "\n",
    "\n",
    "def _paste_dynamic(\n",
    "    data: Frame, dynamic_dfs: Optional[List[pd.DataFrame]], is_distributed: bool\n",
    ") -> pd.DataFrame:\n",
    "    if dynamic_dfs is None:\n",
    "        return data\n",
    "    data = data.reset_index()\n",
    "    for df in dynamic_dfs:\n",
    "        data = data.merge(df, how='left')\n",
    "    kwargs = {}\n",
    "    if is_distributed:\n",
    "        kwargs['sorted'] = True\n",
    "    data = data.set_index('unique_id', **kwargs)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "for data_format in ('csv', 'parquet'):\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        tmp = Path(tmpdir)\n",
    "        series = generate_daily_series(20, n_static_features=2, equal_ends=True)\n",
    "        series = series.rename(columns={'static_1': 'product_id'})\n",
    "        prices = generate_prices_for_series(series)\n",
    "        series = series.reset_index().merge(prices, how='left')\n",
    "        getattr(series, f'to_{data_format}')(tmp / 'train', index=False)\n",
    "        getattr(prices, f'to_{data_format}')(tmp / 'prices', index=False)\n",
    "        data_cfg = DataConfig(\n",
    "            prefix=tmpdir,\n",
    "            input='train',\n",
    "            output='',\n",
    "            format=data_format,\n",
    "            dynamic=['prices'],\n",
    "        )\n",
    "        dynamic_dfs = _read_dynamic(data_cfg)\n",
    "        assert isinstance(dynamic_dfs, list)\n",
    "        test_eq(len(dynamic_dfs), 1)\n",
    "        pd.testing.assert_frame_equal(dynamic_dfs[0], prices)\n",
    "        \n",
    "data_cfg = DataConfig(prefix='', input='', output='', format='csv')\n",
    "assert _read_dynamic(data_cfg) is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def _instantiate_transforms(config: FeaturesConfig) -> Dict:\n",
    "    \"\"\"Turn the function names into the actual functions and make sure their positional arguments are in order.\"\"\"\n",
    "    if config.lag_transforms is None:\n",
    "        return {}\n",
    "    lag_tfms = defaultdict(list)\n",
    "    for lag, tfms in config.lag_transforms.items():\n",
    "        for tfm in tfms:\n",
    "            if isinstance(tfm, dict):\n",
    "                [(tfm_name, tfm_kwargs)] = tfm.items()\n",
    "            else:\n",
    "                tfm_name, tfm_kwargs = tfm, ()\n",
    "            tfm_func = _available_tfms[tfm_name]\n",
    "            tfm_args: Tuple[Any, ...] = ()\n",
    "            for kwarg in _available_tfms_kwargs[tfm_name]:\n",
    "                if kwarg in tfm_kwargs:\n",
    "                    tfm_args += (tfm_kwargs[kwarg],)\n",
    "            lag_tfms[lag].append((tfm_func, *tfm_args))\n",
    "    return lag_tfms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "features_cfg = FeaturesConfig(freq='D',\n",
    "                              lags=[1, 2],\n",
    "                              lag_transforms={\n",
    "                                  1: ['expanding_mean', {'rolling_mean': {'window_size': 7}}],\n",
    "                                  2: [{'rolling_mean': {'min_samples': 2, 'window_size': 3}}]\n",
    "                              })\n",
    "\n",
    "test_eq(_instantiate_transforms(features_cfg),\n",
    "        {\n",
    "            1: [(expanding_mean,), (rolling_mean, 7)],\n",
    "            2: [(rolling_mean, 3, 2)]\n",
    "        })\n",
    "test_eq(_instantiate_transforms(FeaturesConfig(freq='D')), {})\n",
    "test_fail(\n",
    "    lambda: _instantiate_transforms(\n",
    "        FeaturesConfig(freq='D', lag_transforms={1: [{'exp_mean': {}}]})\n",
    "    ),\n",
    "    contains='unexpected value; permitted:'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def _fcst_from_local(model_config: ModelConfig, flow_config: Dict) -> Forecast:\n",
    "    module_name, model_cls = model_config.name.rsplit('.', maxsplit=1)\n",
    "    module = importlib.import_module(module_name)\n",
    "    model = getattr(module, model_cls)(**(model_config.params or {}))\n",
    "    ts = TimeSeries(**flow_config)\n",
    "    return Forecast(model, ts)\n",
    "\n",
    "\n",
    "def _fcst_from_distributed(\n",
    "    model_config: DistributedModelConfig, flow_config: Dict\n",
    ") -> DistributedForecast:\n",
    "    model_params = model_config.params or {}\n",
    "    if model_config.name is DistributedModelName.LGBMForecast:\n",
    "        from mlforecast.distributed.models.lgb import LGBMForecast\n",
    "        \n",
    "        model = LGBMForecast(**model_params)\n",
    "    else:\n",
    "        from mlforecast.distributed.models.xgb import XGBForecast\n",
    "        \n",
    "        model = XGBForecast(**model_params)\n",
    "    ts = TimeSeries(**flow_config)\n",
    "    return DistributedForecast(model, ts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def fcst_from_config(config: FlowConfig) -> Union[Forecast, DistributedForecast]:\n",
    "    \"\"\"Instantiate Forecast class from config.\"\"\"\n",
    "    flow_config = config.features.dict()\n",
    "    flow_config['lag_transforms'] = _instantiate_transforms(config.features)\n",
    "    remove_keys = {'static_features', 'keep_last_n'}\n",
    "    flow_config = {k: v for k, v in flow_config.items() if k not in remove_keys}\n",
    "    \n",
    "    if config.local is not None:\n",
    "        return _fcst_from_local(config.local.model, flow_config)\n",
    "    # because of the config validation, either local or distributed will be not None\n",
    "    # however mypy can't see this, hence the next assert\n",
    "    assert config.distributed is not None\n",
    "    return _fcst_from_distributed(config.distributed.model, flow_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../sample_configs/local.yaml', 'rt') as f:\n",
    "    cfg = FlowConfig(**yaml.safe_load(f))\n",
    "\n",
    "fcst = fcst_from_config(cfg)\n",
    "test_eq(fcst.model.__class__.__name__, cfg.local.model.name.split('.')[-1])\n",
    "model_params = fcst.model.get_params()\n",
    "for param_name, param_value in cfg.local.model.params.items():\n",
    "    test_eq(model_params[param_name], param_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Client(n_workers=2) as client:\n",
    "    with open('../sample_configs/distributed.yaml', 'rt') as f:\n",
    "        cfg = FlowConfig(**yaml.safe_load(f))\n",
    "    fcst = fcst_from_config(cfg)\n",
    "    test_eq(fcst.model.__class__.__name__, cfg.distributed.model.name)\n",
    "    model_params = fcst.model.get_params()\n",
    "    for param_name, param_value in cfg.distributed.model.params.items():\n",
    "        test_eq(model_params[param_name], param_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "with Client(n_workers=2) as client:\n",
    "    with open('../sample_configs/distributed.yaml', 'rt') as f:\n",
    "        cfg = FlowConfig(**yaml.safe_load(f))\n",
    "    cfg.distributed.model.name = DistributedModelName('LGBMForecast')\n",
    "    fcst = fcst_from_config(cfg)\n",
    "    test_eq(fcst.model.__class__.__name__, cfg.distributed.model.name)\n",
    "    model_params = fcst.model.get_params()\n",
    "    for param_name, param_value in cfg.distributed.model.params.items():\n",
    "        test_eq(model_params[param_name], param_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def perform_backtest(\n",
    "    fcst: Union[Forecast, DistributedForecast],\n",
    "    data: Frame,\n",
    "    config: FlowConfig,\n",
    "    output_path: Union[Path, S3Path],\n",
    "    dynamic_dfs: Optional[List[pd.DataFrame]] = None,    \n",
    ") -> None:\n",
    "    \"\"\"Performs backtesting of `fcst` using `data` and the strategy defined in `config`. \n",
    "    Writes the results to `output_path`.\"\"\"\n",
    "    if config.backtest is None:\n",
    "        return\n",
    "    data_is_dask = isinstance(data, dd_Frame)\n",
    "    results = fcst.backtest(\n",
    "        data,\n",
    "        config.backtest.n_windows,\n",
    "        config.backtest.window_size,\n",
    "        static_features=config.features.static_features,\n",
    "        dynamic_dfs=dynamic_dfs,\n",
    "    )\n",
    "    for i, result in enumerate(results):\n",
    "        result = result.fillna(0)\n",
    "        split_path = _path_as_str(output_path / f'valid_{i}')\n",
    "        if not data_is_dask:\n",
    "            split_path += f'.{config.data.format}'\n",
    "        writer = getattr(result, f'to_{config.data.format}')\n",
    "        writer(split_path)\n",
    "        result['sq_err'] = (result['y'] - result['y_pred']).pow(2)\n",
    "        mse = result.groupby(\"unique_id\")[\"sq_err\"].mean().mean()\n",
    "        if data_is_dask:\n",
    "            mse = mse.compute()\n",
    "        print(f'Split {i+1} MSE: {mse:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../sample_configs/local.yaml', 'rt') as f:\n",
    "    cfg = FlowConfig(**yaml.safe_load(f))\n",
    "fcst = fcst_from_config(cfg)\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    series = generate_daily_series(20, 100, 200)\n",
    "    perform_backtest(fcst, series, cfg, Path(tmpdir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "with open(f'../sample_configs/local.yaml', 'rt') as f:\n",
    "    cfg = FlowConfig(**yaml.safe_load(f))\n",
    "cfg.backtest = None\n",
    "fcst = fcst_from_config(cfg)\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    perform_backtest(fcst, series, cfg, Path(tmpdir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distributed\n",
    "with Client(n_workers=2) as client:\n",
    "    with open(f'../sample_configs/distributed.yaml', 'rt') as f:\n",
    "        cfg = FlowConfig(**yaml.safe_load(f))\n",
    "    fcst = fcst_from_config(cfg)\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        perform_backtest(fcst, series_ddf, cfg, Path(tmpdir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def parse_config(config_file: str) -> FlowConfig:\n",
    "    \"\"\"Create a `FlowConfig` object using the contents of `config_file`\"\"\"\n",
    "    with open(config_file, 'r') as f:\n",
    "        config = FlowConfig(**yaml.safe_load(f))\n",
    "    return config\n",
    "\n",
    "\n",
    "def setup_client(config: ClusterConfig) -> Client:\n",
    "    \"\"\"Spins up a cluster with the specifications defined in `config` and returns a client connected to it.\"\"\"\n",
    "    module_name, cluster_cls = config.class_name.rsplit('.', maxsplit=1)\n",
    "    module = importlib.import_module(module_name)\n",
    "    cluster = getattr(module, cluster_cls)(**config.class_kwargs)\n",
    "    client = Client(cluster)\n",
    "    n_workers = config.class_kwargs.get('n_workers', 0)\n",
    "    client.wait_for_workers(n_workers)\n",
    "    return client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distributed\n",
    "client = setup_client(cfg.distributed.cluster)\n",
    "assert isinstance(client.cluster, LocalCluster)\n",
    "assert len(client.scheduler_info()['workers']) == cfg.distributed.cluster.class_kwargs['n_workers']\n",
    "client.cluster.close()\n",
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
