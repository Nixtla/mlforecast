{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f2247f-be0c-4a71-bf79-d260988e8e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp target_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a639d9d1-0b9f-41b2-b154-3172bf8cbebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba00a9c6-845d-4840-ad64-68a8d5430836",
   "metadata": {},
   "source": [
    "# Target transforms\n",
    "Transformations that can be applied to the target before computing the features and restored after computing the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae088aa3-7a4e-4c29-98fe-940277d93c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import abc\n",
    "import copy\n",
    "from typing import Iterable, List, Optional, Sequence\n",
    "\n",
    "import coreforecast.scalers as core_scalers\n",
    "import numpy as np\n",
    "import utilsforecast.processing as ufp\n",
    "from coreforecast.grouped_array import GroupedArray as CoreGroupedArray\n",
    "from sklearn.base import TransformerMixin, clone\n",
    "from utilsforecast.compat import DataFrame\n",
    "\n",
    "from mlforecast.grouped_array import GroupedArray\n",
    "from mlforecast.utils import _ShortSeriesException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0daa3f7-c214-4eb8-bd8d-e12095c0be6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fastcore.test import test_fail\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from utilsforecast.processing import counts_by_id\n",
    "\n",
    "from mlforecast import MLForecast\n",
    "from mlforecast.utils import generate_daily_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c487eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BaseTargetTransform(abc.ABC):\n",
    "    \"\"\"Base class used for target transformations.\"\"\"\n",
    "    def set_column_names(self, id_col: str, time_col: str, target_col: str):\n",
    "        self.id_col = id_col\n",
    "        self.time_col = time_col\n",
    "        self.target_col = target_col\n",
    "\n",
    "    def update(self, df: DataFrame) -> DataFrame:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @staticmethod\n",
    "    def stack(transforms: Sequence[\"BaseTargetTransform\"]) -> \"BaseTargetTransform\":\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def fit_transform(self, df: DataFrame) -> DataFrame:\n",
    "        ...\n",
    "        \n",
    "    @abc.abstractmethod\n",
    "    def inverse_transform(self, df: DataFrame) -> DataFrame:\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76764887-3aff-4e11-80b8-004da868f77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class _BaseGroupedArrayTargetTransform(abc.ABC):\n",
    "    \"\"\"Base class used for target transformations that operate on grouped arrays.\"\"\"\n",
    "    num_threads: int = 1\n",
    "    scaler_: core_scalers._BaseLocalScaler\n",
    "\n",
    "    def set_num_threads(self, num_threads: int) -> None:\n",
    "        self.num_threads = num_threads\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def update(self, ga: GroupedArray) -> GroupedArray:\n",
    "        ...\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def fit_transform(self, ga: GroupedArray) -> GroupedArray:\n",
    "        ...\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def inverse_transform(self, ga: GroupedArray) -> GroupedArray:\n",
    "        ...    \n",
    "\n",
    "    def inverse_transform_fitted(self, ga: GroupedArray) -> GroupedArray:\n",
    "        return self.inverse_transform(ga)\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def take(self, idxs: np.ndarray) -> '_BaseGroupedArrayTargetTransform':\n",
    "        ...\n",
    "\n",
    "    @staticmethod\n",
    "    def stack(scalers: Sequence[\"_BaseGroupedArrayTargetTransform\"]) -> \"_BaseGroupedArrayTargetTransform\":\n",
    "        first_scaler = scalers[0]\n",
    "        core_scaler = first_scaler.scaler_\n",
    "        out = copy.deepcopy(first_scaler)\n",
    "        out.scaler_ = core_scaler.stack([sc.scaler_ for sc in scalers])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad5dfb5-a9fe-4f02-8561-3ad8f14d04cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Differences(_BaseGroupedArrayTargetTransform):\n",
    "    \"\"\"Subtracts previous values of the serie. Can be used to remove trend or seasonalities.\"\"\"\n",
    "    store_fitted = False\n",
    "    \n",
    "    def __init__(self, differences: Iterable[int]):\n",
    "        self.differences = list(differences)\n",
    "\n",
    "    def fit_transform(self, ga: GroupedArray) -> GroupedArray:\n",
    "        self.fitted_: List[GroupedArray] = []\n",
    "        original_sizes = np.diff(ga.indptr)\n",
    "        total_diffs = sum(self.differences)\n",
    "        small_series = original_sizes < total_diffs\n",
    "        if small_series.any():\n",
    "            raise _ShortSeriesException(np.arange(ga.n_groups)[small_series])\n",
    "        self.scalers_ = []\n",
    "        core_ga = CoreGroupedArray(ga.data, ga.indptr, self.num_threads)        \n",
    "        for d in self.differences:\n",
    "            if self.store_fitted:\n",
    "                # these are saved in order to be able to perform a correct\n",
    "                # inverse transform when trying to retrieve the fitted values.\n",
    "                self.fitted_.append(GroupedArray(core_ga.data.copy(), core_ga.indptr.copy()))\n",
    "            scaler = core_scalers.Difference(d)\n",
    "            transformed = scaler.fit_transform(core_ga)\n",
    "            self.scalers_.append(scaler)            \n",
    "            core_ga = core_ga._with_data(transformed)\n",
    "        return GroupedArray(core_ga.data, ga.indptr)\n",
    "\n",
    "    def update(self, ga: GroupedArray) -> GroupedArray:\n",
    "        core_ga = CoreGroupedArray(ga.data, ga.indptr, self.num_threads)\n",
    "        for scaler in self.scalers_:\n",
    "            transformed = scaler.update(core_ga)\n",
    "            core_ga = core_ga._with_data(transformed)\n",
    "        return GroupedArray(transformed, ga.indptr)\n",
    "\n",
    "    def inverse_transform(self, ga: GroupedArray) -> GroupedArray:\n",
    "        core_ga = CoreGroupedArray(ga.data, ga.indptr, self.num_threads)\n",
    "        for scaler in self.scalers_[::-1]:\n",
    "            transformed = scaler.inverse_transform(core_ga)\n",
    "            core_ga = core_ga._with_data(transformed)\n",
    "        return GroupedArray(transformed, ga.indptr)\n",
    "\n",
    "    def inverse_transform_fitted(self, ga: GroupedArray) -> GroupedArray:\n",
    "        ga = copy.copy(ga)\n",
    "        for d, fitted in zip(reversed(self.differences), reversed(self.fitted_)):\n",
    "            fitted.restore_fitted_difference(ga.data, ga.indptr, d)\n",
    "        return ga\n",
    "\n",
    "    def take(self, idxs: np.ndarray) -> \"Differences\":\n",
    "        out = Differences(self.differences)\n",
    "        out.fitted_ = [ga.take(idxs) for ga in self.fitted_]\n",
    "        out.scalers_ = [scaler.take(idxs) for scaler in self.scalers_]\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def stack(scalers: Sequence[\"Differences\"]) -> \"Differences\":  # type: ignore[override]\n",
    "        first_scaler = scalers[0]\n",
    "        core_scaler = first_scaler.scalers_[0]\n",
    "        diffs = first_scaler.differences\n",
    "        out = Differences(diffs)\n",
    "        out.fitted_ = []\n",
    "        for i in range(len(scalers[0].fitted_)):\n",
    "            data = np.hstack([sc.fitted_[i].data for sc in scalers])\n",
    "            sizes = np.hstack([np.diff(sc.fitted_[i].indptr) for sc in scalers])\n",
    "            out.fitted_.append(GroupedArray(data, np.append(0, sizes.cumsum())))\n",
    "        out.scalers_ = [\n",
    "            core_scaler.stack([sc.scalers_[i] for sc in scalers])\n",
    "            for i in range(len(diffs))\n",
    "        ]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f21d990-6a80-400a-9b5c-9cb75131a58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "series = generate_daily_series(10, min_length=50, max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f8ace2-b5c1-4fd2-9f2d-2a3aecae6bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = Differences([1, 2, 5])\n",
    "id_counts = counts_by_id(series, 'unique_id')\n",
    "indptr = np.append(0, id_counts['counts'].cumsum())\n",
    "ga = GroupedArray(series['y'].values, indptr)\n",
    "\n",
    "# differences are applied correctly\n",
    "transformed = diffs.fit_transform(ga)\n",
    "assert diffs.fitted_ == []\n",
    "expected = series.copy()\n",
    "for d in diffs.differences:\n",
    "    expected['y'] -= expected.groupby('unique_id', observed=True)['y'].shift(d)\n",
    "np.testing.assert_allclose(transformed.data, expected['y'].values)\n",
    "\n",
    "# fitted differences are restored correctly\n",
    "diffs.store_fitted = True\n",
    "transformed = diffs.fit_transform(ga)\n",
    "keep_mask = ~np.isnan(transformed.data)\n",
    "restored = diffs.inverse_transform_fitted(transformed)\n",
    "np.testing.assert_allclose(ga.data[keep_mask], restored.data[keep_mask])\n",
    "restored_subs = diffs.inverse_transform_fitted(transformed.take_from_groups(slice(8, None)))\n",
    "np.testing.assert_allclose(ga.data[keep_mask], restored_subs.data)\n",
    "\n",
    "# test transform\n",
    "new_ga = GroupedArray(np.random.rand(10), np.arange(11))\n",
    "prev_orig = [diffs.scalers_[i].tails_[::d].copy() for i, d in enumerate(diffs.differences)]\n",
    "expected = new_ga.data - np.add.reduce(prev_orig)\n",
    "updates = diffs.update(new_ga)\n",
    "np.testing.assert_allclose(expected, updates.data)\n",
    "np.testing.assert_allclose(diffs.scalers_[0].tails_, new_ga.data)\n",
    "np.testing.assert_allclose(diffs.scalers_[1].tails_[1::2], new_ga.data - prev_orig[0])\n",
    "np.testing.assert_allclose(diffs.scalers_[2].tails_[4::5], new_ga.data - np.add.reduce(prev_orig[:2]))\n",
    "# variable sizes\n",
    "diff1 = Differences([1])\n",
    "ga = GroupedArray(np.arange(10), np.array([0, 3, 10]))\n",
    "diff1.fit_transform(ga)\n",
    "new_ga = GroupedArray(np.arange(4), np.array([0, 1, 4]))\n",
    "updates = diff1.update(new_ga)\n",
    "np.testing.assert_allclose(updates.data, np.array([0 - 2, 1 - 9, 2 - 1, 3 - 2]))\n",
    "np.testing.assert_allclose(diff1.scalers_[0].tails_, np.array([0, 3]))\n",
    "\n",
    "# short series\n",
    "ga = GroupedArray(np.arange(20), np.array([0, 2, 20]))\n",
    "test_fail(lambda: diffs.fit_transform(ga), contains=\"[0]\")\n",
    "\n",
    "# stack\n",
    "diffs = Differences([1, 2, 5])\n",
    "ga = GroupedArray(series['y'].values, indptr)\n",
    "diffs.fit_transform(ga)\n",
    "stacked = Differences.stack([diffs, diffs])\n",
    "for i in range(len(diffs.differences)):\n",
    "    np.testing.assert_allclose(\n",
    "        stacked.scalers_[i].tails_,\n",
    "        np.tile(diffs.scalers_[i].tails_, 2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7547f36c-d141-4618-ab1c-e6a5e4a291c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AutoDifferences(_BaseGroupedArrayTargetTransform):\n",
    "    \"\"\"Find and apply the optimal number of differences to each serie.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    max_diffs: int\n",
    "        Maximum number of differences to apply.\"\"\"\n",
    "    def __init__(self, max_diffs: int):\n",
    "        self.scaler_ = core_scalers.AutoDifferences(max_diffs)\n",
    "\n",
    "    def fit_transform(self, ga: GroupedArray) -> GroupedArray:\n",
    "        core_ga = CoreGroupedArray(ga.data, ga.indptr, self.num_threads)\n",
    "        return GroupedArray(self.scaler_.fit_transform(core_ga), ga.indptr)\n",
    "\n",
    "    def update(self, ga: GroupedArray) -> GroupedArray:\n",
    "        core_ga = CoreGroupedArray(ga.data, ga.indptr, self.num_threads)\n",
    "        return GroupedArray(self.scaler_.update(core_ga), ga.indptr)\n",
    "\n",
    "    def inverse_transform(self, ga: GroupedArray) -> GroupedArray:\n",
    "        core_ga = CoreGroupedArray(ga.data, ga.indptr, self.num_threads)\n",
    "        return GroupedArray(self.scaler_.inverse_transform(core_ga), ga.indptr)\n",
    "\n",
    "    def inverse_transform_fitted(self, ga: GroupedArray) -> GroupedArray:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def take(self, idxs: np.ndarray) -> 'AutoDifferences':\n",
    "        out = AutoDifferences(self.scaler_.max_diffs)\n",
    "        out.scaler_ = self.scaler_.take(idxs)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68edc85-3c18-4ded-a5ec-0817937acac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "sc = AutoDifferences(1)\n",
    "ga = GroupedArray(np.arange(10), np.array([0, 10]))\n",
    "transformed = sc.fit_transform(ga)\n",
    "np.testing.assert_allclose(transformed.data, np.append(np.nan, np.ones(9)))\n",
    "np.testing.assert_equal(sc.scaler_.diffs_, np.array([1.0], dtype=np.float32))\n",
    "inv = sc.inverse_transform(ga).data\n",
    "np.testing.assert_allclose(9 + np.arange(10).cumsum(), inv)\n",
    "upd = sc.update(GroupedArray(np.array([10]), np.array([0, 1])))\n",
    "np.testing.assert_equal(np.array([1.0]), upd.data)\n",
    "np.testing.assert_equal(sc.scaler_.tails_[0], np.array([10]))\n",
    "\n",
    "# stack\n",
    "stacked = AutoDifferences.stack([sc, sc])\n",
    "np.testing.assert_allclose(\n",
    "    stacked.scaler_.diffs_,\n",
    "    np.tile(sc.scaler_.diffs_, 2),\n",
    ")\n",
    "np.testing.assert_allclose(\n",
    "    stacked.scaler_.tails_,\n",
    "    np.tile(sc.scaler_.tails_, 2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d82635-4dae-404f-9704-16fedcd4ab6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AutoSeasonalDifferences(AutoDifferences):\n",
    "    \"\"\"Find and apply the optimal number of seasonal differences to each group.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    season_length : int\n",
    "        Length of the seasonal period.\n",
    "    max_diffs : int\n",
    "        Maximum number of differences to apply.\n",
    "    n_seasons : int, optional (default=10)\n",
    "        Number of seasons to use to determine the number of differences. Defaults to 10.\n",
    "        If `None` will use all samples, otherwise `season_length` * `n_seasons samples` will be used for the test.\n",
    "        Smaller values will be faster but could be less accurate.\"\"\"\n",
    "    def __init__(self, season_length: int, max_diffs: int, n_seasons: Optional[int] = 10):      \n",
    "        self.scaler_ = core_scalers.AutoSeasonalDifferences(\n",
    "            season_length=season_length,\n",
    "            max_diffs=max_diffs,\n",
    "            n_seasons=n_seasons,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aab6a4-ef1c-408d-b793-2c3aa768d279",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "sc = AutoSeasonalDifferences(season_length=5, max_diffs=1)\n",
    "ga = GroupedArray(np.arange(5)[np.arange(10) % 5], np.array([0, 10]))\n",
    "transformed = sc.fit_transform(ga)\n",
    "sc.inverse_transform(ga)\n",
    "sc.update(ga)\n",
    "\n",
    "# stack\n",
    "stacked = AutoDifferences.stack([sc, sc])\n",
    "np.testing.assert_allclose(\n",
    "    stacked.scaler_.diffs_,\n",
    "    np.tile(sc.scaler_.diffs_, 2),\n",
    ")\n",
    "np.testing.assert_allclose(\n",
    "    stacked.scaler_.tails_,\n",
    "    np.tile(sc.scaler_.tails_, 2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8e1089-d8a7-4eb7-8ced-ba9f26b95264",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AutoSeasonalityAndDifferences(AutoDifferences):\n",
    "    \"\"\"Find the length of the seasonal period and apply the optimal number of differences to each group.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    max_season_length : int\n",
    "        Maximum length of the seasonal period.\n",
    "    max_diffs : int\n",
    "        Maximum number of differences to apply.\n",
    "    n_seasons : int, optional (default=10)\n",
    "        Number of seasons to use to determine the number of differences. Defaults to 10.\n",
    "        If `None` will use all samples, otherwise `max_season_length` * `n_seasons samples` will be used for the test.\n",
    "        Smaller values will be faster but could be less accurate.\"\"\"\n",
    "    def __init__(self, max_season_length: int, max_diffs: int, n_seasons: Optional[int] = 10):      \n",
    "        self.scaler_ = core_scalers.AutoSeasonalityAndDifferences(\n",
    "            max_season_length=max_season_length,\n",
    "            max_diffs=max_diffs,\n",
    "            n_seasons=n_seasons,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274e5961-ad6a-4f61-8d7a-e956259a3e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "sc = AutoSeasonalityAndDifferences(max_season_length=5, max_diffs=1)\n",
    "ga = GroupedArray(np.arange(5)[np.arange(10) % 5], np.array([0, 10]))\n",
    "transformed = sc.fit_transform(ga)\n",
    "sc.inverse_transform(ga)\n",
    "sc.update(ga)\n",
    "\n",
    "# stack\n",
    "stacked = AutoDifferences.stack([sc, sc])\n",
    "np.testing.assert_allclose(\n",
    "    stacked.scaler_.diffs_,\n",
    "    np.tile(sc.scaler_.diffs_, 2),\n",
    ")\n",
    "np.testing.assert_allclose(\n",
    "    stacked.scaler_.tails_,\n",
    "    np.tile(sc.scaler_.tails_, 2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67473e24-19f4-4c04-bc4b-13c313cbf52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class _BaseLocalScaler(_BaseGroupedArrayTargetTransform):\n",
    "    scaler_factory: type\n",
    "\n",
    "    def update(self, ga: GroupedArray) -> GroupedArray:\n",
    "        ga = CoreGroupedArray(ga.data, ga.indptr, self.num_threads)\n",
    "        return GroupedArray(self.scaler_.transform(ga), ga.indptr)\n",
    "\n",
    "    def fit_transform(self, ga: GroupedArray) -> GroupedArray:\n",
    "        self.scaler_ = self.scaler_factory()\n",
    "        core_ga = CoreGroupedArray(ga.data, ga.indptr, self.num_threads)\n",
    "        transformed = self.scaler_.fit_transform(core_ga)\n",
    "        return GroupedArray(transformed, ga.indptr)\n",
    "\n",
    "    def inverse_transform(self, ga: GroupedArray) -> GroupedArray:\n",
    "        core_ga = CoreGroupedArray(ga.data, ga.indptr, self.num_threads)\n",
    "        transformed = self.scaler_.inverse_transform(core_ga)\n",
    "        return GroupedArray(transformed, ga.indptr)\n",
    "\n",
    "    def take(self, idxs: np.ndarray) -> '_BaseLocalScaler':\n",
    "        out = copy.deepcopy(self)\n",
    "        out.scaler_ = self.scaler_.take(idxs)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2ece47-2f97-43a4-bf52-6a2a9cebf7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_scaler(sc, series):\n",
    "    id_counts = counts_by_id(series, 'unique_id')\n",
    "    indptr = np.append(0, id_counts['counts'].cumsum())\n",
    "    ga = GroupedArray(series['y'].values, indptr)\n",
    "    transformed = sc.fit_transform(ga)\n",
    "    np.testing.assert_allclose(\n",
    "        sc.inverse_transform(transformed).data,\n",
    "        ga.data,\n",
    "    )\n",
    "    transformed2 = sc.update(ga)\n",
    "    np.testing.assert_allclose(transformed.data, transformed2.data)\n",
    "    \n",
    "    idxs = [0, 7]\n",
    "    subset = ga.take(idxs)\n",
    "    transformed_subset = transformed.take(idxs)\n",
    "    subsc = sc.take(idxs)\n",
    "    np.testing.assert_allclose(\n",
    "        subsc.inverse_transform(transformed_subset).data,\n",
    "        subset.data,\n",
    "    )\n",
    "\n",
    "    stacked = sc.stack([sc, sc])\n",
    "    stacked_stats = stacked.scaler_.stats_\n",
    "    np.testing.assert_allclose(\n",
    "        stacked_stats,\n",
    "        np.tile(sc.scaler_.stats_, (2, 1)),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9f8d0e-d2a9-450d-8c95-58e5edbf4ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LocalStandardScaler(_BaseLocalScaler):\n",
    "    \"\"\"Standardizes each serie by subtracting its mean and dividing by its standard deviation.\"\"\"\n",
    "    scaler_factory = core_scalers.LocalStandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79a3c5b-5512-434d-809a-49e8b498c278",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scaler(LocalStandardScaler(), series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e413c9-0125-4551-923b-60209b92b800",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LocalMinMaxScaler(_BaseLocalScaler):\n",
    "    \"\"\"Scales each serie to be in the [0, 1] interval.\"\"\"\n",
    "    scaler_factory = core_scalers.LocalMinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d2b18a-639b-466b-98fc-71a36465c039",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scaler(LocalMinMaxScaler(), series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb33300-24a0-4ba0-9a43-6a98b18c987d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LocalRobustScaler(_BaseLocalScaler):\n",
    "    \"\"\"Scaler robust to outliers.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    scale : str (default='iqr')\n",
    "        Statistic to use for scaling. Can be either 'iqr' (Inter Quartile Range) or 'mad' (Median Asbolute Deviation)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scale: str):\n",
    "        self.scaler_factory = lambda: core_scalers.LocalRobustScaler(scale)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368179bc-ba11-49ec-a015-dca1c0827e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scaler(LocalRobustScaler(scale='iqr'), series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a97f26-3b4f-4441-9887-0dd537c73a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scaler(LocalRobustScaler(scale='mad'), series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844fccb7-7e8c-4cca-a6e4-fbf64d10bfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LocalBoxCox(_BaseLocalScaler):\n",
    "    \"\"\"Finds the optimum lambda for each serie and applies the Box-Cox transformation\"\"\"\n",
    "    def __init__(self):\n",
    "        self.scaler_factory = lambda: core_scalers.LocalBoxCoxScaler(\n",
    "            method='loglik', lower=0.0\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5bf731-2d9a-4c08-9c60-c4fbc9af2549",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scaler(LocalBoxCox(), series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ffb5e1-43db-48b9-8436-29b09e0dded8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class GlobalSklearnTransformer(BaseTargetTransform):\n",
    "    \"\"\"Applies the same scikit-learn transformer to all series.\"\"\"    \n",
    "    def __init__(self, transformer: TransformerMixin):\n",
    "        self.transformer = transformer\n",
    "\n",
    "    def fit_transform(self, df: DataFrame) -> DataFrame:\n",
    "        df = ufp.copy_if_pandas(df, deep=False)\n",
    "        self.transformer_ = clone(self.transformer)\n",
    "        transformed = self.transformer_.fit_transform(df[[self.target_col]].to_numpy())\n",
    "        return ufp.assign_columns(df, self.target_col, transformed[:, 0])\n",
    "\n",
    "    def inverse_transform(self, df: DataFrame) -> DataFrame:\n",
    "        df = ufp.copy_if_pandas(df, deep=False)\n",
    "        cols_to_transform = [\n",
    "            c for c in df.columns if c not in (self.id_col, self.time_col)\n",
    "        ]\n",
    "        transformed = self.transformer_.inverse_transform(\n",
    "            df[cols_to_transform].to_numpy()\n",
    "        )\n",
    "        return ufp.assign_columns(df, cols_to_transform, transformed)\n",
    "\n",
    "    def update(self, df: DataFrame) -> DataFrame:\n",
    "        df = ufp.copy_if_pandas(df, deep=False)\n",
    "        transformed = self.transformer_.transform(df[[self.target_col]].to_numpy())\n",
    "        return ufp.assign_columns(df, self.target_col, transformed[:, 0])\n",
    "\n",
    "    @staticmethod\n",
    "    def stack(transforms: Sequence[\"GlobalSklearnTransformer\"]) -> \"GlobalSklearnTransformer\":  # type: ignore[override]\n",
    "        return transforms[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d714474-fd4e-416a-bd5c-e093423ded38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need this import in order for isinstance to work\n",
    "from mlforecast.target_transforms import Differences as ExportedDifferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1a2af6-53ed-4dc6-8c79-99d21eb97257",
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_boxcox = PowerTransformer(method='box-cox', standardize=False)\n",
    "boxcox_global = GlobalSklearnTransformer(sk_boxcox)\n",
    "single_difference = ExportedDifferences([1])\n",
    "series = generate_daily_series(10)\n",
    "fcst = MLForecast(\n",
    "    models=[LinearRegression()],\n",
    "    freq='D',\n",
    "    lags=[1, 2],\n",
    "    target_transforms=[boxcox_global, single_difference]\n",
    ")\n",
    "prep = fcst.preprocess(series, dropna=False)\n",
    "expected = (\n",
    "    pd.Series(\n",
    "        sk_boxcox.fit_transform(series[['y']])[:, 0], index=series['unique_id']\n",
    "    ).groupby('unique_id', observed=True)\n",
    "    .diff()\n",
    "    .values\n",
    ")\n",
    "np.testing.assert_allclose(prep['y'].values, expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a67713-ac9e-4525-b635-8ac3298262d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| polars\n",
    "series_pl = generate_daily_series(10, engine='polars')\n",
    "fcst_pl = MLForecast(\n",
    "    models=[LinearRegression()],\n",
    "    freq='1d',\n",
    "    lags=[1, 2],\n",
    "    target_transforms=[boxcox_global, single_difference]\n",
    ")\n",
    "prep_pl = fcst_pl.preprocess(series_pl, dropna=False)\n",
    "pd.testing.assert_frame_equal(prep, prep_pl.to_pandas())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
