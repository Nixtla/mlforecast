{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00eb8158-917e-4242-af3f-c18f9ec1a064",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9166cafb-12d3-4953-8c38-8cbf345a7c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8db46f-8a2f-458b-b403-50c1d82fdd6a",
   "metadata": {},
   "source": [
    "# Optimization\n",
    "Utilities for hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651eb16d-b541-4b21-8912-ff8e720d9316",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import copy\n",
    "from typing import Any, Callable, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import optuna\n",
    "import utilsforecast.processing as ufp\n",
    "from sklearn.base import BaseEstimator, clone\n",
    "from utilsforecast.compat import DataFrame\n",
    "\n",
    "from mlforecast import MLForecast\n",
    "from mlforecast.compat import CatBoostRegressor\n",
    "from mlforecast.core import Freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e0e555",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "_TrialToConfig = Callable[[optuna.Trial], Dict[str, Any]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7435127e-e6be-4d4b-8b02-e52538ec60bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mlforecast_objective(\n",
    "    df: DataFrame,\n",
    "    config_fn: _TrialToConfig,\n",
    "    loss: Callable,\n",
    "    model: BaseEstimator,\n",
    "    freq: Freq,\n",
    "    n_windows: int,\n",
    "    h: int,\n",
    "    id_col: str = 'unique_id',\n",
    "    time_col: str = 'ds',\n",
    "    target_col: str = 'y',\n",
    ") -> Callable[[optuna.Trial], float]:\n",
    "    \"\"\"optuna objective function for the MLForecast class\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    config_fn : callable\n",
    "        Function that takes an optuna trial and produces a configuration with the following keys:\n",
    "        - model_params\n",
    "        - mlf_init_params\n",
    "        - mlf_fit_params\n",
    "    loss : callable\n",
    "        Function that takes the validation and train dataframes and produces a float.\n",
    "    model : BaseEstimator\n",
    "        scikit-learn compatible model to be trained\n",
    "    freq : str or int\n",
    "        pandas' or polars' offset alias or integer denoting the frequency of the series.\n",
    "    n_windows : int\n",
    "        Number of windows to evaluate.\n",
    "    h : int\n",
    "        Forecast horizon. \n",
    "    id_col : str (default='unique_id')\n",
    "        Column that identifies each serie.\n",
    "    time_col : str (default='ds')\n",
    "        Column that identifies each timestep, its values can be timestamps or integers.\n",
    "    target_col : str (default='y')\n",
    "        Column that contains the target.        \n",
    "    study_kwargs : dict, optional (default=None)\n",
    "    \"\"\"\n",
    "    def objective(trial: optuna.Trial) -> float:\n",
    "        config = config_fn(trial)\n",
    "        trial.set_user_attr('config', copy.deepcopy(config))\n",
    "        if all(\n",
    "            config['mlf_init_params'].get(k, None) is None\n",
    "            for k in ['lags', 'lag_transforms', 'date_features']\n",
    "        ):\n",
    "            # no features\n",
    "            return np.inf\n",
    "        splits = ufp.backtest_splits(\n",
    "            df,\n",
    "            n_windows=n_windows,\n",
    "            h=h,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            freq=freq,\n",
    "        )\n",
    "        model_copy = clone(model)\n",
    "        model_params = config['model_params']\n",
    "        if (\n",
    "            config['mlf_fit_params'].get('static_features', [])\n",
    "            and isinstance(model, CatBoostRegressor)\n",
    "        ):\n",
    "            # catboost needs the categorical features in the init signature\n",
    "            # we assume all statics are categoricals\n",
    "            model_params['cat_features'] = config['mlf_fit_params']['static_features']\n",
    "        model_copy.set_params(**config['model_params'])\n",
    "        metrics = []\n",
    "        for i, (_, train, valid) in enumerate(splits):\n",
    "            mlf = MLForecast(\n",
    "                models={'model': model_copy},\n",
    "                freq=freq,\n",
    "                **config['mlf_init_params'],\n",
    "            )\n",
    "            mlf.fit(\n",
    "                train,\n",
    "                id_col=id_col,\n",
    "                time_col=time_col,\n",
    "                target_col=target_col,\n",
    "                **config['mlf_fit_params'],\n",
    "            )\n",
    "            static = [c for c in mlf.ts.static_features_.columns if c != id_col]\n",
    "            dynamic = [\n",
    "                c\n",
    "                for c in valid.columns\n",
    "                if c not in static + [id_col, time_col, target_col]\n",
    "            ]\n",
    "            if dynamic:\n",
    "                X_df: Optional[DataFrame] = ufp.drop_columns(\n",
    "                    valid, static + [target_col]\n",
    "                )\n",
    "            else:\n",
    "                X_df = None            \n",
    "            preds = mlf.predict(h=h, X_df=X_df)\n",
    "            result = ufp.join(\n",
    "                valid[[id_col, time_col, target_col]],\n",
    "                preds,\n",
    "                on=[id_col, time_col],\n",
    "            )\n",
    "            if result.shape[0] < valid.shape[0]:\n",
    "                raise ValueError(\n",
    "                    \"Cross validation result produced less results than expected. \"\n",
    "                    \"Please verify that the passed frequency (freq) matches your series' \"\n",
    "                    \"and that there aren't any missing periods.\"                    \n",
    "                )\n",
    "            metric = loss(result, train_df=train)\n",
    "            metrics.append(metric)\n",
    "            trial.report(metric, step=i)\n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "        return np.mean(metrics).item()\n",
    "    return objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca09079",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6c74a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/mlforecast/blob/main/mlforecast/optimization.py#L24){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### mlforecast_objective\n",
       "\n",
       ">      mlforecast_objective\n",
       ">                            (df:Union[pandas.core.frame.DataFrame,polars.datafr\n",
       ">                            ame.frame.DataFrame], config_fn:Callable[[optuna.tr\n",
       ">                            ial._trial.Trial],Dict[str,Any]], loss:Callable[[Un\n",
       ">                            ion[pandas.core.frame.DataFrame,polars.dataframe.fr\n",
       ">                            ame.DataFrame],Union[pandas.core.frame.DataFrame,po\n",
       ">                            lars.dataframe.frame.DataFrame]],float],\n",
       ">                            model:sklearn.base.BaseEstimator,\n",
       ">                            freq:Union[int,str], n_windows:int, h:int,\n",
       ">                            id_col:str='unique_id', time_col:str='ds',\n",
       ">                            target_col:str='y')\n",
       "\n",
       "optuna objective function for the MLForecast class\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | Union |  |  |\n",
       "| config_fn | Callable |  | Function that takes an optuna trial and produces a configuration with the following keys:<br>- model_params<br>- mlf_init_params<br>- mlf_fit_params |\n",
       "| loss | Callable |  | Function that takes the validation and train dataframes and produces a float. |\n",
       "| model | BaseEstimator |  | scikit-learn compatible model to be trained |\n",
       "| freq | Union |  | pandas' or polars' offset alias or integer denoting the frequency of the series. |\n",
       "| n_windows | int |  | Number of windows to evaluate. |\n",
       "| h | int |  | Forecast horizon.  |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| time_col | str | ds | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str | y | Column that contains the target.         |\n",
       "| **Returns** | **Callable** |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/mlforecast/blob/main/mlforecast/optimization.py#L24){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### mlforecast_objective\n",
       "\n",
       ">      mlforecast_objective\n",
       ">                            (df:Union[pandas.core.frame.DataFrame,polars.datafr\n",
       ">                            ame.frame.DataFrame], config_fn:Callable[[optuna.tr\n",
       ">                            ial._trial.Trial],Dict[str,Any]], loss:Callable[[Un\n",
       ">                            ion[pandas.core.frame.DataFrame,polars.dataframe.fr\n",
       ">                            ame.DataFrame],Union[pandas.core.frame.DataFrame,po\n",
       ">                            lars.dataframe.frame.DataFrame]],float],\n",
       ">                            model:sklearn.base.BaseEstimator,\n",
       ">                            freq:Union[int,str], n_windows:int, h:int,\n",
       ">                            id_col:str='unique_id', time_col:str='ds',\n",
       ">                            target_col:str='y')\n",
       "\n",
       "optuna objective function for the MLForecast class\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | Union |  |  |\n",
       "| config_fn | Callable |  | Function that takes an optuna trial and produces a configuration with the following keys:<br>- model_params<br>- mlf_init_params<br>- mlf_fit_params |\n",
       "| loss | Callable |  | Function that takes the validation and train dataframes and produces a float. |\n",
       "| model | BaseEstimator |  | scikit-learn compatible model to be trained |\n",
       "| freq | Union |  | pandas' or polars' offset alias or integer denoting the frequency of the series. |\n",
       "| n_windows | int |  | Number of windows to evaluate. |\n",
       "| h | int |  | Forecast horizon.  |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| time_col | str | ds | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str | y | Column that contains the target.         |\n",
       "| **Returns** | **Callable** |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(mlforecast_objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c507bae-b2c6-4702-b955-3474388f1733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from datasetsforecast.m4 import M4, M4Evaluation, M4Info\n",
    "from utilsforecast.losses import smape\n",
    "\n",
    "from mlforecast.lag_transforms import ExpandingMean, RollingMean\n",
    "from mlforecast.target_transforms import Differences, LocalBoxCox, LocalStandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2697abe1-c951-4738-9676-7efb3dcb8365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_split(group):\n",
    "    df, *_ = M4.load(directory='data', group=group)\n",
    "    df['ds'] = df['ds'].astype('int')\n",
    "    horizon = M4Info[group].horizon\n",
    "    valid = df.groupby('unique_id').tail(horizon)\n",
    "    train = df.drop(valid.index)\n",
    "    return train, valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6af295-3adc-475d-8bbc-8a8b5d5c6df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = M4Info['Weekly'].horizon\n",
    "weekly_train, weekly_valid = train_valid_split('Weekly')\n",
    "weekly_train['unique_id'] = weekly_train['unique_id'].astype('category')\n",
    "weekly_valid['unique_id'] = weekly_valid['unique_id'].astype(weekly_train['unique_id'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857a963f-ad43-42ad-91f9-65f741da00e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_fn(trial):\n",
    "    candidate_lags = [\n",
    "        [1],\n",
    "        [13],\n",
    "        [1, 13],\n",
    "        range(1, 33),\n",
    "    ]\n",
    "    lag_idx = trial.suggest_categorical('lag_idx', range(len(candidate_lags)))\n",
    "    candidate_lag_tfms = [\n",
    "        {\n",
    "            1: [RollingMean(window_size=13)]\n",
    "        },\n",
    "        {\n",
    "            1: [RollingMean(window_size=13)],\n",
    "            13: [RollingMean(window_size=13)],\n",
    "        },\n",
    "        {\n",
    "            13: [RollingMean(window_size=13)],\n",
    "        },\n",
    "        {\n",
    "            4: [ExpandingMean(), RollingMean(window_size=4)],\n",
    "            8: [ExpandingMean(), RollingMean(window_size=4)],\n",
    "        }\n",
    "    ]\n",
    "    lag_tfms_idx = trial.suggest_categorical('lag_tfms_idx', range(len(candidate_lag_tfms)))\n",
    "    candidate_targ_tfms = [\n",
    "        [Differences([1])],\n",
    "        [LocalBoxCox()],\n",
    "        [LocalStandardScaler()],        \n",
    "        [LocalBoxCox(), Differences([1])],\n",
    "        [LocalBoxCox(), LocalStandardScaler()],\n",
    "        [LocalBoxCox(), Differences([1]), LocalStandardScaler()],\n",
    "    ]\n",
    "    targ_tfms_idx = trial.suggest_categorical('targ_tfms_idx', range(len(candidate_targ_tfms)))\n",
    "    return {\n",
    "        'model_params': {\n",
    "            'learning_rate': 0.05,\n",
    "            'objective': 'l1',\n",
    "            'bagging_freq': 1,\n",
    "            'num_threads': 2,\n",
    "            'verbose': -1,\n",
    "            'force_col_wise': True,\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 10, 1000, log=True),            \n",
    "            'num_leaves': trial.suggest_int('num_leaves', 31, 1024, log=True),\n",
    "            'lambda_l1': trial.suggest_float('lambda_l1', 0.01, 10, log=True),\n",
    "            'lambda_l2': trial.suggest_float('lambda_l2', 0.01, 10, log=True),\n",
    "            'bagging_fraction': trial.suggest_float('bagging_fraction', 0.75, 1.0),\n",
    "            'feature_fraction': trial.suggest_float('feature_fraction', 0.75, 1.0),\n",
    "        },\n",
    "        'mlf_init_params': {\n",
    "            'lags': candidate_lags[lag_idx],\n",
    "            'lag_transforms': candidate_lag_tfms[lag_tfms_idx],\n",
    "            'target_transforms': candidate_targ_tfms[targ_tfms_idx],\n",
    "        },\n",
    "        'mlf_fit_params': {\n",
    "            'static_features': ['unique_id'],\n",
    "        }\n",
    "    }\n",
    "\n",
    "def loss(df, train_df):\n",
    "    return smape(df, models=['model'])['model'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23296223-7aad-4333-a43f-0a22d51f1163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SMAPE</th>\n",
       "      <th>MASE</th>\n",
       "      <th>OWA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Weekly</th>\n",
       "      <td>9.261538</td>\n",
       "      <td>2.614473</td>\n",
       "      <td>0.976158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           SMAPE      MASE       OWA\n",
       "Weekly  9.261538  2.614473  0.976158"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "objective = mlforecast_objective(\n",
    "    df=weekly_train,\n",
    "    config_fn=config_fn,\n",
    "    loss=loss,    \n",
    "    model=lgb.LGBMRegressor(),\n",
    "    freq=1,\n",
    "    n_windows=2,\n",
    "    h=h,\n",
    ")\n",
    "study = optuna.create_study(\n",
    "    direction='minimize', sampler=optuna.samplers.TPESampler(seed=0)\n",
    ")\n",
    "study.optimize(objective, n_trials=2)\n",
    "best_cfg = study.best_trial.user_attrs['config']\n",
    "final_model = MLForecast(\n",
    "    models=[lgb.LGBMRegressor(**best_cfg['model_params'])],\n",
    "    freq=1,\n",
    "    **best_cfg['mlf_init_params'],\n",
    ")\n",
    "final_model.fit(weekly_train, **best_cfg['mlf_fit_params'])\n",
    "preds = final_model.predict(h)\n",
    "M4Evaluation.evaluate('data', 'Weekly', preds['LGBMRegressor'].values.reshape(-1, 13))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
