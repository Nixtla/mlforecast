{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf5b1d7-bc13-4ed3-af30-f15cacc861f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp distributed.fugue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc9a984-a442-41d2-8132-471d73e64d96",
   "metadata": {},
   "source": [
    "# Fugue\n",
    "\n",
    "> Distributed fugue backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8108c14-6d27-4dfd-b3e8-a2b315eb5f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import copy\n",
    "from collections import namedtuple\n",
    "from typing import Any, Callable, Iterable, Iterator, List, Optional\n",
    "\n",
    "import cloudpickle\n",
    "try:\n",
    "    import dask.dataframe as dd\n",
    "    DASK_INSTALLED = True\n",
    "except ModuleNotFoundError:\n",
    "    DASK_INSTALLED = False\n",
    "import fugue\n",
    "import fugue.api as fa\n",
    "import pandas as pd\n",
    "try:\n",
    "    from pyspark.ml.feature import VectorAssembler\n",
    "    from pyspark.sql import DataFrame as SparkDataFrame\n",
    "    SPARK_INSTALLED = True\n",
    "except ModuleNotFoundError:\n",
    "    SPARK_INSTALLED = False\n",
    "from sklearn.base import clone\n",
    "\n",
    "from mlforecast.core import (\n",
    "    DateFeature,\n",
    "    Differences,\n",
    "    Freq,\n",
    "    LagTransforms,\n",
    "    Lags,\n",
    "    TimeSeries,\n",
    "    _name_models,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c505b5b8-2c00-456b-8d61-2301516bc347",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "WindowInfo = namedtuple('WindowInfo', ['n_windows', 'window_size', 'step_size', 'i_window'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227f19b4-4582-4c51-94cd-c7c220d35dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class FugueMLForecast:\n",
    "    \"\"\"Multi backend distributed pipeline\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        models,\n",
    "        freq: Optional[Freq] = None,\n",
    "        lags: Optional[Lags] = None,\n",
    "        lag_transforms: Optional[LagTransforms] = None,\n",
    "        date_features: Optional[Iterable[DateFeature]] = None,\n",
    "        differences: Optional[Differences] = None,\n",
    "        num_threads: int = 1,\n",
    "        engine = None,\n",
    "    ):\n",
    "        \"\"\"Create distributed forecast object\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        models : regressor or list of regressors\n",
    "            Models that will be trained and used to compute the forecasts.\n",
    "        freq : str or int, optional (default=None)\n",
    "            Pandas offset alias, e.g. 'D', 'W-THU' or integer denoting the frequency of the series.\n",
    "        lags : list of int, optional (default=None)\n",
    "            Lags of the target to use as features.\n",
    "        lag_transforms : dict of int to list of functions, optional (default=None)\n",
    "            Mapping of target lags to their transformations.\n",
    "        date_features : list of str or callable, optional (default=None)\n",
    "            Features computed from the dates. Can be pandas date attributes or functions that will take the dates as input.\n",
    "        differences : list of int, optional (default=None)\n",
    "            Differences to take of the target before computing the features. These are restored at the forecasting step.\n",
    "        num_threads : int (default=1)\n",
    "            Number of threads to use when computing the features.\n",
    "        engine : fugue execution engine, optional (default=None)\n",
    "            Dask Client, Spark Session, etc to use for the distributed computation.\n",
    "            If None will use default depending on input type.\n",
    "        \"\"\"        \n",
    "        if not isinstance(models, dict) and not isinstance(models, list):\n",
    "            models = [models]\n",
    "        if isinstance(models, list):\n",
    "            model_names = _name_models([m.__class__.__name__ for m in models])\n",
    "            models_with_names = dict(zip(model_names, models))\n",
    "        else:\n",
    "            models_with_names = models\n",
    "        self.models = models_with_names\n",
    "        self._base_ts = TimeSeries(\n",
    "            freq, lags, lag_transforms, date_features, differences, num_threads\n",
    "        )\n",
    "        self.engine = engine\n",
    "        \n",
    "    def __repr__(self) -> str:\n",
    "        return (\n",
    "            f'{self.__class__.__name__}(models=[{\", \".join(self.models.keys())}], '\n",
    "            f\"freq={self._base_ts.freq}, \"\n",
    "            f\"lag_features={list(self._base_ts.transforms.keys())}, \"\n",
    "            f\"date_features={self._base_ts.date_features}, \"\n",
    "            f\"num_threads={self._base_ts.num_threads}, \"\n",
    "            f\"engine={self.engine})\"\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _preprocess_partition(\n",
    "        part: pd.DataFrame,\n",
    "        base_ts: TimeSeries,        \n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "        window_info: Optional[WindowInfo] = None,\n",
    "    ) -> List[List[Any]]:\n",
    "        ts = copy.deepcopy(base_ts)\n",
    "        if window_info is None:\n",
    "            train = part\n",
    "            valid = None\n",
    "        else:\n",
    "            n_windows, window_size, step_size, i_window = window_info\n",
    "            if step_size is None:\n",
    "                step_size = window_size\n",
    "            test_size = window_size + step_size * (n_windows - 1)\n",
    "            offset = test_size - i_window * step_size\n",
    "            max_dates = part.groupby(id_col)[time_col].transform('max')\n",
    "            train_ends = max_dates - offset * base_ts.freq\n",
    "            valid_ends = train_ends + window_size * base_ts.freq\n",
    "            train_mask = part[time_col].le(train_ends)\n",
    "            valid_mask = part[time_col].gt(train_ends) & part[time_col].le(valid_ends)\n",
    "            train = part[train_mask]\n",
    "            valid_keep_cols = part.columns\n",
    "            if static_features is not None:\n",
    "                valid_keep_cols.drop(static_features)\n",
    "            valid = part.loc[valid_mask, valid_keep_cols]\n",
    "        transformed = ts.fit_transform(\n",
    "            train,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            static_features=static_features,\n",
    "            dropna=dropna,\n",
    "            keep_last_n=keep_last_n,\n",
    "        )\n",
    "        return [[cloudpickle.dumps(ts), cloudpickle.dumps(transformed), cloudpickle.dumps(valid)]]\n",
    "\n",
    "    @staticmethod\n",
    "    def _retrieve_df(items: List[List[Any]]) -> Iterable[pd.DataFrame]:\n",
    "        for _, serialized_train, _ in items:\n",
    "            yield cloudpickle.loads(serialized_train)\n",
    "\n",
    "    def _preprocess(\n",
    "        self,\n",
    "        data: fugue.AnyDataFrame,\n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "        window_info: Optional[WindowInfo] = None,\n",
    "    ) -> fugue.AnyDataFrame:\n",
    "        self.id_col = id_col\n",
    "        self.time_col = time_col\n",
    "        self.target_col = target_col\n",
    "        self.partition_results = fa.transform(\n",
    "            data,\n",
    "            FugueMLForecast._preprocess_partition,\n",
    "            params={\n",
    "                'base_ts': self._base_ts,\n",
    "                'id_col': id_col,\n",
    "                'time_col': time_col,\n",
    "                'target_col': target_col,\n",
    "                'static_features': static_features,\n",
    "                'dropna': dropna,\n",
    "                'keep_last_n': keep_last_n,\n",
    "                'window_info': window_info,\n",
    "            },\n",
    "            schema='ts:binary,train:binary,valid:binary',\n",
    "            engine=self.engine,\n",
    "            as_fugue=True,\n",
    "        )\n",
    "        base_schema = str(fa.get_schema(data))\n",
    "        features_schema = ','.join(f'{feat}:double' for feat in self._base_ts.features)\n",
    "        res = fa.transform(\n",
    "            self.partition_results,\n",
    "            FugueMLForecast._retrieve_df,\n",
    "            schema=f'{base_schema},{features_schema}',\n",
    "            engine=self.engine,\n",
    "        )\n",
    "        return fa.get_native_as_df(res)\n",
    "    \n",
    "    def preprocess(\n",
    "        self,\n",
    "        data: fugue.AnyDataFrame,\n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "    ) -> fugue.AnyDataFrame:\n",
    "        \"\"\"Add the features to `data`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : dask or spark DataFrame.\n",
    "            Series data in long format.\n",
    "        id_col : str\n",
    "            Column that identifies each serie. If 'index' then the index is used.\n",
    "        time_col : str\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str\n",
    "            Column that contains the target.\n",
    "        static_features : list of str, optional (default=None)\n",
    "            Names of the features that are static and will be repeated when forecasting.\n",
    "        dropna : bool (default=True)\n",
    "            Drop rows with missing values produced by the transformations.\n",
    "        keep_last_n : int, optional (default=None)\n",
    "            Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        result : same type as input\n",
    "            data with added features.\n",
    "        \"\"\"        \n",
    "        return self._preprocess(\n",
    "            data,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            static_features=static_features,\n",
    "            dropna=dropna,\n",
    "            keep_last_n=keep_last_n,\n",
    "        )\n",
    "    \n",
    "    def _fit(\n",
    "        self,\n",
    "        data: fugue.AnyDataFrame,\n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "        window_info: Optional[WindowInfo] = None,\n",
    "    ) -> 'FugueMLForecast':\n",
    "        prep = self._preprocess(\n",
    "            data,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            static_features=static_features,\n",
    "            dropna=dropna,\n",
    "            keep_last_n=keep_last_n,\n",
    "            window_info=window_info,\n",
    "        )\n",
    "        features = [x for x in prep.columns if x not in {id_col, time_col, target_col}]\n",
    "        self.models_ = {}\n",
    "        if SPARK_INSTALLED and isinstance(data, SparkDataFrame):\n",
    "            try:\n",
    "                import lightgbm as lgb\n",
    "                from synapse.ml.lightgbm import LightGBMRegressor as SynapseLGBMRegressor\n",
    "                LGBM_INSTALLED = True\n",
    "            except ModuleNotFoundError:\n",
    "                LGBM_INSTALLED = False\n",
    "            try:\n",
    "                import xgboost as xgb\n",
    "                from xgboost.spark import SparkXGBRegressor  # type: ignore\n",
    "                XGB_INSTALLED = True\n",
    "            except ModuleNotFoundError:\n",
    "                XGB_INSTALLED = False\n",
    "\n",
    "            featurizer = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "            train_data = featurizer.transform(prep)[target_col, \"features\"]\n",
    "            for name, model in self.models.items():\n",
    "                if LGBM_INSTALLED and isinstance(model, SynapseLGBMRegressor):\n",
    "                    trained_model = model.setLabelCol(target_col).fit(train_data)\n",
    "                    model_str = trained_model.getNativeModel()\n",
    "                    local_model = lgb.Booster(model_str=model_str)                    \n",
    "                elif XGB_INSTALLED and isinstance(model, SparkXGBRegressor):\n",
    "                    model.setParams(label_col=target_col)\n",
    "                    trained_model = model.fit(train_data)\n",
    "                    model_str = trained_model.get_booster().save_raw('ubj')\n",
    "                    local_model = xgb.XGBRegressor()\n",
    "                    local_model.load_model(model_str)\n",
    "                else:\n",
    "                    raise ValueError('Only LightGBMRegressor from SynapseML and SparkXGBRegressor are supported in spark.')\n",
    "                self.models_[name] = local_model\n",
    "        elif DASK_INSTALLED and isinstance(data, dd.DataFrame):\n",
    "            try:\n",
    "                from mlforecast.distributed.models.lgb import LGBMForecast\n",
    "                LGBM_INSTALLED = True\n",
    "            except ModuleNotFoundError:\n",
    "                LGBM_INSTALLED = False\n",
    "            try:\n",
    "                from mlforecast.distributed.models.xgb import XGBForecast\n",
    "                XGB_INSTALLED = True\n",
    "            except ModuleNotFoundError:\n",
    "                XGB_INSTALLED = False\n",
    "            X, y = prep[features], prep[target_col]\n",
    "            for name, model in self.models.items():\n",
    "                if not ((LGBM_INSTALLED and isinstance(model, LGBMForecast)) or (XGB_INSTALLED and isinstance(model, XGBForecast))):\n",
    "                    raise ValueError('Models must be either LGBMForecast or XGBForecast with dask backend.')\n",
    "                self.models_[name] = clone(model).fit(X, y).model_\n",
    "        else:\n",
    "            raise NotImplementedError('Only spark and dask engines are supported.')\n",
    "        return self\n",
    "    \n",
    "    def fit(\n",
    "        self,\n",
    "        data: fugue.AnyDataFrame,\n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,        \n",
    "    ) -> 'FugueMLForecast':\n",
    "        \"\"\"Apply the feature engineering and train the models.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : dask or spark DataFrame\n",
    "            Series data in long format.\n",
    "        id_col : str\n",
    "            Column that identifies each serie. If 'index' then the index is used.\n",
    "        time_col : str\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str\n",
    "            Column that contains the target.\n",
    "        static_features : list of str, optional (default=None)\n",
    "            Names of the features that are static and will be repeated when forecasting.\n",
    "        dropna : bool (default=True)\n",
    "            Drop rows with missing values produced by the transformations.\n",
    "        keep_last_n : int, optional (default=None)\n",
    "            Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : FugueMLForecast\n",
    "            Forecast object with series values and trained models.\n",
    "        \"\"\"        \n",
    "        return self._fit(\n",
    "            data,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            static_features=static_features,\n",
    "            dropna=dropna,\n",
    "            keep_last_n=keep_last_n,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _predict(\n",
    "        items: List[List[Any]],\n",
    "        models,        \n",
    "        horizon,\n",
    "        dynamic_dfs,\n",
    "        before_predict_callback,\n",
    "        after_predict_callback,\n",
    "    ) -> Iterable[pd.DataFrame]:\n",
    "        for serialized_ts, _, serialized_valid in items:\n",
    "            valid = cloudpickle.loads(serialized_valid)\n",
    "            ts = cloudpickle.loads(serialized_ts)\n",
    "            if valid is not None:\n",
    "                dynamic_features = valid.columns.drop(\n",
    "                    [ts.id_col, ts.time_col, ts.target_col]\n",
    "                )\n",
    "                if not dynamic_features.empty:\n",
    "                    dynamic_dfs = [valid.drop(columns=ts.target_col)]\n",
    "            res = ts.predict(\n",
    "                models=models,\n",
    "                horizon=horizon,\n",
    "                dynamic_dfs=dynamic_dfs,\n",
    "                before_predict_callback=before_predict_callback,\n",
    "                after_predict_callback=after_predict_callback,\n",
    "            ).reset_index()\n",
    "            if valid is not None:\n",
    "                res = res.merge(valid, how='left')\n",
    "            yield res\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        horizon: int,\n",
    "        dynamic_dfs: Optional[List[pd.DataFrame]] = None,\n",
    "        before_predict_callback: Optional[Callable] = None,\n",
    "        after_predict_callback: Optional[Callable] = None,\n",
    "    ) -> fugue.AnyDataFrame:\n",
    "        \"\"\"Compute the predictions for the next `horizon` steps.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        horizon : int\n",
    "            Number of periods to predict.\n",
    "        dynamic_dfs : list of pandas DataFrame, optional (default=None)\n",
    "            Future values of the dynamic features, e.g. prices.\n",
    "        before_predict_callback : callable, optional (default=None)\n",
    "            Function to call on the features before computing the predictions.\n",
    "                This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure.\n",
    "                The series identifier is on the index.\n",
    "        after_predict_callback : callable, optional (default=None)\n",
    "            Function to call on the predictions before updating the targets.\n",
    "                This function will take a pandas Series with the predictions and should return another one with the same structure.\n",
    "                The series identifier is on the index.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        result : dask or spark DataFrame\n",
    "            Predictions for each serie and timestep, with one column per model.\n",
    "        \"\"\"        \n",
    "        model_names = self.models.keys()\n",
    "        models_schema = ','.join(f'{model_name}:double' for model_name in model_names)\n",
    "        schema = f'{self.id_col}:string,{self.time_col}:datetime,' + models_schema\n",
    "        if getattr(self, '_n_windows', None) is not None:\n",
    "            schema += f',{self.target_col}:double'\n",
    "        res = fa.transform(\n",
    "            self.partition_results,\n",
    "            FugueMLForecast._predict,\n",
    "            params={\n",
    "                'models': self.models_,\n",
    "                'horizon': horizon,\n",
    "                'dynamic_dfs': dynamic_dfs,\n",
    "                'before_predict_callback': before_predict_callback,\n",
    "                'after_predict_callback': after_predict_callback,\n",
    "            },\n",
    "            schema=schema,\n",
    "            engine=self.engine,\n",
    "        )\n",
    "        return fa.get_native_as_df(res)\n",
    "\n",
    "    def cross_validation(\n",
    "        self,\n",
    "        data: fugue.AnyDataFrame,\n",
    "        n_windows: int,\n",
    "        window_size: int,\n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        step_size: Optional[int] = None, \n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "        before_predict_callback: Optional[Callable] = None,\n",
    "        after_predict_callback: Optional[Callable] = None,\n",
    "    ) -> Iterator[fugue.AnyDataFrame]:\n",
    "        \"\"\"Perform time series cross validation.\n",
    "        Creates `n_windows` splits where each window has `window_size` test periods,\n",
    "        trains the models, computes the predictions and merges the actuals.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : dask DataFrame\n",
    "            Series data in long format.\n",
    "        n_windows : int\n",
    "            Number of windows to evaluate.\n",
    "        window_size : int\n",
    "            Number of test periods in each window.\n",
    "        id_col : str\n",
    "            Column that identifies each serie. If 'index' then the index is used.\n",
    "        time_col : str\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str\n",
    "            Column that contains the target.\n",
    "        step_size : int, optional (default=None)\n",
    "            Step size between each cross validation window. If None it will be equal to `window_size`.\n",
    "        static_features : list of str, optional (default=None)\n",
    "            Names of the features that are static and will be repeated when forecasting.\n",
    "        dropna : bool (default=True)\n",
    "            Drop rows with missing values produced by the transformations.\n",
    "        keep_last_n : int, optional (default=None)\n",
    "            Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it.\n",
    "        before_predict_callback : callable, optional (default=None)\n",
    "            Function to call on the features before computing the predictions.\n",
    "                This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure.\n",
    "                The series identifier is on the index.\n",
    "        after_predict_callback : callable, optional (default=None)\n",
    "            Function to call on the predictions before updating the targets.\n",
    "                This function will take a pandas Series with the predictions and should return another one with the same structure.\n",
    "                The series identifier is on the index.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        result : dask or spark DataFrame\n",
    "            Predictions for each window with the series id, timestamp, target value and predictions from each model.\n",
    "        \"\"\"            \n",
    "        self.cv_models_ = []\n",
    "        for i in range(n_windows):\n",
    "            window_info = WindowInfo(n_windows, window_size, step_size, i)\n",
    "            self._fit(\n",
    "                data,\n",
    "                id_col=id_col,\n",
    "                time_col=time_col,\n",
    "                target_col=target_col,\n",
    "                static_features=static_features,\n",
    "                dropna=dropna,\n",
    "                keep_last_n=keep_last_n,\n",
    "                window_info=window_info,\n",
    "            )\n",
    "            self.cv_models_.append(self.models_)\n",
    "            preds = self.predict(\n",
    "                window_size,\n",
    "                before_predict_callback=before_predict_callback,\n",
    "                after_predict_callback=after_predict_callback,\n",
    "            )\n",
    "            yield preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e350d0b-8a4c-40e0-83be-7c1ef1bf5e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from window_ops.expanding import expanding_mean\n",
    "from mlforecast.utils import generate_daily_series, generate_prices_for_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520e5293-f536-4941-b11d-7414f88096be",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter('ignore', FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552cea5f-b826-4c9b-ae9f-2532f92f31bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>static_0</th>\n",
       "      <th>product_id</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-10-05</td>\n",
       "      <td>3.981198</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>0.856289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-10-06</td>\n",
       "      <td>10.327401</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>0.710628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-10-07</td>\n",
       "      <td>17.657474</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>0.418277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-10-08</td>\n",
       "      <td>25.898790</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>0.363121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-10-09</td>\n",
       "      <td>34.494040</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>0.201515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26998</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-10</td>\n",
       "      <td>45.340051</td>\n",
       "      <td>69</td>\n",
       "      <td>35</td>\n",
       "      <td>0.458545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26999</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-11</td>\n",
       "      <td>3.022948</td>\n",
       "      <td>69</td>\n",
       "      <td>35</td>\n",
       "      <td>0.881891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27000</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-12</td>\n",
       "      <td>10.131371</td>\n",
       "      <td>69</td>\n",
       "      <td>35</td>\n",
       "      <td>0.306065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27001</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-13</td>\n",
       "      <td>14.572434</td>\n",
       "      <td>69</td>\n",
       "      <td>35</td>\n",
       "      <td>0.932251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27002</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-14</td>\n",
       "      <td>22.816357</td>\n",
       "      <td>69</td>\n",
       "      <td>35</td>\n",
       "      <td>0.865276</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27003 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      unique_id         ds          y  static_0  product_id     price\n",
       "0         id_00 2000-10-05   3.981198        79          45  0.856289\n",
       "1         id_00 2000-10-06  10.327401        79          45  0.710628\n",
       "2         id_00 2000-10-07  17.657474        79          45  0.418277\n",
       "3         id_00 2000-10-08  25.898790        79          45  0.363121\n",
       "4         id_00 2000-10-09  34.494040        79          45  0.201515\n",
       "...         ...        ...        ...       ...         ...       ...\n",
       "26998     id_99 2001-05-10  45.340051        69          35  0.458545\n",
       "26999     id_99 2001-05-11   3.022948        69          35  0.881891\n",
       "27000     id_99 2001-05-12  10.131371        69          35  0.306065\n",
       "27001     id_99 2001-05-13  14.572434        69          35  0.932251\n",
       "27002     id_99 2001-05-14  22.816357        69          35  0.865276\n",
       "\n",
       "[27003 rows x 6 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series = (\n",
    "    generate_daily_series(100, n_static_features=2, static_as_categorical=False, equal_ends=True)\n",
    "    .reset_index()\n",
    "    .rename(columns={'static_1': 'product_id'})\n",
    ")\n",
    "prices = generate_prices_for_series(series, horizon=14)\n",
    "series = series.merge(prices, on=['product_id', 'ds'], how='left')\n",
    "series['unique_id'] = series['unique_id'].astype(str)\n",
    "series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bd1c31-9270-4176-b9b1-eaaaeae3cf28",
   "metadata": {},
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81348566-8200-419f-9921-c6b5b655652d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9713172-2263-48a5-99fd-f66b13117cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"MyApp\")\n",
    "    .config(\"spark.jars.packages\", \"com.microsoft.azure:synapseml_2.12:0.10.2\")\n",
    "    .config(\"spark.jars.repositories\", \"https://mmlspark.azureedge.net/maven\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "from synapse.ml.lightgbm import LightGBMRegressor as SynapseLGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9efe100-a3b8-4b23-93c5-8235cbe4e6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_series = spark.createDataFrame(series).repartitionByRange(4, 'unique_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071e6bb6-7cfe-4208-bae9-8e8b4cc0cfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost.spark import SparkXGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba790bc5-5a6d-426d-ad5e-74705f815725",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst = FugueMLForecast(\n",
    "    [\n",
    "        SynapseLGBMRegressor(),\n",
    "        SparkXGBRegressor()\n",
    "    ],\n",
    "    freq='D',\n",
    "    lags=[1],\n",
    "    lag_transforms={\n",
    "        1: [expanding_mean]\n",
    "    },\n",
    "    date_features=['dayofweek'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec77108e-0cc2-4774-9591-9e9480b5438b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[21:02:53] task 0 got new rank 0                                    (0 + 1) / 1]\n",
      "/home/jose/mambaforge/envs/mlforecast/lib/python3.10/site-packages/xgboost/sklearn.py:808: UserWarning: Loading a native XGBoost model with Scikit-Learn interface.\n",
      "  warnings.warn(\"Loading a native XGBoost model with Scikit-Learn interface.\")\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>LightGBMRegressor</th>\n",
       "      <th>SparkXGBRegressor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-15</td>\n",
       "      <td>42.227353</td>\n",
       "      <td>42.170174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-16</td>\n",
       "      <td>49.678239</td>\n",
       "      <td>49.665058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-17</td>\n",
       "      <td>1.425428</td>\n",
       "      <td>2.123648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-18</td>\n",
       "      <td>10.021719</td>\n",
       "      <td>10.150534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-19</td>\n",
       "      <td>18.167309</td>\n",
       "      <td>18.219870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-24</td>\n",
       "      <td>43.879492</td>\n",
       "      <td>43.089096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-25</td>\n",
       "      <td>1.592245</td>\n",
       "      <td>1.300191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-26</td>\n",
       "      <td>8.765065</td>\n",
       "      <td>8.563540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-27</td>\n",
       "      <td>15.720301</td>\n",
       "      <td>15.627042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-28</td>\n",
       "      <td>22.443045</td>\n",
       "      <td>23.139147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1400 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     unique_id         ds  LightGBMRegressor  SparkXGBRegressor\n",
       "0        id_00 2001-05-15          42.227353          42.170174\n",
       "1        id_00 2001-05-16          49.678239          49.665058\n",
       "2        id_00 2001-05-17           1.425428           2.123648\n",
       "3        id_00 2001-05-18          10.021719          10.150534\n",
       "4        id_00 2001-05-19          18.167309          18.219870\n",
       "...        ...        ...                ...                ...\n",
       "1395     id_99 2001-05-24          43.879492          43.089096\n",
       "1396     id_99 2001-05-25           1.592245           1.300191\n",
       "1397     id_99 2001-05-26           8.765065           8.563540\n",
       "1398     id_99 2001-05-27          15.720301          15.627042\n",
       "1399     id_99 2001-05-28          22.443045          23.139147\n",
       "\n",
       "[1400 rows x 4 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcst.fit(\n",
    "    spark_series,\n",
    "    id_col='unique_id',\n",
    "    time_col='ds',\n",
    "    target_col='y',\n",
    "    static_features=['static_0', 'product_id'],\n",
    ")\n",
    "fcst.predict(14, dynamic_dfs=[prices]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05d12b6-67f7-4a57-b341-b9b8d8400c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[21:03:12] task 0 got new rank 0                                    (0 + 1) / 1]\n",
      "/home/jose/mambaforge/envs/mlforecast/lib/python3.10/site-packages/xgboost/sklearn.py:808: UserWarning: Loading a native XGBoost model with Scikit-Learn interface.\n",
      "  warnings.warn(\"Loading a native XGBoost model with Scikit-Learn interface.\")\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>LightGBMRegressor</th>\n",
       "      <th>SparkXGBRegressor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-04-17</td>\n",
       "      <td>41.397289</td>\n",
       "      <td>41.412704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-04-18</td>\n",
       "      <td>50.008758</td>\n",
       "      <td>49.848236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-04-19</td>\n",
       "      <td>1.868972</td>\n",
       "      <td>1.531837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-04-20</td>\n",
       "      <td>10.266771</td>\n",
       "      <td>9.750021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-04-21</td>\n",
       "      <td>18.296489</td>\n",
       "      <td>17.535915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-04-26</td>\n",
       "      <td>43.910674</td>\n",
       "      <td>43.634216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-04-27</td>\n",
       "      <td>1.955158</td>\n",
       "      <td>1.962174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-04-28</td>\n",
       "      <td>8.864172</td>\n",
       "      <td>8.601549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-04-29</td>\n",
       "      <td>15.983674</td>\n",
       "      <td>15.883894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-04-30</td>\n",
       "      <td>22.950702</td>\n",
       "      <td>23.148775</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1400 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     unique_id         ds  LightGBMRegressor  SparkXGBRegressor\n",
       "0        id_00 2001-04-17          41.397289          41.412704\n",
       "1        id_00 2001-04-18          50.008758          49.848236\n",
       "2        id_00 2001-04-19           1.868972           1.531837\n",
       "3        id_00 2001-04-20          10.266771           9.750021\n",
       "4        id_00 2001-04-21          18.296489          17.535915\n",
       "...        ...        ...                ...                ...\n",
       "1395     id_99 2001-04-26          43.910674          43.634216\n",
       "1396     id_99 2001-04-27           1.955158           1.962174\n",
       "1397     id_99 2001-04-28           8.864172           8.601549\n",
       "1398     id_99 2001-04-29          15.983674          15.883894\n",
       "1399     id_99 2001-04-30          22.950702          23.148775\n",
       "\n",
       "[1400 rows x 4 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_res = fcst.cross_validation(spark_series, n_windows=2, window_size=14, id_col='unique_id', time_col='ds', target_col='y')\n",
    "res1 = next(cv_res)\n",
    "res1.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a734f48-fb9b-4d08-9548-ca0b53661563",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[21:03:31] task 0 got new rank 0                                    (0 + 1) / 1]\n",
      "/home/jose/mambaforge/envs/mlforecast/lib/python3.10/site-packages/xgboost/sklearn.py:808: UserWarning: Loading a native XGBoost model with Scikit-Learn interface.\n",
      "  warnings.warn(\"Loading a native XGBoost model with Scikit-Learn interface.\")\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>LightGBMRegressor</th>\n",
       "      <th>SparkXGBRegressor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-01</td>\n",
       "      <td>41.510874</td>\n",
       "      <td>41.991829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-02</td>\n",
       "      <td>49.580176</td>\n",
       "      <td>49.946548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-03</td>\n",
       "      <td>1.699062</td>\n",
       "      <td>1.733775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-04</td>\n",
       "      <td>10.143278</td>\n",
       "      <td>9.953071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-05</td>\n",
       "      <td>18.001024</td>\n",
       "      <td>17.789112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-10</td>\n",
       "      <td>43.846853</td>\n",
       "      <td>43.703159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-11</td>\n",
       "      <td>1.832246</td>\n",
       "      <td>1.999386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-12</td>\n",
       "      <td>8.805302</td>\n",
       "      <td>8.618651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-13</td>\n",
       "      <td>15.946444</td>\n",
       "      <td>15.728466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-14</td>\n",
       "      <td>22.699001</td>\n",
       "      <td>23.113226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1400 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     unique_id         ds  LightGBMRegressor  SparkXGBRegressor\n",
       "0        id_00 2001-05-01          41.510874          41.991829\n",
       "1        id_00 2001-05-02          49.580176          49.946548\n",
       "2        id_00 2001-05-03           1.699062           1.733775\n",
       "3        id_00 2001-05-04          10.143278           9.953071\n",
       "4        id_00 2001-05-05          18.001024          17.789112\n",
       "...        ...        ...                ...                ...\n",
       "1395     id_99 2001-05-10          43.846853          43.703159\n",
       "1396     id_99 2001-05-11           1.832246           1.999386\n",
       "1397     id_99 2001-05-12           8.805302           8.618651\n",
       "1398     id_99 2001-05-13          15.946444          15.728466\n",
       "1399     id_99 2001-05-14          22.699001          23.113226\n",
       "\n",
       "[1400 rows x 4 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res2 = next(cv_res)\n",
    "res2.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf3322f-3a0f-47d8-92f2-689b525111aa",
   "metadata": {},
   "source": [
    "## Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2228d0-e004-4e40-b413-314b54c36056",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "from mlforecast.distributed.models.lgb import LGBMForecast\n",
    "from mlforecast.distributed.models.xgb import XGBForecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18a5876-b898-4b48-940c-d5046397c23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(n_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163e9f0d-9d2e-41f6-a801-8f16395adb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_series = (\n",
    "    dd\n",
    "    .from_pandas(series.set_index('unique_id'), npartitions=4)  # make sure we split by the series identifier\n",
    "    .map_partitions(lambda df: df.reset_index())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f96a1f8-0b77-4327-a5d7-28bc69873466",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst = FugueMLForecast(\n",
    "    [LGBMForecast(), XGBForecast()],\n",
    "    freq='D',\n",
    "    lags=[1],\n",
    "    lag_transforms={\n",
    "        1: [expanding_mean]\n",
    "    },\n",
    "    date_features=['dayofweek'],\n",
    "    engine=client,\n",
    ")\n",
    "_ = fcst.fit(dask_series, id_col='unique_id', time_col='ds', target_col='y', static_features=['static_0', 'product_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fffef75-fcfc-4014-8ac0-e8dcf09c9b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>LGBMForecast</th>\n",
       "      <th>XGBForecast</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-15</td>\n",
       "      <td>42.328150</td>\n",
       "      <td>42.544678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-16</td>\n",
       "      <td>49.966854</td>\n",
       "      <td>50.006393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-17</td>\n",
       "      <td>1.614102</td>\n",
       "      <td>2.404574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-18</td>\n",
       "      <td>10.246828</td>\n",
       "      <td>10.144515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-19</td>\n",
       "      <td>18.183167</td>\n",
       "      <td>17.768204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-24</td>\n",
       "      <td>42.986614</td>\n",
       "      <td>43.913208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-25</td>\n",
       "      <td>1.536043</td>\n",
       "      <td>1.769517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-26</td>\n",
       "      <td>8.651628</td>\n",
       "      <td>8.689207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-27</td>\n",
       "      <td>15.467835</td>\n",
       "      <td>15.858318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-28</td>\n",
       "      <td>22.620816</td>\n",
       "      <td>22.818695</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1400 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    unique_id         ds  LGBMForecast  XGBForecast\n",
       "0       id_00 2001-05-15     42.328150    42.544678\n",
       "1       id_00 2001-05-16     49.966854    50.006393\n",
       "2       id_00 2001-05-17      1.614102     2.404574\n",
       "3       id_00 2001-05-18     10.246828    10.144515\n",
       "4       id_00 2001-05-19     18.183167    17.768204\n",
       "..        ...        ...           ...          ...\n",
       "345     id_99 2001-05-24     42.986614    43.913208\n",
       "346     id_99 2001-05-25      1.536043     1.769517\n",
       "347     id_99 2001-05-26      8.651628     8.689207\n",
       "348     id_99 2001-05-27     15.467835    15.858318\n",
       "349     id_99 2001-05-28     22.620816    22.818695\n",
       "\n",
       "[1400 rows x 4 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcst.predict(14, dynamic_dfs=[prices]).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a9d631-4e90-49be-9a1a-fca5ba9008e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_res = fcst.cross_validation(dask_series, n_windows=2, window_size=14, id_col='unique_id', time_col='ds', target_col='y')\n",
    "res1 = next(cv_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06c89e7-6747-4537-b79b-57027c16d5c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>LGBMForecast</th>\n",
       "      <th>XGBForecast</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-04-17</td>\n",
       "      <td>41.101257</td>\n",
       "      <td>41.509098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-04-18</td>\n",
       "      <td>49.439778</td>\n",
       "      <td>49.994793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-04-19</td>\n",
       "      <td>2.209694</td>\n",
       "      <td>1.885389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-04-20</td>\n",
       "      <td>10.016894</td>\n",
       "      <td>9.791873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-04-21</td>\n",
       "      <td>18.006730</td>\n",
       "      <td>17.518440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-04-26</td>\n",
       "      <td>44.320700</td>\n",
       "      <td>42.248009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-04-27</td>\n",
       "      <td>2.230125</td>\n",
       "      <td>2.526252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-04-28</td>\n",
       "      <td>8.579381</td>\n",
       "      <td>8.485373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-04-29</td>\n",
       "      <td>15.496969</td>\n",
       "      <td>15.847349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-04-30</td>\n",
       "      <td>22.584693</td>\n",
       "      <td>23.249107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1400 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    unique_id         ds  LGBMForecast  XGBForecast\n",
       "0       id_00 2001-04-17     41.101257    41.509098\n",
       "1       id_00 2001-04-18     49.439778    49.994793\n",
       "2       id_00 2001-04-19      2.209694     1.885389\n",
       "3       id_00 2001-04-20     10.016894     9.791873\n",
       "4       id_00 2001-04-21     18.006730    17.518440\n",
       "..        ...        ...           ...          ...\n",
       "345     id_99 2001-04-26     44.320700    42.248009\n",
       "346     id_99 2001-04-27      2.230125     2.526252\n",
       "347     id_99 2001-04-28      8.579381     8.485373\n",
       "348     id_99 2001-04-29     15.496969    15.847349\n",
       "349     id_99 2001-04-30     22.584693    23.249107\n",
       "\n",
       "[1400 rows x 4 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res1.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9c81ac-d5c4-4388-8159-e61d6b2357d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = next(cv_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d253c880-c025-4d8e-8944-a8ef8539b53a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>LGBMForecast</th>\n",
       "      <th>XGBForecast</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-01</td>\n",
       "      <td>42.610164</td>\n",
       "      <td>41.107208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-02</td>\n",
       "      <td>50.179124</td>\n",
       "      <td>50.424072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-03</td>\n",
       "      <td>1.690276</td>\n",
       "      <td>1.991987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-04</td>\n",
       "      <td>10.159849</td>\n",
       "      <td>9.895548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-05</td>\n",
       "      <td>18.321141</td>\n",
       "      <td>17.538578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-10</td>\n",
       "      <td>42.920872</td>\n",
       "      <td>43.232620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-11</td>\n",
       "      <td>1.932801</td>\n",
       "      <td>1.821584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-12</td>\n",
       "      <td>8.724589</td>\n",
       "      <td>8.611847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-13</td>\n",
       "      <td>15.355793</td>\n",
       "      <td>15.651299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-14</td>\n",
       "      <td>22.728723</td>\n",
       "      <td>23.504103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1400 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    unique_id         ds  LGBMForecast  XGBForecast\n",
       "0       id_00 2001-05-01     42.610164    41.107208\n",
       "1       id_00 2001-05-02     50.179124    50.424072\n",
       "2       id_00 2001-05-03      1.690276     1.991987\n",
       "3       id_00 2001-05-04     10.159849     9.895548\n",
       "4       id_00 2001-05-05     18.321141    17.538578\n",
       "..        ...        ...           ...          ...\n",
       "345     id_99 2001-05-10     42.920872    43.232620\n",
       "346     id_99 2001-05-11      1.932801     1.821584\n",
       "347     id_99 2001-05-12      8.724589     8.611847\n",
       "348     id_99 2001-05-13     15.355793    15.651299\n",
       "349     id_99 2001-05-14     22.728723    23.504103\n",
       "\n",
       "[1400 rows x 4 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res2.compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
