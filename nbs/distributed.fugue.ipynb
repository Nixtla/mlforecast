{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf5b1d7-bc13-4ed3-af30-f15cacc861f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp distributed.fugue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc9a984-a442-41d2-8132-471d73e64d96",
   "metadata": {},
   "source": [
    "# Fugue\n",
    "\n",
    "> Distributed fugue backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8108c14-6d27-4dfd-b3e8-a2b315eb5f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import copy\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Iterable, List, Optional\n",
    "\n",
    "import cloudpickle\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "from fugue import transform\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "from mlforecast.core import (\n",
    "    DateFeature,\n",
    "    Differences,\n",
    "    Freq,\n",
    "    LagTransforms,\n",
    "    Lags,\n",
    "    TimeSeries,\n",
    "    _name_models,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227f19b4-4582-4c51-94cd-c7c220d35dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class FugueMLForecast:\n",
    "    def __init__(\n",
    "        self,\n",
    "        models,\n",
    "        freq: Optional[Freq] = None,\n",
    "        lags: Optional[Lags] = None,\n",
    "        lag_transforms: Optional[LagTransforms] = None,\n",
    "        date_features: Optional[Iterable[DateFeature]] = None,\n",
    "        differences: Optional[Differences] = None,\n",
    "        num_threads: int = 1,\n",
    "        engine = None,\n",
    "    ):\n",
    "        if not isinstance(models, dict) and not isinstance(models, list):\n",
    "            models = [models]\n",
    "        if isinstance(models, list):\n",
    "            model_names = _name_models([m.__class__.__name__ for m in models])\n",
    "            models_with_names = dict(zip(model_names, models))\n",
    "        else:\n",
    "            models_with_names = models\n",
    "        self.models = models_with_names\n",
    "        self._base_ts = TimeSeries(\n",
    "            freq, lags, lag_transforms, date_features, differences, num_threads\n",
    "        )\n",
    "        self.engine = engine\n",
    "        \n",
    "    def __repr__(self) -> str:\n",
    "        return (\n",
    "            f'{self.__class__.__name__}(models=[{\", \".join(self.models.keys())}], '\n",
    "            f\"freq={self._base_ts.freq}, \"\n",
    "            f\"lag_features={list(self._base_ts.transforms.keys())}, \"\n",
    "            f\"date_features={self._base_ts.date_features}, \"\n",
    "            f\"num_threads={self._base_ts.num_threads}, \"\n",
    "            f\"engine={self.engine})\"\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _preprocess_partition(\n",
    "        part: pd.DataFrame,\n",
    "        base_ts: TimeSeries,        \n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,        \n",
    "    ) -> List[List[Any]]:\n",
    "        ts = copy.deepcopy(base_ts)\n",
    "        transformed = ts.fit_transform(\n",
    "            part,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            static_features=static_features,\n",
    "            dropna=dropna,\n",
    "            keep_last_n=keep_last_n,\n",
    "        )\n",
    "        return [[cloudpickle.dumps(ts), cloudpickle.dumps(transformed)]]\n",
    "    \n",
    "    @staticmethod\n",
    "    def _retrieve_df(items: List[List[Any]]) -> Iterable[pd.DataFrame]:\n",
    "        for _, serialized_df in items:\n",
    "            yield cloudpickle.loads(serialized_df)\n",
    "\n",
    "    def preprocess(\n",
    "        self,\n",
    "        data,\n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "    ):\n",
    "        self.id_col = id_col\n",
    "        self.time_col = time_col\n",
    "        self.target_col = target_col\n",
    "        self.partition_results = transform(\n",
    "            data,\n",
    "            FugueMLForecast._preprocess_partition,\n",
    "            params={\n",
    "                'base_ts': self._base_ts,\n",
    "                'id_col': id_col,\n",
    "                'time_col': time_col,\n",
    "                'target_col': target_col,\n",
    "                'static_features': static_features,\n",
    "                'dropna': dropna,\n",
    "                'keep_last_n': keep_last_n,\n",
    "            },\n",
    "            schema='ts:binary,df:binary',\n",
    "            engine=self.engine,\n",
    "        )\n",
    "        base_schema = f'{id_col}:string,{time_col}:datetime,{target_col}:double'\n",
    "        features_dtypes = [f'{feat}:double' for feat in self._base_ts.features]        \n",
    "        schema = base_schema + ',' + ','.join(features_dtypes)\n",
    "        return transform(\n",
    "            self.partition_results,\n",
    "            FugueMLForecast._retrieve_df,\n",
    "            schema=schema,\n",
    "            engine=self.engine,\n",
    "        )\n",
    "    \n",
    "    def fit(\n",
    "        self,\n",
    "        data,\n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "    ):\n",
    "        preprocessed = self.preprocess(\n",
    "            data,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            static_features=static_features,\n",
    "            dropna=dropna,\n",
    "            keep_last_n=keep_last_n,\n",
    "        )\n",
    "        feature_cols = [x for x in preprocessed.columns if x not in (id_col, time_col, target_col)]\n",
    "        featurizer = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "        train_data = featurizer.transform(preprocessed)[target_col, \"features\"]\n",
    "        for name, model in self.models.items():\n",
    "            trained_model = model.setLabelCol(target_col).fit(train_data)\n",
    "            # horrible way to get a lightgbm booster, please look away\n",
    "            with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "                trained_model.saveNativeModel(tmp_dir)\n",
    "                tmp_path = Path(tmp_dir)\n",
    "                txt_file = next(tmp_path.glob('*.txt'))\n",
    "                bst = lgb.Booster(model_file=txt_file)\n",
    "            self.models[name] = bst\n",
    "        return self\n",
    "\n",
    "    @staticmethod\n",
    "    def _predict(\n",
    "        items: List[List[Any]],\n",
    "        models,        \n",
    "        horizon,\n",
    "        dynamic_dfs,\n",
    "        before_predict_callback,\n",
    "        after_predict_callback,\n",
    "    ) -> Iterable[pd.DataFrame]:\n",
    "        for serialized_ts, _ in items:\n",
    "            ts = cloudpickle.loads(serialized_ts)\n",
    "            res = ts.predict(\n",
    "                models=models,\n",
    "                horizon=horizon,\n",
    "                dynamic_dfs=dynamic_dfs,\n",
    "                before_predict_callback=before_predict_callback,\n",
    "                after_predict_callback=after_predict_callback,\n",
    "            )\n",
    "            yield res.reset_index()\n",
    "            \n",
    "    def predict(\n",
    "        self,\n",
    "        horizon: int,\n",
    "        dynamic_dfs: Optional[List[pd.DataFrame]] = None,\n",
    "        before_predict_callback: Optional[Callable] = None,\n",
    "        after_predict_callback: Optional[Callable] = None,\n",
    "    ):\n",
    "        model_names = self.models.keys()\n",
    "        models_schema = ','.join(f'{model_name}:double' for model_name in model_names)\n",
    "        schema = f'{self.id_col}:string,{self.time_col}:datetime,' + models_schema\n",
    "        return transform(\n",
    "            self.partition_results,\n",
    "            FugueMLForecast._predict,\n",
    "            params={\n",
    "                'models': self.models,\n",
    "                'horizon': horizon,\n",
    "                'dynamic_dfs': dynamic_dfs,\n",
    "                'before_predict_callback': before_predict_callback,\n",
    "                'after_predict_callback': after_predict_callback,\n",
    "            },\n",
    "            schema=schema,\n",
    "            engine=self.engine,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e350d0b-8a4c-40e0-83be-7c1ef1bf5e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore', FutureWarning)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from window_ops.expanding import expanding_mean\n",
    "\n",
    "from mlforecast.utils import generate_daily_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9713172-2263-48a5-99fd-f66b13117cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"MyApp\")\n",
    "    .config(\"spark.jars.packages\", \"com.microsoft.azure:synapseml_2.12:0.10.2\")\n",
    "    .config(\"spark.jars.repositories\", \"https://mmlspark.azureedge.net/maven\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9efe100-a3b8-4b23-93c5-8235cbe4e6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "series = generate_daily_series(100).reset_index()\n",
    "series['unique_id'] = series['unique_id'].astype(str)\n",
    "spark_series = spark.createDataFrame(series).repartitionByRange(4, 'unique_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbde9e6-5eee-428d-a825-c7f4743387d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from synapse.ml.lightgbm import LightGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a972a6-80c5-40f5-b265-6ea48b083a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst = FugueMLForecast(\n",
    "    [LightGBMRegressor()],\n",
    "    freq='D',\n",
    "    lags=[1],\n",
    "    lag_transforms={\n",
    "        1: [expanding_mean]\n",
    "    },\n",
    "    date_features=['dayofweek'],\n",
    "    engine=spark,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48f00a1-a45e-41d1-b929-13c6523cb228",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FugueMLForecast(models=[LightGBMRegressor], freq=<Day>, lag_features=['lag1', 'expanding_mean_lag1'], date_features=['dayofweek'], num_threads=1, engine=<pyspark.sql.session.SparkSession object>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcst.fit(spark_series, id_col='unique_id', time_col='ds', target_col='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9dbaed-1042-4333-ad47-d6b8a549fed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>LightGBMRegressor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-08-10</td>\n",
       "      <td>5.263606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-08-11</td>\n",
       "      <td>6.255906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-08-12</td>\n",
       "      <td>0.263115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-08-13</td>\n",
       "      <td>1.258064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-08-14</td>\n",
       "      <td>2.249121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2000-07-05</td>\n",
       "      <td>2.249115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2000-07-06</td>\n",
       "      <td>3.241592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2000-07-07</td>\n",
       "      <td>4.246263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2000-07-08</td>\n",
       "      <td>5.260917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2000-07-09</td>\n",
       "      <td>6.247499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    unique_id         ds  LightGBMRegressor\n",
       "0       id_00 2000-08-10           5.263606\n",
       "1       id_00 2000-08-11           6.255906\n",
       "2       id_00 2000-08-12           0.263115\n",
       "3       id_00 2000-08-13           1.258064\n",
       "4       id_00 2000-08-14           2.249121\n",
       "..        ...        ...                ...\n",
       "995     id_99 2000-07-05           2.249115\n",
       "996     id_99 2000-07-06           3.241592\n",
       "997     id_99 2000-07-07           4.246263\n",
       "998     id_99 2000-07-08           5.260917\n",
       "999     id_99 2000-07-09           6.247499\n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcst.predict(10).toPandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
