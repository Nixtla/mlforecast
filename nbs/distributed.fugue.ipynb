{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf5b1d7-bc13-4ed3-af30-f15cacc861f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp distributed.fugue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc9a984-a442-41d2-8132-471d73e64d96",
   "metadata": {},
   "source": [
    "# Fugue\n",
    "\n",
    "> Distributed fugue backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8108c14-6d27-4dfd-b3e8-a2b315eb5f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import copy\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Iterable, List, Optional\n",
    "\n",
    "import cloudpickle\n",
    "try:\n",
    "    import dask.dataframe as dd\n",
    "    DASK_INSTALLED = True\n",
    "except ModuleNotFoundError:\n",
    "    DASK_INSTALLED = False\n",
    "import fugue.api as fa\n",
    "import pandas as pd\n",
    "try:\n",
    "    from pyspark.ml.feature import VectorAssembler\n",
    "    from pyspark.sql import DataFrame as SparkDataFrame\n",
    "    SPARK_INSTALLED = True\n",
    "except ModuleNotFoundError:\n",
    "    SPARK_INSTALLED = False\n",
    "from sklearn.base import clone\n",
    "\n",
    "from mlforecast.core import (\n",
    "    DateFeature,\n",
    "    Differences,\n",
    "    Freq,\n",
    "    LagTransforms,\n",
    "    Lags,\n",
    "    TimeSeries,\n",
    "    _name_models,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227f19b4-4582-4c51-94cd-c7c220d35dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class FugueMLForecast:\n",
    "    def __init__(\n",
    "        self,\n",
    "        models,\n",
    "        freq: Optional[Freq] = None,\n",
    "        lags: Optional[Lags] = None,\n",
    "        lag_transforms: Optional[LagTransforms] = None,\n",
    "        date_features: Optional[Iterable[DateFeature]] = None,\n",
    "        differences: Optional[Differences] = None,\n",
    "        num_threads: int = 1,\n",
    "        engine = None,\n",
    "    ):\n",
    "        if not isinstance(models, dict) and not isinstance(models, list):\n",
    "            models = [models]\n",
    "        if isinstance(models, list):\n",
    "            model_names = _name_models([m.__class__.__name__ for m in models])\n",
    "            models_with_names = dict(zip(model_names, models))\n",
    "        else:\n",
    "            models_with_names = models\n",
    "        self.models = models_with_names\n",
    "        self._base_ts = TimeSeries(\n",
    "            freq, lags, lag_transforms, date_features, differences, num_threads\n",
    "        )\n",
    "        self.engine = engine\n",
    "        \n",
    "    def __repr__(self) -> str:\n",
    "        return (\n",
    "            f'{self.__class__.__name__}(models=[{\", \".join(self.models.keys())}], '\n",
    "            f\"freq={self._base_ts.freq}, \"\n",
    "            f\"lag_features={list(self._base_ts.transforms.keys())}, \"\n",
    "            f\"date_features={self._base_ts.date_features}, \"\n",
    "            f\"num_threads={self._base_ts.num_threads}, \"\n",
    "            f\"engine={self.engine})\"\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _preprocess_partition(\n",
    "        part: pd.DataFrame,\n",
    "        base_ts: TimeSeries,        \n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,        \n",
    "    ) -> List[List[Any]]:\n",
    "        ts = copy.deepcopy(base_ts)\n",
    "        transformed = ts.fit_transform(\n",
    "            part,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            static_features=static_features,\n",
    "            dropna=dropna,\n",
    "            keep_last_n=keep_last_n,\n",
    "        )\n",
    "        return [[cloudpickle.dumps(ts), cloudpickle.dumps(transformed)]]\n",
    "    \n",
    "    @staticmethod\n",
    "    def _retrieve_df(items: List[List[Any]]) -> Iterable[pd.DataFrame]:\n",
    "        for _, serialized_df in items:\n",
    "            yield cloudpickle.loads(serialized_df)\n",
    "\n",
    "    def preprocess(\n",
    "        self,\n",
    "        data,\n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "    ):\n",
    "        self.id_col = id_col\n",
    "        self.time_col = time_col\n",
    "        self.target_col = target_col\n",
    "        self.partition_results = fa.transform(\n",
    "            data,\n",
    "            FugueMLForecast._preprocess_partition,\n",
    "            params={\n",
    "                'base_ts': self._base_ts,\n",
    "                'id_col': id_col,\n",
    "                'time_col': time_col,\n",
    "                'target_col': target_col,\n",
    "                'static_features': static_features,\n",
    "                'dropna': dropna,\n",
    "                'keep_last_n': keep_last_n,\n",
    "            },\n",
    "            schema='ts:binary,df:binary',\n",
    "            engine=self.engine,\n",
    "            as_fugue=True,\n",
    "        )\n",
    "        base_schema = fa.get_schema(data[[id_col, time_col, target_col]])\n",
    "        features_dtypes = [f'{feat}:double' for feat in self._base_ts.features]        \n",
    "        schema = str(base_schema) + ',' + ','.join(features_dtypes)\n",
    "        res = fa.transform(\n",
    "            self.partition_results,\n",
    "            FugueMLForecast._retrieve_df,\n",
    "            schema=schema,\n",
    "            engine=self.engine,\n",
    "        )\n",
    "        return fa.get_native_as_df(res)\n",
    "    \n",
    "    def fit(\n",
    "        self,\n",
    "        data,\n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "    ):\n",
    "        prep = self.preprocess(\n",
    "            data,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            static_features=static_features,\n",
    "            dropna=dropna,\n",
    "            keep_last_n=keep_last_n,\n",
    "        )\n",
    "        features = [x for x in prep.columns if x not in {id_col, time_col, target_col}]\n",
    "        self.models_ = {}\n",
    "        if SPARK_INSTALLED and isinstance(data, SparkDataFrame):\n",
    "            try:\n",
    "                import lightgbm as lgb\n",
    "                from synapse.ml.lightgbm import LightGBMRegressor as SynapseLGBMRegressor\n",
    "                LGBM_INSTALLED = True\n",
    "            except ModuleNotFoundError:\n",
    "                SynapseLGBMRegressor = object\n",
    "                LGBM_INSTALLED = False\n",
    "            try:\n",
    "                import xgboost as xgb\n",
    "                from xgboost.spark import SparkXGBRegressor\n",
    "                XGBOOST_INSTALLED = True\n",
    "            except ModuleNotFoundError:\n",
    "                SparkXGBRegressor = object\n",
    "                XGBOOST_INSTALLED = False\n",
    "\n",
    "            featurizer = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "            train_data = featurizer.transform(prep)[target_col, \"features\"]\n",
    "            for name, model in self.models.items():\n",
    "                if LGBM_INSTALLED and isinstance(model, SynapseLGBMRegressor):\n",
    "                    trained_model = model.setLabelCol(target_col).fit(train_data)\n",
    "                    model_str = trained_model.getNativeModel()\n",
    "                    local_model = lgb.Booster(model_str=model_str)                    \n",
    "                elif XGBOOST_INSTALLED and isinstance(model, SparkXGBRegressor):\n",
    "                    model.setParams(label_col=target_col)\n",
    "                    trained_model = model.fit(train_data)\n",
    "                    model_str = trained_model.get_booster().save_raw('ubj')\n",
    "                    local_model = xgb.XGBRegressor()\n",
    "                    local_model.load_model(model_str)\n",
    "                else:\n",
    "                    raise ValueError('Only LightGBMRegressor from SynapseML and SparkXGBRegressor are supported in spark.')\n",
    "                self.models_[name] = local_model\n",
    "        elif DASK_INSTALLED and isinstance(data, dd.DataFrame):\n",
    "            try:\n",
    "                from mlforecast.distributed.models.lgb import LGBMForecast\n",
    "            except ModuleNotFoundError:\n",
    "                LGBMForecast = object\n",
    "            try:\n",
    "                from mlforecast.distributed.models.xgb import XGBForecast\n",
    "            except ModuleNotFoundError:\n",
    "                XGBForecast = object                \n",
    "            X, y = prep[features], prep[target_col]\n",
    "            for name, model in self.models.items():\n",
    "                if not isinstance(model, (LGBMForecast, XGBForecast)):\n",
    "                    raise ValueError('Models must be either LGBMForecast or XGBForecast with dask backend.')\n",
    "                self.models_[name] = clone(model).fit(X, y).model_\n",
    "        else:\n",
    "            raise NotImplementedError('Only spark and dask engines are supported.')\n",
    "        return self\n",
    "\n",
    "    @staticmethod\n",
    "    def _predict(\n",
    "        items: List[List[Any]],\n",
    "        models,        \n",
    "        horizon,\n",
    "        dynamic_dfs,\n",
    "        before_predict_callback,\n",
    "        after_predict_callback,\n",
    "    ) -> Iterable[pd.DataFrame]:\n",
    "        for serialized_ts, _ in items:\n",
    "            ts = cloudpickle.loads(serialized_ts)\n",
    "            res = ts.predict(\n",
    "                models=models,\n",
    "                horizon=horizon,\n",
    "                dynamic_dfs=dynamic_dfs,\n",
    "                before_predict_callback=before_predict_callback,\n",
    "                after_predict_callback=after_predict_callback,\n",
    "            )\n",
    "            yield res.reset_index()\n",
    "            \n",
    "    def predict(\n",
    "        self,\n",
    "        horizon: int,\n",
    "        dynamic_dfs: Optional[List[pd.DataFrame]] = None,\n",
    "        before_predict_callback: Optional[Callable] = None,\n",
    "        after_predict_callback: Optional[Callable] = None,\n",
    "    ):\n",
    "        model_names = self.models.keys()\n",
    "        models_schema = ','.join(f'{model_name}:double' for model_name in model_names)\n",
    "        schema = f'{self.id_col}:string,{self.time_col}:datetime,' + models_schema\n",
    "        return fa.transform(\n",
    "            self.partition_results,\n",
    "            FugueMLForecast._predict,\n",
    "            params={\n",
    "                'models': self.models_,\n",
    "                'horizon': horizon,\n",
    "                'dynamic_dfs': dynamic_dfs,\n",
    "                'before_predict_callback': before_predict_callback,\n",
    "                'after_predict_callback': after_predict_callback,\n",
    "            },\n",
    "            schema=schema,\n",
    "            engine=self.engine,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e350d0b-8a4c-40e0-83be-7c1ef1bf5e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore', FutureWarning)\n",
    "\n",
    "from window_ops.expanding import expanding_mean\n",
    "from mlforecast.utils import generate_daily_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552cea5f-b826-4c9b-ae9f-2532f92f31bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "series = generate_daily_series(100).reset_index()\n",
    "series['unique_id'] = series['unique_id'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bd1c31-9270-4176-b9b1-eaaaeae3cf28",
   "metadata": {},
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81348566-8200-419f-9921-c6b5b655652d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9713172-2263-48a5-99fd-f66b13117cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"MyApp\")\n",
    "    .config(\"spark.jars.packages\", \"com.microsoft.azure:synapseml_2.12:0.10.2\")\n",
    "    .config(\"spark.jars.repositories\", \"https://mmlspark.azureedge.net/maven\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9efe100-a3b8-4b23-93c5-8235cbe4e6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_series = spark.createDataFrame(series).repartitionByRange(4, 'unique_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071e6bb6-7cfe-4208-bae9-8e8b4cc0cfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from synapse.ml.lightgbm import LightGBMRegressor as SynapseLGBMRegressor\n",
    "from xgboost.spark import SparkXGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db855cb0-0ebe-4f99-b3b8-eeb6fb6dca22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[22:52:33] task 0 got new rank 0                                    (0 + 1) / 1]\n",
      "/home/jose/mambaforge/envs/mlforecast/lib/python3.10/site-packages/xgboost/sklearn.py:808: UserWarning: Loading a native XGBoost model with Scikit-Learn interface.\n",
      "  warnings.warn(\"Loading a native XGBoost model with Scikit-Learn interface.\")\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>LightGBMRegressor</th>\n",
       "      <th>SparkXGBRegressor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-08-10</td>\n",
       "      <td>5.263606</td>\n",
       "      <td>5.291951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-08-11</td>\n",
       "      <td>6.255906</td>\n",
       "      <td>6.277210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-08-12</td>\n",
       "      <td>0.263115</td>\n",
       "      <td>0.274654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-08-13</td>\n",
       "      <td>1.258064</td>\n",
       "      <td>1.216802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-08-14</td>\n",
       "      <td>2.249121</td>\n",
       "      <td>2.268891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2000-07-09</td>\n",
       "      <td>6.247499</td>\n",
       "      <td>6.249135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2000-07-10</td>\n",
       "      <td>0.229369</td>\n",
       "      <td>0.325171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2000-07-11</td>\n",
       "      <td>1.241571</td>\n",
       "      <td>1.234871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2000-07-12</td>\n",
       "      <td>2.249115</td>\n",
       "      <td>2.226775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2000-07-13</td>\n",
       "      <td>3.252963</td>\n",
       "      <td>3.277925</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1400 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     unique_id         ds  LightGBMRegressor  SparkXGBRegressor\n",
       "0        id_00 2000-08-10           5.263606           5.291951\n",
       "1        id_00 2000-08-11           6.255906           6.277210\n",
       "2        id_00 2000-08-12           0.263115           0.274654\n",
       "3        id_00 2000-08-13           1.258064           1.216802\n",
       "4        id_00 2000-08-14           2.249121           2.268891\n",
       "...        ...        ...                ...                ...\n",
       "1395     id_99 2000-07-09           6.247499           6.249135\n",
       "1396     id_99 2000-07-10           0.229369           0.325171\n",
       "1397     id_99 2000-07-11           1.241571           1.234871\n",
       "1398     id_99 2000-07-12           2.249115           2.226775\n",
       "1399     id_99 2000-07-13           3.252963           3.277925\n",
       "\n",
       "[1400 rows x 4 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcst = FugueMLForecast(\n",
    "    [\n",
    "        SynapseLGBMRegressor(),\n",
    "        SparkXGBRegressor()\n",
    "    ],\n",
    "    freq='D',\n",
    "    lags=[1],\n",
    "    lag_transforms={\n",
    "        1: [expanding_mean]\n",
    "    },\n",
    "    date_features=['dayofweek'],\n",
    "    engine=spark,\n",
    ")\n",
    "fcst.fit(spark_series, id_col='unique_id', time_col='ds', target_col='y')\n",
    "fcst.predict(14).as_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf3322f-3a0f-47d8-92f2-689b525111aa",
   "metadata": {},
   "source": [
    "## Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2228d0-e004-4e40-b413-314b54c36056",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "from mlforecast.distributed.models.lgb import LGBMForecast\n",
    "from mlforecast.distributed.models.xgb import XGBForecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18a5876-b898-4b48-940c-d5046397c23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(n_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163e9f0d-9d2e-41f6-a801-8f16395adb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_series = (\n",
    "    dd\n",
    "    .from_pandas(series.set_index('unique_id'), npartitions=4)  # make sure we split by the series identifier\n",
    "    .map_partitions(lambda df: df.reset_index())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f96a1f8-0b77-4327-a5d7-28bc69873466",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst = FugueMLForecast(\n",
    "    [LGBMForecast(), XGBForecast()],\n",
    "    freq='D',\n",
    "    lags=[1],\n",
    "    lag_transforms={\n",
    "        1: [expanding_mean]\n",
    "    },\n",
    "    date_features=['dayofweek'],\n",
    "    engine=client,\n",
    ")\n",
    "_ = fcst.fit(dask_series, id_col='unique_id', time_col='ds', target_col='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fffef75-fcfc-4014-8ac0-e8dcf09c9b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>LGBMForecast</th>\n",
       "      <th>XGBForecast</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-08-10</td>\n",
       "      <td>5.260191</td>\n",
       "      <td>5.286701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-08-11</td>\n",
       "      <td>6.257121</td>\n",
       "      <td>6.265203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-08-12</td>\n",
       "      <td>0.259089</td>\n",
       "      <td>0.259317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-08-13</td>\n",
       "      <td>1.257404</td>\n",
       "      <td>1.254297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-08-14</td>\n",
       "      <td>2.254521</td>\n",
       "      <td>2.207159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2000-07-09</td>\n",
       "      <td>6.239925</td>\n",
       "      <td>6.168726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2000-07-10</td>\n",
       "      <td>0.269222</td>\n",
       "      <td>-0.223779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2000-07-11</td>\n",
       "      <td>1.246913</td>\n",
       "      <td>1.200824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2000-07-12</td>\n",
       "      <td>2.251555</td>\n",
       "      <td>2.232315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2000-07-13</td>\n",
       "      <td>3.241348</td>\n",
       "      <td>3.286217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1400 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     unique_id         ds  LGBMForecast  XGBForecast\n",
       "0        id_00 2000-08-10      5.260191     5.286701\n",
       "1        id_00 2000-08-11      6.257121     6.265203\n",
       "2        id_00 2000-08-12      0.259089     0.259317\n",
       "3        id_00 2000-08-13      1.257404     1.254297\n",
       "4        id_00 2000-08-14      2.254521     2.207159\n",
       "...        ...        ...           ...          ...\n",
       "1395     id_99 2000-07-09      6.239925     6.168726\n",
       "1396     id_99 2000-07-10      0.269222    -0.223779\n",
       "1397     id_99 2000-07-11      1.246913     1.200824\n",
       "1398     id_99 2000-07-12      2.251555     2.232315\n",
       "1399     id_99 2000-07-13      3.241348     3.286217\n",
       "\n",
       "[1400 rows x 4 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcst.predict(14).as_pandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
