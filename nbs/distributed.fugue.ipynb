{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf5b1d7-bc13-4ed3-af30-f15cacc861f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp distributed.fugue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2921fbf1-b7be-4f3d-b8e6-ce58b49fbd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "import warnings\n",
    "\n",
    "from nbdev import show_doc\n",
    "from sklearn import set_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bf1328-ca14-4169-8608-31dbb62107d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "warnings.simplefilter('ignore', FutureWarning)\n",
    "set_config(display='text')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc9a984-a442-41d2-8132-471d73e64d96",
   "metadata": {},
   "source": [
    "# DistributedMLForecast\n",
    "\n",
    "> Distributed pipeline encapsulation\n",
    "\n",
    "**This interface is only tested on Linux**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8108c14-6d27-4dfd-b3e8-a2b315eb5f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import copy\n",
    "from collections import namedtuple\n",
    "from typing import Any, Callable, Dict, Iterable, List, Optional, Union\n",
    "\n",
    "import cloudpickle\n",
    "try:\n",
    "    import dask.dataframe as dd\n",
    "    DASK_INSTALLED = True\n",
    "except ModuleNotFoundError:\n",
    "    DASK_INSTALLED = False\n",
    "import fugue\n",
    "import fugue.api as fa\n",
    "import pandas as pd\n",
    "try:\n",
    "    from pyspark.ml.feature import VectorAssembler\n",
    "    from pyspark.sql import DataFrame as SparkDataFrame\n",
    "    SPARK_INSTALLED = True\n",
    "except ModuleNotFoundError:\n",
    "    SPARK_INSTALLED = False\n",
    "from sklearn.base import BaseEstimator, clone\n",
    "\n",
    "from mlforecast.core import (\n",
    "    DateFeature,\n",
    "    Differences,\n",
    "    Freq,\n",
    "    LagTransforms,\n",
    "    Lags,\n",
    "    TimeSeries,\n",
    "    _name_models,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c505b5b8-2c00-456b-8d61-2301516bc347",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "WindowInfo = namedtuple('WindowInfo', ['n_windows', 'window_size', 'step_size', 'i_window'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227f19b4-4582-4c51-94cd-c7c220d35dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class DistributedMLForecast:\n",
    "    \"\"\"Multi backend distributed pipeline\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        models,\n",
    "        freq: Optional[Freq] = None,\n",
    "        lags: Optional[Lags] = None,\n",
    "        lag_transforms: Optional[LagTransforms] = None,\n",
    "        date_features: Optional[Iterable[DateFeature]] = None,\n",
    "        differences: Optional[Differences] = None,\n",
    "        num_threads: int = 1,\n",
    "        engine = None,\n",
    "    ):\n",
    "        \"\"\"Create distributed forecast object\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        models : regressor or list of regressors\n",
    "            Models that will be trained and used to compute the forecasts.\n",
    "        freq : str or int, optional (default=None)\n",
    "            Pandas offset alias, e.g. 'D', 'W-THU' or integer denoting the frequency of the series.\n",
    "        lags : list of int, optional (default=None)\n",
    "            Lags of the target to use as features.\n",
    "        lag_transforms : dict of int to list of functions, optional (default=None)\n",
    "            Mapping of target lags to their transformations.\n",
    "        date_features : list of str or callable, optional (default=None)\n",
    "            Features computed from the dates. Can be pandas date attributes or functions that will take the dates as input.\n",
    "        differences : list of int, optional (default=None)\n",
    "            Differences to take of the target before computing the features. These are restored at the forecasting step.\n",
    "        num_threads : int (default=1)\n",
    "            Number of threads to use when computing the features.\n",
    "        engine : fugue execution engine, optional (default=None)\n",
    "            Dask Client, Spark Session, etc to use for the distributed computation.\n",
    "            If None will use default depending on input type.\n",
    "        \"\"\"        \n",
    "        if not isinstance(models, dict) and not isinstance(models, list):\n",
    "            models = [models]\n",
    "        if isinstance(models, list):\n",
    "            model_names = _name_models([m.__class__.__name__ for m in models])\n",
    "            models_with_names = dict(zip(model_names, models))\n",
    "        else:\n",
    "            models_with_names = models\n",
    "        self.models = models_with_names\n",
    "        self._base_ts = TimeSeries(\n",
    "            freq, lags, lag_transforms, date_features, differences, num_threads\n",
    "        )\n",
    "        self.engine = engine\n",
    "        \n",
    "    def __repr__(self) -> str:\n",
    "        return (\n",
    "            f'{self.__class__.__name__}(models=[{\", \".join(self.models.keys())}], '\n",
    "            f\"freq={self._base_ts.freq}, \"\n",
    "            f\"lag_features={list(self._base_ts.transforms.keys())}, \"\n",
    "            f\"date_features={self._base_ts.date_features}, \"\n",
    "            f\"num_threads={self._base_ts.num_threads}, \"\n",
    "            f\"engine={self.engine})\"\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _preprocess_partition(\n",
    "        part: pd.DataFrame,\n",
    "        base_ts: TimeSeries,        \n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "        window_info: Optional[WindowInfo] = None,\n",
    "        ts_only: bool = False,\n",
    "    ) -> List[List[Any]]:\n",
    "        ts = copy.deepcopy(base_ts)\n",
    "        if ts_only:\n",
    "            ts._fit(\n",
    "                part,\n",
    "                id_col=id_col,\n",
    "                time_col=time_col,\n",
    "                target_col=target_col,\n",
    "                static_features=static_features,\n",
    "                keep_last_n=keep_last_n,                \n",
    "            )\n",
    "            return [[cloudpickle.dumps(ts), cloudpickle.dumps(None), cloudpickle.dumps(None)]]        \n",
    "        if window_info is None:\n",
    "            train = part\n",
    "            valid = None\n",
    "        else:\n",
    "            n_windows, window_size, step_size, i_window = window_info\n",
    "            if step_size is None:\n",
    "                step_size = window_size\n",
    "            test_size = window_size + step_size * (n_windows - 1)\n",
    "            offset = test_size - i_window * step_size\n",
    "            max_dates = part.groupby(id_col)[time_col].transform('max')\n",
    "            train_ends = max_dates - offset * base_ts.freq\n",
    "            valid_ends = train_ends + window_size * base_ts.freq\n",
    "            train_mask = part[time_col].le(train_ends)\n",
    "            valid_mask = part[time_col].gt(train_ends) & part[time_col].le(valid_ends)\n",
    "            train = part[train_mask]\n",
    "            valid_keep_cols = part.columns\n",
    "            if static_features is not None:\n",
    "                valid_keep_cols.drop(static_features)\n",
    "            valid = part.loc[valid_mask, valid_keep_cols]\n",
    "        transformed = ts.fit_transform(\n",
    "            train,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            static_features=static_features,\n",
    "            dropna=dropna,\n",
    "            keep_last_n=keep_last_n,\n",
    "        )\n",
    "        return [[cloudpickle.dumps(ts), cloudpickle.dumps(transformed), cloudpickle.dumps(valid)]]\n",
    "\n",
    "    @staticmethod\n",
    "    def _retrieve_df(items: List[List[Any]]) -> Iterable[pd.DataFrame]:\n",
    "        for _, serialized_train, _ in items:\n",
    "            yield cloudpickle.loads(serialized_train)\n",
    "            \n",
    "    def _preprocess_partitions(\n",
    "        self,\n",
    "        data: fugue.AnyDataFrame,\n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "        window_info: Optional[WindowInfo] = None,\n",
    "    ) -> List[Any]:\n",
    "        return fa.transform(\n",
    "            data,\n",
    "            DistributedMLForecast._preprocess_partition,\n",
    "            params={\n",
    "                'base_ts': self._base_ts,\n",
    "                'id_col': id_col,\n",
    "                'time_col': time_col,\n",
    "                'target_col': target_col,\n",
    "                'static_features': static_features,\n",
    "                'dropna': dropna,\n",
    "                'keep_last_n': keep_last_n,\n",
    "                'window_info': window_info,\n",
    "            },\n",
    "            schema='ts:binary,train:binary,valid:binary',\n",
    "            engine=self.engine,\n",
    "            as_fugue=True,\n",
    "        )        \n",
    "\n",
    "    def _preprocess(\n",
    "        self,\n",
    "        data: fugue.AnyDataFrame,\n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "        window_info: Optional[WindowInfo] = None,\n",
    "    ) -> fugue.AnyDataFrame:\n",
    "        self.id_col = id_col\n",
    "        self.time_col = time_col\n",
    "        self.target_col = target_col\n",
    "        self.static_features = static_features\n",
    "        self.dropna = dropna\n",
    "        self.keep_last_n = keep_last_n\n",
    "        self.partition_results = self._preprocess_partitions(\n",
    "            data=data,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            static_features=static_features,\n",
    "            dropna=dropna,\n",
    "            keep_last_n=keep_last_n,\n",
    "            window_info=window_info,\n",
    "        )\n",
    "        base_schema = str(fa.get_schema(data))\n",
    "        features_schema = ','.join(f'{feat}:double' for feat in self._base_ts.features)\n",
    "        res = fa.transform(\n",
    "            self.partition_results,\n",
    "            DistributedMLForecast._retrieve_df,\n",
    "            schema=f'{base_schema},{features_schema}',\n",
    "            engine=self.engine,\n",
    "        )\n",
    "        return fa.get_native_as_df(res)\n",
    "    \n",
    "    def preprocess(\n",
    "        self,\n",
    "        data: fugue.AnyDataFrame,\n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "    ) -> fugue.AnyDataFrame:\n",
    "        \"\"\"Add the features to `data`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : dask or spark DataFrame.\n",
    "            Series data in long format.\n",
    "        id_col : str\n",
    "            Column that identifies each serie. If 'index' then the index is used.\n",
    "        time_col : str\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str\n",
    "            Column that contains the target.\n",
    "        static_features : list of str, optional (default=None)\n",
    "            Names of the features that are static and will be repeated when forecasting.\n",
    "        dropna : bool (default=True)\n",
    "            Drop rows with missing values produced by the transformations.\n",
    "        keep_last_n : int, optional (default=None)\n",
    "            Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        result : same type as input\n",
    "            data with added features.\n",
    "        \"\"\"        \n",
    "        return self._preprocess(\n",
    "            data,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            static_features=static_features,\n",
    "            dropna=dropna,\n",
    "            keep_last_n=keep_last_n,\n",
    "        )\n",
    "    \n",
    "    def _fit(\n",
    "        self,\n",
    "        data: fugue.AnyDataFrame,\n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "        window_info: Optional[WindowInfo] = None,\n",
    "    ) -> 'DistributedMLForecast':\n",
    "        prep = self._preprocess(\n",
    "            data,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            static_features=static_features,\n",
    "            dropna=dropna,\n",
    "            keep_last_n=keep_last_n,\n",
    "            window_info=window_info,\n",
    "        )\n",
    "        features = [x for x in prep.columns if x not in {id_col, time_col, target_col}]\n",
    "        self.models_ = {}\n",
    "        if SPARK_INSTALLED and isinstance(data, SparkDataFrame):\n",
    "            try:\n",
    "                import lightgbm as lgb\n",
    "                from synapse.ml.lightgbm import LightGBMRegressor as SynapseLGBMRegressor\n",
    "                LGBM_INSTALLED = True\n",
    "            except ModuleNotFoundError:\n",
    "                LGBM_INSTALLED = False\n",
    "            try:\n",
    "                import xgboost as xgb\n",
    "                from xgboost.spark import SparkXGBRegressor  # type: ignore\n",
    "                XGB_INSTALLED = True\n",
    "            except ModuleNotFoundError:\n",
    "                XGB_INSTALLED = False\n",
    "\n",
    "            featurizer = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "            train_data = featurizer.transform(prep)[target_col, \"features\"]\n",
    "            for name, model in self.models.items():\n",
    "                if LGBM_INSTALLED and isinstance(model, SynapseLGBMRegressor):\n",
    "                    trained_model = model.setLabelCol(target_col).fit(train_data)\n",
    "                    model_str = trained_model.getNativeModel()\n",
    "                    local_model = lgb.Booster(model_str=model_str)                    \n",
    "                elif XGB_INSTALLED and isinstance(model, SparkXGBRegressor):\n",
    "                    model.setParams(label_col=target_col)\n",
    "                    trained_model = model.fit(train_data)\n",
    "                    model_str = trained_model.get_booster().save_raw('ubj')\n",
    "                    local_model = xgb.XGBRegressor()\n",
    "                    local_model.load_model(model_str)\n",
    "                else:\n",
    "                    raise ValueError('Only LightGBMRegressor from SynapseML and SparkXGBRegressor are supported in spark.')\n",
    "                self.models_[name] = local_model\n",
    "        elif DASK_INSTALLED and isinstance(data, dd.DataFrame):\n",
    "            try:\n",
    "                from mlforecast.distributed.models.lgb import LGBMForecast\n",
    "                LGBM_INSTALLED = True\n",
    "            except ModuleNotFoundError:\n",
    "                LGBM_INSTALLED = False\n",
    "            try:\n",
    "                from mlforecast.distributed.models.xgb import XGBForecast\n",
    "                XGB_INSTALLED = True\n",
    "            except ModuleNotFoundError:\n",
    "                XGB_INSTALLED = False\n",
    "            X, y = prep[features], prep[target_col]\n",
    "            for name, model in self.models.items():\n",
    "                if not ((LGBM_INSTALLED and isinstance(model, LGBMForecast)) or (XGB_INSTALLED and isinstance(model, XGBForecast))):\n",
    "                    raise ValueError('Models must be either LGBMForecast or XGBForecast with dask backend.')\n",
    "                self.models_[name] = clone(model).fit(X, y).model_\n",
    "        else:\n",
    "            raise NotImplementedError('Only spark and dask engines are supported.')\n",
    "        return self\n",
    "    \n",
    "    def fit(\n",
    "        self,\n",
    "        data: fugue.AnyDataFrame,\n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,        \n",
    "    ) -> 'DistributedMLForecast':\n",
    "        \"\"\"Apply the feature engineering and train the models.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : dask or spark DataFrame\n",
    "            Series data in long format.\n",
    "        id_col : str\n",
    "            Column that identifies each serie. If 'index' then the index is used.\n",
    "        time_col : str\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str\n",
    "            Column that contains the target.\n",
    "        static_features : list of str, optional (default=None)\n",
    "            Names of the features that are static and will be repeated when forecasting.\n",
    "        dropna : bool (default=True)\n",
    "            Drop rows with missing values produced by the transformations.\n",
    "        keep_last_n : int, optional (default=None)\n",
    "            Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : DistributedMLForecast\n",
    "            Forecast object with series values and trained models.\n",
    "        \"\"\"        \n",
    "        return self._fit(\n",
    "            data,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            static_features=static_features,\n",
    "            dropna=dropna,\n",
    "            keep_last_n=keep_last_n,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _predict(\n",
    "        items: List[List[Any]],\n",
    "        models,\n",
    "        horizon,\n",
    "        dynamic_dfs,\n",
    "        before_predict_callback,\n",
    "        after_predict_callback,\n",
    "    ) -> Iterable[pd.DataFrame]:\n",
    "        for serialized_ts, _, serialized_valid in items:\n",
    "            valid = cloudpickle.loads(serialized_valid)\n",
    "            ts = cloudpickle.loads(serialized_ts)\n",
    "            if valid is not None:\n",
    "                dynamic_features = valid.columns.drop(\n",
    "                    [ts.id_col, ts.time_col, ts.target_col]\n",
    "                )\n",
    "                if not dynamic_features.empty:\n",
    "                    dynamic_dfs = [valid.drop(columns=ts.target_col)]\n",
    "            res = ts.predict(\n",
    "                models=models,\n",
    "                horizon=horizon,\n",
    "                dynamic_dfs=dynamic_dfs,\n",
    "                before_predict_callback=before_predict_callback,\n",
    "                after_predict_callback=after_predict_callback,\n",
    "            ).reset_index()\n",
    "            if valid is not None:\n",
    "                res = res.merge(valid, how='left')\n",
    "            yield res\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        horizon: int,\n",
    "        dynamic_dfs: Optional[List[pd.DataFrame]] = None,\n",
    "        before_predict_callback: Optional[Callable] = None,\n",
    "        after_predict_callback: Optional[Callable] = None,\n",
    "        new_data: Optional[pd.DataFrame] = None,\n",
    "    ) -> fugue.AnyDataFrame:\n",
    "        \"\"\"Compute the predictions for the next `horizon` steps.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        horizon : int\n",
    "            Number of periods to predict.\n",
    "        dynamic_dfs : list of pandas DataFrame, optional (default=None)\n",
    "            Future values of the dynamic features, e.g. prices.\n",
    "        before_predict_callback : callable, optional (default=None)\n",
    "            Function to call on the features before computing the predictions.\n",
    "                This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure.\n",
    "                The series identifier is on the index.\n",
    "        after_predict_callback : callable, optional (default=None)\n",
    "            Function to call on the predictions before updating the targets.\n",
    "                This function will take a pandas Series with the predictions and should return another one with the same structure.\n",
    "                The series identifier is on the index.\n",
    "        new_data : dask or spark DataFrame, optional (default=None)\n",
    "            Series data of new observations for which forecasts are to be generated.\n",
    "                This dataframe should have the same structure as the one used to fit the model, including any features and time series data.\n",
    "                If `new_data` is not None, the method will generate forecasts for the new observations.                \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        result : dask or spark DataFrame\n",
    "            Predictions for each serie and timestep, with one column per model.\n",
    "        \"\"\"        \n",
    "        model_names = self.models.keys()\n",
    "        models_schema = ','.join(f'{model_name}:double' for model_name in model_names)\n",
    "        schema = f'{self.id_col}:string,{self.time_col}:datetime,' + models_schema\n",
    "        if getattr(self, '_n_windows', None) is not None:\n",
    "            schema += f',{self.target_col}:double'\n",
    "        if new_data is not None:\n",
    "            partition_results = self._preprocess_partitions(\n",
    "                data=new_data,\n",
    "                id_col=self.id_col,\n",
    "                time_col=self.time_col,\n",
    "                target_col=self.target_col,\n",
    "                static_features=self.static_features,\n",
    "                dropna=self.dropna,\n",
    "                keep_last_n=self.keep_last_n,\n",
    "            )\n",
    "        else:\n",
    "            partition_results = self.partition_results\n",
    "        res = fa.transform(\n",
    "            partition_results,\n",
    "            DistributedMLForecast._predict,\n",
    "            params={\n",
    "                'models': self.models_,\n",
    "                'horizon': horizon,\n",
    "                'dynamic_dfs': dynamic_dfs,\n",
    "                'before_predict_callback': before_predict_callback,\n",
    "                'after_predict_callback': after_predict_callback,\n",
    "            },\n",
    "            schema=schema,\n",
    "            engine=self.engine,\n",
    "        )\n",
    "        return fa.get_native_as_df(res)\n",
    "\n",
    "    def cross_validation(\n",
    "        self,\n",
    "        data: fugue.AnyDataFrame,\n",
    "        n_windows: int,\n",
    "        window_size: int,\n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        step_size: Optional[int] = None, \n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "        before_predict_callback: Optional[Callable] = None,\n",
    "        after_predict_callback: Optional[Callable] = None,\n",
    "    ) -> fugue.AnyDataFrame:\n",
    "        \"\"\"Perform time series cross validation.\n",
    "        Creates `n_windows` splits where each window has `window_size` test periods,\n",
    "        trains the models, computes the predictions and merges the actuals.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : dask DataFrame\n",
    "            Series data in long format.\n",
    "        n_windows : int\n",
    "            Number of windows to evaluate.\n",
    "        window_size : int\n",
    "            Number of test periods in each window.\n",
    "        id_col : str\n",
    "            Column that identifies each serie. If 'index' then the index is used.\n",
    "        time_col : str\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str\n",
    "            Column that contains the target.\n",
    "        step_size : int, optional (default=None)\n",
    "            Step size between each cross validation window. If None it will be equal to `window_size`.\n",
    "        static_features : list of str, optional (default=None)\n",
    "            Names of the features that are static and will be repeated when forecasting.\n",
    "        dropna : bool (default=True)\n",
    "            Drop rows with missing values produced by the transformations.\n",
    "        keep_last_n : int, optional (default=None)\n",
    "            Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it.\n",
    "        before_predict_callback : callable, optional (default=None)\n",
    "            Function to call on the features before computing the predictions.\n",
    "                This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure.\n",
    "                The series identifier is on the index.\n",
    "        after_predict_callback : callable, optional (default=None)\n",
    "            Function to call on the predictions before updating the targets.\n",
    "                This function will take a pandas Series with the predictions and should return another one with the same structure.\n",
    "                The series identifier is on the index.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        result : dask or spark DataFrame\n",
    "            Predictions for each window with the series id, timestamp, target value and predictions from each model.\n",
    "        \"\"\"            \n",
    "        self.cv_models_ = []\n",
    "        results = []\n",
    "        for i in range(n_windows):\n",
    "            window_info = WindowInfo(n_windows, window_size, step_size, i)\n",
    "            self._fit(\n",
    "                data,\n",
    "                id_col=id_col,\n",
    "                time_col=time_col,\n",
    "                target_col=target_col,\n",
    "                static_features=static_features,\n",
    "                dropna=dropna,\n",
    "                keep_last_n=keep_last_n,\n",
    "                window_info=window_info,\n",
    "            )\n",
    "            self.cv_models_.append(self.models_)\n",
    "            preds = self.predict(\n",
    "                window_size,\n",
    "                before_predict_callback=before_predict_callback,\n",
    "                after_predict_callback=after_predict_callback,\n",
    "            )\n",
    "            results.append(preds)\n",
    "        if len(results) == 1:\n",
    "            return results[0]\n",
    "        if len(results) == 2:\n",
    "            return fa.union(results[0], results[1])\n",
    "        return fa.union(results[0], results[1], results[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18b1388-3c79-4d1c-98cf-d748d64119e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### DistributedMLForecast\n",
       "\n",
       ">      DistributedMLForecast (models,\n",
       ">                             freq:Union[int,str,pandas._libs.tslibs.offsets.Bas\n",
       ">                             eOffset,NoneType]=None,\n",
       ">                             lags:Optional[Iterable[int]]=None, lag_transforms:\n",
       ">                             Optional[Dict[int,List[Union[Callable,Tuple[Callab\n",
       ">                             le,Any]]]]]=None, date_features:Optional[Iterable[\n",
       ">                             Union[str,Callable]]]=None,\n",
       ">                             differences:Optional[Iterable[int]]=None,\n",
       ">                             num_threads:int=1, engine=None)\n",
       "\n",
       "Multi backend distributed pipeline"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### DistributedMLForecast\n",
       "\n",
       ">      DistributedMLForecast (models,\n",
       ">                             freq:Union[int,str,pandas._libs.tslibs.offsets.Bas\n",
       ">                             eOffset,NoneType]=None,\n",
       ">                             lags:Optional[Iterable[int]]=None, lag_transforms:\n",
       ">                             Optional[Dict[int,List[Union[Callable,Tuple[Callab\n",
       ">                             le,Any]]]]]=None, date_features:Optional[Iterable[\n",
       ">                             Union[str,Callable]]]=None,\n",
       ">                             differences:Optional[Iterable[int]]=None,\n",
       ">                             num_threads:int=1, engine=None)\n",
       "\n",
       "Multi backend distributed pipeline"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DistributedMLForecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605d42e8-a340-49c0-8066-c151529eb79b",
   "metadata": {},
   "source": [
    "The `DistributedMLForecast` class is a high level abstraction that encapsulates all the steps in the pipeline (preprocessing, fitting the model and computing predictions) and applies them in a distributed way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f211a3d-684d-4aa0-9e39-9e347ca7d0fe",
   "metadata": {},
   "source": [
    "## Example\n",
    "This shows an example with simulated data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<<<<<<< HEAD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e350d0b-8a4c-40e0-83be-7c1ef1bf5e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from window_ops.expanding import expanding_mean\n",
    "from window_ops.rolling import rolling_mean\n",
    "\n",
    "from mlforecast.utils import backtest_splits, generate_daily_series, generate_prices_for_series\n",
    "from mlforecast.distributed.models.lgb import LGBMForecast\n",
    "from mlforecast.distributed.models.xgb import XGBForecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`=======`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e350d0b-8a4c-40e0-83be-7c1ef1bf5e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dask.distributed import Client\n",
    "from window_ops.expanding import expanding_mean\n",
    "from window_ops.rolling import rolling_mean\n",
    "\n",
    "from mlforecast.utils import backtest_splits, generate_daily_series, generate_prices_for_series\n",
    "from mlforecast.distributed.models.lgb import LGBMForecast\n",
    "from mlforecast.distributed.models.xgb import XGBForecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`>>>>>>> THEIRS`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2eb413-8421-49be-9a51-045ac7332cb6",
   "metadata": {},
   "source": [
    "The different things that you need to use `DistributedMLForecast` (as opposed to `MLForecast`) are:\n",
    "\n",
    "1. You need to set up a cluster. We currently support dask and spark (ray is on the roadmap).\n",
    "2. Your data needs to be a distributed collection. We currently support dask and spark dataframes.\n",
    "3. You need to use a model that implements distributed training in your framework of choice, e.g. SynapseML for LightGBM in spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73eb8f10-1b95-453a-8f1c-91f9c1813af1",
   "metadata": {},
   "source": [
    "### Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92b13a2-f6bd-4d76-840d-cadb1d672147",
   "metadata": {},
   "source": [
    "#### Client setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9fe50e-1b4a-4f58-8bbf-d1c087fb7d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(n_workers=2, threads_per_worker=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e838b0d-a0f3-4672-a794-75301a4aced3",
   "metadata": {},
   "source": [
    "Here we define a client that connects to a `dask.distributed.LocalCluster`, however it could be any other kind of cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f15a4a-79d2-4b72-8b9b-290edefe340f",
   "metadata": {},
   "source": [
    "### Data setup\n",
    "\n",
    "For dask, the data must be a `dask.dataframe.DataFrame`. You need to make sure that each time serie is only in one partition and it is recommended that you have as many partitions as you have workers. If you have more partitions than workers make sure to set `num_threads=1` to avoid having nested parallelism.\n",
    "\n",
    "The required input format is the same as for `MLForecast`, except that it's a `dask.dataframe.DataFrame` instead of a `pandas.Dataframe`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<<<<<<< HEAD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520e5293-f536-4941-b11d-7414f88096be",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter('ignore', FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`=======`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`>>>>>>> THEIRS`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552cea5f-b826-4c9b-ae9f-2532f92f31bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>static_0</th>\n",
       "      <th>static_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=10</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>object</td>\n",
       "      <td>datetime64[ns]</td>\n",
       "      <td>float64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_10</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_89</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_99</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: assign, 5 graph layers</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "               unique_id              ds        y static_0 static_1\n",
       "npartitions=10                                                     \n",
       "id_00             object  datetime64[ns]  float64    int64    int64\n",
       "id_10                ...             ...      ...      ...      ...\n",
       "...                  ...             ...      ...      ...      ...\n",
       "id_89                ...             ...      ...      ...      ...\n",
       "id_99                ...             ...      ...      ...      ...\n",
       "Dask Name: assign, 5 graph layers"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series = generate_daily_series(100, n_static_features=2, equal_ends=True, static_as_categorical=False)\n",
    "partitioned_series = dd.from_pandas(series, npartitions=10).map_partitions(lambda df: df.reset_index())\n",
    "partitioned_series['unique_id'] = partitioned_series['unique_id'].astype(str)\n",
    "partitioned_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf40d477-eddd-4c4d-b6b1-57fd4bd7c58f",
   "metadata": {},
   "source": [
    "#### Models\n",
    "In order to perform distributed forecasting, we need to use a model that is able to train in a distributed way using `dask`. The current implementations are in `LGBMForecast` and `XGBForecast` which are just wrappers around `lightgbm.dask.DaskLGBMRegressor` and `xgboost.dask.DaskXGBRegressor` that add a `model_` property to get the trained model from them and send it to every worker to perform the predictions step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e3eabd-f9d3-4d7e-b1e8-edccf8a27f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [XGBForecast(random_state=0), LGBMForecast(random_state=0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfb3968-0ee4-4141-ad7a-ee202dd8c044",
   "metadata": {},
   "source": [
    "### Training\n",
    "Once we have our models we instantiate a `DistributedMLForecast` object defining our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f56da13-0e3e-43d8-897d-af44fb89b037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistributedMLForecast(models=[XGBForecast, LGBMForecast], freq=<Day>, lag_features=['lag7', 'expanding_mean_lag1', 'rolling_mean_lag7_window_size14'], date_features=['dayofweek', 'month'], num_threads=1, engine=<Client: 'tcp://127.0.0.1:44923' processes=2 threads=2, memory=15.50 GiB>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcst = DistributedMLForecast(\n",
    "    models=models,\n",
    "    freq='D',\n",
    "    lags=[7],\n",
    "    lag_transforms={\n",
    "        1: [expanding_mean],\n",
    "        7: [(rolling_mean, 14)]\n",
    "    },\n",
    "    date_features=['dayofweek', 'month'],\n",
    "    num_threads=1,\n",
    "    engine=client,\n",
    ")\n",
    "fcst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67af6771-3484-4b54-82b1-ef6f284bb289",
   "metadata": {},
   "source": [
    "Here where we say that:\n",
    "\n",
    "* Our series have daily frequency.\n",
    "* We want to use lag 7 as a feature\n",
    "* We want the lag transformations to be:\n",
    "   * expanding mean of the lag 1\n",
    "   * rolling mean of the lag 7 over a window of size 14\n",
    "* We want to use dayofweek and month as date features.\n",
    "* We want to perform the preprocessing and the forecasting steps using 1 thread, because we have 10 partitions and 2 workers.\n",
    "\n",
    "From this point we have two options:\n",
    "\n",
    "1. Compute the features and fit our models.\n",
    "2. Compute the features and get them back as a dataframe to do some custom splitting or adding additional features, then training the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ea6591-d9bd-4273-8920-c40f678e2f2c",
   "metadata": {},
   "source": [
    "#### 1. Using all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3201af6c-4915-4447-8bca-01fe9a1f7cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### DistributedMLForecast.fit\n",
       "\n",
       ">      DistributedMLForecast.fit (data:~AnyDataFrame, id_col:str, time_col:str,\n",
       ">                                 target_col:str,\n",
       ">                                 static_features:Optional[List[str]]=None,\n",
       ">                                 dropna:bool=True,\n",
       ">                                 keep_last_n:Optional[int]=None)\n",
       "\n",
       "Apply the feature engineering and train the models.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| data | AnyDataFrame |  | Series data in long format. |\n",
       "| id_col | str |  | Column that identifies each serie. If 'index' then the index is used. |\n",
       "| time_col | str |  | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str |  | Column that contains the target. |\n",
       "| static_features | Optional | None | Names of the features that are static and will be repeated when forecasting. |\n",
       "| dropna | bool | True | Drop rows with missing values produced by the transformations. |\n",
       "| keep_last_n | Optional | None | Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it. |\n",
       "| **Returns** | **DistributedMLForecast** |  | **Forecast object with series values and trained models.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### DistributedMLForecast.fit\n",
       "\n",
       ">      DistributedMLForecast.fit (data:~AnyDataFrame, id_col:str, time_col:str,\n",
       ">                                 target_col:str,\n",
       ">                                 static_features:Optional[List[str]]=None,\n",
       ">                                 dropna:bool=True,\n",
       ">                                 keep_last_n:Optional[int]=None)\n",
       "\n",
       "Apply the feature engineering and train the models.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| data | AnyDataFrame |  | Series data in long format. |\n",
       "| id_col | str |  | Column that identifies each serie. If 'index' then the index is used. |\n",
       "| time_col | str |  | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str |  | Column that contains the target. |\n",
       "| static_features | Optional | None | Names of the features that are static and will be repeated when forecasting. |\n",
       "| dropna | bool | True | Drop rows with missing values produced by the transformations. |\n",
       "| keep_last_n | Optional | None | Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it. |\n",
       "| **Returns** | **DistributedMLForecast** |  | **Forecast object with series values and trained models.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DistributedMLForecast.fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa359fc-e73b-4178-9421-848742b027ae",
   "metadata": {},
   "source": [
    "Calling `fit` on our data computes the features independently for each partition and performs distributed training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bf69ca-1723-43ef-a527-239aed06bd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst.fit(partitioned_series, id_col='unique_id', time_col='ds', target_col='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06df485-8496-45da-87e0-e45cf9b1f9ea",
   "metadata": {},
   "source": [
    "### Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc8f139-ae37-4927-8449-ed949dc254db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### DistributedMLForecast.predict\n",
       "\n",
       ">      DistributedMLForecast.predict (horizon:int,\n",
       ">                                     dynamic_dfs:Optional[List[pandas.core.fram\n",
       ">                                     e.DataFrame]]=None, before_predict_callbac\n",
       ">                                     k:Optional[Callable]=None, after_predict_c\n",
       ">                                     allback:Optional[Callable]=None, new_data:\n",
       ">                                     Optional[pandas.core.frame.DataFrame]=None\n",
       ">                                     )\n",
       "\n",
       "Compute the predictions for the next `horizon` steps.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| horizon | int |  | Number of periods to predict. |\n",
       "| dynamic_dfs | Optional | None | Future values of the dynamic features, e.g. prices. |\n",
       "| before_predict_callback | Optional | None | Function to call on the features before computing the predictions.<br>    This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure.<br>    The series identifier is on the index. |\n",
       "| after_predict_callback | Optional | None | Function to call on the predictions before updating the targets.<br>    This function will take a pandas Series with the predictions and should return another one with the same structure.<br>    The series identifier is on the index. |\n",
       "| new_data | Optional | None | Series data of new observations for which forecasts are to be generated.<br>    This dataframe should have the same structure as the one used to fit the model, including any features and time series data.<br>    If `new_data` is not None, the method will generate forecasts for the new observations.                 |\n",
       "| **Returns** | **AnyDataFrame** |  | **Predictions for each serie and timestep, with one column per model.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### DistributedMLForecast.predict\n",
       "\n",
       ">      DistributedMLForecast.predict (horizon:int,\n",
       ">                                     dynamic_dfs:Optional[List[pandas.core.fram\n",
       ">                                     e.DataFrame]]=None, before_predict_callbac\n",
       ">                                     k:Optional[Callable]=None, after_predict_c\n",
       ">                                     allback:Optional[Callable]=None, new_data:\n",
       ">                                     Optional[pandas.core.frame.DataFrame]=None\n",
       ">                                     )\n",
       "\n",
       "Compute the predictions for the next `horizon` steps.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| horizon | int |  | Number of periods to predict. |\n",
       "| dynamic_dfs | Optional | None | Future values of the dynamic features, e.g. prices. |\n",
       "| before_predict_callback | Optional | None | Function to call on the features before computing the predictions.<br>    This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure.<br>    The series identifier is on the index. |\n",
       "| after_predict_callback | Optional | None | Function to call on the predictions before updating the targets.<br>    This function will take a pandas Series with the predictions and should return another one with the same structure.<br>    The series identifier is on the index. |\n",
       "| new_data | Optional | None | Series data of new observations for which forecasts are to be generated.<br>    This dataframe should have the same structure as the one used to fit the model, including any features and time series data.<br>    If `new_data` is not None, the method will generate forecasts for the new observations.                 |\n",
       "| **Returns** | **AnyDataFrame** |  | **Predictions for each serie and timestep, with one column per model.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DistributedMLForecast.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8162b4-4dc7-48cd-8a0e-f59f36825f1e",
   "metadata": {},
   "source": [
    "Once we have our fitted models we can compute the predictions for the next 7 timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694a1c30-40d0-4552-ab79-73eaf6179b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>XGBForecast</th>\n",
       "      <th>LGBMForecast</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=10</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>object</td>\n",
       "      <td>datetime64[ns]</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_10</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_89</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_99</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: map, 17 graph layers</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "               unique_id              ds XGBForecast LGBMForecast\n",
       "npartitions=10                                                   \n",
       "id_00             object  datetime64[ns]     float64      float64\n",
       "id_10                ...             ...         ...          ...\n",
       "...                  ...             ...         ...          ...\n",
       "id_89                ...             ...         ...          ...\n",
       "id_99                ...             ...         ...          ...\n",
       "Dask Name: map, 17 graph layers"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = fcst.predict(7)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba35d44-0916-4fb0-9a2d-ee98efbbe931",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "preds = preds.compute()\n",
    "preds2 = fcst.predict(7).compute()\n",
    "preds3 = fcst.predict(7, new_data=partitioned_series).compute()\n",
    "pd.testing.assert_frame_equal(preds, preds2)\n",
    "pd.testing.assert_frame_equal(preds, preds3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d85e0d-b167-43a2-a480-708e20aba44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "non_std_series = partitioned_series.copy()\n",
    "non_std_series['ds'] = non_std_series.map_partitions(lambda part: part.groupby('unique_id').cumcount())\n",
    "non_std_series = non_std_series.rename(columns={'ds': 'time', 'y': 'value', 'unique_id': 'some_id'})\n",
    "flow_params = dict(\n",
    "    models=[XGBForecast(random_state=0)],\n",
    "    lags=[7],\n",
    "    lag_transforms={\n",
    "        1: [expanding_mean],\n",
    "        7: [(rolling_mean, 14)]\n",
    "    },\n",
    "    num_threads=1,\n",
    ")\n",
    "fcst = DistributedMLForecast(freq='D', **flow_params)\n",
    "fcst.fit(partitioned_series, id_col='unique_id', time_col='ds', target_col='y')\n",
    "preds = fcst.predict(7).compute()\n",
    "fcst2 = DistributedMLForecast(**flow_params)\n",
    "fcst2.preprocess(non_std_series, id_col='some_id', time_col='time', target_col='value')\n",
    "fcst2.models_ = fcst.models_  # distributed training can end up with different fits\n",
    "non_std_preds = fcst2.predict(7).compute()\n",
    "pd.testing.assert_frame_equal(\n",
    "    preds.drop(columns='ds'),\n",
    "    non_std_preds.drop(columns='time').rename(columns={'some_id': 'unique_id'})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6438efd-365c-444f-a447-03f44f1cc61a",
   "metadata": {},
   "source": [
    "#### 2. Preprocess and train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b989f8-945e-4864-854d-92e846fe240d",
   "metadata": {},
   "source": [
    "If we only want to perform the preprocessing step we call `preprocess` with our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d825fee7-6b8f-4613-93b0-e5769a7abe39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### DistributedMLForecast.preprocess\n",
       "\n",
       ">      DistributedMLForecast.preprocess (data:~AnyDataFrame, id_col:str,\n",
       ">                                        time_col:str, target_col:str, static_fe\n",
       ">                                        atures:Optional[List[str]]=None,\n",
       ">                                        dropna:bool=True,\n",
       ">                                        keep_last_n:Optional[int]=None)\n",
       "\n",
       "Add the features to `data`.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| data | AnyDataFrame |  | Series data in long format. |\n",
       "| id_col | str |  | Column that identifies each serie. If 'index' then the index is used. |\n",
       "| time_col | str |  | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str |  | Column that contains the target. |\n",
       "| static_features | Optional | None | Names of the features that are static and will be repeated when forecasting. |\n",
       "| dropna | bool | True | Drop rows with missing values produced by the transformations. |\n",
       "| keep_last_n | Optional | None | Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it. |\n",
       "| **Returns** | **AnyDataFrame** |  | **data with added features.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### DistributedMLForecast.preprocess\n",
       "\n",
       ">      DistributedMLForecast.preprocess (data:~AnyDataFrame, id_col:str,\n",
       ">                                        time_col:str, target_col:str, static_fe\n",
       ">                                        atures:Optional[List[str]]=None,\n",
       ">                                        dropna:bool=True,\n",
       ">                                        keep_last_n:Optional[int]=None)\n",
       "\n",
       "Add the features to `data`.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| data | AnyDataFrame |  | Series data in long format. |\n",
       "| id_col | str |  | Column that identifies each serie. If 'index' then the index is used. |\n",
       "| time_col | str |  | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str |  | Column that contains the target. |\n",
       "| static_features | Optional | None | Names of the features that are static and will be repeated when forecasting. |\n",
       "| dropna | bool | True | Drop rows with missing values produced by the transformations. |\n",
       "| keep_last_n | Optional | None | Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it. |\n",
       "| **Returns** | **AnyDataFrame** |  | **data with added features.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DistributedMLForecast.preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbd1a34-f3ad-4aca-8d94-62a2bc13299e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>static_0</th>\n",
       "      <th>static_1</th>\n",
       "      <th>lag7</th>\n",
       "      <th>expanding_mean_lag1</th>\n",
       "      <th>rolling_mean_lag7_window_size14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-10-25</td>\n",
       "      <td>49.766844</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>50.694639</td>\n",
       "      <td>25.001367</td>\n",
       "      <td>26.320060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-10-26</td>\n",
       "      <td>3.918347</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>3.887780</td>\n",
       "      <td>26.180675</td>\n",
       "      <td>26.313387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-10-27</td>\n",
       "      <td>9.437778</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>11.512774</td>\n",
       "      <td>25.168751</td>\n",
       "      <td>26.398056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-10-28</td>\n",
       "      <td>17.923574</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>18.038498</td>\n",
       "      <td>24.484796</td>\n",
       "      <td>26.425272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-10-29</td>\n",
       "      <td>26.754645</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>24.222859</td>\n",
       "      <td>24.211411</td>\n",
       "      <td>26.305563</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   unique_id         ds          y  static_0  static_1       lag7  \\\n",
       "20     id_00 2000-10-25  49.766844        79        45  50.694639   \n",
       "21     id_00 2000-10-26   3.918347        79        45   3.887780   \n",
       "22     id_00 2000-10-27   9.437778        79        45  11.512774   \n",
       "23     id_00 2000-10-28  17.923574        79        45  18.038498   \n",
       "24     id_00 2000-10-29  26.754645        79        45  24.222859   \n",
       "\n",
       "    expanding_mean_lag1  rolling_mean_lag7_window_size14  \n",
       "20            25.001367                        26.320060  \n",
       "21            26.180675                        26.313387  \n",
       "22            25.168751                        26.398056  \n",
       "23            24.484796                        26.425272  \n",
       "24            24.211411                        26.305563  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_ddf = fcst.preprocess(partitioned_series, id_col='unique_id', time_col='ds', target_col='y')\n",
    "features_ddf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15a97f1-7285-48fc-a016-6b8076c154ec",
   "metadata": {},
   "source": [
    "This is useful if we want to inspect the data the model will be trained. If we do this we must manually train our models and add a local version of them to the `models_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece0fb49-c02c-4336-af7c-e007bd19aaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = features_ddf.drop(columns=['unique_id', 'ds', 'y']), features_ddf['y']\n",
    "model = XGBForecast(random_state=0).fit(X, y)\n",
    "fcst.models_ = {'XGBForecast': model.model_}\n",
    "fcst.predict(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0380885-e914-4caf-ad85-7d9d1e6c0b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "fcst.models_ = fcst2.models_\n",
    "preds2 = fcst.predict(7).compute()\n",
    "pd.testing.assert_frame_equal(preds, preds2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e6f46c-8222-4cef-8955-a2b9a80a4735",
   "metadata": {},
   "source": [
    "#### Dynamic features\n",
    "By default the predict method repeats the static features and updates the transformations and the date features. If you have dynamic features like prices or a calendar with holidays you can pass them as a list to the `dynamic_dfs` argument of `DistributedMLForecast.predict`, which will call `pd.DataFrame.merge` on each of them in order.\n",
    "\n",
    "Here's an example:\n",
    "\n",
    "Suppose that we have a `product_id` column and we have a catalog for prices based on that `product_id` and the date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a3b1df-530e-4673-b500-e26ffd378914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>product_id</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-06-09</td>\n",
       "      <td>1</td>\n",
       "      <td>0.548814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-06-10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.715189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-06-11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.602763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-06-12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.544883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-06-13</td>\n",
       "      <td>1</td>\n",
       "      <td>0.423655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20180</th>\n",
       "      <td>2001-05-17</td>\n",
       "      <td>99</td>\n",
       "      <td>0.223520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20181</th>\n",
       "      <td>2001-05-18</td>\n",
       "      <td>99</td>\n",
       "      <td>0.446104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20182</th>\n",
       "      <td>2001-05-19</td>\n",
       "      <td>99</td>\n",
       "      <td>0.044783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20183</th>\n",
       "      <td>2001-05-20</td>\n",
       "      <td>99</td>\n",
       "      <td>0.483216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20184</th>\n",
       "      <td>2001-05-21</td>\n",
       "      <td>99</td>\n",
       "      <td>0.799660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20185 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ds  product_id     price\n",
       "0     2000-06-09           1  0.548814\n",
       "1     2000-06-10           1  0.715189\n",
       "2     2000-06-11           1  0.602763\n",
       "3     2000-06-12           1  0.544883\n",
       "4     2000-06-13           1  0.423655\n",
       "...          ...         ...       ...\n",
       "20180 2001-05-17          99  0.223520\n",
       "20181 2001-05-18          99  0.446104\n",
       "20182 2001-05-19          99  0.044783\n",
       "20183 2001-05-20          99  0.483216\n",
       "20184 2001-05-21          99  0.799660\n",
       "\n",
       "[20185 rows x 3 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynamic_series = series.rename(columns={'static_1': 'product_id'})\n",
    "prices_catalog = generate_prices_for_series(dynamic_series)\n",
    "prices_catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5aec529-856e-4028-b8aa-4347eefd422f",
   "metadata": {},
   "source": [
    "And you have already merged these prices into your series dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0557a5a8-bfad-4c6e-a0a5-a3a2181d6c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>static_0</th>\n",
       "      <th>product_id</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-10-05</td>\n",
       "      <td>3.981198</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>0.570826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-10-06</td>\n",
       "      <td>10.327401</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>0.260562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-10-07</td>\n",
       "      <td>17.657474</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>0.274048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-10-08</td>\n",
       "      <td>25.898790</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>0.433878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-10-09</td>\n",
       "      <td>34.494040</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>0.653738</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  unique_id         ds          y  static_0  product_id     price\n",
       "0     id_00 2000-10-05   3.981198        79          45  0.570826\n",
       "1     id_00 2000-10-06  10.327401        79          45  0.260562\n",
       "2     id_00 2000-10-07  17.657474        79          45  0.274048\n",
       "3     id_00 2000-10-08  25.898790        79          45  0.433878\n",
       "4     id_00 2000-10-09  34.494040        79          45  0.653738"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynamic_series = partitioned_series.rename(columns={'static_1': 'product_id'})\n",
    "dynamic_series = dynamic_series\n",
    "series_with_prices = dynamic_series.merge(prices_catalog, how='left')\n",
    "series_with_prices.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951f5066-3685-43d0-91a6-09867790ab35",
   "metadata": {},
   "source": [
    "This dataframe will be passed to `DistributedMLForecast.fit` (or `DistributedMLForecast.preprocess`), however since the price is dynamic we have to tell that method that only `static_0` and `product_id` are static and we'll have to update `price` in every timestep, which basically involves merging the updated features with the prices catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ca112d-cbdb-41dc-88b4-fad613cb8b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst = DistributedMLForecast(\n",
    "    models,\n",
    "    freq='D',\n",
    "    lags=[7],\n",
    "    lag_transforms={\n",
    "        1: [expanding_mean],\n",
    "        7: [(rolling_mean, 14)]\n",
    "    },\n",
    "    date_features=['dayofweek', 'month'],\n",
    "    num_threads=1,\n",
    ")\n",
    "series_with_prices = series_with_prices\n",
    "fcst.fit(\n",
    "    series_with_prices,\n",
    "    id_col='unique_id',\n",
    "    time_col='ds',\n",
    "    target_col='y',\n",
    "    static_features=['static_0', 'product_id'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4191f9c1-d2cd-4896-9ff2-18c078cfc503",
   "metadata": {},
   "source": [
    "So in order to update the price in each timestep we just call `DistributedForecast.predict` with our forecast horizon and pass the prices catalog as a dynamic dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c4e580-73af-4b0f-943a-32bc3838cbb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>XGBForecast</th>\n",
       "      <th>LGBMForecast</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-15</td>\n",
       "      <td>42.223095</td>\n",
       "      <td>42.709877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-16</td>\n",
       "      <td>50.528976</td>\n",
       "      <td>49.982335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-17</td>\n",
       "      <td>2.072457</td>\n",
       "      <td>1.954048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-18</td>\n",
       "      <td>10.141087</td>\n",
       "      <td>10.343824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-19</td>\n",
       "      <td>18.437445</td>\n",
       "      <td>18.491415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-17</td>\n",
       "      <td>43.931427</td>\n",
       "      <td>44.330152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-18</td>\n",
       "      <td>2.001306</td>\n",
       "      <td>2.101588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-19</td>\n",
       "      <td>8.775194</td>\n",
       "      <td>9.288952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-20</td>\n",
       "      <td>15.422138</td>\n",
       "      <td>15.439463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-21</td>\n",
       "      <td>22.713383</td>\n",
       "      <td>22.907743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>700 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   unique_id         ds  XGBForecast  LGBMForecast\n",
       "0      id_00 2001-05-15    42.223095     42.709877\n",
       "1      id_00 2001-05-16    50.528976     49.982335\n",
       "2      id_00 2001-05-17     2.072457      1.954048\n",
       "3      id_00 2001-05-18    10.141087     10.343824\n",
       "4      id_00 2001-05-19    18.437445     18.491415\n",
       "..       ...        ...          ...           ...\n",
       "72     id_99 2001-05-17    43.931427     44.330152\n",
       "73     id_99 2001-05-18     2.001306      2.101588\n",
       "74     id_99 2001-05-19     8.775194      9.288952\n",
       "75     id_99 2001-05-20    15.422138     15.439463\n",
       "76     id_99 2001-05-21    22.713383     22.907743\n",
       "\n",
       "[700 rows x 4 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = fcst.predict(7, dynamic_dfs=[prices_catalog])\n",
    "preds.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06edbe8-27e1-4662-bf5a-60a67c7f6713",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test we can compute cross validation with\n",
    "# exougenous variables without adding extra information\n",
    "# later a more robust test is performed\n",
    "cv_with_ex = fcst.cross_validation(\n",
    "    series_with_prices,\n",
    "    window_size=7,\n",
    "    n_windows=2,\n",
    "    id_col='unique_id',\n",
    "    time_col='ds',\n",
    "    target_col='y',\n",
    "    static_features=['static_0', 'product_id'],\n",
    ").compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c37303b-d2a9-4041-8cbe-f7921e5a7316",
   "metadata": {},
   "source": [
    "#### Custom predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3063a465-11bb-4e85-be4e-a3f4978e6fc7",
   "metadata": {},
   "source": [
    "If you want to do something like scaling the predictions you can define a function and pass it to `DistributedMLForecast.predict` as described in <a href=\"/forecast.html#custom-predictions\">Custom predictions</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9461e8-2891-4c52-ab1c-0b1417c9ce60",
   "metadata": {},
   "source": [
    "### Cross validation\n",
    "Refer to `MLForecast.cross_validation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3faef3-89d4-45dd-bcda-78aba4414a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### DistributedMLForecast.cross_validation\n",
       "\n",
       ">      DistributedMLForecast.cross_validation (data:~AnyDataFrame,\n",
       ">                                              n_windows:int, window_size:int,\n",
       ">                                              id_col:str, time_col:str,\n",
       ">                                              target_col:str,\n",
       ">                                              step_size:Optional[int]=None, sta\n",
       ">                                              tic_features:Optional[List[str]]=\n",
       ">                                              None, dropna:bool=True,\n",
       ">                                              keep_last_n:Optional[int]=None, b\n",
       ">                                              efore_predict_callback:Optional[C\n",
       ">                                              allable]=None, after_predict_call\n",
       ">                                              back:Optional[Callable]=None)\n",
       "\n",
       "Perform time series cross validation.\n",
       "Creates `n_windows` splits where each window has `window_size` test periods,\n",
       "trains the models, computes the predictions and merges the actuals.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| data | AnyDataFrame |  | Series data in long format. |\n",
       "| n_windows | int |  | Number of windows to evaluate. |\n",
       "| window_size | int |  | Number of test periods in each window. |\n",
       "| id_col | str |  | Column that identifies each serie. If 'index' then the index is used. |\n",
       "| time_col | str |  | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str |  | Column that contains the target. |\n",
       "| step_size | Optional | None | Step size between each cross validation window. If None it will be equal to `window_size`. |\n",
       "| static_features | Optional | None | Names of the features that are static and will be repeated when forecasting. |\n",
       "| dropna | bool | True | Drop rows with missing values produced by the transformations. |\n",
       "| keep_last_n | Optional | None | Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it. |\n",
       "| before_predict_callback | Optional | None | Function to call on the features before computing the predictions.<br>    This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure.<br>    The series identifier is on the index. |\n",
       "| after_predict_callback | Optional | None | Function to call on the predictions before updating the targets.<br>    This function will take a pandas Series with the predictions and should return another one with the same structure.<br>    The series identifier is on the index. |\n",
       "| **Returns** | **Iterator** |  | **Predictions for each window with the series id, timestamp, target value and predictions from each model.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### DistributedMLForecast.cross_validation\n",
       "\n",
       ">      DistributedMLForecast.cross_validation (data:~AnyDataFrame,\n",
       ">                                              n_windows:int, window_size:int,\n",
       ">                                              id_col:str, time_col:str,\n",
       ">                                              target_col:str,\n",
       ">                                              step_size:Optional[int]=None, sta\n",
       ">                                              tic_features:Optional[List[str]]=\n",
       ">                                              None, dropna:bool=True,\n",
       ">                                              keep_last_n:Optional[int]=None, b\n",
       ">                                              efore_predict_callback:Optional[C\n",
       ">                                              allable]=None, after_predict_call\n",
       ">                                              back:Optional[Callable]=None)\n",
       "\n",
       "Perform time series cross validation.\n",
       "Creates `n_windows` splits where each window has `window_size` test periods,\n",
       "trains the models, computes the predictions and merges the actuals.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| data | AnyDataFrame |  | Series data in long format. |\n",
       "| n_windows | int |  | Number of windows to evaluate. |\n",
       "| window_size | int |  | Number of test periods in each window. |\n",
       "| id_col | str |  | Column that identifies each serie. If 'index' then the index is used. |\n",
       "| time_col | str |  | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_col | str |  | Column that contains the target. |\n",
       "| step_size | Optional | None | Step size between each cross validation window. If None it will be equal to `window_size`. |\n",
       "| static_features | Optional | None | Names of the features that are static and will be repeated when forecasting. |\n",
       "| dropna | bool | True | Drop rows with missing values produced by the transformations. |\n",
       "| keep_last_n | Optional | None | Keep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it. |\n",
       "| before_predict_callback | Optional | None | Function to call on the features before computing the predictions.<br>    This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure.<br>    The series identifier is on the index. |\n",
       "| after_predict_callback | Optional | None | Function to call on the predictions before updating the targets.<br>    This function will take a pandas Series with the predictions and should return another one with the same structure.<br>    The series identifier is on the index. |\n",
       "| **Returns** | **Iterator** |  | **Predictions for each window with the series id, timestamp, target value and predictions from each model.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DistributedMLForecast.cross_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8eb4ac-1622-4eed-8892-e1272e131925",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_windows = 2\n",
    "window_size = 14\n",
    "\n",
    "cv_results = fcst.cross_validation(\n",
    "    partitioned_series,\n",
    "    n_windows,\n",
    "    window_size,\n",
    "    id_col='unique_id',\n",
    "    time_col='ds',\n",
    "    target_col='y',\n",
    ")\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a173f86-eeca-4e66-9df3-9282bf4afcdb",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DistributedMLForecast.cross_validation() got an unexpected keyword argument 'refit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[163], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#| hide\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m cv_results_no_refit \u001b[38;5;241m=\u001b[39m \u001b[43mfcst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_validation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartitioned_series\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_windows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mid_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43munique_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mds\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrefit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m cv_results_df \u001b[38;5;241m=\u001b[39m cv_results\u001b[38;5;241m.\u001b[39mcompute()\n\u001b[1;32m     12\u001b[0m cv_results_no_refit_df \u001b[38;5;241m=\u001b[39m cv_results_no_refit\u001b[38;5;241m.\u001b[39mcompute()\n",
      "\u001b[0;31mTypeError\u001b[0m: DistributedMLForecast.cross_validation() got an unexpected keyword argument 'refit'"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "cv_results_no_refit = fcst.cross_validation(\n",
    "    partitioned_series,\n",
    "    n_windows,\n",
    "    window_size,\n",
    "    id_col='unique_id',\n",
    "    time_col='ds',\n",
    "    target_col='y',\n",
    "    refit=False\n",
    ")\n",
    "cv_results_df = cv_results.compute()\n",
    "cv_results_no_refit_df = cv_results_no_refit.compute()\n",
    "# test we recover the same \"metadata\"\n",
    "models = ['XGBForecast', 'LGBMForecast']\n",
    "test_eq(\n",
    "    cv_results_no_refit_df.drop(columns=models),\n",
    "    cv_results_df.drop(columns=models)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df86990-35ff-4b44-b96a-cabd22b03adb",
   "metadata": {},
   "source": [
    "We can aggregate these by date to get a rough estimate of how our model is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1776f7a5-996d-4598-90d6-c5217b6da9e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['cutoff'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[164], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m agg_results \u001b[38;5;241m=\u001b[39m \u001b[43mcv_results\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcutoff\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mds\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m      2\u001b[0m agg_results\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/mambaforge/envs/mlforecast/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/mlforecast/lib/python3.10/site-packages/pandas/core/frame.py:5396\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5248\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   5249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m   5250\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5257\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5258\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5259\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5260\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5261\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5394\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5395\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5398\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5402\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5403\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5404\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/mlforecast/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/mlforecast/lib/python3.10/site-packages/pandas/core/generic.py:4505\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4503\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4504\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4505\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4508\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m~/mambaforge/envs/mlforecast/lib/python3.10/site-packages/pandas/core/generic.py:4546\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4544\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4545\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4546\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4547\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4549\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4550\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/mambaforge/envs/mlforecast/lib/python3.10/site-packages/pandas/core/indexes/base.py:6977\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6975\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   6976\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 6977\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(labels[mask])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6978\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   6979\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['cutoff'] not found in axis\""
     ]
    }
   ],
   "source": [
    "agg_results = cv_results.compute().drop(columns='cutoff').groupby('ds').mean()\n",
    "agg_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccb8da2-6009-4143-8c3b-3425921f52de",
   "metadata": {},
   "source": [
    "We can also compute the error for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a449b9e-7b68-443a-b3c2-4b0977d32143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'XGBForecast': 0.87, 'LGBMForecast': 0.9}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mse_from_dask_dataframe(ddf):\n",
    "    mses = {}\n",
    "    for model_name in ddf.columns.drop(['ds', 'y', 'cutoff']):\n",
    "        mses[model_name] = (ddf['y'] - ddf[model_name]).pow(2).mean()\n",
    "    return client.gather(client.compute(mses))\n",
    "\n",
    "{k: round(v, 2) for k, v in mse_from_dask_dataframe(cv_results).items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d370fa9b-37bc-45d2-a35d-4902eaa456f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "def test_cross_validation(data=non_std_series, add_exogenous=False):\n",
    "    n_windows = 2\n",
    "    window_size = 14\n",
    "    fcst = DistributedMLForecast(XGBForecast(random_state=0), lags=[7, 14])\n",
    "    if add_exogenous:\n",
    "        data.map_partitions(lambda x: x.assign(ex1=lambda y: np.arange(0, len(y))))\n",
    "    backtest_results = fcst.cross_validation(\n",
    "        data,\n",
    "        n_windows,\n",
    "        window_size,\n",
    "        id_col='some_id',\n",
    "        time_col='time',\n",
    "        target_col='value',\n",
    "        static_features=['static_0', 'static_1'],    \n",
    "    ).compute()\n",
    "    renamer = {'some_id': 'unique_id', 'time': 'ds', 'value': 'y'}\n",
    "    backtest_results = backtest_results.rename(columns=renamer).set_index('unique_id')\n",
    "    renamed = data.rename(columns=renamer).set_index('unique_id')\n",
    "    cv_models = fcst.cv_models_\n",
    "    manual_results = []\n",
    "    for i, (cutoff, train, valid) in enumerate(backtest_splits(renamed, n_windows, window_size, 1)):\n",
    "        fcst.preprocess(train)\n",
    "        fcst.models_ = cv_models[i]\n",
    "        if add_exogenous:\n",
    "            dynamic_dfs = [valid.drop(columns=['y', 'static_0', 'static_1']).reset_index().compute()]\n",
    "        else:\n",
    "            dynamic_dfs = None\n",
    "        pred = fcst.predict(window_size, dynamic_dfs=dynamic_dfs).compute()\n",
    "        res = valid[['ds', 'y']].compute()\n",
    "        res['cutoff'] = cutoff\n",
    "        res = res.merge(pred, on=['unique_id', 'ds'], how='left')\n",
    "        manual_results.append(res)\n",
    "    manual_results = pd.concat(manual_results)\n",
    "    pd.testing.assert_frame_equal(backtest_results, manual_results)\n",
    "test_cross_validation()\n",
    "test_cross_validation(add_exogenous=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa70236c-dfd0-4cc7-9a0a-46a9769567ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bd1c31-9270-4176-b9b1-eaaaeae3cf28",
   "metadata": {},
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81348566-8200-419f-9921-c6b5b655652d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9713172-2263-48a5-99fd-f66b13117cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"MyApp\")\n",
    "    .config(\"spark.jars.packages\", \"com.microsoft.azure:synapseml_2.12:0.10.2\")\n",
    "    .config(\"spark.jars.repositories\", \"https://mmlspark.azureedge.net/maven\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "from synapse.ml.lightgbm import LightGBMRegressor as SynapseLGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9efe100-a3b8-4b23-93c5-8235cbe4e6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_series = spark.createDataFrame(series).repartitionByRange(4, 'unique_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071e6bb6-7cfe-4208-bae9-8e8b4cc0cfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost.spark import SparkXGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba790bc5-5a6d-426d-ad5e-74705f815725",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst = DistributedMLForecast(\n",
    "    [\n",
    "        SynapseLGBMRegressor(),\n",
    "        SparkXGBRegressor()\n",
    "    ],\n",
    "    freq='D',\n",
    "    lags=[1],\n",
    "    lag_transforms={\n",
    "        1: [expanding_mean]\n",
    "    },\n",
    "    date_features=['dayofweek'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec77108e-0cc2-4774-9591-9e9480b5438b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[21:02:53] task 0 got new rank 0                                    (0 + 1) / 1]\n",
      "/home/jose/mambaforge/envs/mlforecast/lib/python3.10/site-packages/xgboost/sklearn.py:808: UserWarning: Loading a native XGBoost model with Scikit-Learn interface.\n",
      "  warnings.warn(\"Loading a native XGBoost model with Scikit-Learn interface.\")\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>LightGBMRegressor</th>\n",
       "      <th>SparkXGBRegressor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-15</td>\n",
       "      <td>42.227353</td>\n",
       "      <td>42.170174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-16</td>\n",
       "      <td>49.678239</td>\n",
       "      <td>49.665058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-17</td>\n",
       "      <td>1.425428</td>\n",
       "      <td>2.123648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-18</td>\n",
       "      <td>10.021719</td>\n",
       "      <td>10.150534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-19</td>\n",
       "      <td>18.167309</td>\n",
       "      <td>18.219870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-24</td>\n",
       "      <td>43.879492</td>\n",
       "      <td>43.089096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-25</td>\n",
       "      <td>1.592245</td>\n",
       "      <td>1.300191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-26</td>\n",
       "      <td>8.765065</td>\n",
       "      <td>8.563540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-27</td>\n",
       "      <td>15.720301</td>\n",
       "      <td>15.627042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-28</td>\n",
       "      <td>22.443045</td>\n",
       "      <td>23.139147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1400 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     unique_id         ds  LightGBMRegressor  SparkXGBRegressor\n",
       "0        id_00 2001-05-15          42.227353          42.170174\n",
       "1        id_00 2001-05-16          49.678239          49.665058\n",
       "2        id_00 2001-05-17           1.425428           2.123648\n",
       "3        id_00 2001-05-18          10.021719          10.150534\n",
       "4        id_00 2001-05-19          18.167309          18.219870\n",
       "...        ...        ...                ...                ...\n",
       "1395     id_99 2001-05-24          43.879492          43.089096\n",
       "1396     id_99 2001-05-25           1.592245           1.300191\n",
       "1397     id_99 2001-05-26           8.765065           8.563540\n",
       "1398     id_99 2001-05-27          15.720301          15.627042\n",
       "1399     id_99 2001-05-28          22.443045          23.139147\n",
       "\n",
       "[1400 rows x 4 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcst.fit(\n",
    "    spark_series,\n",
    "    id_col='unique_id',\n",
    "    time_col='ds',\n",
    "    target_col='y',\n",
    "    static_features=['static_0', 'product_id'],\n",
    ")\n",
    "fcst.predict(14, dynamic_dfs=[prices]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05d12b6-67f7-4a57-b341-b9b8d8400c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[21:03:12] task 0 got new rank 0                                    (0 + 1) / 1]\n",
      "/home/jose/mambaforge/envs/mlforecast/lib/python3.10/site-packages/xgboost/sklearn.py:808: UserWarning: Loading a native XGBoost model with Scikit-Learn interface.\n",
      "  warnings.warn(\"Loading a native XGBoost model with Scikit-Learn interface.\")\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>LightGBMRegressor</th>\n",
       "      <th>SparkXGBRegressor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-04-17</td>\n",
       "      <td>41.397289</td>\n",
       "      <td>41.412704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-04-18</td>\n",
       "      <td>50.008758</td>\n",
       "      <td>49.848236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-04-19</td>\n",
       "      <td>1.868972</td>\n",
       "      <td>1.531837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-04-20</td>\n",
       "      <td>10.266771</td>\n",
       "      <td>9.750021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-04-21</td>\n",
       "      <td>18.296489</td>\n",
       "      <td>17.535915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-04-26</td>\n",
       "      <td>43.910674</td>\n",
       "      <td>43.634216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-04-27</td>\n",
       "      <td>1.955158</td>\n",
       "      <td>1.962174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-04-28</td>\n",
       "      <td>8.864172</td>\n",
       "      <td>8.601549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-04-29</td>\n",
       "      <td>15.983674</td>\n",
       "      <td>15.883894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-04-30</td>\n",
       "      <td>22.950702</td>\n",
       "      <td>23.148775</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1400 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     unique_id         ds  LightGBMRegressor  SparkXGBRegressor\n",
       "0        id_00 2001-04-17          41.397289          41.412704\n",
       "1        id_00 2001-04-18          50.008758          49.848236\n",
       "2        id_00 2001-04-19           1.868972           1.531837\n",
       "3        id_00 2001-04-20          10.266771           9.750021\n",
       "4        id_00 2001-04-21          18.296489          17.535915\n",
       "...        ...        ...                ...                ...\n",
       "1395     id_99 2001-04-26          43.910674          43.634216\n",
       "1396     id_99 2001-04-27           1.955158           1.962174\n",
       "1397     id_99 2001-04-28           8.864172           8.601549\n",
       "1398     id_99 2001-04-29          15.983674          15.883894\n",
       "1399     id_99 2001-04-30          22.950702          23.148775\n",
       "\n",
       "[1400 rows x 4 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_res = fcst.cross_validation(spark_series, n_windows=2, window_size=14, id_col='unique_id', time_col='ds', target_col='y')\n",
    "res1 = next(cv_res)\n",
    "res1.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a734f48-fb9b-4d08-9548-ca0b53661563",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[21:03:31] task 0 got new rank 0                                    (0 + 1) / 1]\n",
      "/home/jose/mambaforge/envs/mlforecast/lib/python3.10/site-packages/xgboost/sklearn.py:808: UserWarning: Loading a native XGBoost model with Scikit-Learn interface.\n",
      "  warnings.warn(\"Loading a native XGBoost model with Scikit-Learn interface.\")\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>LightGBMRegressor</th>\n",
       "      <th>SparkXGBRegressor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-01</td>\n",
       "      <td>41.510874</td>\n",
       "      <td>41.991829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-02</td>\n",
       "      <td>49.580176</td>\n",
       "      <td>49.946548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-03</td>\n",
       "      <td>1.699062</td>\n",
       "      <td>1.733775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-04</td>\n",
       "      <td>10.143278</td>\n",
       "      <td>9.953071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-05</td>\n",
       "      <td>18.001024</td>\n",
       "      <td>17.789112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-10</td>\n",
       "      <td>43.846853</td>\n",
       "      <td>43.703159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-11</td>\n",
       "      <td>1.832246</td>\n",
       "      <td>1.999386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-12</td>\n",
       "      <td>8.805302</td>\n",
       "      <td>8.618651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-13</td>\n",
       "      <td>15.946444</td>\n",
       "      <td>15.728466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-14</td>\n",
       "      <td>22.699001</td>\n",
       "      <td>23.113226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1400 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     unique_id         ds  LightGBMRegressor  SparkXGBRegressor\n",
       "0        id_00 2001-05-01          41.510874          41.991829\n",
       "1        id_00 2001-05-02          49.580176          49.946548\n",
       "2        id_00 2001-05-03           1.699062           1.733775\n",
       "3        id_00 2001-05-04          10.143278           9.953071\n",
       "4        id_00 2001-05-05          18.001024          17.789112\n",
       "...        ...        ...                ...                ...\n",
       "1395     id_99 2001-05-10          43.846853          43.703159\n",
       "1396     id_99 2001-05-11           1.832246           1.999386\n",
       "1397     id_99 2001-05-12           8.805302           8.618651\n",
       "1398     id_99 2001-05-13          15.946444          15.728466\n",
       "1399     id_99 2001-05-14          22.699001          23.113226\n",
       "\n",
       "[1400 rows x 4 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res2 = next(cv_res)\n",
    "res2.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf3322f-3a0f-47d8-92f2-689b525111aa",
   "metadata": {},
   "source": [
    "## Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2228d0-e004-4e40-b413-314b54c36056",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "from mlforecast.distributed.models.lgb import LGBMForecast\n",
    "from mlforecast.distributed.models.xgb import XGBForecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18a5876-b898-4b48-940c-d5046397c23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(n_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163e9f0d-9d2e-41f6-a801-8f16395adb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_series = (\n",
    "    dd\n",
    "    .from_pandas(series.set_index('unique_id'), npartitions=4)  # make sure we split by the series identifier\n",
    "    .map_partitions(lambda df: df.reset_index())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f96a1f8-0b77-4327-a5d7-28bc69873466",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst = DistributedMLForecast(\n",
    "    [LGBMForecast(), XGBForecast()],\n",
    "    freq='D',\n",
    "    lags=[1],\n",
    "    lag_transforms={\n",
    "        1: [expanding_mean]\n",
    "    },\n",
    "    date_features=['dayofweek'],\n",
    "    engine=client,\n",
    ")\n",
    "_ = fcst.fit(dask_series, id_col='unique_id', time_col='ds', target_col='y', static_features=['static_0', 'product_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fffef75-fcfc-4014-8ac0-e8dcf09c9b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>LGBMForecast</th>\n",
       "      <th>XGBForecast</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-15</td>\n",
       "      <td>42.328150</td>\n",
       "      <td>42.544678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-16</td>\n",
       "      <td>49.966854</td>\n",
       "      <td>50.006393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-17</td>\n",
       "      <td>1.614102</td>\n",
       "      <td>2.404574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-18</td>\n",
       "      <td>10.246828</td>\n",
       "      <td>10.144515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-19</td>\n",
       "      <td>18.183167</td>\n",
       "      <td>17.768204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-24</td>\n",
       "      <td>42.986614</td>\n",
       "      <td>43.913208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-25</td>\n",
       "      <td>1.536043</td>\n",
       "      <td>1.769517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-26</td>\n",
       "      <td>8.651628</td>\n",
       "      <td>8.689207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-27</td>\n",
       "      <td>15.467835</td>\n",
       "      <td>15.858318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-28</td>\n",
       "      <td>22.620816</td>\n",
       "      <td>22.818695</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1400 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    unique_id         ds  LGBMForecast  XGBForecast\n",
       "0       id_00 2001-05-15     42.328150    42.544678\n",
       "1       id_00 2001-05-16     49.966854    50.006393\n",
       "2       id_00 2001-05-17      1.614102     2.404574\n",
       "3       id_00 2001-05-18     10.246828    10.144515\n",
       "4       id_00 2001-05-19     18.183167    17.768204\n",
       "..        ...        ...           ...          ...\n",
       "345     id_99 2001-05-24     42.986614    43.913208\n",
       "346     id_99 2001-05-25      1.536043     1.769517\n",
       "347     id_99 2001-05-26      8.651628     8.689207\n",
       "348     id_99 2001-05-27     15.467835    15.858318\n",
       "349     id_99 2001-05-28     22.620816    22.818695\n",
       "\n",
       "[1400 rows x 4 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcst.predict(14, dynamic_dfs=[prices]).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a9d631-4e90-49be-9a1a-fca5ba9008e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_res = fcst.cross_validation(dask_series, n_windows=2, window_size=14, id_col='unique_id', time_col='ds', target_col='y')\n",
    "res1 = next(cv_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06c89e7-6747-4537-b79b-57027c16d5c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>LGBMForecast</th>\n",
       "      <th>XGBForecast</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-04-17</td>\n",
       "      <td>41.101257</td>\n",
       "      <td>41.509098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-04-18</td>\n",
       "      <td>49.439778</td>\n",
       "      <td>49.994793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-04-19</td>\n",
       "      <td>2.209694</td>\n",
       "      <td>1.885389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-04-20</td>\n",
       "      <td>10.016894</td>\n",
       "      <td>9.791873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-04-21</td>\n",
       "      <td>18.006730</td>\n",
       "      <td>17.518440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-04-26</td>\n",
       "      <td>44.320700</td>\n",
       "      <td>42.248009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-04-27</td>\n",
       "      <td>2.230125</td>\n",
       "      <td>2.526252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-04-28</td>\n",
       "      <td>8.579381</td>\n",
       "      <td>8.485373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-04-29</td>\n",
       "      <td>15.496969</td>\n",
       "      <td>15.847349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-04-30</td>\n",
       "      <td>22.584693</td>\n",
       "      <td>23.249107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1400 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    unique_id         ds  LGBMForecast  XGBForecast\n",
       "0       id_00 2001-04-17     41.101257    41.509098\n",
       "1       id_00 2001-04-18     49.439778    49.994793\n",
       "2       id_00 2001-04-19      2.209694     1.885389\n",
       "3       id_00 2001-04-20     10.016894     9.791873\n",
       "4       id_00 2001-04-21     18.006730    17.518440\n",
       "..        ...        ...           ...          ...\n",
       "345     id_99 2001-04-26     44.320700    42.248009\n",
       "346     id_99 2001-04-27      2.230125     2.526252\n",
       "347     id_99 2001-04-28      8.579381     8.485373\n",
       "348     id_99 2001-04-29     15.496969    15.847349\n",
       "349     id_99 2001-04-30     22.584693    23.249107\n",
       "\n",
       "[1400 rows x 4 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res1.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9c81ac-d5c4-4388-8159-e61d6b2357d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = next(cv_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d253c880-c025-4d8e-8944-a8ef8539b53a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>LGBMForecast</th>\n",
       "      <th>XGBForecast</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-01</td>\n",
       "      <td>42.610164</td>\n",
       "      <td>41.107208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-02</td>\n",
       "      <td>50.179124</td>\n",
       "      <td>50.424072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-03</td>\n",
       "      <td>1.690276</td>\n",
       "      <td>1.991987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-04</td>\n",
       "      <td>10.159849</td>\n",
       "      <td>9.895548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2001-05-05</td>\n",
       "      <td>18.321141</td>\n",
       "      <td>17.538578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-10</td>\n",
       "      <td>42.920872</td>\n",
       "      <td>43.232620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-11</td>\n",
       "      <td>1.932801</td>\n",
       "      <td>1.821584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-12</td>\n",
       "      <td>8.724589</td>\n",
       "      <td>8.611847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-13</td>\n",
       "      <td>15.355793</td>\n",
       "      <td>15.651299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>id_99</td>\n",
       "      <td>2001-05-14</td>\n",
       "      <td>22.728723</td>\n",
       "      <td>23.504103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1400 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    unique_id         ds  LGBMForecast  XGBForecast\n",
       "0       id_00 2001-05-01     42.610164    41.107208\n",
       "1       id_00 2001-05-02     50.179124    50.424072\n",
       "2       id_00 2001-05-03      1.690276     1.991987\n",
       "3       id_00 2001-05-04     10.159849     9.895548\n",
       "4       id_00 2001-05-05     18.321141    17.538578\n",
       "..        ...        ...           ...          ...\n",
       "345     id_99 2001-05-10     42.920872    43.232620\n",
       "346     id_99 2001-05-11      1.932801     1.821584\n",
       "347     id_99 2001-05-12      8.724589     8.611847\n",
       "348     id_99 2001-05-13     15.355793    15.651299\n",
       "349     id_99 2001-05-14     22.728723    23.504103\n",
       "\n",
       "[1400 rows x 4 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res2.compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
