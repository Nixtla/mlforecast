{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccb7770-5b63-4263-a196-855b012d9451",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp cli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36212c1f-6071-4910-b0dd-abc10d97f7b5",
   "metadata": {},
   "source": [
    "# CLI\n",
    "\n",
    "> Run the forecasting pipeline with configuration files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b586d9a4-005d-41a3-9599-fe0986a243d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import typer\n",
    "\n",
    "from mlforecast.api import (\n",
    "    S3Path,\n",
    "    _is_s3_path,\n",
    "    _path_as_str,\n",
    "    fcst_from_config,\n",
    "    parse_config,\n",
    "    perform_backtest,\n",
    "    read_data,\n",
    "    setup_client,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ee6a57-4a8a-466d-83dd-e01bdd231a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "app = typer.Typer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da347be-7fa1-4b17-ba36-aac9c256efeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@app.command()\n",
    "def run_forecast(config_file: str):\n",
    "    \"\"\"Run the forecasting pipeline using the configuration defined in `config_file`.\"\"\"\n",
    "    config = parse_config(config_file)\n",
    "    is_distributed = config.distributed is not None\n",
    "    if config.distributed is not None:  # mypy\n",
    "        client = setup_client(config.distributed.cluster)\n",
    "    try:\n",
    "        data = read_data(config.data, is_distributed)\n",
    "        prefix = config.data.prefix\n",
    "        path = S3Path.from_uri(prefix) if _is_s3_path(prefix) else Path(prefix)\n",
    "        output_path = path / config.data.output\n",
    "        output_path.mkdir(exist_ok=True)\n",
    "\n",
    "        fcst = fcst_from_config(config)\n",
    "        if config.backtest is not None:\n",
    "            perform_backtest(fcst, data, config, output_path)\n",
    "        if config.forecast is not None:\n",
    "            fcst.fit(data)\n",
    "            preds = fcst.predict(config.forecast.horizon)\n",
    "            writer = getattr(preds, f'to_{config.data.format}')\n",
    "            write_path = _path_as_str(output_path / 'forecast')\n",
    "            if isinstance(data, pd.DataFrame):\n",
    "                write_path += f'.{config.data.format}'\n",
    "            writer(write_path)\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "    finally:\n",
    "        if is_distributed:\n",
    "            client.cluster.close()\n",
    "            client.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf5f891-2848-4321-a3eb-3b0c05ee7085",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import shutil\n",
    "import tempfile\n",
    "from pprint import pprint\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import yaml\n",
    "from mlforecast.utils import generate_daily_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253266d6-3f16-48c7-8deb-c53bd2ee3c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "series = generate_daily_series(20, 100, 200)\n",
    "\n",
    "for data_format in ('csv', 'parquet'):\n",
    "    config_name = 'local.yaml'\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        tmpdir = Path(tmpdir)\n",
    "        train_path = f'train.{data_format}'\n",
    "        config_path = tmpdir/config_name\n",
    "        writer = getattr(series, f'to_{data_format}')\n",
    "        writer(tmpdir/train_path)\n",
    "\n",
    "        with open(f'../sample_configs/{config_name}', 'rt') as f:\n",
    "            cfg = yaml.safe_load(f)\n",
    "        cfg['data']['prefix'] = str(tmpdir)\n",
    "        cfg['data']['input'] = train_path\n",
    "        cfg['data']['format'] = data_format\n",
    "        with open(config_path, 'wt') as f:\n",
    "            yaml.dump(cfg, f)\n",
    "        run_forecast(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74f6bd3-07a5-4419-b52e-c54c65fd0a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#distributed\n",
    "#hide\n",
    "series_ddf = dd.from_pandas(series, npartitions=2)\n",
    "\n",
    "for data_format in ('csv', 'parquet'):\n",
    "    config_name = 'distributed.yaml'\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        tmpdir = Path(tmpdir)\n",
    "        train_path = 'train'\n",
    "        config_path = tmpdir/config_name\n",
    "        writer = getattr(series_ddf, f'to_{data_format}')\n",
    "        writer(tmpdir/train_path)\n",
    "\n",
    "        with open(f'../sample_configs/{config_name}', 'rt') as f:\n",
    "            cfg = yaml.safe_load(f)\n",
    "        cfg['data']['prefix'] = str(tmpdir)\n",
    "        cfg['data']['input'] = train_path\n",
    "        cfg['data']['format'] = data_format\n",
    "        with open(config_path, 'wt') as f:\n",
    "            yaml.dump(cfg, f)\n",
    "        run_forecast(config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0f1304-9ca5-4101-8bfe-3c8c073446d7",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9add99-c498-445a-97c1-d4b86aa04927",
   "metadata": {},
   "source": [
    "If you want to run the forecasting pipeline from the CLI you first need to save your data and define a configuration file. Sample configurations are provided in `sample_configs/local.yaml` and `sample_configs/distributed.yaml`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5eedd5-41eb-4c00-af8f-f42c6730f20d",
   "metadata": {},
   "source": [
    "### Local"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d323a8e3-fb7a-4af9-b95a-f2c9e8ffba20",
   "metadata": {},
   "source": [
    "We can run the forecasting pipeline defined in `sample_configs/local.yaml` by saving our data and using the following configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b8bdae-05ec-4564-8ce3-89ef7863b6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../sample_configs/local.yaml', 'rt') as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "pprint(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06cbcdc-f34e-43cc-85f1-7771240922b5",
   "metadata": {},
   "source": [
    "This will look for a single file in the directory `data` called `train` (`data.prefix/data.input`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a889065a-666d-4e07-bb03-c346d3ffba24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "data_path = Path('data')\n",
    "data_path.mkdir()\n",
    "series.to_parquet(data_path/'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60399a42-71a8-4032-8752-52b4ad4d7794",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path('data/train').is_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175c816a-63e5-4511-8beb-bb695b118a0a",
   "metadata": {},
   "source": [
    "Then we just call `mlforecast` with this configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b4556a-addf-4a8e-b99b-306f75fd55bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mlforecast ../sample_configs/local.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6a17dd-787f-40c0-a308-8a79384834c2",
   "metadata": {},
   "source": [
    "We can see our results have been saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5626bab-1cd5-4b81-bcd3-bfad16c0f8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(Path('data/outputs').iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bee3824-0ed1-46ed-b032-8a0dfe1e987b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "assert 'forecast.parquet' in [file.name for file in (data_path/'outputs').iterdir()]\n",
    "shutil.rmtree(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6b93f5-c640-4033-a3f1-b88225aad636",
   "metadata": {},
   "source": [
    "### Distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0b4233-8ec9-4c33-96e3-621ad9671f51",
   "metadata": {},
   "source": [
    "We can also use the CLI to run the distributed forecasting pipeline. To do this we need to save our data in partitions and fill the distributed key of the configuration file (instead of local). A sample configuration is provided in `sample_configs/distributed.yaml`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd6ddb9-3984-4314-b2e2-667b5da402b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../sample_configs/distributed.yaml', 'rt') as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "pprint(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f07562-ae1b-413a-acc4-44254ebe3b43",
   "metadata": {},
   "source": [
    "Notice that we use `cluster.class_name` to define the cluster that we want to instantiate, as well as its keyword arguments. This example will use a `dask.distributed.LocalCluster`, however you can fill any other cluster you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482bad6b-60cf-469a-853f-8e7b621ae563",
   "metadata": {},
   "outputs": [],
   "source": [
    "#distributed\n",
    "#hide\n",
    "data_path = Path('data')\n",
    "data_path.mkdir()\n",
    "series_ddf = dd.from_pandas(series, npartitions=2)\n",
    "series_ddf.to_parquet('data/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8151ad-22ad-43ec-a2d6-d3833f4eee00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#distributed\n",
    "list(Path('data/train').iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db07f5d-a000-4a48-ac40-7e58b2adb3d5",
   "metadata": {},
   "source": [
    "Note that we have split our data in two partitions and have specified that we want two workers from our cluster (`distributed.cluster.class_kwargs.n_workers`). **If you're using a remote cluster use a remote storage like S3**.\n",
    "\n",
    "To run the pipeline we just call `mlforecast` with this configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6ae3b5-2d45-4f3e-9c83-4ad7c2231b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#distributed\n",
    "!mlforecast ../sample_configs/distributed.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa08497-dd03-4349-82b2-bf4dfccc5a9c",
   "metadata": {},
   "source": [
    "We can see our results have been saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2131525-9c59-4dd5-88b8-b35d2bd90f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#distributed\n",
    "list(Path('data/outputs').iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b2a9ed-0057-4b9b-9e10-1b5faf0c8892",
   "metadata": {},
   "outputs": [],
   "source": [
    "#distributed\n",
    "#hide\n",
    "assert (data_path/'outputs'/'forecast').is_dir()\n",
    "shutil.rmtree(data_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
