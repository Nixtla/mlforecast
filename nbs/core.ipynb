{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import copy\n",
    "import inspect\n",
    "import reprlib\n",
    "import warnings\n",
    "from collections import Counter, OrderedDict\n",
    "from contextlib import contextmanager\n",
    "from pathlib import Path\n",
    "from typing import (\n",
    "    Any,\n",
    "    Callable,\n",
    "    Dict,\n",
    "    Iterable,\n",
    "    Iterator,\n",
    "    List,\n",
    "    Mapping,\n",
    "    Optional,\n",
    "    Tuple,\n",
    "    Union,\n",
    ")\n",
    "\n",
    "import cloudpickle\n",
    "import fsspec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utilsforecast.processing as ufp\n",
    "from sklearn.base import BaseEstimator, clone\n",
    "from sklearn.pipeline import Pipeline\n",
    "from utilsforecast.compat import (\n",
    "    DFType,\n",
    "    DataFrame,\n",
    "    pl,\n",
    "    pl_DataFrame,\n",
    "    pl_Series,\n",
    ")\n",
    "from utilsforecast.validation import validate_format, validate_freq\n",
    "\n",
    "from mlforecast.grouped_array import GroupedArray\n",
    "from mlforecast.lag_transforms import _BaseLagTransform, Lag\n",
    "from mlforecast.target_transforms import (\n",
    "    _BaseGroupedArrayTargetTransform,\n",
    "    BaseTargetTransform,\n",
    ")\n",
    "from mlforecast.utils import _ShortSeriesException, _ensure_shallow_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import tempfile\n",
    "from nbdev import show_doc\n",
    "from fastcore.test import test_eq, test_fail, test_warns\n",
    "from window_ops.expanding import expanding_mean\n",
    "from window_ops.rolling import rolling_mean\n",
    "from window_ops.shift import shift_array\n",
    "\n",
    "from mlforecast.callbacks import SaveFeatures\n",
    "from mlforecast.lag_transforms import ExpandingMean, RollingMean\n",
    "from mlforecast.target_transforms import Differences, LocalStandardScaler\n",
    "from mlforecast.utils import generate_daily_series, generate_prices_for_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The required input format is a dataframe with at least the following columns:\n",
    "* `unique_id` with a unique identifier for each time serie\n",
    "* `ds` with the datestamp and a column\n",
    "* `y` with the values of the serie.\n",
    "\n",
    "Every other column is considered a static feature unless stated otherwise in `TimeSeries.fit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>static_0</th>\n",
       "      <th>static_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>7.404529</td>\n",
       "      <td>27</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-01-02</td>\n",
       "      <td>35.952624</td>\n",
       "      <td>27</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>68.958353</td>\n",
       "      <td>27</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-01-04</td>\n",
       "      <td>84.994505</td>\n",
       "      <td>27</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-01-05</td>\n",
       "      <td>113.219810</td>\n",
       "      <td>27</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4869</th>\n",
       "      <td>id_19</td>\n",
       "      <td>2000-03-25</td>\n",
       "      <td>400.606807</td>\n",
       "      <td>97</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4870</th>\n",
       "      <td>id_19</td>\n",
       "      <td>2000-03-26</td>\n",
       "      <td>538.794824</td>\n",
       "      <td>97</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4871</th>\n",
       "      <td>id_19</td>\n",
       "      <td>2000-03-27</td>\n",
       "      <td>620.202104</td>\n",
       "      <td>97</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4872</th>\n",
       "      <td>id_19</td>\n",
       "      <td>2000-03-28</td>\n",
       "      <td>20.625426</td>\n",
       "      <td>97</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4873</th>\n",
       "      <td>id_19</td>\n",
       "      <td>2000-03-29</td>\n",
       "      <td>141.513169</td>\n",
       "      <td>97</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4874 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     unique_id         ds           y static_0 static_1\n",
       "0        id_00 2000-01-01    7.404529       27       53\n",
       "1        id_00 2000-01-02   35.952624       27       53\n",
       "2        id_00 2000-01-03   68.958353       27       53\n",
       "3        id_00 2000-01-04   84.994505       27       53\n",
       "4        id_00 2000-01-05  113.219810       27       53\n",
       "...        ...        ...         ...      ...      ...\n",
       "4869     id_19 2000-03-25  400.606807       97       45\n",
       "4870     id_19 2000-03-26  538.794824       97       45\n",
       "4871     id_19 2000-03-27  620.202104       97       45\n",
       "4872     id_19 2000-03-28   20.625426       97       45\n",
       "4873     id_19 2000-03-29  141.513169       97       45\n",
       "\n",
       "[4874 rows x 5 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series = generate_daily_series(20, n_static_features=2)\n",
    "series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity we'll just take one time serie here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>static_0</th>\n",
       "      <th>static_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>7.404529</td>\n",
       "      <td>27</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-01-02</td>\n",
       "      <td>35.952624</td>\n",
       "      <td>27</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>68.958353</td>\n",
       "      <td>27</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-01-04</td>\n",
       "      <td>84.994505</td>\n",
       "      <td>27</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-01-05</td>\n",
       "      <td>113.219810</td>\n",
       "      <td>27</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-08-05</td>\n",
       "      <td>13.263188</td>\n",
       "      <td>27</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-08-06</td>\n",
       "      <td>38.231981</td>\n",
       "      <td>27</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-08-07</td>\n",
       "      <td>59.555183</td>\n",
       "      <td>27</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-08-08</td>\n",
       "      <td>86.986368</td>\n",
       "      <td>27</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>id_00</td>\n",
       "      <td>2000-08-09</td>\n",
       "      <td>119.254810</td>\n",
       "      <td>27</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>222 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    unique_id         ds           y static_0 static_1\n",
       "0       id_00 2000-01-01    7.404529       27       53\n",
       "1       id_00 2000-01-02   35.952624       27       53\n",
       "2       id_00 2000-01-03   68.958353       27       53\n",
       "3       id_00 2000-01-04   84.994505       27       53\n",
       "4       id_00 2000-01-05  113.219810       27       53\n",
       "..        ...        ...         ...      ...      ...\n",
       "217     id_00 2000-08-05   13.263188       27       53\n",
       "218     id_00 2000-08-06   38.231981       27       53\n",
       "219     id_00 2000-08-07   59.555183       27       53\n",
       "220     id_00 2000-08-08   86.986368       27       53\n",
       "221     id_00 2000-08-09  119.254810       27       53\n",
       "\n",
       "[222 rows x 5 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uids = series['unique_id'].unique()\n",
    "serie = series[series['unique_id'].eq(uids[0])]\n",
    "serie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "date_features_dtypes = {\n",
    "    \"year\": np.uint16,\n",
    "    \"month\": np.uint8,\n",
    "    \"day\": np.uint8,\n",
    "    \"hour\": np.uint8,\n",
    "    \"minute\": np.uint8,\n",
    "    \"second\": np.uint8,\n",
    "    \"dayofyear\": np.uint16,\n",
    "    \"day_of_year\": np.uint16,\n",
    "    \"weekofyear\": np.uint8,\n",
    "    \"week\": np.uint8,\n",
    "    \"dayofweek\": np.uint8,\n",
    "    \"day_of_week\": np.uint8,\n",
    "    \"weekday\": np.uint8,\n",
    "    \"quarter\": np.uint8,\n",
    "    \"daysinmonth\": np.uint8,\n",
    "    \"is_month_start\": np.uint8,\n",
    "    \"is_month_end\": np.uint8,\n",
    "    \"is_quarter_start\": np.uint8,\n",
    "    \"is_quarter_end\": np.uint8,\n",
    "    \"is_year_start\": np.uint8,\n",
    "    \"is_year_end\": np.uint8,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def _build_function_transform_name(tfm: Callable, lag: int, *args) -> str:\n",
    "    \"\"\"Creates a name for a transformation based on `lag`, the name of the function and its arguments.\"\"\"\n",
    "    tfm_name = f\"{tfm.__name__}_lag{lag}\"\n",
    "    func_params = inspect.signature(tfm).parameters\n",
    "    func_args = list(func_params.items())[1:]  # remove input array argument\n",
    "    changed_params = [\n",
    "        f\"{name}{value}\"\n",
    "        for value, (name, arg) in zip(args, func_args)\n",
    "        if arg.default != value\n",
    "    ]\n",
    "    if changed_params:\n",
    "        tfm_name += \"_\" + \"_\".join(changed_params)\n",
    "    return tfm_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "test_eq(_build_function_transform_name(expanding_mean, 1), 'expanding_mean_lag1')\n",
    "test_eq(_build_function_transform_name(rolling_mean, 2, 7), 'rolling_mean_lag2_window_size7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _build_lag_transform_name(tfm: _BaseLagTransform, lag: int) -> str:\n",
    "    return tfm._get_name(lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_eq(_build_lag_transform_name(ExpandingMean(), 1), 'expanding_mean_lag1')\n",
    "test_eq(_build_lag_transform_name(RollingMean(7), 2), 'rolling_mean_lag2_window_size7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _build_transform_name(\n",
    "    tfm: Union[Callable, _BaseLagTransform], lag: int, *args\n",
    ") -> str:\n",
    "    if callable(tfm):\n",
    "        name = _build_function_transform_name(tfm, lag, *args)\n",
    "    else:\n",
    "        name = _build_lag_transform_name(tfm, lag)\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def _get_model_name(model) -> str:\n",
    "    if isinstance(model, Pipeline):\n",
    "        return _get_model_name(model.steps[-1][1])\n",
    "    return model.__class__.__name__\n",
    "\n",
    "def _name_models(current_names):\n",
    "    ctr = Counter(current_names)\n",
    "    if not ctr:\n",
    "        return []\n",
    "    if max(ctr.values()) < 2:\n",
    "        return current_names\n",
    "    names = current_names.copy()\n",
    "    for i, x in enumerate(reversed(current_names), start=1):\n",
    "        count = ctr[x]\n",
    "        if count > 1:\n",
    "            name = f\"{x}{count}\"\n",
    "            ctr[x] -= 1\n",
    "        else:\n",
    "            name = x\n",
    "        names[-i] = name\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# one duplicate\n",
    "names = ['a', 'b', 'a', 'c']\n",
    "expected = ['a', 'b', 'a2', 'c']\n",
    "actual = _name_models(names)\n",
    "assert actual == expected\n",
    "\n",
    "# no duplicates\n",
    "names = ['a', 'b', 'c']\n",
    "actual = _name_models(names)\n",
    "assert actual == names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _as_tuple(x):\n",
    "    \"\"\"Return a tuple from the input.\"\"\"\n",
    "    if isinstance(x, tuple):\n",
    "        return x\n",
    "    return (x,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "Freq = Union[int, str]\n",
    "Lags = Iterable[int]\n",
    "LagTransform = Union[Callable, Tuple[Callable, Any]]\n",
    "LagTransforms = Dict[int, List[LagTransform]]\n",
    "DateFeature = Union[str, Callable]\n",
    "Models = Union[BaseEstimator, List[BaseEstimator], Dict[str, BaseEstimator]]\n",
    "TargetTransform = Union[BaseTargetTransform, _BaseGroupedArrayTargetTransform]\n",
    "Transforms = Dict[str, Union[Tuple[Any, ...], _BaseLagTransform]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _parse_transforms(\n",
    "    lags: Lags,\n",
    "    lag_transforms: LagTransforms,\n",
    "    namer: Optional[Callable] = None,\n",
    ") -> Transforms:\n",
    "    transforms: Transforms = OrderedDict()\n",
    "    if namer is None:\n",
    "        namer = _build_transform_name\n",
    "    for lag in lags:\n",
    "        transforms[f'lag{lag}'] = Lag(lag)\n",
    "    for lag in lag_transforms.keys():\n",
    "        for tfm in lag_transforms[lag]:\n",
    "            if isinstance(tfm, _BaseLagTransform):\n",
    "                tfm_name = namer(tfm, lag)\n",
    "                transforms[tfm_name] = clone(tfm)._set_core_tfm(lag)\n",
    "            else:\n",
    "                tfm, *args = _as_tuple(tfm)\n",
    "                assert callable(tfm)\n",
    "                tfm_name = namer(tfm, lag, *args)\n",
    "                transforms[tfm_name] = (lag, tfm, *args)\n",
    "    return transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TimeSeries:\n",
    "    \"\"\"Utility class for storing and transforming time series data.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        freq: Freq,\n",
    "        lags: Optional[Lags] = None,\n",
    "        lag_transforms: Optional[LagTransforms] = None,\n",
    "        date_features: Optional[Iterable[DateFeature]] = None,\n",
    "        num_threads: int = 1,\n",
    "        target_transforms: Optional[List[TargetTransform]] = None,\n",
    "        lag_transforms_namer: Optional[Callable] = None,\n",
    "    ):\n",
    "        self.freq = freq\n",
    "        if not isinstance(num_threads, int) or num_threads < 1:\n",
    "            warnings.warn('Setting num_threads to 1.')\n",
    "            num_threads = 1\n",
    "        self.lags = [] if lags is None else list(lags)\n",
    "        for lag in self.lags:\n",
    "            if lag <= 0 or not isinstance(lag, int):\n",
    "                raise ValueError('lags must be positive integers.')\n",
    "        self.lag_transforms = {} if lag_transforms is None else lag_transforms\n",
    "        for lag in self.lag_transforms.keys():\n",
    "            if lag <= 0 or not isinstance(lag, int):\n",
    "                raise ValueError('keys of lag_transforms must be positive integers.')\n",
    "        self.date_features = [] if date_features is None else list(date_features)\n",
    "        self.num_threads = num_threads\n",
    "        self.target_transforms = target_transforms\n",
    "        if self.target_transforms is not None:\n",
    "            for tfm in self.target_transforms:\n",
    "                if isinstance(tfm, _BaseGroupedArrayTargetTransform):\n",
    "                    tfm.set_num_threads(num_threads)\n",
    "        for feature in self.date_features:\n",
    "            if callable(feature) and feature.__name__ == '<lambda>':\n",
    "                raise ValueError(\n",
    "                    \"Can't use a lambda as a date feature because the function name gets used as the feature name.\"\n",
    "                )\n",
    "        self.lag_transforms_namer = lag_transforms_namer\n",
    "        self.transforms = _parse_transforms(\n",
    "            lags=self.lags, lag_transforms=self.lag_transforms, namer=lag_transforms_namer\n",
    "        )\n",
    "        self.ga: GroupedArray\n",
    "\n",
    "    def _get_core_lag_tfms(self) -> Dict[str, _BaseLagTransform]:\n",
    "        return {k: v for k, v in self.transforms.items() if isinstance(v, _BaseLagTransform)}\n",
    "\n",
    "    @property\n",
    "    def _date_feature_names(self):\n",
    "        return [f.__name__ if callable(f) else f for f in self.date_features]\n",
    "        \n",
    "    @property\n",
    "    def features(self) -> List[str]:\n",
    "        \"\"\"Names of all computed features.\"\"\"\n",
    "        return list(self.transforms.keys()) + self._date_feature_names\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            f\"TimeSeries(freq={self.freq}, \"\n",
    "            f\"transforms={list(self.transforms.keys())}, \"\n",
    "            f\"date_features={self._date_feature_names}, \"\n",
    "            f\"num_threads={self.num_threads})\"\n",
    "        )\n",
    "\n",
    "    def _fit(\n",
    "        self,\n",
    "        df: DataFrame,\n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "    ) -> 'TimeSeries':\n",
    "        \"\"\"Save the series values, ids and last dates.\"\"\"\n",
    "        validate_format(df, id_col, time_col, target_col)\n",
    "        validate_freq(df[time_col], self.freq)\n",
    "        if ufp.is_nan_or_none(df[target_col]).any():\n",
    "            raise ValueError(f'{target_col} column contains null values.')\n",
    "        self.id_col = id_col\n",
    "        self.target_col = target_col\n",
    "        self.time_col = time_col\n",
    "        self.keep_last_n = keep_last_n\n",
    "        self.static_features = static_features\n",
    "        sorted_df = df[[id_col, time_col, target_col]]\n",
    "        sorted_df = ufp.copy_if_pandas(sorted_df, deep=False)\n",
    "        uids, times, data, indptr, self._sort_idxs = ufp.process_df(\n",
    "            df=sorted_df,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "        )\n",
    "        if data.ndim == 2:\n",
    "            data = data[:, 0]\n",
    "        ga = GroupedArray(data, indptr)\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            self.uids = pd.Index(uids)\n",
    "            self.last_dates = pd.Index(times)\n",
    "        else:\n",
    "            self.uids = uids\n",
    "            self.last_dates = pl_Series(times)\n",
    "        if self._sort_idxs is not None:\n",
    "            self._restore_idxs: Optional[np.ndarray] = np.empty(df.shape[0], dtype=np.int32)\n",
    "            self._restore_idxs[self._sort_idxs] = np.arange(df.shape[0])\n",
    "            sorted_df = ufp.take_rows(sorted_df, self._sort_idxs)\n",
    "        else:\n",
    "            self._restore_idxs = None\n",
    "        if self.target_transforms is not None:\n",
    "            for tfm in self.target_transforms:\n",
    "                if isinstance(tfm, _BaseGroupedArrayTargetTransform):\n",
    "                    try:\n",
    "                        ga = tfm.fit_transform(ga)\n",
    "                    except _ShortSeriesException as exc:\n",
    "                        tfm_name = tfm.__class__.__name__\n",
    "                        uids = reprlib.repr(list(self.uids[exc.args]))\n",
    "                        raise ValueError(\n",
    "                            f\"The following series are too short for the '{tfm_name}' transformation: {uids}.\"\n",
    "                        ) from None\n",
    "                    sorted_df = ufp.assign_columns(sorted_df, target_col, ga.data)\n",
    "                else:\n",
    "                    tfm.set_column_names(id_col, time_col, target_col)\n",
    "                    sorted_df = tfm.fit_transform(sorted_df)\n",
    "                    ga.data = sorted_df[target_col].to_numpy()\n",
    "        to_drop = [id_col, time_col, target_col]\n",
    "        if static_features is None:\n",
    "            static_features = [c for c in df.columns if c not in [time_col, target_col]]\n",
    "        elif id_col not in static_features:\n",
    "            static_features = [id_col] + static_features\n",
    "        else:  # static_features defined and contain id_col\n",
    "            to_drop = [time_col, target_col]\n",
    "        self.ga = ga\n",
    "        series_starts = ga.indptr[:-1]\n",
    "        series_ends = ga.indptr[1:] - 1\n",
    "        if self._sort_idxs is not None:\n",
    "            series_starts = self._sort_idxs[series_starts]\n",
    "            series_ends = self._sort_idxs[series_ends]\n",
    "        statics_on_starts = ufp.drop_index_if_pandas(\n",
    "            ufp.take_rows(df, series_starts)[static_features]\n",
    "        )\n",
    "        statics_on_ends = ufp.drop_index_if_pandas(\n",
    "            ufp.take_rows(df, series_ends)[static_features]\n",
    "        )\n",
    "        for feat in static_features:\n",
    "            if (statics_on_starts[feat] != statics_on_ends[feat]).any():\n",
    "                raise ValueError(\n",
    "                    f'{feat} is declared as a static feature but its values change '\n",
    "                    'over time. Please set the `static_features` argument to '\n",
    "                    'indicate which features are static.\\nIf all of your features '\n",
    "                    'are dynamic please set `static_features=[]`.'\n",
    "                )\n",
    "        self.static_features_ = statics_on_ends\n",
    "        self.features_order_ = [\n",
    "            c for c in df.columns if c not in to_drop\n",
    "        ] + [f for f in self.features if f not in df.columns]\n",
    "        return self\n",
    "\n",
    "    def _compute_transforms(\n",
    "        self,\n",
    "        transforms: Mapping[str, Union[Tuple[Any, ...], _BaseLagTransform]],\n",
    "        updates_only: bool,\n",
    "    ) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Compute the transformations defined in the constructor.\n",
    "\n",
    "        If `self.num_threads > 1` these are computed using multithreading.\"\"\"\n",
    "        if self.num_threads == 1 or len(transforms) == 1:\n",
    "            out = self.ga.apply_transforms(\n",
    "                transforms=transforms, updates_only=updates_only\n",
    "            )\n",
    "        else:\n",
    "            out = self.ga.apply_multithreaded_transforms(\n",
    "                transforms=transforms,\n",
    "                num_threads=self.num_threads,\n",
    "                updates_only=updates_only,\n",
    "            )\n",
    "        return out\n",
    "    \n",
    "    def _compute_date_feature(self, dates, feature): \n",
    "        if callable(feature):\n",
    "            feat_name = feature.__name__\n",
    "            feat_vals = feature(dates)\n",
    "        else:\n",
    "            feat_name = feature\n",
    "            if isinstance(dates, pd.DatetimeIndex):\n",
    "                if feature in ('week', 'weekofyear'):\n",
    "                    dates = dates.isocalendar()\n",
    "                feat_vals = getattr(dates, feature)\n",
    "            else:\n",
    "                feat_vals = getattr(dates.dt, feature)()\n",
    "        if isinstance(feat_vals, (pd.Index, pd.Series)):\n",
    "            feat_vals = np.asarray(feat_vals)\n",
    "            feat_dtype = date_features_dtypes.get(feature)\n",
    "            if feat_dtype is not None:\n",
    "                feat_vals = feat_vals.astype(feat_dtype)\n",
    "        return feat_name, feat_vals\n",
    "\n",
    "    def _transform(\n",
    "        self,\n",
    "        df: DFType,\n",
    "        dropna: bool = True,\n",
    "        max_horizon: Optional[int] = None,\n",
    "        return_X_y: bool = False,\n",
    "        as_numpy: bool = False,\n",
    "    ) -> DFType:\n",
    "        \"\"\"Add the features to `df`.\n",
    "        \n",
    "        if `dropna=True` then all the null rows are dropped.\"\"\"\n",
    "        # we need to compute all transformations in case they save state\n",
    "        features = self._compute_transforms(\n",
    "            transforms=self.transforms,\n",
    "            updates_only=False\n",
    "        )\n",
    "        # filter out the features that already exist in df to avoid overwriting them\n",
    "        features = {k: v for k, v in features.items() if k not in df}\n",
    "        if self._restore_idxs is not None:\n",
    "            for k, v in features.items():\n",
    "                features[k] = v[self._restore_idxs]\n",
    "\n",
    "        # target\n",
    "        self.max_horizon = max_horizon\n",
    "        if max_horizon is None:\n",
    "            target = self.ga.data\n",
    "        else:\n",
    "            target = self.ga.expand_target(max_horizon)\n",
    "        if self._restore_idxs is not None:\n",
    "            target = target[self._restore_idxs]       \n",
    "\n",
    "        # determine rows to keep\n",
    "        if dropna:\n",
    "            feature_nulls = np.full(df.shape[0], False)\n",
    "            for feature_vals in features.values():\n",
    "                feature_nulls |= np.isnan(feature_vals)\n",
    "            target_nulls = np.isnan(target)\n",
    "            if target_nulls.ndim == 2:\n",
    "                # target nulls for each horizon are dropped in MLForecast.fit_models\n",
    "                # we just drop rows here for which all the target values are null\n",
    "                target_nulls = target_nulls.all(axis=1)\n",
    "            keep_rows = ~(feature_nulls | target_nulls)\n",
    "            for k, v in features.items():\n",
    "                features[k] = v[keep_rows]\n",
    "            target = target[keep_rows]\n",
    "            df = ufp.filter_with_mask(df, keep_rows)\n",
    "            df = ufp.copy_if_pandas(df, deep=False)\n",
    "            last_idxs = self.ga.indptr[1:] - 1\n",
    "            if self._sort_idxs is not None:\n",
    "                last_idxs = self._sort_idxs[last_idxs]\n",
    "            last_vals_nan = ~keep_rows[last_idxs]\n",
    "            if last_vals_nan.any():\n",
    "                self._dropped_series: Optional[np.ndarray] = np.where(last_vals_nan)[0]                \n",
    "                dropped_ids = reprlib.repr(list(self.uids[self._dropped_series]))\n",
    "                warnings.warn(\n",
    "                    \"The following series were dropped completely \"\n",
    "                    f\"due to the transformations and features: {dropped_ids}.\\n\"\n",
    "                    \"These series won't show up if you use `MLForecast.forecast_fitted_values()`.\\n\"\n",
    "                    \"You can set `dropna=False` or use transformations that require less samples to mitigate this\"\n",
    "                )\n",
    "            else:\n",
    "                self._dropped_series = None\n",
    "        elif isinstance(df, pd.DataFrame):\n",
    "            df = df.copy(deep=False)\n",
    "            self._dropped_series = None\n",
    "\n",
    "        # once we've computed the features and target we can slice the series\n",
    "        if self.keep_last_n is not None:\n",
    "            self.ga = self.ga.take_from_groups(slice(-self.keep_last_n, None))         \n",
    "        del self._restore_idxs, self._sort_idxs\n",
    "\n",
    "        # lag transforms\n",
    "        for feat in self.transforms.keys():\n",
    "            if feat in features:\n",
    "                df = ufp.assign_columns(df, feat, features[feat])\n",
    "\n",
    "        # date features\n",
    "        names = [f.__name__ if callable(f) else f for f in self.date_features]\n",
    "        date_features = [f for f, name in zip(self.date_features, names) if name not in df]\n",
    "        if date_features:\n",
    "            unique_dates = df[self.time_col].unique()\n",
    "            if isinstance(df, pd.DataFrame):\n",
    "                # all kinds of trickery to make this fast\n",
    "                unique_dates = pd.Index(unique_dates)\n",
    "                date2pos = {date: i for i, date in enumerate(unique_dates)}\n",
    "                restore_idxs = df[self.time_col].map(date2pos)\n",
    "                for feature in date_features:\n",
    "                    feat_name, feat_vals = self._compute_date_feature(unique_dates, feature)\n",
    "                    df[feat_name] = feat_vals[restore_idxs]\n",
    "            elif isinstance(df, pl_DataFrame):\n",
    "                exprs = []\n",
    "                for feat in date_features:  # type: ignore\n",
    "                    name, vals = self._compute_date_feature(pl.col(self.time_col), feat)\n",
    "                    exprs.append(vals.alias(name))\n",
    "                feats = unique_dates.to_frame().with_columns(*exprs)\n",
    "                df = df.join(feats, on=self.time_col, how='left')\n",
    "\n",
    "        # assemble return\n",
    "        if return_X_y:\n",
    "            X = df[self.features_order_]\n",
    "            if as_numpy:\n",
    "                X = ufp.to_numpy(X)\n",
    "            return X, target\n",
    "        if max_horizon is not None:\n",
    "            # remove original target\n",
    "            out_cols = [c for c in df.columns if c != self.target_col]\n",
    "            df = df[out_cols]\n",
    "            target_names = [f\"{self.target_col}{i}\" for i in range(max_horizon)]\n",
    "            df = ufp.assign_columns(df, target_names, target)\n",
    "        else:\n",
    "            if isinstance(df, pd.DataFrame):\n",
    "                df = _ensure_shallow_copy(df)\n",
    "            df = ufp.assign_columns(df, self.target_col, target)\n",
    "        return df\n",
    "\n",
    "\n",
    "    def fit_transform(\n",
    "        self,\n",
    "        data: DFType,\n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "        max_horizon: Optional[int] = None,\n",
    "        return_X_y: bool = False,\n",
    "        as_numpy: bool = False,\n",
    "    ) -> Union[DFType, Tuple[DFType, np.ndarray]]:\n",
    "        \"\"\"Add the features to `data` and save the required information for the predictions step.\n",
    "        \n",
    "        If not all features are static, specify which ones are in `static_features`.\n",
    "        If you don't want to drop rows with null values after the transformations set `dropna=False`\n",
    "        If `keep_last_n` is not None then that number of observations is kept across all series for updates.\n",
    "        \"\"\"\n",
    "        self.dropna = dropna\n",
    "        self.as_numpy = as_numpy\n",
    "        self._fit(\n",
    "            df=data,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            static_features=static_features,\n",
    "            keep_last_n=keep_last_n,\n",
    "        )\n",
    "        return self._transform(\n",
    "            df=data,\n",
    "            dropna=dropna,\n",
    "            max_horizon=max_horizon,\n",
    "            return_X_y=return_X_y,\n",
    "            as_numpy=as_numpy,\n",
    "        )\n",
    "\n",
    "    def _update_y(self, new: np.ndarray) -> None:\n",
    "        \"\"\"Appends the elements of `new` to every time serie.\n",
    "\n",
    "        These values are used to update the transformations and are stored as predictions.\"\"\"\n",
    "        if not hasattr(self, 'y_pred'):\n",
    "            self.y_pred = []\n",
    "        self.y_pred.append(new)\n",
    "        new_arr = np.asarray(new)\n",
    "        self.ga = self.ga.append(new_arr)\n",
    "        \n",
    "    def _update_features(self) -> DataFrame:\n",
    "        \"\"\"Compute the current values of all the features using the latest values of the time series.\"\"\"\n",
    "        self.curr_dates: Union[pd.Index, pl_Series] = ufp.offset_times(self.curr_dates, self.freq, 1)\n",
    "        self.test_dates.append(self.curr_dates)\n",
    " \n",
    "        features = self._compute_transforms(self.transforms, updates_only=True)\n",
    " \n",
    "        for feature in self.date_features:\n",
    "            feat_name, feat_vals = self._compute_date_feature(self.curr_dates, feature)\n",
    "            features[feat_name] = feat_vals\n",
    "\n",
    "        if isinstance(self.last_dates, pl_Series):\n",
    "            df_constructor = pl_DataFrame\n",
    "        else:\n",
    "            df_constructor = pd.DataFrame\n",
    "        features_df = df_constructor(features)[self.features]\n",
    "        return ufp.horizontal_concat([self.static_features_, features_df])\n",
    "\n",
    "    def _get_raw_predictions(self) -> np.ndarray:\n",
    "        return np.array(self.y_pred).ravel('F')\n",
    "\n",
    "    def _get_future_ids(self, h: int):\n",
    "        if isinstance(self.uids, pl_Series):\n",
    "            uids = pl.concat([self.uids for _ in range(h)]).sort()\n",
    "        else:\n",
    "            uids = pd.Series(\n",
    "                np.repeat(self.uids, h), name=self.id_col, dtype=self.uids.dtype\n",
    "            )\n",
    "        return uids\n",
    "\n",
    "    def _get_predictions(self) -> DataFrame:\n",
    "        \"\"\"Get all the predicted values with their corresponding ids and datestamps.\"\"\"\n",
    "        h = len(self.y_pred)\n",
    "        if isinstance(self.uids, pl_Series):\n",
    "            df_constructor = pl_DataFrame\n",
    "        else:\n",
    "            df_constructor = pd.DataFrame\n",
    "        uids = self._get_future_ids(h)\n",
    "        df = df_constructor(\n",
    "            {\n",
    "                self.id_col: uids,\n",
    "                self.time_col: np.array(self.test_dates).ravel('F'),\n",
    "                f'{self.target_col}_pred': self._get_raw_predictions(),\n",
    "            },\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    def _get_features_for_next_step(self, X_df=None):\n",
    "        new_x = self._update_features()\n",
    "        if X_df is not None:\n",
    "            n_series = len(self.uids)\n",
    "            h = X_df.shape[0] // n_series\n",
    "            rows = np.arange(self._h, X_df.shape[0], h)\n",
    "            X = ufp.take_rows(X_df, rows)\n",
    "            X = ufp.drop_index_if_pandas(X)\n",
    "            new_x = ufp.horizontal_concat([new_x, X])\n",
    "        if isinstance(new_x, pd.DataFrame):\n",
    "            nulls = new_x.isnull().any()\n",
    "            cols_with_nulls = nulls[nulls].index.tolist()\n",
    "        else:\n",
    "            nulls = new_x.select(pl.all().is_null().any())\n",
    "            cols_with_nulls = [k for k, v in nulls.to_dicts()[0].items() if v]\n",
    "        if cols_with_nulls:\n",
    "            warnings.warn(\n",
    "                f'Found null values in {\", \".join(cols_with_nulls)}.'\n",
    "            )\n",
    "        self._h += 1\n",
    "        new_x = new_x[self.features_order_]\n",
    "        if self.as_numpy:\n",
    "            new_x = ufp.to_numpy(new_x)\n",
    "        return new_x\n",
    "\n",
    "    @contextmanager\n",
    "    def _backup(self) -> Iterator[None]:\n",
    "        # this gets modified during predict because the predictions are appended\n",
    "        ga = copy.copy(self.ga)\n",
    "        # if these save state (like ExpandingMean) they'll get modified by the updates\n",
    "        lag_tfms = copy.deepcopy(self.transforms)\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            self.ga = ga\n",
    "            self.transforms = lag_tfms\n",
    "\n",
    "    def _predict_setup(self) -> None:\n",
    "        # TODO: move to utils\n",
    "        if isinstance(self.last_dates, pl_Series):\n",
    "            self.curr_dates  = self.last_dates.clone()\n",
    "        else:\n",
    "            self.curr_dates  = self.last_dates.copy()\n",
    "        self.test_dates: List[Union[pd.Index, pl_Series]] = []\n",
    "        self.y_pred = []\n",
    "        self._h = 0\n",
    "\n",
    "    def _predict_recursive(\n",
    "        self,\n",
    "        models: Dict[str, BaseEstimator],\n",
    "        horizon: int,\n",
    "        before_predict_callback: Optional[Callable] = None,\n",
    "        after_predict_callback: Optional[Callable] = None,\n",
    "        X_df: Optional[DFType] = None,\n",
    "    ) -> DFType:\n",
    "        \"\"\"Use `model` to predict the next `horizon` timesteps.\"\"\"\n",
    "        for i, (name, model) in enumerate(models.items()):\n",
    "            with self._backup():\n",
    "                self._predict_setup()\n",
    "                for _ in range(horizon):\n",
    "                    new_x = self._get_features_for_next_step(X_df)\n",
    "                    if before_predict_callback is not None:\n",
    "                        new_x = before_predict_callback(new_x)\n",
    "                    predictions = model.predict(new_x)\n",
    "                    if after_predict_callback is not None:\n",
    "                        predictions = after_predict_callback(predictions)\n",
    "                    self._update_y(predictions)\n",
    "                if i == 0:\n",
    "                    preds = self._get_predictions()\n",
    "                    rename_dict = {f'{self.target_col}_pred': name}\n",
    "                    preds = ufp.rename(preds, rename_dict)\n",
    "                else:\n",
    "                    raw_preds = self._get_raw_predictions()\n",
    "                    preds = ufp.assign_columns(preds, name, raw_preds)\n",
    "        return preds\n",
    "\n",
    "    def _predict_multi(\n",
    "        self,\n",
    "        models: Dict[str, BaseEstimator],\n",
    "        horizon: int,\n",
    "        before_predict_callback: Optional[Callable] = None,\n",
    "        X_df: Optional[DFType] = None,\n",
    "    ) -> DFType:\n",
    "        assert self.max_horizon is not None\n",
    "        if horizon > self.max_horizon:\n",
    "            raise ValueError(f'horizon must be at most max_horizon ({self.max_horizon})')\n",
    "        self._predict_setup()\n",
    "        uids = self._get_future_ids(horizon)\n",
    "        starts = ufp.offset_times(self.curr_dates, self.freq, 1)\n",
    "        dates = ufp.time_ranges(starts, self.freq, periods=horizon)\n",
    "        if isinstance(self.curr_dates, pl_Series):\n",
    "            df_constructor = pl_DataFrame\n",
    "        else:\n",
    "            df_constructor = pd.DataFrame\n",
    "        result = df_constructor({self.id_col: uids, self.time_col: dates})\n",
    "        for name, model in models.items():\n",
    "            with self._backup():\n",
    "                new_x = self._get_features_for_next_step(X_df)\n",
    "                if before_predict_callback is not None:\n",
    "                    new_x = before_predict_callback(new_x)\n",
    "                predictions = np.empty((new_x.shape[0], horizon))\n",
    "                for i in range(horizon):\n",
    "                    predictions[:, i] = model[i].predict(new_x)\n",
    "                raw_preds = predictions.ravel()\n",
    "                result = ufp.assign_columns(result, name, raw_preds)\n",
    "        return result\n",
    "\n",
    "    def _has_ga_target_tfms(self):\n",
    "        return any(isinstance(tfm, _BaseGroupedArrayTargetTransform) for tfm in self.target_transforms)\n",
    "\n",
    "    @contextmanager\n",
    "    def _maybe_subset(self, idxs: Optional[np.ndarray]) -> Iterator[None]:\n",
    "        # save original\n",
    "        ga = self.ga\n",
    "        uids = self.uids\n",
    "        statics = self.static_features_\n",
    "        last_dates = self.last_dates\n",
    "        targ_tfms = copy.copy(self.target_transforms)\n",
    "        lag_tfms = copy.deepcopy(self.transforms)\n",
    "\n",
    "        if idxs is not None:\n",
    "            # assign subsets\n",
    "            self.ga = self.ga.take(idxs)            \n",
    "            self.uids = uids[idxs]\n",
    "            self.static_features_ = ufp.take_rows(statics, idxs)\n",
    "            self.static_features_ = ufp.drop_index_if_pandas(self.static_features_)\n",
    "            self.last_dates = last_dates[idxs]\n",
    "            if self.target_transforms is not None:\n",
    "                for i, tfm in enumerate(self.target_transforms):\n",
    "                    if isinstance(tfm, _BaseGroupedArrayTargetTransform):\n",
    "                        self.target_transforms[i] = tfm.take(idxs)\n",
    "            for name, lag_tfm in self.transforms.items():\n",
    "                if isinstance(lag_tfm, _BaseLagTransform):\n",
    "                    lag_tfm = lag_tfm.take(idxs)\n",
    "                self.transforms[name] = lag_tfm\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            self.ga = ga\n",
    "            self.uids = uids\n",
    "            self.static_features_ = statics\n",
    "            self.last_dates = last_dates\n",
    "            self.target_transforms = targ_tfms\n",
    "            self.lag_tfms = lag_tfms\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        models: Dict[str, Union[BaseEstimator, List[BaseEstimator]]],\n",
    "        horizon: int,\n",
    "        before_predict_callback: Optional[Callable] = None,\n",
    "        after_predict_callback: Optional[Callable] = None,\n",
    "        X_df: Optional[DFType] = None,\n",
    "        ids: Optional[List[str]] = None,\n",
    "    ) -> DFType:\n",
    "        if ids is not None:\n",
    "            unseen = set(ids) - set(self.uids)\n",
    "            if unseen:\n",
    "                raise ValueError(f\"The following ids weren't seen during training and thus can't be forecasted: {unseen}\")\n",
    "            idxs: Optional[np.ndarray] = np.where(ufp.is_in(self.uids, ids))[0]\n",
    "        else:\n",
    "            idxs = None\n",
    "        with self._maybe_subset(idxs):\n",
    "            if X_df is not None:\n",
    "                if self.id_col not in X_df or self.time_col not in X_df:\n",
    "                    raise ValueError(f\"X_df must have '{self.id_col}' and '{self.time_col}' columns.\")\n",
    "                if X_df.shape[1] < 3:\n",
    "                    raise ValueError(\"Found no exogenous features in `X_df`.\")\n",
    "                statics = [c for c in self.static_features_.columns if c != self.id_col]\n",
    "                dynamics = [c for c in X_df.columns if c not in [self.id_col, self.time_col]]\n",
    "                common = [c for c in dynamics if c in statics]\n",
    "                if common:\n",
    "                    raise ValueError(\n",
    "                        f\"The following features were provided through `X_df` but were considered as static during fit: {common}.\\n\"\n",
    "                        \"Please re-run the fit step using the `static_features` argument to indicate which features are static. \"\n",
    "                        \"If all your features are dynamic please pass an empty list (static_features=[]).\"\n",
    "                    )\n",
    "                starts = ufp.offset_times(self.last_dates, self.freq, 1)\n",
    "                ends = ufp.offset_times(self.last_dates, self.freq, horizon)\n",
    "                dates_validation = type(X_df)(\n",
    "                    {\n",
    "                        self.id_col: self.uids,\n",
    "                        '_start': starts,\n",
    "                        '_end': ends,\n",
    "                    }\n",
    "                )\n",
    "                X_df = ufp.join(X_df, dates_validation, on=self.id_col)\n",
    "                mask = ufp.between(X_df[self.time_col], X_df['_start'], X_df['_end'])\n",
    "                X_df = ufp.filter_with_mask(X_df, mask)\n",
    "                if X_df.shape[0] != len(self.uids) * horizon:\n",
    "                    msg = (\n",
    "                        \"Found missing inputs in X_df. \"\n",
    "                        \"It should have one row per id and time for the complete forecasting horizon.\\n\"\n",
    "                        \"You can get the expected structure by running `MLForecast.make_future_dataframe(h)` \"\n",
    "                        \"or get the missing combinatins in your current `X_df` by running `MLForecast.get_missing_future(h, X_df)`.\"\n",
    "                    ) \n",
    "                    raise ValueError(msg)\n",
    "                drop_cols = [self.id_col, self.time_col, '_start', '_end']\n",
    "                X_df = ufp.sort(X_df, [self.id_col, self.time_col])\n",
    "                X_df = ufp.drop_columns(X_df, drop_cols)\n",
    "            if getattr(self, 'max_horizon', None) is None:\n",
    "                preds = self._predict_recursive(\n",
    "                    models=models,\n",
    "                    horizon=horizon,\n",
    "                    before_predict_callback=before_predict_callback,\n",
    "                    after_predict_callback=after_predict_callback,\n",
    "                    X_df=X_df,\n",
    "                )\n",
    "            else:\n",
    "                preds = self._predict_multi(\n",
    "                    models=models,\n",
    "                    horizon=horizon,\n",
    "                    before_predict_callback=before_predict_callback,\n",
    "                    X_df=X_df,\n",
    "                )\n",
    "            if self.target_transforms is not None:\n",
    "                if self._has_ga_target_tfms():\n",
    "                    model_cols = [c for c in preds.columns if c not in (self.id_col, self.time_col)]\n",
    "                    indptr = np.arange(0, horizon * (len(self.uids) + 1), horizon)\n",
    "                for tfm in self.target_transforms[::-1]:\n",
    "                    if isinstance(tfm, _BaseGroupedArrayTargetTransform):\n",
    "                        for col in model_cols:\n",
    "                            ga = GroupedArray(preds[col].to_numpy().astype(self.ga.data.dtype), indptr)\n",
    "                            ga = tfm.inverse_transform(ga)\n",
    "                            preds = ufp.assign_columns(preds, col, ga.data)\n",
    "                    else:\n",
    "                        preds = tfm.inverse_transform(preds)\n",
    "        return preds\n",
    "\n",
    "    def save(self, path: Union[str, Path]) -> None:\n",
    "        with fsspec.open(path, 'wb') as f:\n",
    "            cloudpickle.dump(self, f)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(\n",
    "        path: Union[str, Path], protocol: Optional[str] = None\n",
    "    ) -> 'TimeSeries':\n",
    "        with fsspec.open(path, 'rb', protocol=protocol) as f:\n",
    "            ts = cloudpickle.load(f)\n",
    "        return ts\n",
    "\n",
    "    def update(self, df: DataFrame) -> None:\n",
    "        \"\"\"Update the values of the stored series.\"\"\"\n",
    "        validate_format(df, self.id_col, self.time_col, self.target_col)\n",
    "        uids = self.uids\n",
    "        if isinstance(uids, pd.Index):\n",
    "            uids = pd.Series(uids)\n",
    "        uids, new_ids = ufp.match_if_categorical(uids, df[self.id_col])\n",
    "        df = ufp.copy_if_pandas(df, deep=False)\n",
    "        df = ufp.assign_columns(df, self.id_col, new_ids)\n",
    "        df = ufp.sort(df, by=[self.id_col, self.time_col])\n",
    "        values = df[self.target_col].to_numpy()\n",
    "        values = values.astype(self.ga.data.dtype, copy=False)\n",
    "        id_counts = ufp.counts_by_id(df, self.id_col)\n",
    "        try:\n",
    "            sizes = ufp.join(uids, id_counts, on=self.id_col, how='outer_coalesce')\n",
    "        except (KeyError, ValueError):\n",
    "            # pandas raises key error, polars before coalesce raises value error\n",
    "            sizes = ufp.join(uids, id_counts, on=self.id_col, how='outer')\n",
    "        sizes = ufp.fill_null(sizes, {'counts': 0})\n",
    "        sizes = ufp.sort(sizes, by=self.id_col)\n",
    "        new_groups = ~ufp.is_in(sizes[self.id_col], uids)\n",
    "        last_dates = ufp.group_by_agg(df, self.id_col, {self.time_col: 'max'})\n",
    "        last_dates = ufp.join(sizes, last_dates, on=self.id_col, how='left')\n",
    "        curr_last_dates = type(df)({self.id_col: uids, '_curr': self.last_dates})\n",
    "        last_dates = ufp.join(last_dates, curr_last_dates, on=self.id_col, how='left')\n",
    "        last_dates = ufp.fill_null(last_dates, {self.time_col: last_dates['_curr']})\n",
    "        last_dates = ufp.sort(last_dates, by=self.id_col)\n",
    "        self.last_dates = ufp.cast(last_dates[self.time_col], self.last_dates.dtype)\n",
    "        self.uids = ufp.sort(sizes[self.id_col])\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            self.uids = pd.Index(self.uids)\n",
    "            self.last_dates = pd.Index(self.last_dates)\n",
    "        if new_groups.any():\n",
    "            if self.target_transforms is not None:\n",
    "                raise ValueError('Can not update target_transforms with new series.')\n",
    "            new_ids = ufp.filter_with_mask(sizes[self.id_col], new_groups)\n",
    "            new_ids_df = ufp.filter_with_mask(df, ufp.is_in(df[self.id_col], new_ids))\n",
    "            new_ids_counts = ufp.counts_by_id(new_ids_df, self.id_col)\n",
    "            new_statics = ufp.take_rows(df, new_ids_counts[\"counts\"].to_numpy().cumsum() - 1)\n",
    "            new_statics = new_statics[self.static_features_.columns]\n",
    "            self.static_features_ = ufp.vertical_concat([self.static_features_, new_statics])\n",
    "            self.static_features_ = ufp.sort(self.static_features_, self.id_col)\n",
    "        if self.target_transforms is not None:\n",
    "            if self._has_ga_target_tfms():            \n",
    "                indptr = np.append(0, id_counts['counts']).cumsum()\n",
    "            for tfm in self.target_transforms:\n",
    "                if isinstance(tfm, _BaseGroupedArrayTargetTransform):\n",
    "                    ga = GroupedArray(values, indptr)\n",
    "                    ga = tfm.update(ga)\n",
    "                    df = ufp.assign_columns(df, self.target_col, ga.data)\n",
    "                else:\n",
    "                    df = tfm.update(df)\n",
    "                values = df[self.target_col].to_numpy()                    \n",
    "        self.ga = self.ga.append_several(\n",
    "            new_sizes=sizes['counts'].to_numpy().astype(np.int32),\n",
    "            new_values=values,\n",
    "            new_groups=new_groups.to_numpy(),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_fail(lambda: TimeSeries(freq='D', lags=list(range(2))), contains='lags must be positive integers')\n",
    "test_fail(lambda: TimeSeries(freq='D', lag_transforms={0: 1}), contains='keys of lag_transforms must be positive integers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# differences\n",
    "n = 7 * 14\n",
    "x = pd.DataFrame(\n",
    "    {\n",
    "        'id': np.repeat(0, n),\n",
    "        'ds': np.arange(n),\n",
    "        'y': np.arange(7)[[x % 7 for x in np.arange(n)]]\n",
    "    },\n",
    ")\n",
    "x['y'] = x['ds'] * 0.1 + x['y']\n",
    "ts = TimeSeries(freq=1, target_transforms=[Differences([1, 7])])\n",
    "ts._fit(x.iloc[:-14], id_col='id', time_col='ds', target_col='y')\n",
    "ts.as_numpy = False\n",
    "np.testing.assert_allclose(\n",
    "    x['y'].diff(1).diff(7).values[:-14],\n",
    "    ts.ga.data,\n",
    ")\n",
    "ts.y_pred = np.zeros(14)\n",
    "class A:\n",
    "    def fit(self, X):\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        return np.zeros(X.shape[0])\n",
    "xx = ts.predict({'A': A()}, 14)\n",
    "np.testing.assert_allclose(xx['A'], x['y'].tail(14).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# tfms namer\n",
    "def namer(f, lag, *args):\n",
    "    return f'hello_from_{f.__name__}'\n",
    "\n",
    "ts = TimeSeries(\n",
    "    freq=1,\n",
    "    lag_transforms={1: [(rolling_mean, 7), expanding_mean]},\n",
    "    lag_transforms_namer=namer,\n",
    ")\n",
    "transformed = ts.fit_transform(x, id_col='id', time_col='ds', target_col='y')\n",
    "test_eq(\n",
    "    transformed.columns.tolist(),\n",
    "    ['id', 'ds', 'y', 'hello_from_rolling_mean', 'hello_from_expanding_mean'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_fail(lambda: TimeSeries(freq=1, date_features=[lambda: 1]), contains=\"Can't use a lambda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `TimeSeries` class takes care of defining the transformations to be performed (`lags`, `lag_transforms` and `date_features`). The transformations can be computed using multithreading if `num_threads > 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeSeries(freq=W-THU, transforms=['lag7', 'expanding_mean_lag1', 'rolling_mean_lag1_window_size7'], date_features=['dayofweek', 'week', 'month_start_or_end'], num_threads=1)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def month_start_or_end(dates):\n",
    "    return dates.is_month_start | dates.is_month_end\n",
    "\n",
    "flow_config = dict(\n",
    "    freq='W-THU',\n",
    "    lags=[7],\n",
    "    lag_transforms={\n",
    "        1: [expanding_mean, (rolling_mean, 7)]\n",
    "    },\n",
    "    date_features=['dayofweek', 'week', month_start_or_end]\n",
    ")\n",
    "\n",
    "ts = TimeSeries(**flow_config)\n",
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_eq(\n",
    "    TimeSeries(freq=ts.freq).freq,\n",
    "    TimeSeries(freq='W-THU').freq\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frequency is converted to an offset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(ts.freq, pd.tseries.frequencies.to_offset(flow_config['freq']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The date features are stored as they were passed to the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(ts.date_features, flow_config['date_features'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformations are stored as a dictionary where the key is the name of the transformation (name of the column in the dataframe with the computed features), which is built using `build_transform_name` and the value is a tuple where the first element is the lag it is applied to, then the function and then the function arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(\n",
    "    ts.transforms, \n",
    "    {\n",
    "        'lag7': Lag(7),\n",
    "        'expanding_mean_lag1': (1, expanding_mean), \n",
    "        'rolling_mean_lag1_window_size7': (1, rolling_mean, 7)\n",
    "        \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for `lags` we define the transformation as the identity function applied to its corresponding lag. This is because `_transform_series` takes the lag as an argument and shifts the array before computing the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# int y is converted to float32\n",
    "serie2 = serie.copy()\n",
    "serie2['y'] = serie2['y'].astype(int)\n",
    "ts = TimeSeries(num_threads=1, freq='D')\n",
    "ts._fit(serie2, id_col='unique_id', time_col='ds', target_col='y')\n",
    "test_eq(ts.ga.data.dtype, np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# _compute_transforms\n",
    "y = serie.y.values\n",
    "lag_1 = shift_array(y, 1)\n",
    "\n",
    "for num_threads in (1, 2):\n",
    "    ts = TimeSeries(**flow_config)\n",
    "    ts._fit(serie, id_col='unique_id', time_col='ds', target_col='y')\n",
    "    transforms = ts._compute_transforms(ts.transforms, updates_only=False)\n",
    "\n",
    "    np.testing.assert_equal(transforms['lag7'], shift_array(y, 7))\n",
    "    np.testing.assert_equal(transforms['expanding_mean_lag1'], expanding_mean(lag_1))\n",
    "    np.testing.assert_equal(transforms['rolling_mean_lag1_window_size7'], rolling_mean(lag_1, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# update_y\n",
    "ts = TimeSeries(freq='D', lags=[1])\n",
    "ts._fit(serie, id_col='unique_id', time_col='ds', target_col='y')\n",
    "\n",
    "max_size = np.diff(ts.ga.indptr)\n",
    "ts._update_y([1])\n",
    "ts._update_y([2])\n",
    "\n",
    "test_eq(np.diff(ts.ga.indptr), max_size + 2)\n",
    "test_eq(ts.ga.data[-2:], [1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# _update_features\n",
    "ts = TimeSeries(**flow_config)\n",
    "ts._fit(serie, id_col='unique_id', time_col='ds', target_col='y')\n",
    "ts._predict_setup()\n",
    "updates = ts._update_features()\n",
    "\n",
    "last_date = serie['ds'].max()\n",
    "first_prediction_date = last_date + pd.offsets.Day()\n",
    "\n",
    "# these have an offset becase we can now \"see\" our last y value\n",
    "expected = pd.DataFrame({\n",
    "    'unique_id': ts.uids,\n",
    "    'lag7': shift_array(y, 6)[-1],\n",
    "    'expanding_mean_lag1': expanding_mean(y)[-1],\n",
    "    'rolling_mean_lag1_window_size7': rolling_mean(y, 7)[-1],\n",
    "    'dayofweek': np.uint8([getattr(first_prediction_date, 'dayofweek')]),\n",
    "    'week': np.uint8([first_prediction_date.isocalendar()[1]]),\n",
    "    'month_start_or_end': month_start_or_end(first_prediction_date)\n",
    "})\n",
    "statics = serie.tail(1).drop(columns=['ds', 'y'])\n",
    "pd.testing.assert_frame_equal(updates, statics.merge(expected))\n",
    "\n",
    "\n",
    "test_eq(ts.curr_dates[0], first_prediction_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# _get_predictions\n",
    "ts = TimeSeries(freq='D', lags=[1])\n",
    "ts._fit(serie, id_col='unique_id', time_col='ds', target_col='y')\n",
    "ts._predict_setup()\n",
    "ts._update_features()\n",
    "ts._update_y([1.])\n",
    "preds = ts._get_predictions()\n",
    "\n",
    "last_ds = serie['ds'].max()\n",
    "expected = pd.DataFrame({'unique_id': serie['unique_id'][[0]], 'ds': [last_ds + pd.offsets.Day()], 'y_pred': [1.]})\n",
    "pd.testing.assert_frame_equal(preds, expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/mlforecast/blob/main/mlforecast/core.py#L486){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "## TimeSeries.fit_transform\n",
       "\n",
       ">      TimeSeries.fit_transform (data:~DFType, id_col:str, time_col:str,\n",
       ">                                target_col:str,\n",
       ">                                static_features:Optional[List[str]]=None,\n",
       ">                                dropna:bool=True,\n",
       ">                                keep_last_n:Optional[int]=None,\n",
       ">                                max_horizon:Optional[int]=None,\n",
       ">                                return_X_y:bool=False, as_numpy:bool=False)\n",
       "\n",
       "*Add the features to `data` and save the required information for the predictions step.\n",
       "\n",
       "If not all features are static, specify which ones are in `static_features`.\n",
       "If you don't want to drop rows with null values after the transformations set `dropna=False`\n",
       "If `keep_last_n` is not None then that number of observations is kept across all series for updates.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/mlforecast/blob/main/mlforecast/core.py#L486){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "## TimeSeries.fit_transform\n",
       "\n",
       ">      TimeSeries.fit_transform (data:~DFType, id_col:str, time_col:str,\n",
       ">                                target_col:str,\n",
       ">                                static_features:Optional[List[str]]=None,\n",
       ">                                dropna:bool=True,\n",
       ">                                keep_last_n:Optional[int]=None,\n",
       ">                                max_horizon:Optional[int]=None,\n",
       ">                                return_X_y:bool=False, as_numpy:bool=False)\n",
       "\n",
       "*Add the features to `data` and save the required information for the predictions step.\n",
       "\n",
       "If not all features are static, specify which ones are in `static_features`.\n",
       "If you don't want to drop rows with null values after the transformations set `dropna=False`\n",
       "If `keep_last_n` is not None then that number of observations is kept across all series for updates.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TimeSeries.fit_transform, title_level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_config = dict(\n",
    "    freq='D',\n",
    "    lags=[7, 14],\n",
    "    lag_transforms={\n",
    "        2: [\n",
    "            (rolling_mean, 7),\n",
    "            (rolling_mean, 14),\n",
    "        ]\n",
    "    },\n",
    "    date_features=['dayofweek', 'month', 'year'],\n",
    "    num_threads=2\n",
    ")\n",
    "\n",
    "ts = TimeSeries(**flow_config)\n",
    "_ = ts.fit_transform(series, id_col='unique_id', time_col='ds', target_col='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The series values are stored as a GroupedArray in an attribute `ga`. If the data type of the series values is an int then it is converted to `np.float32`, this is because lags generate `np.nan`s so we need a float data type for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_equal(ts.ga.data, series.y.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The series ids are stored in an `uids` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(ts.uids, series['unique_id'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each time serie, the last observed date is stored so that predictions start from the last date + the frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(ts.last_dates, series.groupby('unique_id', observed=True)['ds'].max().values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last row of every serie without the `y` and `ds` columns are taken as static features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.testing.assert_frame_equal(\n",
    "    ts.static_features_,\n",
    "    series.groupby('unique_id', observed=True).tail(1).drop(columns=['ds', 'y']).reset_index(drop=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you pass `static_features` to `TimeSeries.fit_transform` then only these are kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.fit_transform(series, id_col='unique_id', time_col='ds', target_col='y', static_features=['static_0'])\n",
    "\n",
    "pd.testing.assert_frame_equal(\n",
    "    ts.static_features_,\n",
    "    series.groupby('unique_id', observed=True).tail(1)[['unique_id', 'static_0']].reset_index(drop=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also specify keep_last_n in TimeSeries.fit_transform, which means that after computing the features for training we want to keep only the last n samples of each time serie for computing the updates. This saves both memory and time, since the updates are performed by running the transformation functions on all time series again and keeping only the last value (the update).\n",
    "\n",
    "If you have very long time series and your updates only require a small sample it's recommended that you set keep_last_n to the minimum number of samples required to compute the updates, which in this case is 15 since we have a rolling mean of size 14 over the lag 2 and in the first update the lag 2 becomes the lag 1. This is because in the first update the lag 1 is the last value of the series (or the lag 0), the lag 2 is the lag 1 and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_last_n = 15\n",
    "\n",
    "ts = TimeSeries(**flow_config)\n",
    "df = ts.fit_transform(series, id_col='unique_id', time_col='ds', target_col='y', keep_last_n=keep_last_n)\n",
    "ts._predict_setup()\n",
    "\n",
    "expected_lags = ['lag7', 'lag14']\n",
    "expected_transforms = ['rolling_mean_lag2_window_size7', \n",
    "                       'rolling_mean_lag2_window_size14']\n",
    "expected_date_features = ['dayofweek', 'month', 'year']\n",
    "\n",
    "test_eq(ts.features, expected_lags + expected_transforms + expected_date_features)\n",
    "test_eq(ts.static_features_.columns.tolist() + ts.features, df.columns.drop(['ds', 'y']).tolist())\n",
    "# we dropped 2 rows because of the lag 2 and 13 more to have the window of size 14\n",
    "test_eq(df.shape[0], series.shape[0] - (2 + 13) * ts.ga.n_groups)\n",
    "test_eq(ts.ga.data.size, ts.ga.n_groups * keep_last_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TimeSeries.fit_transform` requires that the *y* column doesn't have any null values. This is because the transformations could propagate them forward, so if you have null values in the *y* column you'll get an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_with_nulls = series.copy()\n",
    "series_with_nulls.loc[1, 'y'] = np.nan\n",
    "test_fail(\n",
    "    lambda: ts.fit_transform(series_with_nulls, id_col='unique_id', time_col='ds', target_col='y'),\n",
    "    contains='y column contains null values'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# unsorted df\n",
    "ts = TimeSeries(**flow_config)\n",
    "df = ts.fit_transform(series, id_col='unique_id', time_col='ds', target_col='y')\n",
    "unordered_series = series.sample(frac=1.0)\n",
    "assert not unordered_series.set_index('ds', append=True).index.is_monotonic_increasing\n",
    "df2 = ts.fit_transform(unordered_series, id_col='unique_id', time_col='ds', target_col='y')\n",
    "pd.testing.assert_frame_equal(\n",
    "    df.reset_index(drop=True),\n",
    "    df2.sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# existing features arent recomputed\n",
    "df_with_features = pd.DataFrame({\n",
    "    'unique_id': [1, 1, 1],\n",
    "    'ds': pd.date_range('2000-01-01', freq='D', periods=3),\n",
    "    'y': [10., 11., 12.],\n",
    "    'lag1': [1, 1, 1],\n",
    "    'month': [12, 12, 12],\n",
    "    \n",
    "})\n",
    "ts = TimeSeries(freq='D', lags=[1, 2], date_features=['year', 'month'])\n",
    "transformed = ts.fit_transform(df_with_features, id_col='unique_id', time_col='ds', target_col='y', dropna=False)\n",
    "pd.testing.assert_series_equal(transformed['lag1'], df_with_features['lag1'])\n",
    "pd.testing.assert_series_equal(transformed['month'], df_with_features['month'])\n",
    "np.testing.assert_array_equal(transformed['year'], 3 * [2000])\n",
    "np.testing.assert_array_equal(transformed['lag2'].values, [np.nan, np.nan, 10.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# non-standard df\n",
    "ts = TimeSeries(**flow_config)\n",
    "df = ts.fit_transform(series, id_col='unique_id', time_col='ds', target_col='y')\n",
    "non_std_series = series.reset_index().rename(columns={'unique_id': 'some_id', 'ds': 'timestamp', 'y': 'value'})\n",
    "non_std_res = ts.fit_transform(\n",
    "    non_std_series, id_col='some_id', time_col='timestamp', target_col='value', static_features=[]\n",
    ")\n",
    "non_std_res = non_std_res.reset_index(drop=True)\n",
    "pd.testing.assert_frame_equal(\n",
    "    df.reset_index(),\n",
    "    non_std_res.rename(columns={'timestamp': 'ds', 'value': 'y', 'some_id': 'unique_id'})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# integer timestamps\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "flow_config_int_ds = copy.deepcopy(flow_config)\n",
    "flow_config_int_ds['date_features'] = [identity]\n",
    "flow_config_int_ds['freq'] = 1\n",
    "ts = TimeSeries(**flow_config_int_ds)\n",
    "int_ds_series = series.copy()\n",
    "int_ds_series['ds'] = int_ds_series['ds'].astype('int64')\n",
    "int_ds_res = ts.fit_transform(int_ds_series, id_col='unique_id', time_col='ds', target_col='y')\n",
    "int_ds_res['ds'] = pd.to_datetime(int_ds_res['ds'])\n",
    "int_ds_res['identity'] = pd.to_datetime(int_ds_res['ds'])\n",
    "df2 = df.drop(columns=flow_config['date_features'])\n",
    "df2['identity'] = df2['ds']\n",
    "pd.testing.assert_frame_equal(df2, int_ds_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(TimeSeries.predict, title_level=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a trained model we can use `TimeSeries.predict` passing the model and the horizon to get the predictions back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyModel:\n",
    "    def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
    "        return X['lag7'].values\n",
    "\n",
    "horizon = 7\n",
    "model = DummyModel()\n",
    "ts = TimeSeries(**flow_config)\n",
    "ts.fit_transform(series, id_col='unique_id', time_col='ds', target_col='y')\n",
    "predictions = ts.predict({'DummyModel': model}, horizon)\n",
    "\n",
    "grouped_series = series.groupby('unique_id', observed=True)\n",
    "expected_preds = grouped_series['y'].tail(7)  # the model predicts the lag-7\n",
    "last_dates = grouped_series['ds'].max()\n",
    "expected_dsmin = last_dates + pd.offsets.Day()\n",
    "expected_dsmax = last_dates + horizon * pd.offsets.Day()\n",
    "grouped_preds = predictions.groupby('unique_id', observed=True)\n",
    "\n",
    "np.testing.assert_allclose(predictions['DummyModel'], expected_preds)\n",
    "pd.testing.assert_series_equal(grouped_preds['ds'].min(), expected_dsmin)\n",
    "pd.testing.assert_series_equal(grouped_preds['ds'].max(), expected_dsmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "model = DummyModel()\n",
    "ts = TimeSeries(**flow_config)\n",
    "ts.fit_transform(series, id_col='unique_id', time_col='ds', target_col='y')\n",
    "predictions = ts.predict({'DummyModel': model}, horizon=horizon)\n",
    "ts = TimeSeries(**flow_config_int_ds)\n",
    "ts.fit_transform(int_ds_series, id_col='unique_id', time_col='ds', target_col='y')\n",
    "int_ds_predictions = ts.predict({'DummyModel': model}, horizon=horizon)\n",
    "pd.testing.assert_frame_equal(predictions.drop(columns='ds'), int_ds_predictions.drop(columns='ds'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have dynamic features we can pass them to `X_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictPrice:\n",
    "    def predict(self, X):\n",
    "        return X['price']\n",
    "\n",
    "series = generate_daily_series(20, n_static_features=2, equal_ends=True)\n",
    "dynamic_series = series.rename(columns={'static_1': 'product_id'})\n",
    "prices_catalog = generate_prices_for_series(dynamic_series)\n",
    "series_with_prices = dynamic_series.merge(prices_catalog, how='left')\n",
    "\n",
    "model = PredictPrice()\n",
    "ts = TimeSeries(**flow_config)\n",
    "ts.fit_transform(\n",
    "    series_with_prices,\n",
    "    id_col='unique_id',\n",
    "    time_col='ds',\n",
    "    target_col='y',\n",
    "    static_features=['static_0', 'product_id'],\n",
    ")\n",
    "predictions = ts.predict({'PredictPrice': model}, horizon=1, X_df=prices_catalog)\n",
    "pd.testing.assert_frame_equal(\n",
    "    predictions.rename(columns={'PredictPrice': 'price'}),\n",
    "    prices_catalog.merge(predictions[['unique_id', 'ds']])[['unique_id', 'ds', 'price']]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# predicting for a subset\n",
    "sample_ids = ['id_00', 'id_16']\n",
    "sample_preds = ts.predict({'price': model}, 1, X_df=prices_catalog, ids=sample_ids)\n",
    "pd.testing.assert_frame_equal(\n",
    "    sample_preds,\n",
    "    prices_catalog.merge(predictions[predictions['unique_id'].isin(sample_ids)][['unique_id', 'ds']])[['unique_id', 'ds', 'price']]\n",
    ")\n",
    "test_fail(lambda: ts.predict({'y': model}, 1, ids=['bonjour']), contains=\"{'bonjour'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(TimeSeries.update, title_level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "class SeasonalNaiveModel:\n",
    "    def predict(self, X):\n",
    "        return X['lag7']\n",
    "\n",
    "class NaiveModel:\n",
    "    def predict(self, X: pd.DataFrame):\n",
    "        return X['lag1']\n",
    "\n",
    "two_series = series[series['unique_id'].isin(['id_00', 'id_19'])].copy()\n",
    "two_series['unique_id'] = pd.Categorical(two_series['unique_id'], ['id_00', 'id_19'])\n",
    "ts = TimeSeries(freq='D', lags=[1], date_features=['dayofweek'])\n",
    "ts.fit_transform(\n",
    "    two_series,\n",
    "    id_col='unique_id',\n",
    "    time_col='ds',\n",
    "    target_col='y',\n",
    ")\n",
    "last_vals_two_series = two_series.groupby('unique_id', observed=True).tail(1)\n",
    "last_val_id0 = last_vals_two_series[lambda x: x['unique_id'].eq('id_00')].copy()\n",
    "new_values = last_val_id0.copy()\n",
    "new_values['ds'] += pd.offsets.Day()\n",
    "new_serie = pd.DataFrame({\n",
    "    'unique_id': ['new_idx', 'new_idx'],\n",
    "    'ds': pd.to_datetime(['2020-01-01', '2020-01-02']),\n",
    "    'y': [5.0, 6.0],\n",
    "    'static_0': [0, 0],\n",
    "    'static_1': [1, 1],\n",
    "})\n",
    "new_values = pd.concat([new_values, new_serie])\n",
    "ts.update(new_values)\n",
    "preds = ts.predict({'Naive': NaiveModel()}, 1)\n",
    "expected_id0 = last_val_id0.copy()\n",
    "expected_id0['ds'] += pd.offsets.Day()\n",
    "expected_id1 = last_vals_two_series[lambda x: x['unique_id'].eq('id_19')].copy()\n",
    "last_val_new_serie = new_serie.tail(1)[['unique_id', 'ds', 'y']]\n",
    "expected = pd.concat([expected_id0, expected_id1, last_val_new_serie])\n",
    "expected = expected[['unique_id', 'ds', 'y']]\n",
    "expected = expected.rename(columns={'y': 'Naive'}).reset_index(drop=True)\n",
    "expected['unique_id'] = pd.Categorical(expected['unique_id'], categories=['id_00', 'id_19', 'new_idx'])\n",
    "expected['ds'] += pd.offsets.Day()\n",
    "pd.testing.assert_frame_equal(preds, expected)\n",
    "pd.testing.assert_frame_equal(\n",
    "    ts.static_features_,\n",
    "    (\n",
    "        pd.concat([last_vals_two_series, new_serie.tail(1)])\n",
    "        [['unique_id', 'static_0', 'static_1']]\n",
    "        .astype(ts.static_features_.dtypes)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    ")\n",
    "# with target transforms\n",
    "ts = TimeSeries(\n",
    "    freq='D',\n",
    "    lags=[7],\n",
    "    target_transforms=[Differences([1, 2]), LocalStandardScaler()],\n",
    ")\n",
    "ts.fit_transform(two_series, id_col='unique_id', time_col='ds', target_col='y')\n",
    "new_values = two_series.groupby('unique_id', observed=True).tail(7).copy()\n",
    "new_values['ds'] += 7 * pd.offsets.Day()\n",
    "orig_last7 = ts.ga.take_from_groups(slice(-7, None)).data\n",
    "ts.update(new_values)\n",
    "preds = ts.predict({'SeasonalNaive': SeasonalNaiveModel()}, 7)\n",
    "np.testing.assert_allclose(\n",
    "    new_values['y'].values,\n",
    "    preds['SeasonalNaive'].values,\n",
    ")\n",
    "last7 = ts.ga.take_from_groups(slice(-7, None)).data\n",
    "assert 0 < np.abs(last7 / orig_last7 - 1).mean() < 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| polars\n",
    "two_series = generate_daily_series(2, n_static_features=2, engine='polars')\n",
    "ts = TimeSeries(freq='1d', lags=[1], date_features=['weekday'])\n",
    "ts.fit_transform(\n",
    "    two_series,\n",
    "    id_col='unique_id',\n",
    "    time_col='ds',\n",
    "    target_col='y',\n",
    ")\n",
    "last_vals_two_series = two_series.join(\n",
    "    two_series.group_by('unique_id').agg(pl.col('ds').max()), on=['unique_id', 'ds']\n",
    ")\n",
    "last_val_id0 = last_vals_two_series.filter(pl.col('unique_id') == 'id_0')\n",
    "new_values = last_val_id0.with_columns(\n",
    "    pl.col('unique_id').cast(pl.Categorical),\n",
    "    pl.col('ds').dt.offset_by('1d'),\n",
    "    pl.col('static_0').cast(pl.Int64),\n",
    "    pl.col('static_1').cast(pl.Int64),\n",
    ")\n",
    "new_serie = pl.DataFrame({\n",
    "    'unique_id': ['new_idx', 'new_idx'],\n",
    "    'ds': [datetime.datetime(2020, 1, 1), datetime.datetime(2020, 1, 2)],\n",
    "    'y': [5.0, 6.0],\n",
    "    'static_0': [0, 0],\n",
    "    'static_1': [1, 1],\n",
    "}).with_columns(\n",
    "    pl.col('ds').dt.cast_time_unit('ns'),\n",
    "    pl.col('unique_id').cast(pl.Categorical),\n",
    ")\n",
    "new_values = pl.concat([new_values, new_serie])\n",
    "ts.update(new_values)\n",
    "preds = ts.predict({'Naive': NaiveModel()}, 1)\n",
    "expected_id0 = last_val_id0.with_columns(pl.col('ds').dt.offset_by('1d'))\n",
    "expected_id1 = last_vals_two_series.filter(pl.col('unique_id') == 'id_1')\n",
    "last_val_new_serie = new_serie.tail(1)\n",
    "expected = pl.concat([expected_id0, expected_id1])\n",
    "expected = ufp.vertical_concat([expected, last_val_new_serie])\n",
    "pd.testing.assert_series_equal(\n",
    "    expected['unique_id'].cat.get_categories().to_pandas(),\n",
    "    pd.Series(['id_0', 'id_1', 'new_idx'], name='unique_id')\n",
    ")\n",
    "expected = expected[['unique_id', 'ds', 'y']]\n",
    "expected = ufp.rename(expected, {'y': 'Naive'})\n",
    "expected = expected.with_columns(pl.col('ds').dt.offset_by('1d'))\n",
    "pd.testing.assert_frame_equal(preds.to_pandas(), expected.to_pandas())\n",
    "pd.testing.assert_frame_equal(\n",
    "    ts.static_features_.to_pandas(),\n",
    "    (\n",
    "        ufp.vertical_concat([last_vals_two_series, new_serie.tail(1)])\n",
    "        [['unique_id', 'static_0', 'static_1']]\n",
    "        .to_pandas()\n",
    "        .astype(ts.static_features_.to_pandas().dtypes)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    ")\n",
    "# with target transforms\n",
    "ts = TimeSeries(\n",
    "    freq='1d',\n",
    "    lags=[7],\n",
    "    target_transforms=[Differences([1, 2]), LocalStandardScaler()],\n",
    ")\n",
    "ts.fit_transform(two_series, id_col='unique_id', time_col='ds', target_col='y')\n",
    "new_values = two_series.group_by('unique_id').tail(7)\n",
    "new_values = new_values.with_columns(pl.col('ds').dt.offset_by('7d'))\n",
    "orig_last7 = ts.ga.take_from_groups(slice(-7, None)).data\n",
    "ts.update(new_values)\n",
    "preds = ts.predict({'SeasonalNaive': SeasonalNaiveModel()}, 7)\n",
    "np.testing.assert_allclose(\n",
    "    new_values['y'].to_numpy(),\n",
    "    preds['SeasonalNaive'].to_numpy(),\n",
    ")\n",
    "last7 = ts.ga.take_from_groups(slice(-7, None)).data\n",
    "assert 0 < np.abs(last7 / orig_last7 - 1).mean() < 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# target_transform with keep_last_n\n",
    "ts = TimeSeries(freq='D', lags=[1], target_transforms=[LocalStandardScaler()])\n",
    "ts.fit_transform(series, id_col='unique_id', time_col='ds', target_col='y', keep_last_n=10)\n",
    "preds = ts.predict({'y': NaiveModel()}, 1)\n",
    "expected = series.groupby('unique_id', observed=True).tail(1)[['unique_id', 'ds', 'y']].reset_index(drop=True)\n",
    "expected['ds'] += pd.offsets.Day()\n",
    "pd.testing.assert_frame_equal(preds, expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# raise error when omitting the static_features argument and passing them as dynamic in predict\n",
    "valid = series.groupby('unique_id', observed=True).tail(10)\n",
    "train = series.drop(valid.index)\n",
    "ts = TimeSeries(freq='D', lags=[1], target_transforms=[LocalStandardScaler()])\n",
    "ts.fit_transform(train, id_col='unique_id', time_col='ds', target_col='y', keep_last_n=10)\n",
    "test_fail(lambda: ts.predict({'y': NaiveModel()}, 1, X_df=valid.drop(columns=['y'])), contains=\"['static_0', 'static_1']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| polars\n",
    "series_pl = generate_daily_series(5, static_as_categorical=False, n_static_features=5, engine='polars')\n",
    "series_pd = generate_daily_series(5, static_as_categorical=False, n_static_features=5, engine='pandas')\n",
    "series_pl = series_pl.with_columns(pl.col('unique_id').cast(str))\n",
    "series_pd['unique_id'] = series_pd['unique_id'].astype(str)\n",
    "\n",
    "cfg = dict(\n",
    "    lags=[1, 2, 3, 4],\n",
    "    lag_transforms={\n",
    "        1: [expanding_mean, (rolling_mean, 7), (rolling_mean, 14)],\n",
    "        2: [expanding_mean, (rolling_mean, 7), (rolling_mean, 14)],\n",
    "        3: [expanding_mean, (rolling_mean, 7), (rolling_mean, 14)],\n",
    "        4: [expanding_mean, (rolling_mean, 7), (rolling_mean, 14)],\n",
    "    },\n",
    "    date_features=['day', 'month', 'quarter', 'year'],\n",
    "    target_transforms=[Differences([1])],\n",
    ")\n",
    "feats_pl = SaveFeatures()\n",
    "ts_pl = TimeSeries(freq='1d', **cfg)\n",
    "prep_pl = ts_pl.fit_transform(series_pl, 'unique_id', 'ds', 'y')\n",
    "fcst_pl = ts_pl.predict({'y': NaiveModel()}, 2, before_predict_callback=feats_pl)\n",
    "\n",
    "feats_pd = SaveFeatures()\n",
    "ts_pd = TimeSeries(freq='1D', **cfg)\n",
    "prep_pd = ts_pd.fit_transform(series_pd, 'unique_id', 'ds', 'y')\n",
    "fcst_pd = ts_pd.predict({'y': NaiveModel()}, 2, before_predict_callback=feats_pd)\n",
    "\n",
    "prep_pd = prep_pd.reset_index(drop=True)\n",
    "prep_pl = prep_pl.to_pandas()\n",
    "fcst_pl = fcst_pl.to_pandas()\n",
    "# date features have different dtypes\n",
    "pd.testing.assert_frame_equal(prep_pl, prep_pd, check_dtype=False)\n",
    "pd.testing.assert_frame_equal(\n",
    "    feats_pl.get_features(with_step=True).to_pandas(),\n",
    "    feats_pd.get_features(with_step=True),\n",
    "    check_dtype=False,\n",
    ")\n",
    "pd.testing.assert_frame_equal(fcst_pl, fcst_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# dropped series\n",
    "for ordered in [True, False]:\n",
    "    series = generate_daily_series(10, min_length=5, max_length=20)\n",
    "    if not ordered:\n",
    "        series = series.sample(frac=1.0, random_state=40)\n",
    "    ts = TimeSeries(freq='D', lags=[10])\n",
    "    with warnings.catch_warnings(record=True):\n",
    "        prep = ts.fit_transform(series, 'unique_id', 'ds', 'y')\n",
    "    dropped = ts.uids[ts._dropped_series].tolist()\n",
    "    assert not prep['unique_id'].isin(dropped).any()\n",
    "    assert set(prep['unique_id'].unique().tolist() + dropped) == set(series['unique_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# short series exception\n",
    "series = generate_daily_series(2, min_length=5, max_length=15)\n",
    "ts = TimeSeries(freq='D', lags=[1], target_transforms=[Differences([20])])\n",
    "test_fail(\n",
    "    lambda: ts.fit_transform(series, 'unique_id', 'ds', 'y'),\n",
    "    contains=\"are too short for the 'Differences' transformation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test predict\n",
    "class Lag1PlusOneModel:\n",
    "    def predict(self, X):\n",
    "        return X['lag1'] + 1\n",
    "\n",
    "ts = TimeSeries(freq='D', lags=[1])\n",
    "for max_horizon in [None, 2]:\n",
    "    if max_horizon is None:\n",
    "        mod1 = Lag1PlusOneModel()\n",
    "        mod2 = Lag1PlusOneModel()\n",
    "    else:\n",
    "        mod1 = [Lag1PlusOneModel() for _ in range(max_horizon)]\n",
    "        mod2 = [Lag1PlusOneModel() for _ in range(max_horizon)]\n",
    "    ts.fit_transform(train, 'unique_id', 'ds', 'y', max_horizon=max_horizon)\n",
    "    # each model gets the correct historic values\n",
    "    preds = ts.predict(models={'mod1': mod1, 'mod2': mod2}, horizon=2)\n",
    "    np.testing.assert_allclose(preds['mod1'], preds['mod2'])\n",
    "    # idempotency\n",
    "    preds2 = ts.predict(models={'mod1': mod1, 'mod2': mod2}, horizon=2)\n",
    "    np.testing.assert_allclose(preds2['mod1'], preds2['mod2'])\n",
    "    pd.testing.assert_frame_equal(preds, preds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# save & load\n",
    "series = generate_daily_series(2, n_static_features=2)\n",
    "ts = TimeSeries(\n",
    "    freq='D',\n",
    "    lags=[1, 2],\n",
    "    date_features=['dayofweek'],\n",
    "    lag_transforms={\n",
    "        1: [RollingMean(1)]\n",
    "    },\n",
    "    target_transforms=[Differences([20])],\n",
    ")\n",
    "ts.fit_transform(series, 'unique_id', 'ds', 'y')\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    fname = Path(tmpdir) / 'hi'\n",
    "    ts.save(fname)\n",
    "    ts2 = TimeSeries.load(fname)\n",
    "preds = ts.predict({'model': NaiveModel()}, 10)\n",
    "preds2 = ts2.predict({'model': NaiveModel()}, 10)\n",
    "pd.testing.assert_frame_equal(preds, preds2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
