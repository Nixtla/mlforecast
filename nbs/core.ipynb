{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import concurrent.futures\n",
    "import copy\n",
    "import inspect\n",
    "import os\n",
    "import warnings\n",
    "from collections import OrderedDict\n",
    "from itertools import chain\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fastcore.foundation import patch, tuplify\n",
    "from numba import njit\n",
    "from window_ops.shift import shift_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev import *\n",
    "from window_ops.expanding import *\n",
    "from window_ops.ewm import *\n",
    "from window_ops.rolling import *\n",
    "\n",
    "from mlforecast.utils import generate_daily_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The required input format is a dataframe with an index named `unique_id` with a unique identifier for each time serie, a column `ds` with the datestamp and a column `y` with the values of the serie. Every other column is considered a static feature unless stated otherwise in the flow configuration for `preprocessing_flow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = generate_daily_series(20, n_static_features=2)\n",
    "series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity we'll just take one time serie here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uids = series.index.unique(level='unique_id')\n",
    "serie = series.loc[[uids[0]]]\n",
    "serie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "date_features_dtypes = {\n",
    "    'year': np.uint16,\n",
    "    'month': np.uint8,\n",
    "    'day': np.uint8,\n",
    "    'hour': np.uint8,\n",
    "    'minute': np.uint8,\n",
    "    'second': np.uint8,\n",
    "    'dayofyear': np.uint16,\n",
    "    'day_of_year': np.uint16,\n",
    "    'weekofyear': np.uint8,\n",
    "    'week': np.uint8,\n",
    "    'dayofweek': np.uint8,\n",
    "    'day_of_week': np.uint8,\n",
    "    'weekday': np.uint8,\n",
    "    'quarter': np.uint8,\n",
    "    'daysinmonth': np.uint8,\n",
    "    'is_month_start': np.uint8,\n",
    "    'is_month_end': np.uint8,\n",
    "    'is_quarter_start': np.uint8,\n",
    "    'is_quarter_end': np.uint8,\n",
    "    'is_year_start': np.uint8,\n",
    "    'is_year_end': np.uint8,\n",
    "}\n",
    "\n",
    "\n",
    "@njit\n",
    "def _append_new(data, indptr, new):\n",
    "    \"\"\"Append each value of new to each group in data formed by indptr.\"\"\"\n",
    "    n_series = len(indptr) - 1\n",
    "    new_data = np.empty(data.size + new.size, dtype=data.dtype)\n",
    "    new_indptr = indptr.copy()\n",
    "    new_indptr[1:] += np.arange(1, n_series + 1)\n",
    "    for i in range(n_series):\n",
    "        new_data[new_indptr[i] : new_indptr[i+1] - 1] = data[indptr[i] : indptr[i + 1]]\n",
    "        new_data[new_indptr[i+1] - 1] = new[i]\n",
    "    return new_data, new_indptr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class GroupedArray:\n",
    "    \"\"\"Array made up of different groups. Can be thought of (and iterated) as a list of arrays.\n",
    "    \n",
    "    All the data is stored in a single 1d array `data`.\n",
    "    The indices for the group boundaries are stored in another 1d array `indptr`.\"\"\"\n",
    "    \n",
    "    def __init__(self, data: np.ndarray, indptr: np.ndarray):\n",
    "        self.data = data\n",
    "        self.indptr = indptr\n",
    "        self.ngroups = len(indptr) - 1\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return self.ngroups\n",
    "        \n",
    "    def __getitem__(self, idx: int) -> np.ndarray:\n",
    "        return self.data[self.indptr[idx]:self.indptr[idx+1]]\n",
    "        \n",
    "    def take_from_groups(self, idx: Union[int, slice]) -> 'GroupedArray':\n",
    "        \"\"\"Takes `idx` from each group in the array.\"\"\"\n",
    "        ranges = [range(self.indptr[i], self.indptr[i+1])[idx] for i in range(self.ngroups)]\n",
    "        items = [self.data[rng] for rng in ranges]\n",
    "        sizes = np.array([item.size for item in items])\n",
    "        data = np.hstack(items)\n",
    "        indptr = np.append(0, sizes.cumsum())\n",
    "        return GroupedArray(data, indptr)\n",
    "        \n",
    "    def append(self, new: np.ndarray) -> 'GroupedArray':\n",
    "        \"\"\"Appends each element of `new` to each existing group. Returns a copy.\"\"\"\n",
    "        if new.size != self.ngroups:\n",
    "            raise ValueError(f'new must be of size {self.ngroups}')\n",
    "        new_data, new_indptr = _append_new(self.data, self.indptr, new)\n",
    "        return GroupedArray(new_data, new_indptr)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'GroupedArray(ndata={self.data.size}, ngroups={self.ngroups})'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `GroupedArray` is used internally for storing the series values and performing transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.arange(10, dtype=np.float32)\n",
    "indptr = np.array([0, 2, 10])  # group 1: [0, 1], group 2: [2..9]\n",
    "ga = GroupedArray(data, indptr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate through the groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ga_iter = iter(ga)\n",
    "np.testing.assert_equal(next(ga_iter), np.array([0, 1]))\n",
    "np.testing.assert_equal(next(ga_iter), np.arange(2, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the last two observations from every group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_2 = ga.take_from_groups(slice(-2, None))\n",
    "np.testing.assert_equal(last_2.data, np.array([0, 1, 8, 9]))\n",
    "np.testing.assert_equal(last_2.indptr, np.array([0, 2, 4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the last four observations from every group. Note that since group 1 only has two elements, only these are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_4 = ga.take_from_groups(slice(-4, None))\n",
    "np.testing.assert_equal(last_4.data, np.array([0, 1, 6, 7, 8, 9]))\n",
    "np.testing.assert_equal(last_4.indptr, np.array([0, 2, 6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "@njit\n",
    "def _identity(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Do nothing to the input.\"\"\"\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@njit(nogil=True)\n",
    "def transform_series(data, indptr, updates_only, lag, func, *args) -> np.ndarray:\n",
    "    \"\"\"Shifts every group in `data` by `lag` and computes `func(shifted, *args)`.\n",
    "    \n",
    "    If `updates_only=True` only last value of the transformation for each group is returned, \n",
    "    otherwise the full transformation is returned\"\"\"\n",
    "    n_series = len(indptr) - 1\n",
    "    if updates_only:\n",
    "        out = np.empty_like(data[:n_series])\n",
    "        for i in range(n_series):\n",
    "            lagged = shift_array(data[indptr[i]:indptr[i+1]], lag)\n",
    "            out[i] = func(lagged, *args)[-1]        \n",
    "    else:\n",
    "        out = np.empty_like(data)\n",
    "        for i in range(n_series):\n",
    "            lagged = shift_array(data[indptr[i]:indptr[i+1]], lag)\n",
    "            out[indptr[i]:indptr[i+1]] = func(lagged, *args)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the function that is used to compute the transformations, both for training and inference. `func` has to be a jitted function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def mycumsum(x):\n",
    "    return np.cumsum(x)\n",
    "\n",
    "actual = transform_series(ga.data, ga.indptr, True, 0, mycumsum)\n",
    "expected = np.array([ga.data[:ga.indptr[1]].sum(), ga.data[ga.indptr[1]:].sum()])\n",
    "np.testing.assert_equal(actual, expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def build_transform_name(lag, tfm, *args) -> str:\n",
    "    \"\"\"Creates a name for a transformation based on `lag`, the name of the function and its arguments.\"\"\"\n",
    "    if lag == 0:\n",
    "        return f'lag-{args[0]}'\n",
    "    tfm_name = f'{tfm.__name__}_lag-{lag}'\n",
    "    func_params = list(inspect.signature(tfm).parameters.items())[1:]  # remove input array argument\n",
    "    changed_params = [f'{name}-{value}' for value, (name, param) in zip(args, func_params) if param.default != value]\n",
    "    if changed_params:\n",
    "        tfm_name += '_' + '_'.join(changed_params)\n",
    "    return tfm_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(build_transform_name(1, expanding_mean), 'expanding_mean_lag-1')\n",
    "test_eq(build_transform_name(2, rolling_mean, 7), 'rolling_mean_lag-2_window_size-7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TimeSeries:\n",
    "    \"\"\"Utility class for storing and transforming time series data.\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 series_df: pd.DataFrame,\n",
    "                 freq: str = 'D',\n",
    "                 lags: List[int] = [],\n",
    "                 lag_transforms: Dict[int, List[Tuple]] = {},\n",
    "                 date_features: List[str] = [],\n",
    "                 static_features: Optional[List[str]] = None,\n",
    "                 num_threads: Optional[int] = None):\n",
    "        if not series_df.index.is_monotonic_increasing:\n",
    "            series_df = series_df.sort_index()\n",
    "        data = series_df.y.values\n",
    "        if data.dtype not in (np.float32, np.float64):\n",
    "            data = data.astype(np.float32)\n",
    "        sizes = series_df.groupby('unique_id').size().values\n",
    "        cumsizes = sizes.cumsum()\n",
    "        indptr = np.append(0, cumsizes)\n",
    "        self.ga = GroupedArray(data, indptr)\n",
    "        self.uids = series_df.index.unique(level='unique_id')\n",
    "        self.last_dates = series_df.index.get_level_values('ds')[cumsizes - 1]\n",
    "        self.freq = pd.tseries.frequencies.to_offset(freq)\n",
    "        self.static_features = series_df.iloc[cumsizes - 1].reset_index('ds', drop=True).drop(columns='y')\n",
    "        if static_features is not None:\n",
    "            self.static_features = self.static_features[static_features]\n",
    "        self.num_threads = num_threads or os.cpu_count()\n",
    "        self.date_features = date_features\n",
    "        \n",
    "        self.transforms: Dict[str, Tuple[Any, ...]] = OrderedDict()\n",
    "        for lag in lags:\n",
    "            self.transforms[f'lag-{lag}'] = (lag, _identity)\n",
    "        for lag in lag_transforms.keys():\n",
    "            for tfm_args in lag_transforms[lag]:\n",
    "                tfm, *args = tuplify(tfm_args)\n",
    "                tfm_name = build_transform_name(lag, tfm, *args)\n",
    "                self.transforms[tfm_name] = (lag, tfm, *args)\n",
    "                \n",
    "        self.y_pred: List[np.ndarray] = []\n",
    "        self.curr_dates = self.last_dates\n",
    "        self.test_dates: List[pd.DatetimeIndex] = []\n",
    "\n",
    "    @property\n",
    "    def n_series(self):\n",
    "        return self.ga.ngroups\n",
    "                \n",
    "    @property\n",
    "    def features(self):\n",
    "        return list(self.transforms.keys()) + self.date_features\n",
    "                \n",
    "    def __repr__(self):\n",
    "        return f'TimeSeries(n_series={self.n_series}, freq={self.freq}, transforms={self.transforms.keys()}, date_features={self.date_features})'\n",
    "    \n",
    "    def _apply_transforms(self, updates_only: bool = False):\n",
    "        results = {}\n",
    "        offset = 1 if updates_only else 0\n",
    "        for tfm_name, (lag, tfm, *args) in self.transforms.items():\n",
    "            results[tfm_name] =  transform_series(self.ga.data, self.ga.indptr, updates_only, lag - offset, tfm, *args)\n",
    "        return results\n",
    "\n",
    "    def _apply_multithreaded_transforms(self, updates_only: bool = False):\n",
    "        future_to_result = {}\n",
    "        results = {}\n",
    "        offset = 1 if updates_only else 0        \n",
    "        with concurrent.futures.ThreadPoolExecutor(self.num_threads) as executor:\n",
    "            for tfm_name, (lag, tfm, *args) in self.transforms.items():\n",
    "                future = executor.submit(transform_series, self.ga.data, self.ga.indptr, updates_only, lag - offset, tfm, *args)\n",
    "                future_to_result[future] = tfm_name\n",
    "            for future in concurrent.futures.as_completed(future_to_result):\n",
    "                tfm_name = future_to_result[future]\n",
    "                results[tfm_name] = future.result()\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `TimeSeries` class has to extract each series values and store it in a contiguous numpy 1d-array. Before doing that, we must ensure that the input dataframe is sorted by `unique_id` and `ds`, so we make them both indices and check if the index is sorted, otherwise it is sorted before storing the values in the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serie = serie.set_index('ds', append=True)\n",
    "serie.index.is_monotonic_increasing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `TimeSeries` class takes care of defining the transformations to be performed (`lags`, `lag_transforms` and `date_features`) as well as storing the necessary data to update them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = [7]\n",
    "lag_transforms = {\n",
    "    1: [\n",
    "        expanding_mean, \n",
    "        (rolling_mean, 7)\n",
    "    ]\n",
    "}\n",
    "date_features = ['dayofweek']\n",
    "\n",
    "ts = TimeSeries(serie, lags=lags, lag_transforms=lag_transforms, date_features=date_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The series values are stored as a `GroupedArray` in an attribute `ga`. If the data type of the series values is an int then it is converted to `np.float32`, this is because lags generate `np.nan`s so we need a float data type for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_equal(ts.ga.data, serie.y.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The series ids are stored in `TimeSeries.uids`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(ts.uids, ['id_00'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each time serie, the last observed date is stored so that predictions start from the last date + the frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(ts.last_dates, [serie.index.get_level_values('ds').max()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frequency is converted to an offset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(ts.freq, pd.tseries.frequencies.to_offset('D'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last row of every serie without the `y` column is taken as static features. The `ds` index level is dropped as well because these will be replicated in every update and the datestamps will be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert ts.static_features.equals(serie.tail(1).reset_index('ds', drop=True).drop(columns='y'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you pass `static_features` to the `TimeSeries` constructor then only these are kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts2 = TimeSeries(serie, static_features=['static_0'])\n",
    "\n",
    "test_eq(ts2.static_features.columns, ['static_0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The date features are stored as they were passed to the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(ts.date_features, date_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformations are stored as a dictionary where the key is the name of the transformation (name of the column in the dataframe with the computed features), which is built using `build_transform_name` and the value is a tuple where the first element is the lag it is applied to, then the function and then the function arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(ts.transforms, \n",
    "        {'lag-7': (7, _identity), \n",
    "        'expanding_mean_lag-1': (1, expanding_mean), \n",
    "        'rolling_mean_lag-1_window_size-7': (1, rolling_mean, 7)\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for `lags` we define the transformation as the identity function applied to its corresponding lag. This is because `transform_series` takes the lag as an argument and shifts the array before computing the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def compute_transforms(self: TimeSeries) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"Compute the transformations defined in the constructor.\n",
    "    \n",
    "    If `num_threads > 1` these are computed using multithreading.\"\"\"\n",
    "    if self.num_threads == 1 or len(self.transforms) == 1:\n",
    "        return self._apply_transforms()\n",
    "    return self._apply_multithreaded_transforms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we instantiate a `TimeSeries` class we can call `compute_transforms` to get the values of all the transformations. These are returned in a dictionary where the keys are the name that was assigned the each transformation and the values are the result of the transformation applied to the time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = serie.y.values\n",
    "lag_1 = shift_array(y, 1)\n",
    "\n",
    "for num_threads in (1, 2):\n",
    "    ts = TimeSeries(serie, lags=lags, lag_transforms=lag_transforms, num_threads=num_threads)\n",
    "    transforms = ts.compute_transforms()\n",
    "\n",
    "    np.testing.assert_equal(transforms['lag-7'], shift_array(y, 7))\n",
    "    np.testing.assert_equal(transforms['expanding_mean_lag-1'], expanding_mean(lag_1))\n",
    "    np.testing.assert_equal(transforms['rolling_mean_lag-1_window_size-7'], rolling_mean(lag_1, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def update_y(self: TimeSeries, new: np.ndarray) -> None:\n",
    "    \"\"\"Appends the elements of `new` to every time serie.\n",
    "    \n",
    "    These values are used to update the transformations and are stored as predictions.\"\"\"\n",
    "    if len(self.y_pred) == 0:\n",
    "        self.y_pred = []\n",
    "    self.y_pred.append(new)\n",
    "    new_arr = np.asarray(new)\n",
    "    self.ga = self.ga.append(new_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = TimeSeries(serie)\n",
    "max_size = np.diff(ts.ga.indptr)\n",
    "ts.update_y([1])\n",
    "ts.update_y([2])\n",
    "\n",
    "test_eq(np.diff(ts.ga.indptr), max_size + 2)\n",
    "test_eq(ts.ga.data[-1], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def update_features(self: TimeSeries) -> pd.DataFrame:\n",
    "    \"\"\"Compute the current values of all the features using the latest values of the time series.\"\"\"\n",
    "    if self.curr_dates.equals(self.last_dates):\n",
    "        self.curr_dates = self.last_dates.copy()\n",
    "        self.test_dates = []\n",
    "    self.curr_dates += self.freq\n",
    "    self.test_dates.append(self.curr_dates)\n",
    "    \n",
    "    if self.num_threads == 1 or len(self.transforms) == 1:\n",
    "        features = self._apply_transforms(updates_only=True)\n",
    "    else:\n",
    "        features = self._apply_multithreaded_transforms(updates_only=True)\n",
    "    \n",
    "    for feature in self.date_features:\n",
    "        feat_vals = getattr(self.curr_dates, feature).values\n",
    "        features[feature] = feat_vals.astype(date_features_dtypes[feature])\n",
    "        \n",
    "    features_df = pd.DataFrame(features, columns=self.features, index=self.uids)\n",
    "    nulls_in_cols = features_df.isnull().any()\n",
    "    if any(nulls_in_cols):\n",
    "        warnings.warn(f'Found null values in {\", \".join(nulls_in_cols[nulls_in_cols].index)}.')\n",
    "    results_df = self.static_features.join(features_df)\n",
    "    results_df['ds'] = self.curr_dates\n",
    "    results_df = results_df.set_index('ds', append=True)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = TimeSeries(serie, lags=lags, lag_transforms=lag_transforms, date_features=date_features)\n",
    "updates = ts.update_features()\n",
    "\n",
    "# these have an offset becase we can now \"see\" our last y value\n",
    "last_date = serie.index.get_level_values('ds').max()\n",
    "first_prediction_date = last_date + ts.freq\n",
    "expected_idx = pd.MultiIndex.from_tuples([(ts.uids[0], first_prediction_date)],\n",
    "                                         names=['unique_id', 'ds'])\n",
    "expected = pd.DataFrame({\n",
    "    'lag-7': shift_array(y, 6)[-1],\n",
    "    'expanding_mean_lag-1': expanding_mean(y)[-1],\n",
    "    'rolling_mean_lag-1_window_size-7': rolling_mean(y, 7)[-1],\n",
    "    'dayofweek': np.uint8([getattr(first_prediction_date, 'dayofweek')])},\n",
    "    index=expected_idx\n",
    ")\n",
    "statics = serie.tail(1).drop('y', 1).reset_index('ds', drop=True)\n",
    "assert updates.equals(statics.join(expected))\n",
    "\n",
    "test_eq(ts.curr_dates[0], first_prediction_date)  # current dates of the series (last_dates + num_preds * freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def get_predictions(self: TimeSeries) -> pd.DataFrame:\n",
    "    \"\"\"Get all the predicted values with their corresponding ids and datestamps.\"\"\"\n",
    "    n_preds = len(self.y_pred)\n",
    "    idx = pd.Index(chain.from_iterable([uid] * n_preds for uid in self.uids), name='unique_id', dtype=self.uids.dtype)\n",
    "    df = pd.DataFrame({\n",
    "        'ds': np.array(self.test_dates).ravel('F'), \n",
    "        'y_pred': np.array(self.y_pred).ravel('F')},\n",
    "        index=idx)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = TimeSeries(serie)\n",
    "ts.update_features()\n",
    "ts.update_y([1.])\n",
    "preds = ts.get_predictions()\n",
    "\n",
    "_, last_ds = serie.index[-1]\n",
    "expected_idx = serie.index.get_level_values('unique_id')[[0]]\n",
    "expected = pd.DataFrame({'ds': [last_ds + ts.freq], 'y_pred': [1.]},\n",
    "                        index=expected_idx)\n",
    "assert preds.equals(expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def preprocessing_flow(df: pd.DataFrame,\n",
    "                       freq: str = 'D',\n",
    "                       lags: List[int] = [],\n",
    "                       lag_transforms: Dict[int, List[Tuple]] = {},\n",
    "                       date_features: List[str] = [],\n",
    "                       static_features: Optional[List[str]] = None,\n",
    "                       dropna: bool = True,\n",
    "                       keep_last_n: Optional[int] = None,\n",
    "                       num_threads: Optional[int] = os.cpu_count()) -> Tuple[TimeSeries, pd.DataFrame]:\n",
    "    \"\"\"Standard preprocessing flow.\n",
    "    \n",
    "    Returns a `TimeSeries` object for the forecasting step and a pandas DataFrame for training.\"\"\"\n",
    "    df = df.set_index('ds', append=True).sort_index()\n",
    "    series = TimeSeries(df, freq, lags, lag_transforms, date_features, \n",
    "                        static_features, num_threads)\n",
    "    df = df.reset_index('ds')\n",
    "    \n",
    "    features = series.compute_transforms()  # type: ignore\n",
    "    for k in series.transforms.keys():\n",
    "        df[k] = features[k]\n",
    "  \n",
    "    if dropna:\n",
    "        df.dropna(inplace=True)\n",
    "\n",
    "    for feature in date_features:\n",
    "        feat_vals = getattr(df.ds.dt, feature).values\n",
    "        df[feature] = feat_vals.astype(date_features_dtypes[feature])\n",
    "    \n",
    "    if keep_last_n is not None:\n",
    "        series.ga = series.ga.take_from_groups(slice(-keep_last_n, None))\n",
    "\n",
    "    return series, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this function has a lot of arguments we define them in a dict than then gets unpacked in the function call. This pattern is used both in `Forecast` and `DistributedForecast` to specify the flow configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    freq='D',\n",
    "    lags=[7, 14],\n",
    "    lag_transforms={\n",
    "        1: [\n",
    "            expanding_mean,\n",
    "            expanding_std\n",
    "        ],\n",
    "        2: [\n",
    "            (rolling_mean, 7),\n",
    "            (rolling_mean, 14),\n",
    "        ]\n",
    "    },\n",
    "    date_features=['dayofweek', 'month', 'year'],\n",
    "    keep_last_n=15,\n",
    "    num_threads=2\n",
    ")\n",
    "ts, df = preprocessing_flow(series, **config)\n",
    "\n",
    "expected_lags = ['lag-7', 'lag-14']\n",
    "expected_transforms = ['expanding_mean_lag-1', \n",
    "                       'expanding_std_lag-1', \n",
    "                       'rolling_mean_lag-2_window_size-7', \n",
    "                       'rolling_mean_lag-2_window_size-14']\n",
    "expected_date_features = ['dayofweek', 'month', 'year']\n",
    "\n",
    "test_eq(ts.features, expected_lags + expected_transforms + expected_date_features)\n",
    "test_eq(ts.static_features.columns.tolist() + ts.features, df.columns.drop(['ds', 'y']).tolist())\n",
    "# we dropped 2 rows because of the lag 2 and 13 more to have the window of size 14\n",
    "test_eq(df.shape[0], series.shape[0] - (2 + 13) * ts.n_series)\n",
    "test_eq(ts.ga.data.size, ts.ga.ngroups * config['keep_last_n'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we define `keep_last_n=15` here, which means that after computing the features for training we want to keep only the last 15 samples of each time serie. This saves both memory and time, since the updates are performed by running the transformation functions on all time series again and keeping only the last value (the update).\n",
    "\n",
    "If you have very long time series and your updates only require a small sample it's recommended that you set `keep_last_n` to the minimum number of samples required to compute the updates, which in this case is 15 since we have a rolling mean of size 14 over the lag 2 and in the first update the lag 2 becomes the lag 1. This is because in the first update the lag 1 is the last value of the series (or the lag 0), the lag 2 is the lag 1 and so on. \n",
    "\n",
    "If you get the minimum number of samples that your transformation needs wrong i.e. you generate `np.nan`s in the update, you'll get a warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_config = dict(\n",
    "    lag_transforms={2: [(rolling_mean, 14)]},\n",
    "    keep_last_n=14\n",
    ")\n",
    "ts, _ = preprocessing_flow(series, **bad_config)\n",
    "test_warns(lambda: ts.update_features())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a rule of thumb the minimal number of samples you need is max(lags) + max(window size for that lag) - 1 for rolling operations. **If you have expanding features you need all samples to get the correct value**. This won't raise a warning because there won't be null values, however the feature value will be wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def predictions_flow(series: TimeSeries,\n",
    "                     model,\n",
    "                     horizon: int) -> pd.DataFrame:\n",
    "    \"\"\"Standard predictions flow.\n",
    "    \n",
    "    Uses `model` to compute the predictions for the next `horizon` steps for every serie in `series`.\"\"\"\n",
    "    series = copy.copy(series)\n",
    "    # this avoids installing xgboost only to check if the model is instance of xgb.Booster\n",
    "    model_is_xgb_booster = type(model).__module__ == 'xgboost.core' and type(model).__name__ == 'Booster'\n",
    "    for _ in range(horizon):\n",
    "        new_x = series.update_features()  # type: ignore\n",
    "        if model_is_xgb_booster:\n",
    "            import xgboost as xgb\n",
    "            new_x = xgb.DMatrix(new_x)\n",
    "        predictions = model.predict(new_x)\n",
    "        series.update_y(predictions)  # type: ignore\n",
    "    return series.get_predictions()  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyModel:\n",
    "    \n",
    "    def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
    "        return X['lag-7'].values\n",
    "    \n",
    "horizon = 7\n",
    "model = DummyModel()\n",
    "ts, _ = preprocessing_flow(series, **config)\n",
    "predictions = predictions_flow(ts, model, horizon)\n",
    "\n",
    "grouped_series = series.groupby('unique_id')\n",
    "expected_preds = grouped_series['y'].tail(horizon)\n",
    "last_dates = grouped_series['ds'].max()\n",
    "expected_dsmin = last_dates + ts.freq\n",
    "expected_dsmax = last_dates + horizon * ts.freq\n",
    "grouped_preds = predictions.groupby('unique_id')\n",
    "\n",
    "assert predictions['y_pred'].equals(expected_preds)\n",
    "assert grouped_preds['ds'].min().equals(expected_dsmin)\n",
    "assert grouped_preds['ds'].max().equals(expected_dsmax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
