{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import concurrent.futures\n",
    "import inspect\n",
    "import warnings\n",
    "from collections import OrderedDict\n",
    "from itertools import chain\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numba import njit\n",
    "from window_ops.shift import shift_array\n",
    "\n",
    "from mlforecast.utils import data_indptr_from_sorted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev import show_doc\n",
    "from fastcore.test import test_eq, test_fail, test_warns\n",
    "from window_ops.expanding import expanding_mean\n",
    "from window_ops.rolling import rolling_mean\n",
    "\n",
    "from mlforecast.utils import generate_daily_series, generate_prices_for_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The required input format is a dataframe with an index named `unique_id` with a unique identifier for each time serie, a column `ds` with the datestamp and a column `y` with the values of the serie. Every other column is considered a static feature unless stated otherwise in `TimeSeries.fit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>static_0</th>\n",
       "      <th>static_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>2.380027</td>\n",
       "      <td>49</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2000-01-02</td>\n",
       "      <td>11.556201</td>\n",
       "      <td>49</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>22.165185</td>\n",
       "      <td>49</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2000-01-04</td>\n",
       "      <td>27.319662</td>\n",
       "      <td>49</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2000-01-05</td>\n",
       "      <td>36.392082</td>\n",
       "      <td>49</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_19</th>\n",
       "      <td>2000-03-25</td>\n",
       "      <td>8.175649</td>\n",
       "      <td>12</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_19</th>\n",
       "      <td>2000-03-26</td>\n",
       "      <td>10.995813</td>\n",
       "      <td>12</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_19</th>\n",
       "      <td>2000-03-27</td>\n",
       "      <td>12.657186</td>\n",
       "      <td>12</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_19</th>\n",
       "      <td>2000-03-28</td>\n",
       "      <td>0.420927</td>\n",
       "      <td>12</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_19</th>\n",
       "      <td>2000-03-29</td>\n",
       "      <td>2.888024</td>\n",
       "      <td>12</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4874 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ds          y static_0 static_1\n",
       "unique_id                                        \n",
       "id_00     2000-01-01   2.380027       49       79\n",
       "id_00     2000-01-02  11.556201       49       79\n",
       "id_00     2000-01-03  22.165185       49       79\n",
       "id_00     2000-01-04  27.319662       49       79\n",
       "id_00     2000-01-05  36.392082       49       79\n",
       "...              ...        ...      ...      ...\n",
       "id_19     2000-03-25   8.175649       12       81\n",
       "id_19     2000-03-26  10.995813       12       81\n",
       "id_19     2000-03-27  12.657186       12       81\n",
       "id_19     2000-03-28   0.420927       12       81\n",
       "id_19     2000-03-29   2.888024       12       81\n",
       "\n",
       "[4874 rows x 4 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series = generate_daily_series(20, n_static_features=2)\n",
    "series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity we'll just take one time serie here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>static_0</th>\n",
       "      <th>static_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>2.380027</td>\n",
       "      <td>49</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2000-01-02</td>\n",
       "      <td>11.556201</td>\n",
       "      <td>49</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>22.165185</td>\n",
       "      <td>49</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2000-01-04</td>\n",
       "      <td>27.319662</td>\n",
       "      <td>49</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2000-01-05</td>\n",
       "      <td>36.392082</td>\n",
       "      <td>49</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2000-08-05</td>\n",
       "      <td>4.263168</td>\n",
       "      <td>49</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2000-08-06</td>\n",
       "      <td>12.288851</td>\n",
       "      <td>49</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2000-08-07</td>\n",
       "      <td>19.142737</td>\n",
       "      <td>49</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2000-08-08</td>\n",
       "      <td>27.959904</td>\n",
       "      <td>49</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_00</th>\n",
       "      <td>2000-08-09</td>\n",
       "      <td>38.331903</td>\n",
       "      <td>49</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>222 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ds          y static_0 static_1\n",
       "unique_id                                        \n",
       "id_00     2000-01-01   2.380027       49       79\n",
       "id_00     2000-01-02  11.556201       49       79\n",
       "id_00     2000-01-03  22.165185       49       79\n",
       "id_00     2000-01-04  27.319662       49       79\n",
       "id_00     2000-01-05  36.392082       49       79\n",
       "...              ...        ...      ...      ...\n",
       "id_00     2000-08-05   4.263168       49       79\n",
       "id_00     2000-08-06  12.288851       49       79\n",
       "id_00     2000-08-07  19.142737       49       79\n",
       "id_00     2000-08-08  27.959904       49       79\n",
       "id_00     2000-08-09  38.331903       49       79\n",
       "\n",
       "[222 rows x 4 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uids = series.index.unique(level='unique_id')\n",
    "serie = series.loc[[uids[0]]]\n",
    "serie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "date_features_dtypes = {\n",
    "    'year': np.uint16,\n",
    "    'month': np.uint8,\n",
    "    'day': np.uint8,\n",
    "    'hour': np.uint8,\n",
    "    'minute': np.uint8,\n",
    "    'second': np.uint8,\n",
    "    'dayofyear': np.uint16,\n",
    "    'day_of_year': np.uint16,\n",
    "    'weekofyear': np.uint8,\n",
    "    'week': np.uint8,\n",
    "    'dayofweek': np.uint8,\n",
    "    'day_of_week': np.uint8,\n",
    "    'weekday': np.uint8,\n",
    "    'quarter': np.uint8,\n",
    "    'daysinmonth': np.uint8,\n",
    "    'is_month_start': np.uint8,\n",
    "    'is_month_end': np.uint8,\n",
    "    'is_quarter_start': np.uint8,\n",
    "    'is_quarter_end': np.uint8,\n",
    "    'is_year_start': np.uint8,\n",
    "    'is_year_end': np.uint8,\n",
    "}\n",
    "\n",
    "\n",
    "@njit\n",
    "def _append_new(data, indptr, new):\n",
    "    \"\"\"Append each value of new to each group in data formed by indptr.\"\"\"\n",
    "    n_series = len(indptr) - 1\n",
    "    new_data = np.empty(data.size + new.size, dtype=data.dtype)\n",
    "    new_indptr = indptr.copy()\n",
    "    new_indptr[1:] += np.arange(1, n_series + 1)\n",
    "    for i in range(n_series):\n",
    "        prev_slice = slice(indptr[i], indptr[i + 1])\n",
    "        new_slice = slice(new_indptr[i], new_indptr[i + 1] - 1)\n",
    "        new_data[new_slice] = data[prev_slice]\n",
    "        new_data[new_indptr[i + 1] - 1] = new[i]\n",
    "    return new_data, new_indptr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "class GroupedArray:\n",
    "    \"\"\"Array made up of different groups. Can be thought of (and iterated) as a list of arrays.\n",
    "    \n",
    "    All the data is stored in a single 1d array `data`.\n",
    "    The indices for the group boundaries are stored in another 1d array `indptr`.\"\"\"\n",
    "    \n",
    "    def __init__(self, data: np.ndarray, indptr: np.ndarray):\n",
    "        self.data = data\n",
    "        self.indptr = indptr\n",
    "        self.ngroups = len(indptr) - 1\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return self.ngroups\n",
    "        \n",
    "    def __getitem__(self, idx: int) -> np.ndarray:\n",
    "        return self.data[self.indptr[idx] : self.indptr[idx + 1]]\n",
    "        \n",
    "    def take_from_groups(self, idx: Union[int, slice]) -> 'GroupedArray':\n",
    "        \"\"\"Takes `idx` from each group in the array.\"\"\"\n",
    "        ranges = [\n",
    "            range(self.indptr[i], self.indptr[i + 1])[idx] for i in range(self.ngroups)\n",
    "        ]\n",
    "        items = [self.data[rng] for rng in ranges]\n",
    "        sizes = np.array([item.size for item in items])\n",
    "        data = np.hstack(items)\n",
    "        indptr = np.append(0, sizes.cumsum())\n",
    "        return GroupedArray(data, indptr)\n",
    "        \n",
    "    def append(self, new: np.ndarray) -> 'GroupedArray':\n",
    "        \"\"\"Appends each element of `new` to each existing group. Returns a copy.\"\"\"\n",
    "        if new.size != self.ngroups:\n",
    "            raise ValueError(f'new must be of size {self.ngroups}')\n",
    "        new_data, new_indptr = _append_new(self.data, self.indptr, new)\n",
    "        return GroupedArray(new_data, new_indptr)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'GroupedArray(ndata={self.data.size}, ngroups={self.ngroups})'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# The `GroupedArray` is used internally for storing the series values and performing transformations.\n",
    "data = np.arange(10, dtype=np.float32)\n",
    "indptr = np.array([0, 2, 10])  # group 1: [0, 1], group 2: [2..9]\n",
    "ga = GroupedArray(data, indptr)\n",
    "test_eq(len(ga), 2)\n",
    "test_eq(str(ga), 'GroupedArray(ndata=10, ngroups=2)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# Iterate through the groups\n",
    "ga_iter = iter(ga)\n",
    "np.testing.assert_equal(next(ga_iter), np.array([0, 1]))\n",
    "np.testing.assert_equal(next(ga_iter), np.arange(2, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# Take the last two observations from every group\n",
    "last_2 = ga.take_from_groups(slice(-2, None))\n",
    "np.testing.assert_equal(last_2.data, np.array([0, 1, 8, 9]))\n",
    "np.testing.assert_equal(last_2.indptr, np.array([0, 2, 4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# Take the last four observations from every group. Note that since group 1 only has two elements, only these are returned.\n",
    "last_4 = ga.take_from_groups(slice(-4, None))\n",
    "np.testing.assert_equal(last_4.data, np.array([0, 1, 6, 7, 8, 9]))\n",
    "np.testing.assert_equal(last_4.indptr, np.array([0, 2, 6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# try to append new values that don't match the number of groups\n",
    "test_fail(lambda: ga.append(np.array([1., 2., 3.])), contains='new must be of size 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "@njit\n",
    "def _identity(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Do nothing to the input.\"\"\"\n",
    "    return x\n",
    "\n",
    "\n",
    "def _as_tuple(x):\n",
    "    \"\"\"Return a tuple from the input.\"\"\"\n",
    "    if isinstance(x, tuple):\n",
    "        return x\n",
    "    return (x,)\n",
    "\n",
    "\n",
    "@njit(nogil=True)\n",
    "def _transform_series(data, indptr, updates_only, lag, func, *args) -> np.ndarray:\n",
    "    \"\"\"Shifts every group in `data` by `lag` and computes `func(shifted, *args)`.\n",
    "    \n",
    "    If `updates_only=True` only last value of the transformation for each group is returned, \n",
    "    otherwise the full transformation is returned\"\"\"\n",
    "    n_series = len(indptr) - 1\n",
    "    if updates_only:\n",
    "        out = np.empty_like(data[:n_series])\n",
    "        for i in range(n_series):\n",
    "            lagged = shift_array(data[indptr[i] : indptr[i + 1]], lag)\n",
    "            out[i] = func(lagged, *args)[-1]        \n",
    "    else:\n",
    "        out = np.empty_like(data)\n",
    "        for i in range(n_series):\n",
    "            lagged = shift_array(data[indptr[i] : indptr[i + 1]], lag)\n",
    "            out[indptr[i] : indptr[i + 1]] = func(lagged, *args)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def _build_transform_name(lag, tfm, *args) -> str:\n",
    "    \"\"\"Creates a name for a transformation based on `lag`, the name of the function and its arguments.\"\"\"\n",
    "    tfm_name = f'{tfm.__name__}_lag-{lag}'\n",
    "    func_params = inspect.signature(tfm).parameters\n",
    "    func_args = list(func_params.items())[1:]  # remove input array argument\n",
    "    changed_params = [\n",
    "        f'{name}-{value}'\n",
    "        for value, (name, arg) in zip(args, func_args)\n",
    "        if arg.default != value\n",
    "    ]\n",
    "    if changed_params:\n",
    "        tfm_name += '_' + '_'.join(changed_params)\n",
    "    return tfm_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "test_eq(_build_transform_name(1, expanding_mean), 'expanding_mean_lag-1')\n",
    "test_eq(_build_transform_name(2, rolling_mean, 7), 'rolling_mean_lag-2_window_size-7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def simple_predict(model, new_x: pd.DataFrame, *args, **kwargs) -> np.ndarray:\n",
    "    \"\"\"Drop the ds column from `new_x` and call `model.predict` on it.\"\"\"\n",
    "    new_x = new_x.drop(columns='ds')\n",
    "    return model.predict(new_x)\n",
    "\n",
    "\n",
    "def merge_predict(\n",
    "    model,\n",
    "    new_x: pd.DataFrame,\n",
    "    dynamic_dfs: List[pd.DataFrame],\n",
    "    features_order: List[str],\n",
    "    **kwargs,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Perform left join on each of `dynamic_dfs` and call model.predict.\"\"\"\n",
    "    new_x = new_x.reset_index('unique_id')\n",
    "    for df in dynamic_dfs:\n",
    "        new_x = new_x.merge(df, how='left')\n",
    "    new_x = new_x.sort_values('unique_id')\n",
    "    new_x = new_x[features_order]\n",
    "    return model.predict(new_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class TimeSeries:\n",
    "    \"\"\"Utility class for storing and transforming time series data.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        freq: str = 'D',\n",
    "        lags: List[int] = [],\n",
    "        lag_transforms: Dict[int, List[Tuple]] = {},\n",
    "        date_features: List[str] = [],\n",
    "        num_threads: int = 1,\n",
    "    ):\n",
    "        self.freq = pd.tseries.frequencies.to_offset(freq)\n",
    "        self.num_threads = num_threads\n",
    "        self.date_features = date_features\n",
    "        \n",
    "        self.transforms: Dict[str, Tuple[Any, ...]] = OrderedDict()\n",
    "        for lag in lags:\n",
    "            self.transforms[f'lag-{lag}'] = (lag, _identity)\n",
    "        for lag in lag_transforms.keys():\n",
    "            for tfm_args in lag_transforms[lag]:\n",
    "                tfm, *args = _as_tuple(tfm_args)\n",
    "                tfm_name = _build_transform_name(lag, tfm, *args)\n",
    "                self.transforms[tfm_name] = (lag, tfm, *args)\n",
    "                \n",
    "        self.ga: GroupedArray\n",
    "                \n",
    "    @property\n",
    "    def features(self) -> List[str]:\n",
    "        \"\"\"Names of all computed features.\"\"\"\n",
    "        return list(self.transforms.keys()) + self.date_features\n",
    "                \n",
    "    def __repr__(self):\n",
    "        return f'TimeSeries(freq={self.freq}, transforms={list(self.transforms.keys())}, date_features={self.date_features}, num_threads={self.num_threads})'\n",
    "    \n",
    "    def _apply_transforms(self, updates_only: bool = False) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Apply the transformations using the main process.\n",
    "        \n",
    "        If `updates_only` then only the updates are returned.\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        offset = 1 if updates_only else 0\n",
    "        for tfm_name, (lag, tfm, *args) in self.transforms.items():\n",
    "            results[tfm_name] = _transform_series(\n",
    "                self.ga.data, self.ga.indptr, updates_only, lag - offset, tfm, *args\n",
    "            )\n",
    "        return results\n",
    "\n",
    "    def _apply_multithreaded_transforms(\n",
    "        self, updates_only: bool = False\n",
    "    ) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Apply the transformations using multithreading.\n",
    "        \n",
    "        If `updates_only` then only the updates are returned.\n",
    "        \"\"\"        \n",
    "        future_to_result = {}\n",
    "        results = {}\n",
    "        offset = 1 if updates_only else 0        \n",
    "        with concurrent.futures.ThreadPoolExecutor(self.num_threads) as executor:\n",
    "            for tfm_name, (lag, tfm, *args) in self.transforms.items():\n",
    "                future = executor.submit(\n",
    "                    _transform_series,\n",
    "                    self.ga.data,\n",
    "                    self.ga.indptr,\n",
    "                    updates_only,\n",
    "                    lag - offset,\n",
    "                    tfm,\n",
    "                    *args,\n",
    "                )\n",
    "                future_to_result[future] = tfm_name\n",
    "            for future in concurrent.futures.as_completed(future_to_result):\n",
    "                tfm_name = future_to_result[future]\n",
    "                results[tfm_name] = future.result()\n",
    "        return results\n",
    "    \n",
    "    def _compute_transforms(self) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Compute the transformations defined in the constructor.\n",
    "\n",
    "        If `self.num_threads > 1` these are computed using multithreading.\"\"\"\n",
    "        if self.num_threads == 1 or len(self.transforms) == 1:\n",
    "            return self._apply_transforms()\n",
    "        return self._apply_multithreaded_transforms()\n",
    "    \n",
    "    def _update_y(self, new: np.ndarray) -> None:\n",
    "        \"\"\"Appends the elements of `new` to every time serie.\n",
    "\n",
    "        These values are used to update the transformations and are stored as predictions.\"\"\"\n",
    "        if not hasattr(self, 'y_pred'):\n",
    "            self.y_pred = []\n",
    "        self.y_pred.append(new)\n",
    "        new_arr = np.asarray(new)\n",
    "        self.ga = self.ga.append(new_arr)     \n",
    "        \n",
    "    def _update_features(self) -> pd.DataFrame:\n",
    "        \"\"\"Compute the current values of all the features using the latest values of the time series.\"\"\"\n",
    "        if not hasattr(self, 'curr_dates'):\n",
    "            self.curr_dates = self.last_dates.copy()\n",
    "            self.test_dates = []\n",
    "        self.curr_dates += self.freq\n",
    "        self.test_dates.append(self.curr_dates)\n",
    "\n",
    "        if self.num_threads == 1 or len(self.transforms) == 1:\n",
    "            features = self._apply_transforms(updates_only=True)\n",
    "        else:\n",
    "            features = self._apply_multithreaded_transforms(updates_only=True)\n",
    "\n",
    "        for feature in self.date_features:\n",
    "            feat_vals = getattr(self.curr_dates, feature).values\n",
    "            features[feature] = feat_vals.astype(date_features_dtypes[feature])\n",
    "\n",
    "        features_df = pd.DataFrame(features, columns=self.features, index=self.uids)\n",
    "        nulls_in_cols = features_df.isnull().any()\n",
    "        if any(nulls_in_cols):\n",
    "            warnings.warn(\n",
    "                f'Found null values in {\", \".join(nulls_in_cols[nulls_in_cols].index)}.'\n",
    "            )\n",
    "        results_df = self.static_features.join(features_df)\n",
    "        results_df['ds'] = self.curr_dates\n",
    "        return results_df\n",
    "    \n",
    "    def _get_predictions(self) -> pd.DataFrame:\n",
    "        \"\"\"Get all the predicted values with their corresponding ids and datestamps.\"\"\"\n",
    "        n_preds = len(self.y_pred)\n",
    "        idx = pd.Index(\n",
    "            chain.from_iterable([uid] * n_preds for uid in self.uids),\n",
    "            name='unique_id',\n",
    "            dtype=self.uids.dtype,\n",
    "        )\n",
    "        df = pd.DataFrame(\n",
    "            {\n",
    "                'ds': np.array(self.test_dates).ravel('F'), \n",
    "                'y_pred': np.array(self.y_pred).ravel('F'),\n",
    "            },\n",
    "            index=idx,\n",
    "        )\n",
    "        return df\n",
    "    \n",
    "    def _fit(self, df: pd.DataFrame) -> 'TimeSeries':\n",
    "        \"\"\"Save the series values, ids and last dates.\"\"\"\n",
    "        data, indptr = data_indptr_from_sorted_df(df)\n",
    "        if data.dtype not in (np.float32, np.float64):\n",
    "            # since all transformations generate nulls, we need a float dtype\n",
    "            data = data.astype(np.float32)\n",
    "        self.ga = GroupedArray(data, indptr)\n",
    "        self.uids = df.index.unique(level='unique_id')\n",
    "        self.last_dates = df.index.get_level_values('ds')[indptr[1:] - 1]\n",
    "        return self\n",
    "\n",
    "    def _transform(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Add the features to `df`.\n",
    "        \n",
    "        if `dropna=True` then all the null rows are dropped.\n",
    "        if `keep_last_n` is not None then that number of observations is kept across all series.\"\"\"\n",
    "        df = df.copy()\n",
    "        features = self._compute_transforms()\n",
    "        for feat in self.transforms.keys():\n",
    "            df[feat] = features[feat][self.restore_idxs]\n",
    "\n",
    "        if dropna:\n",
    "            df.dropna(inplace=True)\n",
    "\n",
    "        for feature in self.date_features:\n",
    "            feat_vals = getattr(df.ds.dt, feature).values\n",
    "            df[feature] = feat_vals.astype(date_features_dtypes[feature])\n",
    "\n",
    "        if keep_last_n is not None:\n",
    "            self.ga = self.ga.take_from_groups(slice(-keep_last_n, None))\n",
    "            \n",
    "        self._ga = GroupedArray(self.ga.data, self.ga.indptr)\n",
    "        self.features_order_ = df.columns.drop(['ds', 'y'])\n",
    "        return df\n",
    "\n",
    "    def fit_transform(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        static_features: Optional[List[str]] = None,\n",
    "        dropna: bool = True,\n",
    "        keep_last_n: Optional[int] = None, \n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Add the features to `data` and save the required information for the predictions step.\n",
    "        \n",
    "        If not all features are static, specify which ones are in `static_features`.\n",
    "        If you don't want to drop rows with null values after the transformations set `dropna=False`.\n",
    "        If you want to keep only the last `n` values of each time serie set `keep_last_n=n`.\n",
    "        \"\"\"\n",
    "        if data.index.name != 'unique_id' or 'ds' not in data or 'y' not in data:\n",
    "            raise ValueError(\n",
    "                'data must have an index named unique_id and ds and y columns.'\n",
    "            )\n",
    "        if data['y'].isnull().any():\n",
    "            raise ValueError('y column contains null values.')\n",
    "        if static_features is None:\n",
    "            static_features = data.columns.drop(['ds', 'y'])\n",
    "        self.static_features = (\n",
    "            data[static_features].reset_index().drop_duplicates().set_index('unique_id')\n",
    "        )\n",
    "        sort_idxs = pd.core.sorting.lexsort_indexer([data.index, data['ds']])        \n",
    "        sorted_data = data[['ds', 'y']].set_index('ds', append=True).iloc[sort_idxs]\n",
    "        self.restore_idxs = np.empty(data.shape[0], dtype=np.int32)\n",
    "        self.restore_idxs[sort_idxs] = np.arange(data.shape[0])\n",
    "        self._fit(sorted_data)\n",
    "        return self._transform(data, dropna, keep_last_n)\n",
    "    \n",
    "    def _predict_setup(self) -> None:\n",
    "        self.curr_dates = self.last_dates.copy()\n",
    "        self.test_dates = []\n",
    "        self.y_pred = []\n",
    "        self.ga = GroupedArray(self._ga.data, self._ga.indptr)\n",
    "        \n",
    "    def _define_predict_fn(self, predict_fn, dynamic_dfs) -> Callable:\n",
    "        if predict_fn is not None:\n",
    "            return predict_fn\n",
    "        if dynamic_dfs is None:\n",
    "            return simple_predict\n",
    "        return merge_predict\n",
    "    \n",
    "    def predict(\n",
    "        self,\n",
    "        model,\n",
    "        horizon: int,\n",
    "        dynamic_dfs: Optional[List[pd.DataFrame]] = None,\n",
    "        predict_fn: Optional[Callable] = None,\n",
    "        **predict_fn_kwargs,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Use `model` to predict the next `horizon` timesteps.\"\"\"    \n",
    "        self._predict_setup()\n",
    "        predict_fn = self._define_predict_fn(predict_fn, dynamic_dfs)   \n",
    "        for _ in range(horizon):\n",
    "            new_x = self._update_features()\n",
    "            predictions = predict_fn(\n",
    "                model,\n",
    "                new_x,\n",
    "                dynamic_dfs,\n",
    "                self.features_order_,\n",
    "                **predict_fn_kwargs,\n",
    "            )\n",
    "            self._update_y(predictions)\n",
    "        return self._get_predictions()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `TimeSeries` class takes care of defining the transformations to be performed (`lags`, `lag_transforms` and `date_features`). The transformations can be computed using multithreading if `num_threads > 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeSeries(freq=<Week: weekday=3>, transforms=['lag-7', 'expanding_mean_lag-1', 'rolling_mean_lag-1_window_size-7'], date_features=['dayofweek'], num_threads=1)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow_config = dict(\n",
    "    freq='W-THU',\n",
    "    lags=[7],\n",
    "    lag_transforms={\n",
    "        1: [expanding_mean, (rolling_mean, 7)]\n",
    "    },\n",
    "    date_features=['dayofweek']\n",
    ")\n",
    "\n",
    "ts = TimeSeries(**flow_config)\n",
    "ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frequency is converted to an offset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(ts.freq, pd.tseries.frequencies.to_offset(flow_config['freq']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The date features are stored as they were passed to the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(ts.date_features, flow_config['date_features'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformations are stored as a dictionary where the key is the name of the transformation (name of the column in the dataframe with the computed features), which is built using `build_transform_name` and the value is a tuple where the first element is the lag it is applied to, then the function and then the function arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(\n",
    "    ts.transforms, \n",
    "    {\n",
    "        'lag-7': (7, _identity),\n",
    "        'expanding_mean_lag-1': (1, expanding_mean), \n",
    "        'rolling_mean_lag-1_window_size-7': (1, rolling_mean, 7)\n",
    "        \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for `lags` we define the transformation as the identity function applied to its corresponding lag. This is because `_transform_series` takes the lag as an argument and shifts the array before computing the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# int y is converted to float32\n",
    "serie2 = serie.copy()\n",
    "serie2['y'] = serie2['y'].astype(int)\n",
    "ts = TimeSeries(num_threads=1)\n",
    "ts._fit(serie2.set_index('ds', append=True))\n",
    "test_eq(ts.ga.data.dtype, np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# _compute_transforms\n",
    "y = serie.y.values\n",
    "lag_1 = shift_array(y, 1)\n",
    "\n",
    "for num_threads in (1, 2):\n",
    "    ts = TimeSeries(**flow_config)\n",
    "    ts._fit(serie.set_index('ds', append=True))\n",
    "    transforms = ts._compute_transforms()\n",
    "\n",
    "    np.testing.assert_equal(transforms['lag-7'], shift_array(y, 7))\n",
    "    np.testing.assert_equal(transforms['expanding_mean_lag-1'], expanding_mean(lag_1))\n",
    "    np.testing.assert_equal(transforms['rolling_mean_lag-1_window_size-7'], rolling_mean(lag_1, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# update_y\n",
    "ts = TimeSeries()\n",
    "ts._fit(serie.set_index('ds', append=True))\n",
    "max_size = np.diff(ts.ga.indptr)\n",
    "ts._update_y([1])\n",
    "ts._update_y([2])\n",
    "\n",
    "test_eq(np.diff(ts.ga.indptr), max_size + 2)\n",
    "test_eq(ts.ga.data[-2:], [1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# _update_features\n",
    "ts = TimeSeries(**flow_config)\n",
    "ts.fit_transform(serie)\n",
    "updates = ts._update_features().drop(columns='ds')\n",
    "\n",
    "last_date = serie['ds'].max()\n",
    "first_prediction_date = last_date + ts.freq\n",
    "expected_idx = pd.Index(ts.uids, name='unique_id')\n",
    "\n",
    "# these have an offset becase we can now \"see\" our last y value\n",
    "expected = pd.DataFrame({\n",
    "    'lag-7': shift_array(y, 6)[-1],\n",
    "    'expanding_mean_lag-1': expanding_mean(y)[-1],\n",
    "    'rolling_mean_lag-1_window_size-7': rolling_mean(y, 7)[-1],\n",
    "    'dayofweek': np.uint8([getattr(first_prediction_date, 'dayofweek')])},\n",
    "    index=expected_idx\n",
    ")\n",
    "statics = serie.tail(1).drop(columns=['ds', 'y'])\n",
    "assert updates.equals(statics.join(expected))\n",
    "\n",
    "test_eq(ts.curr_dates[0], first_prediction_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# _get_predictions\n",
    "ts = TimeSeries()\n",
    "ts.fit_transform(serie)\n",
    "ts._update_features()\n",
    "ts._update_y([1.])\n",
    "preds = ts._get_predictions()\n",
    "\n",
    "last_ds = serie['ds'].max()\n",
    "expected_idx = serie.index.get_level_values('unique_id')[[0]]\n",
    "expected = pd.DataFrame({'ds': [last_ds + ts.freq], 'y_pred': [1.]},\n",
    "                        index=expected_idx)\n",
    "assert preds.equals(expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "#### TimeSeries.fit_transform\n",
       "\n",
       ">      TimeSeries.fit_transform (data:pandas.core.frame.DataFrame,\n",
       ">                                static_features:Optional[List[str]]=None,\n",
       ">                                dropna:bool=True,\n",
       ">                                keep_last_n:Optional[int]=None)\n",
       "\n",
       "Add the features to `data` and save the required information for the predictions step."
      ],
      "text/plain": [
       "<nbdev.showdoc.BasicMarkdownRenderer>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TimeSeries.fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_config = dict(\n",
    "    freq='D',\n",
    "    lags=[7, 14],\n",
    "    lag_transforms={\n",
    "        1: [expanding_mean],\n",
    "        2: [\n",
    "            (rolling_mean, 7),\n",
    "            (rolling_mean, 14),\n",
    "        ]\n",
    "    },\n",
    "    date_features=['dayofweek', 'month', 'year'],\n",
    "    num_threads=2\n",
    ")\n",
    "\n",
    "ts = TimeSeries(**flow_config)\n",
    "_ = ts.fit_transform(series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The series values are stored as a GroupedArray in an attribute `ga`. If the data type of the series values is an int then it is converted to `np.float32`, this is because lags generate `np.nan`s so we need a float data type for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_equal(ts.ga.data, series.y.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The series ids are stored in an `uids` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(ts.uids, series.index.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each time serie, the last observed date is stored so that predictions start from the last date + the frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(ts.last_dates, series.groupby('unique_id')['ds'].max().values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last row of every serie without the `y` and `ds` columns are taken as static features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert ts.static_features.equals(series.groupby('unique_id').tail(1).drop(columns=['ds', 'y']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you pass `static_features` to `TimeSeries.fit_transform` then only these are kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.fit_transform(series, static_features=['static_0'])\n",
    "\n",
    "pd.testing.assert_frame_equal(\n",
    "    ts.static_features,\n",
    "    series.groupby('unique_id').tail(1)[['static_0']],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also specify `keep_last_n` in `TimeSeries.fit_transform`, which means that after computing the features for training we want to keep only the last `n` samples of each time serie for computing the updates. This saves both memory and time, since the updates are performed by running the transformation functions on all time series again and keeping only the last value (the update).\n",
    "\n",
    "If you have very long time series and your updates only require a small sample it's recommended that you set `keep_last_n` to the minimum number of samples required to compute the updates, which in this case is 15 since we have a rolling mean of size 14 over the lag 2 and in the first update the lag 2 becomes the lag 1. This is because in the first update the lag 1 is the last value of the series (or the lag 0), the lag 2 is the lag 1 and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_last_n = 15\n",
    "\n",
    "ts = TimeSeries(**flow_config)\n",
    "df = ts.fit_transform(series, keep_last_n=keep_last_n)\n",
    "\n",
    "expected_lags = ['lag-7', 'lag-14']\n",
    "expected_transforms = ['expanding_mean_lag-1', \n",
    "                       'rolling_mean_lag-2_window_size-7', \n",
    "                       'rolling_mean_lag-2_window_size-14']\n",
    "expected_date_features = ['dayofweek', 'month', 'year']\n",
    "\n",
    "test_eq(ts.features, expected_lags + expected_transforms + expected_date_features)\n",
    "test_eq(ts.static_features.columns.tolist() + ts.features, df.columns.drop(['ds', 'y']).tolist())\n",
    "# we dropped 2 rows because of the lag 2 and 13 more to have the window of size 14\n",
    "test_eq(df.shape[0], series.shape[0] - (2 + 13) * ts.ga.ngroups)\n",
    "test_eq(ts.ga.data.size, ts.ga.ngroups * keep_last_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get the minimum number of samples that your transformation needs wrong i.e. you generate `np.nan`s in the update, you'll get a warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'UserWarning'>: Found null values in rolling_mean_lag-2_window_size-14.\n"
     ]
    }
   ],
   "source": [
    "ts = TimeSeries(lag_transforms={2: [(rolling_mean, 14)]})\n",
    "_ = ts.fit_transform(series, keep_last_n=14)\n",
    "test_warns(lambda: ts._update_features(), show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a rule of thumb the minimum number of samples you need is max(lag + window_size) - 1 for rolling operations. **If you have expanding features you need all samples to get the correct value**, this won't raise a warning because there won't be null values, however the feature value will be wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TimeSeries.fit_transform` requires that the *y* column doesn't have any null values. This is because the transformations could propagate them forward, so if you have null values in the *y* column you'll get an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_with_nulls = series.copy()\n",
    "series_with_nulls.iloc[1, 1] = np.nan\n",
    "test_fail(lambda: ts.fit_transform(series_with_nulls), contains='y column contains null values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# unsorted df\n",
    "ts = TimeSeries(**flow_config)\n",
    "df = ts.fit_transform(series)\n",
    "unordered_series = series.sample(frac=1.0)\n",
    "assert not unordered_series.set_index('ds', append=True).index.is_monotonic_increasing\n",
    "df2 = ts.fit_transform(unordered_series)\n",
    "pd.testing.assert_frame_equal(df, df2.sort_values(['unique_id', 'ds']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "#### TimeSeries.predict\n",
       "\n",
       ">      TimeSeries.predict (model, horizon:int,\n",
       ">                          dynamic_dfs:Optional[List[pandas.core.frame.DataFrame\n",
       ">                          ]]=None, predict_fn:Optional[Callable]=None,\n",
       ">                          **predict_fn_kwargs)\n",
       "\n",
       "Use `model` to predict the next `horizon` timesteps."
      ],
      "text/plain": [
       "<nbdev.showdoc.BasicMarkdownRenderer>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TimeSeries.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a trained model we can use `TimeSeries.predict` passing the model and the horizon to get the predictions back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyModel:\n",
    "    def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
    "        return X['lag-7'].values\n",
    "\n",
    "horizon = 7\n",
    "model = DummyModel()\n",
    "ts = TimeSeries(**flow_config)\n",
    "ts.fit_transform(series)\n",
    "predictions = ts.predict(model, horizon)\n",
    "\n",
    "grouped_series = series.groupby('unique_id')\n",
    "expected_preds = grouped_series['y'].tail(7)  # the model predicts the lag-7\n",
    "last_dates = grouped_series['ds'].max()\n",
    "expected_dsmin = last_dates + ts.freq\n",
    "expected_dsmax = last_dates + horizon * ts.freq\n",
    "grouped_preds = predictions.groupby('unique_id')\n",
    "\n",
    "assert predictions['y_pred'].equals(expected_preds)\n",
    "assert grouped_preds['ds'].min().equals(expected_dsmin)\n",
    "assert grouped_preds['ds'].max().equals(expected_dsmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have dynamic features we can pass them to `dynamic_dfs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictPrice:\n",
    "    def predict(self, X):\n",
    "        return X['price']\n",
    "\n",
    "series = generate_daily_series(20, n_static_features=2, equal_ends=True)\n",
    "dynamic_series = series.rename(columns={'static_1': 'product_id'})\n",
    "prices_catalog = generate_prices_for_series(dynamic_series)\n",
    "series_with_prices = dynamic_series.reset_index().merge(prices_catalog, how='left')\n",
    "series_with_prices.set_index('unique_id', inplace=True)\n",
    "\n",
    "model = PredictPrice()\n",
    "ts = TimeSeries(**flow_config)\n",
    "ts.fit_transform(series_with_prices, static_features=['static_0', 'product_id'])\n",
    "predictions = ts.predict(model, horizon=1, dynamic_dfs=[prices_catalog])\n",
    "\n",
    "expected_prices = series_with_prices.reset_index()[['unique_id', 'product_id']].drop_duplicates()\n",
    "expected_prices['ds'] = series_with_prices['ds'].max() + ts.freq\n",
    "expected_prices = expected_prices.reset_index()\n",
    "expected_prices = expected_prices.merge(prices_catalog, on=['product_id', 'ds'], how='left')\n",
    "expected_prices = expected_prices.set_index('unique_id')[['ds', 'price']]\n",
    "\n",
    "assert predictions.rename(columns={'y_pred': 'price'}).equals(expected_prices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
